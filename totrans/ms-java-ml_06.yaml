- en: Chapter 6. Probabilistic Graph Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章. 概率图建模
- en: '**Probabilistic graph models** (**PGMs**), also known as graph models, capture
    the relationship between different variables and represent the probability distributions.
    PGMs capture joint probability distributions and can be used to answer different
    queries and make inferences that allow us to make predictions on unseen data.
    PGMs have the great advantage of capturing domain knowledge of experts and the
    causal relationship between variables to model systems. PGMs represent the structure
    and they can capture knowledge in a representational framework that makes it easier
    to share and understand the domain and models. PGMs capture the uncertainty or
    the probabilistic nature very well and are thus very useful in applications that
    need scoring or uncertainty-based approaches. PGMs are used in a wide variety
    of applications that use machine learning such as applications to domains of language
    processing, text mining and information extraction, computer vision, disease diagnosis,
    and DNA structure predictions.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率图模型**（**PGMs**），也称为图模型，捕捉不同变量之间的关系并表示概率分布。PGMs捕捉联合概率分布，可用于回答不同查询并做出推断，使我们能够对未见数据做出预测。PGMs具有捕捉专家领域知识和变量之间的因果关系以建模系统的巨大优势。PGMs表示结构，并且可以在一个表示框架中捕捉知识，使其更容易共享和理解领域和模型。PGMs很好地捕捉了不确定性或概率性质，因此在需要评分或基于不确定性的方法的应用中非常有用。PGMs被广泛应用于各种使用机器学习的应用中，如语言处理、文本挖掘和信息提取、计算机视觉、疾病诊断和DNA结构预测等领域。'
- en: Judea Pearl is the pioneer in the area of PGMs and was the first to introduce
    the topic of Bayesian Networks (*References* [2] and [7]). Although covering all
    there is to know about PGMs is beyond the scope of this chapter, our goal is to
    cover the most important aspects of PGMs—Bayes network and directed PGMs—in some
    detail. We will divide the subject into the areas of representation, inference,
    and learning and will discuss specific algorithms and sub-topics in each of these
    areas. We will cover Markov Networks and undirected PGMs, summarizing some differences
    and similarities with PGMs, and addressing related areas such as inference and
    learning. Finally, we will discuss specialized networks such as **tree augmented**
    **networks** (**TAN**), Markov chains and **hidden Markov models** (**HMM**).
    For an in-depth treatment of the subject, see *Probabilistic Graphical Models*,
    by Koller and Friedman (*References* [1]).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 朱迪亚·佩尔是PGMs领域的先驱，也是第一个引入贝叶斯网络主题的人（*参考文献* [2] 和 [7]）。尽管涵盖PGMs的所有知识超出了本章的范围，但我们的目标是详细阐述PGMs最重要的方面——贝叶斯网络和有向PGMs。我们将把主题分为表示、推理和学习三个领域，并将在每个领域讨论具体的算法和子主题。我们将涵盖马尔可夫网络和无向PGMs，总结与PGMs的一些异同，并探讨相关领域，如推理和学习。最后，我们将讨论专门的网络，如**树增强网络**（**TAN**）、马尔可夫链和**隐马尔可夫模型**（**HMM**）。对于该主题的深入探讨，请参阅Koller和Friedman的*概率图模型*（*参考文献*
    [1]）。
- en: Probability revisited
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率回顾
- en: Many basic concepts of probability are detailed in [Appendix B](apb.html "Appendix B. Probability"),
    *Probability*. Some of the key ideas in probability theory form the building blocks
    of probabilistic graph models. A good grasp of the relevant theory can help a
    great deal in understanding PGMs and how they are used to make inferences from
    data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 概率论中的许多基本概念在[附录B](apb.html "附录 B. 概率") *概率*中进行了详细阐述。概率论中的某些关键思想构成了概率图模型的基础。对相关理论的良好掌握有助于极大地理解PGMs及其如何从数据中做出推断。
- en: Concepts in probability
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率论中的概念
- en: In this section, we will discuss important concepts related to probability theory
    that will be used in the discussion later in this chapter.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论与概率论相关的重要概念，这些概念将在本章后面的讨论中使用。
- en: Conditional probability
  id: totrans-7
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 条件概率
- en: 'The essence of conditional probability, given two related events a and ß, is
    to capture how we assign a value for one of the events when the other is known
    to have occurred. The conditional probability, or the conditional distribution,
    is represented by *P*(*a* | *ß*), that is, the probability of event *a* happening
    given that the event *ß* has occurred (equivalently, given that *ß* is true) and
    is formally defined as:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个相关事件a和ß的条件概率的本质，在于捕捉当我们知道另一个事件已经发生时，如何为其中一个事件分配一个值。条件概率或条件分布用*P*(*a* | *ß*)表示，即事件*ß*发生时事件*a*发生的概率（等价于*ß*为真时），其形式定义如下：
- en: '![Conditional probability](img/B05137_06_018.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![条件概率](img/B05137_06_018.jpg)'
- en: The *P*(*a* n *ß*) captures the events where both a and ß occur.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*a* n *ß*)捕捉了同时发生a和ß的事件。'
- en: Chain rule and Bayes' theorem
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链式法则和贝叶斯定理
- en: 'The conditional probability definition gives rise to the chain rule of conditional
    probabilities that says that when there are multiple events α[1], α[2]….α[n] then:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 条件概率的定义导致了条件概率的链式法则，该法则指出，当存在多个事件α[1]，α[2]….α[n]时：
- en: '*P*(*α*[1] ∩ *α*[2] ∩….∩ *α*[n] ) = *P*(*α*[1] )*P*(*α*[2] ¦ *α*[1])*P*(*α*[3]
    | *α*[1] ∩ *α*[2])..*P*(*α*[∩] |*α*[1] ∩ *α*[2] ∩….∩ *α*[n-1])'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*α*[1] ∩ *α*[2] ∩….∩ *α*[n] ) = *P*(*α*[1] )*P*(*α*[2] ¦ *α*[1])*P*(*α*[3]
    | *α*[1] ∩ *α*[2])..*P*(*α*[∩] |*α*[1] ∩ *α*[2] ∩….∩ *α*[n-1])'
- en: The probability of several events can be expressed as the probability of the
    first times the probability of the second given the first, and so on. Thus, the
    probability of *α*[n] depends on everything α[1] to α[n] and is independent of
    the order of the events.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 几个事件的概率可以表示为第一个事件的概率乘以在第一个事件给定的第二个事件的概率，依此类推。因此，*α*[n]的概率取决于α[1]到α[n]的所有事件，并且与事件的顺序无关。
- en: 'Bayes rule also follows from the conditional probability rule and can be given
    formally as:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯法则也遵循条件概率规则，可以正式表示为：
- en: '![Chain rule and Bayes'' theorem](img/B05137_06_024.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![链式法则和贝叶斯定理](img/B05137_06_024.jpg)'
- en: Random variables, joint, and marginal distributions
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机变量、联合分布和边缘分布
- en: It is natural to map the event spaces and outcomes by considering them as attributes
    and values. Random variables are defined as attributes with different known specific
    values. For example, if *Grade* is an attribute associated with *Student*, and
    has values *{A, B, C}*, then *P(Grade = A)* represents a random variable with
    an outcome.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将事件空间和结果视为属性和值，自然地映射事件空间和结果。随机变量被定义为具有不同已知特定值的属性。例如，如果*成绩*是与*学生*相关联的属性，并且具有值*{A,
    B, C}*，那么*P(Grade = A)*表示一个具有结果的随机变量。
- en: 'Random variables are generally denoted by capital letters, such as *X*, *Y*,
    and *Z* and values taken by them are denoted by *Val(X) = x*. In this chapter,
    we will primarily discuss values that are categorical in nature, that is, that
    take a fixed number of discrete values. In the real world, the variables can have
    continuous representations too. The distribution of a variable with categories
    {x¹, x² …x^n} can be represented as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量通常用大写字母表示，如*X*，*Y*，和*Z*，它们所取的值用*Val(X) = x*表示。在本章中，我们将主要讨论本质上为分类的值，即取固定数量的离散值。在现实世界中，变量也可以有连续的表示。具有类别{x¹,
    x² …x^n}的变量的分布可以表示为：
- en: '![Random variables, joint, and marginal distributions](img/B05137_06_030.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![随机变量、联合分布和边缘分布](img/B05137_06_030.jpg)'
- en: Such a distribution over many categories is called a **multinomial distribution**.
    In the special case when there are only two categories, the distribution is said
    to be the **Bernoulli distribution**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一个跨越许多类别的分布称为**多项分布**。在只有两个类别的特殊情况下，该分布被称为**伯努利分布**。
- en: Given a random variable, a probability distribution over all the events described
    by that variable is known as the marginal distribution. For example, if Grade
    is the random variable, the marginal distribution can be defined as *(Grade =
    A) = 0.25, P(Grade = b) = 0.37 and P(Grade = C) = 0.38*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个随机变量，该变量描述的所有事件的概率分布称为边缘分布。例如，如果成绩是随机变量，边缘分布可以定义为*(Grade = A) = 0.25, P(Grade
    = b) = 0.37 和 P(Grade = C) = 0.38*。
- en: In many real-world models, there are more than one random variables and the
    distribution that considers all of these random variable is called the **joint
    distribution**. For example, if *Intelligence* of student is considered as another
    variable and denoted by *P(Intelligence)* or *P(I)* and has binary outcomes *{low,
    high}*, then the distribution considering *Intelligence* and *Grade* represented
    as *P(Intelligence, Grade)* or *P(I, G)*, is the joint distribution.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的模型中，存在多个随机变量，考虑所有这些随机变量的分布称为**联合分布**。例如，如果将学生的*智力*视为另一个变量，并用*P(Intelligence)*或*P(I)*表示，并且具有二进制结果*{低，高}*，那么考虑*智力*和*成绩*的分布，表示为*P(Intelligence,
    Grade)*或*P(I, G)*，就是联合分布。
- en: Marginal distribution of one random variable can be computed from the joint
    distribution by summing up the values over all of the other variables. The marginal
    distribution over grade can be obtained by summing over all the rows as shown
    in *Table 1* and that over the intelligence can be obtained by summation over
    the columns.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机变量的边缘分布可以通过对所有其他变量的值求和从联合分布中计算得出。通过将所有行求和，如 *表1* 所示，可以得到成绩的边缘分布，通过将列求和可以得到智力的边缘分布。
- en: '![Random variables, joint, and marginal distributions](img/B05137_06_037.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![随机变量、联合分布和边缘分布](img/B05137_06_037.jpg)'
- en: Table 1\. Marginal distributions over I and G
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. I 和 G 的边缘分布
- en: Marginal independence and conditional independence
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘独立性和条件独立性
- en: Marginal Independence is defined as follows. Consider two random variables *X*
    and *Y*; then *P(X|Y) = P(X)* means random variable *X* is independent of *Y*.
    It is formally represented as ![Marginal independence and conditional independence](img/B05137_06_042.jpg)
    (*P* satisfies *X* is independent of *Y*).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘独立性定义为以下内容。考虑两个随机变量 *X* 和 *Y*；那么 *P(X|Y) = P(X)* 意味着随机变量 *X* 与 *Y* 独立。它形式上表示为
    ![边缘独立性和条件独立性](img/B05137_06_042.jpg) (*P* 满足 *X* 与 *Y* 独立)。
- en: 'This means the joint distribution can be given as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着联合分布可以表示为：
- en: '*P(X, Y) = P(X)P(Y)*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X, Y) = P(X)P(Y)*'
- en: If the difficulty level of the exam (*D*) and the intelligence of the student
    (*I*) determine the grade (*G*), we know that the difficulty level of the exam
    is independent of the intelligence of the student and (*D* ⊥ *I*) also implies
    *P(D, I) = P(D)P(I)*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果考试的难度水平 (*D*) 和学生的智力 (*I*) 决定了成绩 (*G*)，我们知道考试的难度水平与学生的智力独立，并且 (*D* ⊥ *I*)
    也意味着 *P(D, I) = P(D)P(I)*。
- en: When two random variables are independent given a third variable, the independence
    is called conditional independence. Given a set of three random variables *X*,
    *Y*, and *Z*, we can say that ![Marginal independence and conditional independence](img/B05137_06_051.jpg);
    that is, variable *X* is independent of *Y* given *Z*. The necessary condition
    for conditional independence is
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个随机变量在第三个变量给定的情况下是独立的，这种独立性称为条件独立性。给定三个随机变量 *X*，*Y* 和 *Z* 的集合，我们可以说 ![边缘独立性和条件独立性](img/B05137_06_051.jpg)；也就是说，变量
    *X* 在给定 *Z* 的情况下与 *Y* 独立。条件独立的必要条件是
- en: '![Marginal independence and conditional independence](img/B05137_06_052.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![边缘独立性和条件独立性](img/B05137_06_052.jpg)'
- en: Factors
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因素
- en: Factors are the basic building blocks for defining the probability distributions
    in high-dimensional (large number of variables) spaces. They give basic operations
    that help in manipulating the probability distributions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因素是定义高维（大量变量）空间中概率分布的基本构建块。它们提供了基本操作，有助于操作概率分布。
- en: A "factor" is defined as a function that takes as input the random variables
    known as "scope" and gives a real-value output.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: “因素”被定义为一种函数，它接受称为“作用域”的随机变量作为输入，并给出一个实值输出。
- en: Formally, a factor is represented as ![Factors](img/B05137_06_053.jpg) where
    scope is (X[1], *X*[2], ….*X*[k] ).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，一个因素表示为 ![因素](img/B05137_06_053.jpg) 其中作用域是 (X[1], *X*[2], ….*X*[k] )。
- en: Factor types
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因素类型
- en: 'Different types of factors are as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的因素类型如下：
- en: '**Joint distribution**: For every combination of variables, you get a real-valued
    output.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联合分布**: 对于变量的每一个组合，你都会得到一个实值输出。'
- en: '**Unnormalized measure**: When, in a joint distribution, one of the variables
    is constant, the output is also real-valued, but it is unnormalized as it doesn''t
    sum to one. However, it is still a factor.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未归一化度量**: 当在一个联合分布中，其中一个变量是常数时，输出也是实值，但由于它不总和为1，因此它是未归一化的。然而，它仍然是一个因素。'
- en: '**Conditional probability distribution**: A probability distribution of the
    form *P(G|I)* is also a factor.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件概率分布**: 形式为 *P(G|I)* 的概率分布也是一个因素。'
- en: 'Various operations are performed on factors, such as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在因素上执行各种操作，例如：
- en: '**Factor product**: If two factors *ϕ*[1] (*X*[1], *X*[2]) and *ϕ*[2] (*X*[2],
    *X*[3]) are multiplied, it gives rise to *ϕ*[3] (*X*[1], *X*[2], *X*[3]). In effect,
    it is taking tables corresponding to *ϕ*[1] and multiplying it with *ϕ*[2]'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因素乘积**: 如果两个因素 *ϕ*[1] (*X*[1], *X*[2]) 和 *ϕ*[2] (*X*[2], *X*[3]) 相乘，它会产生
    *ϕ*[3] (*X*[1], *X*[2], *X*[3])。实际上，这是将对应于 *ϕ*[1] 的表与 *ϕ*[2] 相乘。'
- en: '**Factor marginalization**: This is the same as marginalization, where *ϕ*[1]
    (*X*[1], *X*[2], *X*[3]) can be marginalized over a variable, say *X*[2], to give
    *ϕ*[2] (*X*[1], *X*[3]).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因素边缘化**: 这与边缘化相同，其中 *ϕ*[1] (*X*[1], *X*[2], *X*[3]) 可以对变量进行边缘化，例如 *X*[2]，以给出
    *ϕ*[2] (*X*[1], *X*[3])。'
- en: '**Factor reduction**: This is only taking the values of other variables when
    one of the variables is constant.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因子减少**：这只在其他变量中的一个变量为常数时取其他变量的值。'
- en: Distribution queries
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布查询
- en: Given the probability over random variables, many queries can be performed to
    answer certain questions. Some common types of queries are explained in the subsequent
    sections.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 给定随机变量的概率，可以执行许多查询来回答某些问题。一些常见的查询类型将在后续章节中解释。
- en: Probabilistic queries
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概率查询
- en: 'This is one of the most common types of query and it has two parts:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的查询类型之一，它有两个部分：
- en: '**Evidence**: A subset of variables with a well-known outcome or category.
    For example, a random variable **E = e**.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**证据**：具有已知结果或类别的变量子集。例如，随机变量 **E = e**。'
- en: '**Query**: A random variable from the rest of the variables. For example, a
    random variable **X**.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询**：来自其他变量的随机变量。例如，一个随机变量 **X**。'
- en: '*P*(**X**|**E** = **e**)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*P*(**X**|**E** = **e**)'
- en: Examples of probabilistic queries are posterior marginal estimations such as
    *P(I = high|L = bad, S = low) = ?* and evidential probability such as *P(L = bad,
    S = low) = ?*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 概率查询的例子包括后验边缘估计，如 *P(I = high|L = bad, S = low) = ?* 和证据概率，如 *P(L = bad, S =
    low) = ?*。
- en: MAP queries and marginal MAP queries
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: MAP 查询和边缘 MAP 查询
- en: MAP queries are used to find the probability assignment to the subset of variables
    that are most likely and hence are also called **most probable explanation** (**MPE**).
    The difference between these and probabilistic queries is that, instead of getting
    the probability, we get the most likely values for all the variables.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: MAP 查询用于找到最可能变量的子集的概率赋值，因此也被称为**最可能解释**（**MPE**）。这些与概率查询的区别在于，我们不是得到概率，而是得到所有变量的最可能值。
- en: Formally, if we have variables *W= X – E*, where we have *E = e* as the evidence
    and are interested in finding the most likely assignment to the variables in *W*,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，如果我们有变量 *W= X – E*，其中 *E = e* 作为证据，并且我们感兴趣于找到 *W* 中变量的最可能赋值，
- en: '*MAP*(**W**|**e**) = *argmax*[w]*P*(**w**,**e**)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*MAP*(**W**|**e**) = *argmax*[w]*P*(**w**,**e**)'
- en: 'A much more general form of marginal query is when we have a subset of variables,
    say given by *Y* that forms our query and with evidence of *E = e*, we are interested
    in finding most likely assignments to the variables in *Y*. Using the MAP definition,
    we get:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘查询的一种更普遍的形式是当我们有一个变量的子集，比如说由 *Y* 给出，形成我们的查询，并且有证据 *E = e*，我们感兴趣于找到 *Y* 中变量的最可能赋值。使用
    MAP 定义，我们得到：
- en: '*MAP*(**Y**|**e**) = *argmax*[y]*P*(**y**|**e**)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*MAP*(**Y**|**e**) = *argmax*[y]*P*(**y**|**e**)'
- en: 'Let''s say, *Z= X – Y – E*, then the marginal MAP query is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，*Z= X – Y – E*，那么边缘 MAP 查询是：
- en: '![MAP queries and marginal MAP queries](img/B05137_06_424a.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![MAP 查询和边缘 MAP 查询](img/B05137_06_424a.jpg)'
- en: Graph concepts
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图概念
- en: Next, we will briefly revisit the concepts from graph theory and some of the
    definitions that we will use in this chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将简要回顾图论中的概念以及我们将在本章中使用的某些定义。
- en: Graph structure and properties
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图结构和属性
- en: A graph is defined as a data structure containing nodes and edges connecting
    these nodes. In the context of this chapter, the random variables are represented
    as nodes, and edges show connections between the random variables.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图被定义为包含节点和连接这些节点的边的数据结构。在本章的上下文中，随机变量被表示为节点，边显示了随机变量之间的连接。
- en: Formally, if *X = {X*[1]*, X*[2]*,….X*[k]*}* where *X*[1]*, X*[2]*,….X*[k] are
    random variables representing the nodes, then there can either be a directed edge
    belonging to the set e, for example, between the nodes given by ![Graph structure
    and properties](img/B05137_06_079.jpg) or an **undirected edge** ![Graph structure
    and properties](img/B05137_06_079.jpg), and the graph is defined as a data structure
    ![Graph structure and properties](img/B05137_06_081.jpg). A graph is said to be
    a **directed graph** when every edge in the set e between nodes from set **X**
    is directed and similarly an **undirected graph** is one where every edge between
    the nodes is undirected as shown in *Figure 1*. Also, if there is a graph that
    has both directed and undirected edges, the notation of ![Graph structure and
    properties](img/B05137_06_084.jpg) represents an edge that may be directed or
    undirected.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，如果 *X = {X*[1]*, X*[2]*,….X*[k]*}* 其中 *X*[1]*, X*[2]*,….X*[k] 是代表节点的随机变量，那么集合
    e 中可以存在一个有向边，例如，在由 ![图结构和性质](img/B05137_06_079.jpg) 给出的节点之间，或者存在一个 **无向边** ![图结构和性质](img/B05137_06_079.jpg)，并且图被定义为一种数据结构
    ![图结构和性质](img/B05137_06_081.jpg)。当集合 e 中节点从集合 **X** 之间的每一条边都是有向的时，称图为一个 **有向图**；同样，如果节点之间的每一条边都是无向的，则称该图为
    **无向图**，如图 1 所示。此外，如果一个图既有有向边又有无向边，![图结构和性质](img/B05137_06_084.jpg) 的表示法代表一个可能是有向或无向的边。
- en: '![Graph structure and properties](img/B05137_06_086.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图结构和性质](img/B05137_06_086.jpg)'
- en: Figure 1\. Directed, undirected, and partially-directed graphs
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 有向、无向和部分有向图
- en: If a directed edge ![Graph structure and properties](img/B05137_06_087.jpg)
    exists in the graph, the node *X*[i] is called the *parent* node and the node
    *X*[j] is called the *child* node.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图中存在一个有向边 ![图结构和性质](img/B05137_06_087.jpg)，则节点 *X*[i] 被称为 *父节点*，而节点 *X*[j]
    被称为 *子节点*。
- en: In the case of an undirected graph, if there is an edge *X*[i] *– X*[j], the
    nodes *X*[i] and *X*[j] are said to be neighbors.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在无向图的情况下，如果存在边 *X*[i] *– X*[j]，则节点 *X*[i] 和 *X*[j] 被称为相邻节点。
- en: The set of parents of node *X* in a directed graph is called the boundary of
    the node *X* and similarly, adjacent nodes in an undirected graph form each other's
    boundary. The degree of the node *X* is the number of edges it participates in.
    The indegree of the node *X* is the number of edges in the directed graph that
    have a relationship to node *X* such that the edge is between node *Y* and node
    *X* and *X* → *Y*. The degree of the graph is the maximal degree of the node in
    that graph.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在有向图中，节点 *X* 的父节点集合被称为节点 *X* 的边界，同样地，在无向图中，相邻节点形成彼此的边界。节点 *X* 的度是它参与的边的数量。节点
    *X* 的入度是在有向图中与节点 *X* 有关系的边的数量，这些边位于节点 *Y* 和节点 *X* 之间，且 *X* → *Y*。图的度是该图中节点的最大度。
- en: Subgraphs and cliques
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子图和团
- en: A subgraph is part of the graph that represents some of the nodes from the entire
    set. A **clique** is a subset of vertices in an undirected graph such that every
    two distinct vertices are adjacent.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 子图是表示整个集合中的一些节点的图的一部分。**团**是无向图中顶点的子集，其中每两个不同的顶点都是相邻的。
- en: Path, trail, and cycles
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 路径、迹和环
- en: 'If there are variables *X*[1], *X*[2], …. *X*[k] in the graph *K = (X, E)*,
    it forms a path if, for every *i* = 1, 2 ... *k* – 1, we have either ![Path, trail,
    and cycles](img/B05137_06_093.jpg) or *X*[i] *–X*[j]; that is, there is either
    a directed edge or an undirected edge between the variables—recall this can be
    depicted as *X*[i] ? *X*[j]. A directed path has at least one directed edge: ![Path,
    trail, and cycles](img/B05137_06_093.jpg).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图中 *K = (X, E)* 存在变量 *X*[1]，*X*[2]，…，*X*[k]，那么当对于每一个 *i* = 1, 2 ... *k* –
    1，我们都有 ![路径、迹和环](img/B05137_06_093.jpg) 或 *X*[i] *–X*[j]；也就是说，变量之间存在有向边或无向边——记住这可以表示为
    *X*[i] ? *X*[j]。一个有向路径至少有一个有向边：![路径、迹和环](img/B05137_06_093.jpg)。
- en: If there are variables *X*[1], *X*[2], …. *X*[k] in the graph *K = (X, E)* it
    forms a *trail* if for every *i* = 1, 2 ... *k* – 1, we have either ![Path, trail,
    and cycles](img/B05137_06_094.jpg).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图中 *K = (X, E)* 存在变量 *X*[1]，*X*[2]，…，*X*[k]，那么当对于每一个 *i* = 1, 2 ... *k* –
    1，我们都有 ![路径、迹和环](img/B05137_06_094.jpg)。
- en: A graph is called a **connected** graph, if for every *X*[i], ….*X*[j] there
    is a trail between *X*[i] and *X*[j].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于图中的每一个 *X*[i]，…，*X*[j]，都存在一条从 *X*[i] 到 *X*[j] 的路径，则称该图为 **连通图**。
- en: In a graph *K = (X, e)*, if there is a directed path between nodes *X* and *Y*,
    *X* is called the ancestor of *Y* and *Y* is called the *descendant* of *X*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 *K = (X, e)* 中，如果节点 *X* 和 *Y* 之间存在有向路径，则 *X* 被称为 *Y* 的 *祖先*，而 *Y* 被称为 *X*
    的 *后代*。
- en: If a graph *K* has a directed path *X*[1], *X*[2], …. *X*[k] where *X*[1] ?
    *X*[k], the path is called a **cycle**. Conversely, a graph with no cycles is
    called an **acyclic** graph.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图*K*有一个有向路径X[1]，X[2]，……，X[k]，其中X[1] ? X[k]，则该路径称为**循环**。相反，没有循环的图称为**无环图**。
- en: Bayesian networks
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: 'Generally, all Probabilistic Graphical Models have three basic elements that
    form the important sections:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有概率图模型都有三个基本元素，构成了重要的部分：
- en: '**Representation**: This answers the question of what does the model mean or
    represent. The idea is how to represent and store the probability distribution
    of *P(X*[1], *X*[2], …. *X*[n]*)*.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：这回答了模型意味着什么或代表什么的问题。想法是如何表示和存储*P(X[1]，X[2]，……，X[n]*)的概率分布。'
- en: '**Inference**: This answers the question: given the model, how do we perform
    queries and get answers. This gives us the ability to infer the values of the
    unknown from the known evidence given the structure of the models. Motivating
    the main discussion points are various forms of inferences involving trade-offs
    between computational and correctness concerns.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**：这回答了以下问题：给定模型，我们如何进行查询并获得答案。这使我们能够从已知证据中推断未知值的值，前提是模型的结构。推动主要讨论点的动机是涉及计算和正确性之间权衡的各种推理形式。'
- en: '**Learning**: This answers the question of what model is right given the data.
    Learning is divided into two main parts:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习**：这回答了给定数据的情况下哪个模型是正确的问题。学习分为两个主要部分：'
- en: Learning the parameters given the structure and data
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定结构和数据的情况下学习参数
- en: Learning the structure with parameters given the data
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定数据的情况下学习结构
- en: 'We will use the well-known student network as an example of a Bayesian network
    in our discussions to illustrate the concepts and theory. The student network
    has five random variables capturing the relationship between various attributes
    defined as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用众所周知的**学生网络**作为贝叶斯网络的例子，在我们的讨论中说明概念和理论。学生网络有五个随机变量，捕捉了以下定义的各种属性之间的关系：
- en: Difficulty of the exam (*D*)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考试难度（D**）'
- en: Intelligence of the student (*I*)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学生智力（I**）'
- en: Grade the student gets (*G*)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生获得的分数（G）
- en: SAT score of the student (*S*)
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学生的SAT分数（S）
- en: Recommendation Letter the student gets based on grade (*L*).
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于成绩的学生推荐信（L**）'
- en: Each of these attributes has binary categorical values, for example, the variable
    *Difficulty* (*D*) has two categories (*d*0, *d*1) corresponding to low and high,
    respectively. *Grades* (*G*) has three categorical values corresponding to the
    grades *(A, B, C)*. The arrows as indicated in the section on graphs indicate
    the dependencies encoded from the domain knowledge—for example, *Grade* can be
    determined given that we know the *Difficulty* of the exam and *Intelligence*
    of the student while the *Recommendation Letter* is completely determined if we
    know just the *Grade* (*Figure 2*). It can be further observed that no explicit
    edge between the variables indicates that they are independent of each other—for
    example, the *Difficulty* of the exam and *Intelligence* of the student are independent
    variables.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性中的每一个都具有二进制分类值，例如，变量**难度（D**）有两个类别（d0，d1），分别对应低和高。**成绩（G**）有三个分类值，对应于成绩（A，B，C）。图中所示箭头表示从领域知识中编码的依赖关系——例如，如果我们知道考试的**难度**和学生**智力**，则可以确定**成绩**；如果我们只知道**成绩**，则**推荐信**完全确定（图2）。可以进一步观察到，变量之间没有显式边表示它们相互独立——例如，考试的**难度**和学生的**智力**是独立变量。
- en: '![Bayesian networks](img/B05137_06_107.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络](img/B05137_06_107.jpg)'
- en: '*Figure 2\. The "Student" network*'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*图2. “学生”网络*'
- en: Representation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**表示**'
- en: A graph compactly represents the complex relationships between random variables,
    allowing fast algorithms to make queries where a full enumeration would be prohibitive.
    In the concepts defined here, we show how directed acyclic graph structures and
    conditional independence make problems involving large numbers of variables tractable.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图形紧凑地表示随机变量之间的复杂关系，允许快速算法进行查询，而完整枚举将是不可行的。在本节定义的概念中，我们展示了如何通过有向无环图结构和条件独立性使涉及大量变量的问题变得可处理。
- en: Definition
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义
- en: 'A Bayesian network is defined as a model of a system with:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络定义为具有以下特征的系统模型：
- en: A number of random variables {*X*[1], *X*[2], …. *X*[k]}
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一系列随机变量{X[1]，X[2]，……，X[k]}
- en: A **Directed** **Acyclic Graph** (**DAG**) with nodes representing random variables.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**有向**的**无环图**（**DAG**），节点表示随机变量。
- en: A local **conditional probability distribution** (**CPD**) for each node with
    dependence to its parent nodes *P(X*[i] *| parent(X*[i]*))*.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个节点的一个局部**条件概率分布**（**CPD**），与父节点相关联 *P(X*[i] *| parent(X*[i]*))*。
- en: A joint probability distribution obtained using the chain rule of distribution
    is a factor given as:![Definition](img/B05137_06_110.jpg)
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布的链式规则获得的联合概率分布是一个因素，表示为：![定义](img/B05137_06_110.jpg)
- en: 'For the student network defined, the joint distribution capturing all nodes
    can be represented as:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于定义的学生网络，捕获所有节点的联合分布可以表示为：
- en: '*P(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*P(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
- en: Reasoning patterns
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理模式
- en: The Bayesian networks help in answering various queries given some data and
    facts, and these reasoning patterns are discussed here.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络有助于回答在给定一些数据和事实的情况下提出的各种查询，这里讨论了这些推理模式。
- en: Causal or predictive reasoning
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因果或预测推理
- en: If evidence is given as, for example, "low intelligence", then what would be
    the chances of getting a "good letter" as shown in *Figure 3*, in the top right
    quadrant? This is addressed by causal reasoning. As shown in the first quadrant,
    causal reasoning flows from the top down.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果证据是例如“低智商”，那么在*图3*右上象限中显示的获得“好信”的机会有多大？这是通过因果推理来解决的。如图一象限所示，因果推理是从上到下流动的。
- en: Evidential or diagnostic reasoning
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 证据或诊断推理
- en: If evidence such as a "bad letter" is given, what would be the chances that
    the student got a "good grade"? This question, as shown in *Figure 3* in the top
    left quadrant, is addressed by evidential reasoning. As shown in the second quadrant,
    evidential reasoning flows from the bottom up.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果给出诸如“坏信”之类的证据，学生获得“好成绩”的机会有多大？这个问题，如图3左上象限所示，是通过证据推理来解决的。如图二象限所示，证据推理是从下到上流动的。
- en: Intercausal reasoning
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 互因果推理
- en: Obtaining interesting patterns from finding a "related cause" is the objective
    of intercausal reasoning. If evidence of "grade C" and "high intelligence" is
    given, then what would be the chance of course difficulty being "high"? This type
    of reasoning is also called "explaining away" as one cause explains the reason
    for another cause and this is illustrated in the third quadrant, in the bottom-left
    of *Figure 3*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从寻找“相关原因”中获取有趣的模式是互因果推理的目标。如果给出“C级”和“高智商”的证据，那么课程难度为“高”的机会有多大？这种推理也称为“解释”，因为一个原因解释了另一个原因，这在图3的第三象限左下角得到了说明。
- en: Combined reasoning
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组合推理
- en: 'If a student takes an "easy" course and has a "bad letter", what would be the
    chances of him getting a "grade C" ? This is explained by queries with combined
    reasoning patterns. Note that it has mixed information and does not a flow in
    a single fixed direction as in the case of other reasoning patterns and is shown
    in the bottom-right of the figure, in quadrant 4:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个学生选修了一门“容易”的课程并且有“坏信”，他获得“C级”成绩的机会有多大？这是通过具有组合推理模式的查询来解释的。请注意，它包含混合信息，并且不像其他推理模式那样在一个固定的方向上流动，如图中右下角的象限4所示：
- en: '![Combined reasoning](img/B05137_06_112.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![联合推理](img/B05137_06_112.jpg)'
- en: '*Figure 3\. Reasoning patterns*'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*图3. 推理模式*'
- en: Independencies, flow of influence, D-Separation, I-Map
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立性、影响流、D-分离、I-图
- en: The conditional independencies between the nodes can be exploited to reduce
    the computations when performing queries. In this section, we will discuss some
    of the important concepts that are associated with independencies.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 节点之间的条件独立性可以在执行查询时用于减少计算。在本节中，我们将讨论与独立性相关的一些重要概念。
- en: Flow of influence
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 影响流
- en: '**Influence** is the effect of how the condition or outcome of one variable
    changes the value or the belief associated with another variable. We have seen
    this from the reasoning patterns that influence flows from variables in direct
    relationships (parent/child), causal/evidential (parent and child with intermediates)
    and in combined structures.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**影响**是指一个变量的条件或结果如何改变与另一个变量相关的值或信念的影响。我们从影响从直接关系（父/子）、因果/证据（父和子以及中间变量）以及组合结构中的变量流动的推理模式中看到了这一点。'
- en: The only case where the influence doesn't flow is when there is a "v-structure".
    That is, given edges between three variables ![Flow of influence](img/B05137_06_113.jpg)
    there is a v-structure and no influence flows between *X*[i - 1] and *X*[i + 1].
    For example, no influence flows between the Difficulty of the course and the Intelligence
    of the student.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一没有影响流动的情况是存在一个“v-结构”。也就是说，给定三个变量之间的边![影响流动](img/B05137_06_113.jpg)，存在一个v-结构，并且*X*[i
    - 1]和*X*[i + 1]之间没有影响流动。例如，课程难度和学生的智力之间没有影响流动。
- en: D-Separation
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D-Separation
- en: 'Random variables *X* and *Y* are said to be d-separated in the graph **G**,
    given there is no active trail between **X** and **Y** in **G** given **Z**. It
    is formally denoted by:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随机变量*X*和*Y*在图**G**中被说成是d-separated的，前提是在**G**中给定**Z**的情况下，**X**和**Y**之间没有活跃路径。它正式表示为：
- en: '*dsep*[G] *(X,Y|Z)*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*dsep*[G] *(X,Y|Z)*'
- en: The point of d-separation is that it maps perfectly to the conditional independence
    between the points. This gives to an interesting property that in a Bayesian network
    any variable is independent of its non-descendants given the parents of the node.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: d-separation的意义在于它完美地映射到点之间的条件独立性。这给出一个有趣的性质，即在贝叶斯网络中，任何变量在给定节点的父节点的情况下与其非后裔变量是独立的。
- en: In the Student network example, the node/variable Letter is d-separated from
    Difficulty, Intelligence, and SAT given the grade.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在学生网络示例中，节点/变量Letter在给定成绩的情况下与Difficulty、Intelligence和SAT是d-separated的。
- en: I-Map
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: I-Map
- en: 'From the d-separation, in graph **G**, we can collect all the independencies
    from the d-separations and these independencies are formally represented as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从d-separation来看，在图**G**中，我们可以收集所有d-separation产生的独立性，这些独立性正式表示为：
- en: '![I-Map](img/B05137_06_117.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![I-Map](img/B05137_06_117.jpg)'
- en: If *P* satisfies *I*(**G**) then we say the **G** is an independency-map or
    I-Map of *P*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*P*满足*I*(**G**)，那么我们说**G**是*P*的一个独立性映射或I-Map。
- en: The main point of I-Map is it can be formally proven that a factorization relationship
    to the independency holds. The converse can also be proved.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: I-Map的主要观点是，可以形式化地证明因式分解关系与独立性之间的关系成立。反之也可以证明。
- en: In simple terms, one can read in the Bayesian network graph G, all the independencies
    that hold in the distribution P regardless of any parameters!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个人可以读取贝叶斯网络图G，所有在分布P中成立的独立性，无论任何参数！
- en: 'Consider the student network—its whole distribution can be shown as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑学生网络——其整个分布可以表示为：
- en: '*P(D,I,G,S,L) = P(D)P(I|D)P(G¦D,I)P(S¦D,I,G)P(L¦D,I,G,S)*'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(D,I,G,S,L) = P(D)P(I|D)P(G¦D,I)P(S¦D,I,G)P(L¦D,I,G,S)*'
- en: 'Now, consider the independence from I-Maps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑I-Map的独立性：
- en: Variables *I* and *D* are non-descendants and not conditional on parents so
    *P(I|D) = P(I)*
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量*I*和*D*是非后裔变量，并且不依赖于父节点，所以*P(I|D) = P(I)*
- en: Variable *S* is independent of its non-descendants *D* and *G*, given its parent
    *I*. *P(S¦D,I,G)=P(S|I)*
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量*S*在给定其父节点*I*的情况下与其非后裔变量*D*和*G*是独立的。*P(S¦D,I,G)=P(S|I)*
- en: Variable *L* is independent of its non-descendants *D*, *I*, and *S*, given
    its parent *G*. *P(L¦D,I,G,S)=P(L|G)*
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量*L*在给定其父节点*G*的情况下与其非后裔变量*D*、*I*和*S*是独立的。*P(L¦D,I,G,S)=P(L|G)*
- en: '*(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
- en: Thus, we have shown that I-Map helps in factorization given just the graph network!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经证明I-Map仅通过图网络就能帮助进行因式分解！
- en: Inference
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: The biggest advantage of probabilistic graph models is their ability to answer
    probability queries in the form of conditional or MAP or marginal MAP, given some
    evidence.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 概率图模型最大的优点是它们能够以条件或MAP或边际MAP的形式回答概率查询，给定一些证据。
- en: 'Formally, the probability of evidence **E = e** is given by:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，证据**E = e**的概率由以下公式给出：
- en: '![Inference](img/B05137_06_126.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![推理](img/B05137_06_126.jpg)'
- en: 'But the problem has been shown to be NP-Hard (*Reference* [3]) or specifically,
    #P-complete. This means that it is intractable when there are a large number of
    trees or variables. Even for a tree-width (number of variables in the largest
    clique) of 25, the problem seems to be intractable—most real-world models have
    tree-widths larger than this.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个问题已被证明是NP-Hard（*参考* [3]）或更具体地说，#P-complete。这意味着当存在大量树或变量时，它是不可行的。即使对于树宽（最大团中的变量数）为25，这个问题似乎也是不可行的——大多数现实世界的模型具有比这更大的树宽。
- en: So if the exact inference discussed before is intractable, can some approximations
    be used so that within some bounds of the error, we can make the problem tractable?
    It has been shown that even an approximate algorithm to compute inferences with
    an error *?* < 0.5, so that we find a number *p* such that |*P*(**E** = **e**)
    – *p*|< *?*, is also NP-Hard.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果之前讨论的精确推理是不可行的，是否可以使用一些近似来使问题在一定的误差范围内可行？已经证明，即使是一个计算推理的近似算法，其误差 *?* <
    0.5，这样我们找到一个数 *p*，使得 |*P*(**E** = **e**) – *p*|< *?*，也是NP-Hard。
- en: But the good news is that this is among the "worst case" results that show exponential
    time complexity. In the "general case" there can be heuristics applied to reduce
    the computation time both for exact and approximate algorithms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 但好消息是，这是“最坏情况”结果之一，表明指数时间复杂度。在“一般情况”中，可以应用启发式方法来减少精确和近似算法的计算时间。
- en: Some of the well-known techniques for performing exact and approximate inferencing
    are depicted in *Figure 4*, which covers most probabilistic graph models in addition
    to Bayesian networks.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一些执行精确和近似推理的知名技术如图 *4* 所示，它涵盖了除了贝叶斯网络之外的大多数概率图模型。
- en: '![Inference](img/B05137_06_130.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![推理](img/B05137_06_130.jpg)'
- en: Figure 4\. Exact and approximate inference techniques
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 精确和近似推理技术
- en: It is beyond the scope of this chapter to discuss each of these in detail. We
    will explain a few of the algorithms in some detail accompanied by references
    to give the reader a better understanding.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论这些算法的细节超出了本章的范围。我们将详细解释一些算法，并附上参考文献，以便读者更好地理解。
- en: Elimination-based inference
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于消除的推理
- en: Here we will describe two techniques, the variable elimination algorithm and
    the clique-tree or junction-tree algorithm.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将描述两种技术，即变量消除算法和团树或连接树算法。
- en: Variable elimination algorithm
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 变量消除算法
- en: 'The basics of the **Variable elimination** (**VE**) algorithm lie in the distributive
    property as shown:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**变量消除**（**VE**）算法的基本原理在于分配性质，如下所示：'
- en: '*(ab+ac+ad)= a (b+c+d)*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*(ab+ac+ad)= a (b+c+d)*'
- en: In other words, five arithmetic operations of three multiplications and two
    additions can be reduced to four arithmetic operations of one multiplication and
    three additions by taking a common factor *a* out.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过提取一个公共因子 *a*，五个算术运算（三个乘法和两个加法）可以减少到四个算术运算（一个乘法和三个加法）。
- en: Let us understand the reduction of the computations by taking a simple example
    in the student network. If we have to compute a probability query such as the
    difficulty of the exam given the letter was good, that is, *P(D¦L=good)=?*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在学生网络中举一个简单的例子来理解计算的简化。如果我们必须计算一个概率查询，例如，考试难度给定信件是好的，即 *P(D¦L=good)=?*。
- en: 'Using Bayes theorem:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯定理：
- en: '![Variable elimination algorithm](img/B05137_06_134.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![变量消除算法](img/B05137_06_134.jpg)'
- en: 'To compute *P(D¦L=good)=?* we can use the chain rule and joint probability:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 *P(D¦L=good)=?*，我们可以使用链式法则和联合概率：
- en: '![Variable elimination algorithm](img/B05137_06_136.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![变量消除算法](img/B05137_06_136.jpg)'
- en: 'If we rearrange the terms on the right-hand side:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新排列右侧的项：
- en: '![Variable elimination algorithm](img/B05137_06_137.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![变量消除算法](img/B05137_06_137.jpg)'
- en: 'If we now replace ![Variable elimination algorithm](img/B05137_06_138.jpg)
    since the factor is independent of the variable *I* that *S* is conditioned on,
    we get:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在用 ![变量消除算法](img/B05137_06_138.jpg) 替换，因为该因子与 *S* 条件下的变量 *I* 独立，我们得到：
- en: '![Variable elimination algorithm](img/B05137_06_140.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![变量消除算法](img/B05137_06_140.jpg)'
- en: Thus, if we proceed carefully, eliminating one variable at a time, we have effectively
    converted *O(2*^n*)* factors to *O(nk*²*)* factors where *n* is the number of
    variables and *k* is the number of observed values for each.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们小心地一次消除一个变量，我们实际上已经将 *O(2^n)* 个因子转换为 *O(nk²)* 个因子，其中 *n* 是变量的数量，*k* 是每个观察值的数量。
- en: Thus, the main idea of the VE algorithm is to impose an order on the variables
    such that the query variable comes last. A list of factors is maintained over
    the ordered list of variables and summation is performed. Generally, we use dynamic
    programming in the implementation of the VE algorithm (*References* [4]).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，VE算法的主要思想是对变量施加一个顺序，使得查询变量最后。在有序变量列表上维护一个因子列表，并执行求和。通常，我们在VE算法的实现中使用动态规划（*参考文献*
    [4]）。
- en: Input and output
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Inputs:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: List of Condition Probability Distribution/Table **F**
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率分布/表 **F** 列表
- en: List of query variables **Q**
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询变量 **Q** 的列表
- en: List of observed variables **E** and the observed value **e**
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察变量 **E** 和观察值 **e** 的列表
- en: 'Output:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '*P*(**Q**|**E** = *e*)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(**Q**|**E** = *e*)'
- en: How does it work?
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The algorithm calls the `eliminate` function in a loop, as shown here:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在循环中调用 `eliminate` 函数，如下所示：
- en: '*VariableElimination*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*VariableElimination*：'
- en: While *?*, the set of all random variables in the Bayesian network is not empty
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *?*，贝叶斯网络中所有随机变量的集合不为空时
- en: Remove the first variable **Z** from *?*
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *?* 中移除第一个变量 **Z**
- en: '*eliminate*(*F*, **Z**)'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*消除*(*F*, **Z**)'
- en: end loop.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束循环。
- en: Set *?* product of all factors in *F*
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *?* 设置为 *F* 中所有因子的乘积
- en: Instantiate observed variables in *?* to their observed values.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *?* 中实例化观察变量到它们的观察值。
- en: return ![How does it work?](img/B05137_06_155.jpg) (renormalization)
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 ![如何工作？](img/B05137_06_155.jpg)（归一化）
- en: '*eliminate* (*F*, **Z**)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*消除* (*F*, **Z**)'
- en: Remove from the *F* all functions, for example, *X*[1], *X*[2], …. *X*[k] that
    involve **Z**.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *F* 中移除所有涉及 **Z** 的函数，例如，*X*[1]，*X*[2]，…，*X*[k]。
- en: Compute new function ![How does it work?](img/B05137_06_429.jpg)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新函数 ![如何工作？](img/B05137_06_429.jpg)
- en: Compute new function ![How does it work?](img/B05137_06_425.jpg)
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新函数 ![如何工作？](img/B05137_06_425.jpg)
- en: Add new function *?* to *F*
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新函数 *?* 到 *F*
- en: Return *F*
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 *F*
- en: Consider the same example of the student network with *P(D, L = good)* as the
    goal.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑具有 *P(D, L = good)* 作为目标的相同学生网络示例。
- en: 'Pick a variable ordering list: *S*, *I*, *L*, *G*, and *D*'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个变量排序列表：*S*，*I*，*L*，*G*，和 *D*
- en: 'Initialize the active factor list and introduce the evidence:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化活动因子列表并引入证据：
- en: 'List: *P(S¦I)P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)*'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表：*P(S¦I)P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)*
- en: Eliminate the variable SAT or **S** off the list![How does it work?](img/B05137_06_161.jpg)
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表中消除变量 SAT 或 **S** ![如何工作？](img/B05137_06_161.jpg)
- en: 'List: *P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)* *?*1 *(I)*'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表：*P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)* *?*1 *(I)*
- en: Eliminate the variable Intelligence or *I*![How does it work?](img/B05137_06_163.jpg)
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消除变量 Intelligence 或 *I* ![如何工作？](img/B05137_06_163.jpg)
- en: 'List: *P(D)P(L¦G)d(L = good)* *?*2 *(G,D)*'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表：*P(D)P(L¦G)d(L = good)* *?*2 *(G,D)*
- en: Eliminate the variable Letter or *L*![How does it work?](img/B05137_06_166.jpg)
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消除变量 Letter 或 *L* ![如何工作？](img/B05137_06_166.jpg)
- en: 'List: *P(D)* *?*[3] *(G)* *?*[2] *(G,D)*'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表：*P(D)* *?*[3] *(G)* *?*[2] *(G,D)*
- en: Eliminate the variable Grade or *G*![How does it work?](img/B05137_06_169.jpg)
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消除变量 Grade 或 *G* ![如何工作？](img/B05137_06_169.jpg)
- en: 'List: *P(D)* *?*[4] *(D)*'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表：*P(D)* *?*[4] *(D)*
- en: Thus with two values, *P(D=high)* *?*[4] *(D=high)* and *P(D=low)* *?*[4] *(D=low)*,
    we get the answer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过两个值，*P(D=high)* *?*[4] *(D=high)* 和 *P(D=low)* *?*[4] *(D=low)*，我们得到答案。
- en: Advantages and limitations
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优势和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 优势和局限性如下：
- en: The main advantage of the VE algorithm is its simplicity and generality that
    can be applied to many networks.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VE算法的主要优势是其简单性和通用性，可以应用于许多网络。
- en: The computational reduction advantage of VE seems to go away when there are
    many connections in the network.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当网络中有许多连接时，VE的计算减少优势似乎消失了。
- en: The choice of optimal ordering of variables is very important for the computational
    benefit.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量最优排序的选择对计算效益非常重要。
- en: Clique tree or junction tree algorithm
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 团树或桥接树算法
- en: Junction tree or Clique Trees are more efficient forms of variable elimination-based
    techniques.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接树或团树是变量消除技术的更有效形式。
- en: Input and output
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Inputs:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: List of Condition Probability Distribution/Table **F**
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率分布/表 **F** 的列表
- en: List of query variables **Q**
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询变量 **Q** 的列表
- en: List of observed variables **E** and the observed value **e**
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察变量 **E** 和观察值 **e** 的列表
- en: 'Output:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '*P*(**Q|**E = *e*)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(**Q|**E = *e*)'
- en: How does it work?
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The steps involved are as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及的步骤如下：
- en: '**Moralization**: This is a process of converting a directed graph into an
    undirected graph with the following two steps:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**道德化**：这是一个将有向图转换为无向图的过程，以下有两个步骤：'
- en: Replace directed edges with undirected edges between the nodes.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用无向边替换节点之间的有向边。
- en: 'If there are two nodes or vertices that are not connected but have a common
    child, add an edge connecting them. (Note the edge between *V*[4] and *V*[5] and
    *V*[2] and *V*[3] in *Figure* 5):'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有两个节点或顶点没有连接但有共同子节点，则添加一个连接它们的边。（注意图5中 *V*[4] 和 *V*[5] 以及 *V*[2] 和 *V*[3]
    之间的边）：
- en: '![How does it work?](img/B05137_06_178.jpg)'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_178.jpg)'
- en: Figure 5\. Graph moralization of DAG showing in green how the directional edges
    are changed and red edges showing new additions.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5. DAG的图道德化，用绿色显示方向边的改变，用红色显示新添加的边。
- en: '**Triangulation**: For understanding triangulation, chords must be formed.
    The chord of a cycle is a pair of vertices *V*[i] and *V*[j] of non-consecutive
    vertices that have an edge between them. A graph is called a **chordal or triangulated
    graph** if every cycle of length ≥ 4 has chords. Note the edge between *V*[1]
    and *V*[5] in *Figure 6* forming chords to make the moralized graph a chordal/triangulated
    graph:![How does it work?](img/B05137_06_184.jpg)'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**三角化**：为了理解三角化，必须形成弦。循环的弦是非连续顶点 *V*[i] 和 *V*[j] 的一对，它们之间有边。如果一个图称为**弦图或三角化图**，如果长度
    ≥ 4 的每个循环都有弦。注意图6中 *V*[1] 和 *V*[5] 之间的边形成弦，使道德化图成为弦图/三角化图：![如何工作？](img/B05137_06_184.jpg)'
- en: Figure 6\. Graph triangulation with blue edge addition to convert a moralized
    graph to a chordal graph.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6. 图三角化，通过添加蓝色边将道德化图转换为弦图。
- en: '**Junction Tree**: From the chordal graphs a junction tree is formed using
    the following steps:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连接树**：通过以下步骤从弦图形成连接树：'
- en: Find all the cliques in the graph and make them nodes with the cluster of all
    vertices. A clique is a subgraph where an edge exists between each pair of nodes.
    If two nodes have one or more common vertices create an edge consisting of the
    intersecting vertices as a separator or sepset. For example, the cycle with edges
    *V*[1], *V*[4], *V*[5] and *V*[6], *V*[4], *V*[5] that have a common edge between
    *V*[4], *V*[5] can be reduced to a clique as shown with the common edge as separator.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图中找到所有完全子图，并将它们作为所有顶点的簇作为节点。完全子图是一个子图，其中每对节点之间都存在边。如果两个节点有一个或多个共同顶点，则创建一个由相交顶点组成的边作为分隔符或sepset。例如，具有边
    *V*[1]、*V*[4]、*V*[5] 和 *V*[6]、*V*[4]、*V*[5] 的循环，其中 *V*[4]、*V*[5] 之间存在公共边，可以简化为一个完全子图，如图中所示，公共边作为分隔符。
- en: 'If the preceding graph contains a cycle, all separators in the cycle contain
    the same variable. Remove the cycle in the graph by creating a minimum spanning
    tree, while including maximum separators. The entire transformation process is
    shown in Figure 7:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果前面的图包含一个循环，循环中的所有分隔符都包含相同的变量。通过创建最小生成树来移除图中的循环，同时包括最大分隔符。整个转换过程如图7所示：
- en: '![How does it work?](img/B05137_06_189.jpg)'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_189.jpg)'
- en: Figure 7\. Formation of a Junction Tree
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7. 连接树的形成
- en: '**Run the Message Passing algorithm on Junction Tree**: Junction tree can be
    used to compute the joint distribution using factorization of cliques and separators
    as![How does it work?](img/B05137_06_190.jpg)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在连接树上运行消息传递算法**：连接树可以用来通过完全子图和分隔符的分解来计算联合分布![如何工作？](img/B05137_06_190.jpg)'
- en: '**Compute the parameters of Junction Tree**: The junction tree parameters can
    be obtained per node using the parent nodes in the original Bayesian network and
    are called clique potentials, as shown here:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算连接树的参数**：连接树的参数可以通过使用原始贝叶斯网络中的父节点按节点获得，称为完全子图势，如图所示：'
- en: (*?*[1] (*V*[2],*V*[3],*V*[5]) = *P*(*V*[5] |*V*[2],*V*[3])*P*(*V*[3])(Note
    in the original Bayesian network edge *V*[5] is dependent on *V*[2], *V*[3], whereas
    *V*[3] is independent)
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (*?*[1] (*V*[2],*V*[3],*V*[5]) = *P*(*V*[5] |*V*[2],*V*[3])*P*(*V*[3])（注意在原始贝叶斯网络中，边
    *V*[5] 依赖于 *V*[2]、*V*[3]，而 *V*[3] 是独立的）
- en: '![How does it work?](img/B05137_06_194.jpg)'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_194.jpg)'
- en: '![How does it work?](img/B05137_06_196.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_196.jpg)'
- en: '![How does it work?](img/B05137_06_197.jpg)'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_197.jpg)'
- en: '**Message Passing between nodes/cliques in Junction Tree**: A node in the junction
    tree, represented by clique *C*[i], multiplies all incoming messages from its
    neighbors with its own clique potential, resulting in a factor ![How does it work?](img/B05137_06_199.jpg)
    whose scope is the clique. It then sums out all the variables except the ones
    on sepset or separators *S*[i,j] between *C*[i] and *C*[j] and then sends the
    resulting factor as a message to *C*[j].![How does it work?](img/B05137_06_202.jpg)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**节点/完全子图在连接树之间的消息传递**：连接树中的一个节点，用完全子图 *C*[i] 表示，将其邻居发送的所有消息与其自己的完全子图势相乘，得到一个因子
    ![如何工作？](img/B05137_06_199.jpg)，其作用域是连接子图。然后，它对所有变量求和，除了在 *C*[i] 和 *C*[j] 之间的分隔符或分隔符
    *S*[i,j] 上的变量，然后将得到的因子作为消息发送到 *C*[j]。![如何工作？](img/B05137_06_202.jpg)'
- en: Figure 8\. Message passing between nodes/cliques in Junction Tree
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8. 节点/完全子图在连接树之间的消息传递
- en: '![How does it work?](img/B05137_06_203.jpg)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_203.jpg)'
- en: Thus, when the message passing reaches the tree root, the joint probability
    distribution is completed.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当消息传递到达树根时，联合概率分布就完成了。
- en: Advantages and limitations
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitaions are as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: The algorithm has a theoretical upper bound on the computations that are related
    to the tree width in the junction tree.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该算法在树宽相关的计算上有理论上的上限。
- en: Multiplication of each potential in the cliques can result in numeric overflow
    and underflow.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个团中每个势的乘积可能导致数值溢出和下溢。
- en: Propagation-based techniques
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于传播的技术
- en: Here we discuss belief propagation, a commonly used message passing algorithm
    for doing inference by introducing factor graphs and the messages that can flow
    in these graphs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论信念传播，这是一种常用的消息传递算法，通过引入因子图和在这些图中可以流动的消息来进行推理。
- en: Belief propagation
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 信念传播
- en: Belief propagation is one of the most practical inference techniques that has
    applicability across most probabilistic graph models including directed, undirected,
    chain-based, and temporal graphs. To understand the belief propagation algorithm,
    we need to first define factor graphs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 信念传播是最实用的推理技术之一，它适用于大多数概率图模型，包括有向、无向、基于链和时序图。为了理解信念传播算法，我们首先需要定义因子图。
- en: Factor graph
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因子图
- en: We know from basic probability theory that the entire joint distribution can
    be represented as a factor over a subset of variables as
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基本的概率论中知道，整个联合分布可以表示为变量子集上的一个因子，如下所示
- en: '![Factor graph](img/B05137_06_205.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![因子图](img/B05137_06_205.jpg)'
- en: In DAG or Bayesian networks *f*[s](**X**[s]) is a conditional distribution.
    Thus, there is a great advantage in expressing the joint distribution over factors
    over the subset of variables.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在DAG或贝叶斯网络中，*f*[s](**X**[s])是一个条件分布。因此，在变量子集上表达联合分布相对于因子有很大的优势。
- en: Factor graph is a representation of the network where the variables and the
    factors involving the variables are both made into explicit nodes (*References*
    [11]). In a simplified student network from the previous section, the factor graph
    is shown in *Figure 9*.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因子图是网络的表示，其中涉及变量的变量和因子都被制成显式节点（*参考文献* [11]）。在前一节的简化学生网络中，因子图显示在*图9*中。
- en: '![Factor graph](img/B05137_06_208.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![因子图](img/B05137_06_208.jpg)'
- en: Figure 9\. Factor graph of simplified "Student" network
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 简化“学生”网络的因子图
- en: A factor graph is a bipartite graph, that is, it has two types of nodes, variables
    and factors.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因子图是一个二分图，即它有两种类型的节点，变量和因子。
- en: The edges flow between two opposite types, that is, from variables to factors
    and vice versa.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 边在两种相反类型之间流动，即从变量到因子，反之亦然。
- en: Converting the Bayesian network to a factor graph is a straightforward procedure
    as shown previously where you start adding variable nodes and conditional probability
    distributions as factor nodes. The relationship between the Bayesian network and
    factor graphs is one-to-many, that is, the same Bayesian network can be represented
    in many factor graphs and is not unique.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将贝叶斯网络转换为因子图是一个简单的步骤，如前所述，其中你开始添加变量节点和条件概率分布作为因子节点。贝叶斯网络和因子图之间的关系是一对多，也就是说，同一个贝叶斯网络可以用多个因子图表示，并且不是唯一的。
- en: Messaging in factor graph
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因子图中的消息传递
- en: There are two distinct messages that flow in these factor graphs that form the
    bulk of all computations through communication.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些因子图中流动着两种不同的消息，这些消息构成了所有计算的大部分，通过通信实现。
- en: '**Message from factor nodes to variable nodes**: The message that is sent from
    a factor node to the variable node can be mathematically represented as follows:![Messaging
    in factor graph](img/B05137_06_209.jpg)![Messaging in factor graph](img/B05137_06_426.jpg)![Messaging
    in factor graph](img/B05137_06_213.jpg) where ![Messaging in factor graph](img/B05137_06_214.jpg)
    Thus, ![Messaging in factor graph](img/B05137_06_215.jpg) is the message from
    factor node *f*[s] to *x* and the product of all such messages from neighbors
    of *x* to *x* gives the combined probability to *x:*![Messaging in factor graph](img/B05137_06_218.jpg)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从因子节点到变量节点的消息**：从因子节点发送到变量节点的消息可以用以下方式数学表示：![因子图中的消息传递](img/B05137_06_209.jpg)![因子图中的消息传递](img/B05137_06_426.jpg)![因子图中的消息传递](img/B05137_06_213.jpg)
    其中 ![因子图中的消息传递](img/B05137_06_214.jpg) 因此，![因子图中的消息传递](img/B05137_06_215.jpg)
    是从因子节点 *f*[s] 到 *x* 的消息，以及从 *x* 的邻居到 *x* 的所有此类消息的乘积给出了 *x* 的联合概率：![因子图中的消息传递](img/B05137_06_218.jpg)'
- en: Figure 10\. Message-passing from factor node to variable node
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 10\. 从因子节点到变量节点的消息传递
- en: '**Message from variable nodes to factor nodes**: Similar to the previous example,
    messages from variable to factor can be shown to be![Messaging in factor graph](img/B05137_06_219.jpg)![Messaging
    in factor graph](img/B05137_06_220.jpg)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从变量节点到因子节点的消息**：与前面的例子类似，变量到因子的消息可以展示为![因子图中的消息](img/B05137_06_219.jpg)![因子图中的消息](img/B05137_06_220.jpg)'
- en: Thus, all the factors coming to the node *x*[m] are multiplied except for the
    factor it is sending to.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了发送给它的因子外，所有到达节点 *x*[m] 的因子都被相乘。
- en: '![Messaging in factor graph](img/B05137_06_222.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![因子图中的消息](img/B05137_06_222.jpg)'
- en: Figure 11\. Message-passing from variable node to factor node
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11\. 从变量节点到因子节点的消息传递
- en: Input and output
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Inputs:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: List of Condition Probability Distribution/Table (CPD/CPT) *F*
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率分布/表 (CPD/CPT) *F* 列表
- en: List of query variables **Q**
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询变量 **Q** 列表
- en: List of observed variables **E** and the observed value **e**
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察变量 **E** 和观察值 **e**
- en: 'Output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '*P*(**Q|**E = *e*)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(**Q|**E = *e*)'
- en: How does it work?
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Create a factor graph from the Bayesian network as discussed previously.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前面讨论的贝叶斯网络创建因子图。
- en: View the node **Q** as the root of the graph.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将节点 **Q** 视为图的根。
- en: 'Initialize all the leaf nodes, that is: ![How does it work?](img/B05137_06_223.jpg)
    and ![How does it work?](img/B05137_06_224.jpg)'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化所有叶节点，即：![它是如何工作的？](img/B05137_06_223.jpg)和![它是如何工作的？](img/B05137_06_224.jpg)
- en: Apply message passing from a leaf to the next node in a recursive manner.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以递归方式应用从叶节点到下一个节点的消息传递。
- en: Move to the next node, until root is reached.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移动到下一个节点，直到到达根节点。
- en: Marginal at the root node gives the result.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根节点的边际给出结果。
- en: Advantages and limitations
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitaions are as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: This algorithm as discussed is very generic and can be used for most graph models.
    This algorithm gives exact inference in directed trees when there are no cycles.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，此算法非常通用，可用于大多数图模型。当没有循环时，此算法在有向树中进行精确推理。
- en: This can be easily implemented in parallel and helps in scalability. Based on
    connectivity, the memory requirement can be very high.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以很容易地并行实现，有助于可扩展性。根据连通性，内存需求可能非常高。
- en: Sampling-based techniques
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于采样的技术
- en: We will discuss a simple approach using particles and sampling to illustrate
    the process of generating the distribution *P(X)* from the random variables. The
    idea is to repeatedly sample from the Bayesian network and use the samples with
    counts to approximate the inferences.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论一个使用粒子采样来展示从随机变量生成分布 *P(X)* 的简单方法。想法是重复从贝叶斯网络中采样，并使用计数样本来近似推理。
- en: Forward sampling with rejection
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带拒绝的前向采样
- en: The key idea is to generate i.i.d. samples iterating over the variables using
    a topological order. In case of some evidence, for example, *P(X|E = e)* that
    contradicts the sample generated, the easiest way is to reject the sample and
    proceed.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是使用拓扑顺序迭代变量生成独立同分布 (i.i.d.) 样本。在存在某些证据的情况下，例如，*P(X|E = e)* 与生成的样本相矛盾，最简单的方法是拒绝样本并继续。
- en: Input and output
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Inputs:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: List of Condition Probability Distribution/Table *F*
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件概率分布/表 *F* 列表
- en: List of query variables **Q**
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询变量 **Q** 列表
- en: List of observed variables **E** and the observed value **e**
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察变量 **E** 和观察值 **e**
- en: 'Output:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '*P*(**Q|**E = *e*)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(**Q|**E = *e*)'
- en: How does it work?
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: For *j* = 1 to *m* //number of samples
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *j* = 1 到 *m* //样本数量
- en: Create a topological order of variables, say **X**[1], **X**[2], **… X**[n].
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建变量的拓扑顺序，例如 **X**[1]，**X**[2]，**… X**[n]。
- en: For *i* = 1 to *n*
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i* = 1 到 *n*
- en: '**u**[i] ? **X***(parent*(**X**[i])) //assign *parent*(**X**[i]) to variables'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**u**[i] ? **X***(parent*(**X**[i])) //assign *parent*(**X**[i]) to variables'
- en: '*sample*(**x**[i], *P*(**X**[i] | **u**[i]) //sample **X**[i] given parent
    assignments'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*sample*(**x**[i], *P*(**X**[i] | **u**[i]) //sample **X**[i] given parent
    assignments'
- en: if(**x**[i] ?, *P*(**X**[i] | **E** = **e**) reject and go to 1.1.2\. //reject
    sample if it doesn't agree with the evidence.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: if(**x**[i] ?, *P*(**X**[i] | **E** = **e**) reject and go to 1.1.2\. //reject
    sample if it doesn't agree with the evidence.
- en: Return (**X**[1], **X**[2],….**X**[n]) as sample.
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 (**X**[1]，**X**[2]，…**X**[n]) 作为样本返回。
- en: Compute *P*(**Q** | **E** = e) using counts from the samples.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用样本的计数计算 *P*(**Q** | **E** = e)。
- en: An example of one sample generated for the student network can be by sampling
    Difficulty and getting Low, next, sampling Intelligence and getting High, next,
    sampling grade using the CPD table for Difficulty=low and Intelligence=High and
    getting Grade=A, sampling SAT using CPD for Intelligence=High and getting SAT=good
    and finally, using Grade=A to sample from Letter and getting Letter=Good. Thus,
    we get first sample (Difficulty=low, Intelligence=High, Grade=A, SAT=good, Letter=Good)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为学生网络生成一个样本的例子可以是先采样难度得到低，然后采样智力得到高，接着使用CPD表对难度=低和智力=高进行采样得到成绩=A，使用CPD对智力=高进行SAT采样得到SAT=好，最后使用成绩=A从字母中进行采样得到Letter=好。因此，我们得到第一个样本（难度=低，智力=高，成绩=A，SAT=好，Letter=好）
- en: Advantages and limitations
  id: totrans-310
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: This technique is fairly simple to implement and execute. It requires a large
    number of samples to be approximate within the bounds.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种技术实现和执行起来相当简单。它需要大量的样本来在界限内近似。
- en: When evidence set is large, the rejection process becomes costly.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当证据集很大时，拒绝过程变得成本高昂。
- en: Learning
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习
- en: The idea behind learning is to generate either a structure or find parameters
    or both, given the data and the domain experts.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 学习背后的思想是在数据和领域专家的指导下生成结构或找到参数或两者兼而有之。
- en: 'The goals of learning are as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 学习的目标如下：
- en: To facilitate inference in Bayesian networks. The pre-requisite of inferencing
    is that the structure and parameters are known, which are the output of learning.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于在贝叶斯网络中进行推理。推理的前提是结构和参数已知，这是学习的结果。
- en: To facilitate prediction using Bayesian networks. Given observed variables **X**,
    predict the target variables **Y**.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于使用贝叶斯网络进行预测。给定观察变量**X**，预测目标变量**Y**。
- en: To facilitate knowledge discovery using Bayesian networks. This means understanding
    causality, relationships, and other features from the data.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于使用贝叶斯网络进行知识发现。这意味着从数据中理解因果关系、关系和其他特征。
- en: Learning, in general, can be characterized by *Figure 12*. The assumption is
    that there is a known probability distribution *P^** that may or may not have
    been generated from a Bayesian network *G^**. The observed data samples are assumed
    to be generated or sampled from that known probability distribution *P^**. The
    domain expert may or may not be present to include the knowledge or prior beliefs
    about the structure. Bayesian networks are one of the few techniques where domain
    experts' inputs in terms of relationships in variables or prior probabilities
    can be used directly, in contrast to other machine learning algorithms. At the
    end of the process of knowledge elicitation and learning from data, we get as
    an output a Bayesian network with defined structure and parameters (CPTs).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，学习可以通过*图12*来表征。假设存在一个已知的概率分布*P^*，它可能或可能不是由贝叶斯网络*G^*生成的。观察到的数据样本被假定为从这个已知的概率分布*P^*中生成或采样。领域专家可能或可能不存在，以包括关于结构的知识或先验信念。贝叶斯网络是少数几种可以直接使用领域专家关于变量关系或先验概率的输入的技术之一，与其他机器学习算法形成对比。在知识获取和学习数据的过程结束时，我们得到一个具有定义结构和参数（CPTs）的贝叶斯网络作为输出。
- en: '![Learning](img/B05137_06_237.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![学习](img/B05137_06_237.jpg)'
- en: Figure 12\. Elements of learning with Bayesian networks
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图12. 贝叶斯网络学习要素
- en: 'Based on data quality (missing data or complete data) and knowledge of structure
    from the expert (unknown and known), the following are four classes that Learning
    in Bayesian networks fall into, as shown in *Table 2*:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据质量（缺失数据或完整数据）以及专家对结构的了解（未知和已知），贝叶斯网络中的学习可以分为以下四类，如*表2*所示：
- en: '| Data | Structure |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 结构 |'
- en: '| Known Structure(Learn Parameters) | Unknown Structure(Learn Structure and
    Parameters) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 已知结构（学习参数） | 未知结构（学习结构和参数） |'
- en: '| Complete Data | Parameter Estimation(Maximum Likelihood, Bayesian Estimation)
    | Optimization(Search and Scoring Techniques) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 完整数据 | 参数估计（最大似然，贝叶斯估计） | 优化（搜索和评分技术） |'
- en: '| Incomplete Data | Non-Linear Parametric Optimization(Expectation Maximization,
    Gradient Descent) | Structure and Parameter Optimization(Structural EM, Mixture
    Models) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 不完整数据 | 非线性参数优化（期望最大化，梯度下降） | 结构和参数优化（结构EM，混合模型） |'
- en: '*Table 2\. Classes of Bayesian network learning*'
  id: totrans-328
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表2. 贝叶斯网络学习类别*'
- en: Learning parameters
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习参数
- en: In this section, we will discuss two broadly used methodologies to estimate
    parameters given the structure. We will discuss only with the complete data and
    readers can refer to the discussion in (*References* [8]) for incomplete data
    parameter estimation.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种广泛使用的方法来估计给定结构下的参数。我们只讨论完整数据，读者可以参考（*参考文献* [8]）中关于不完整数据参数估计的讨论。
- en: Maximum likelihood estimation for Bayesian networks
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯网络的极大似然估计
- en: '**Maximum** **likelihood estimation** (**MLE**) is a very generic method and
    it can be defined as: given a data set *D*, choose parameters ![Maximum likelihood
    estimation for Bayesian networks](img/B05137_06_239.jpg) that satisfy:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大** **似然估计** (**MLE**) 是一个非常通用的方法，它可以定义为：给定一个数据集 *D*，选择满足以下条件的参数 ![贝叶斯网络的极大似然估计](img/B05137_06_239.jpg)：'
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_240.jpg)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_240.jpg)'
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_241.jpg)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_241.jpg)'
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_242.jpg)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_242.jpg)'
- en: Maximum likelihood is the technique of choosing parameters of the Bayesian network
    given the training data. For a detailed discussion, see (*References* [6]).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 极大似然是给定训练数据选择贝叶斯网络参数的技术。对于详细讨论，请参阅（*参考文献* [6]）。
- en: 'Given the known Bayesian network structure of graph *G* and training data ![Maximum
    likelihood estimation for Bayesian networks](img/B05137_06_243.jpg), we want to
    learn the parameters or CPDs—or CPTs to be precise. This can be formulated as:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 给定图 *G* 的已知贝叶斯网络结构和训练数据![贝叶斯网络的极大似然估计](img/B05137_06_243.jpg)，我们想要学习参数或CPDs——更准确地说，CPTs。这可以表示为：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_244.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_244.jpg)'
- en: 'Now each example or instance ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_245.jpg)
    can be written in terms of variables. If there are *i* variables represented by
    *x*[i] and the parents of each is given by *parent*[Xi] then:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个示例或实例![贝叶斯网络的极大似然估计](img/B05137_06_245.jpg)可以用变量来表示。如果有 *i* 个变量由 *x*[i]
    表示，并且每个变量的父节点由 *parent*[Xi] 给出，那么：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_249.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_249.jpg)'
- en: 'Interchanging the variables and instances:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 交换变量和实例：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_250.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_250.jpg)'
- en: 'The term is:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 该术语是：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_251.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_251.jpg)'
- en: 'This is the conditional likelihood of a particular variable *x* *[i]* given
    its parents *parent*[Xi]. Thus, parameters for these conditional likelihoods are
    a subset of parameters given by ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_252.jpg).
    Thus:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给定其父节点 *parent*[Xi] 的特定变量 *x* *[i]* 的条件似然。因此，这些条件似然的参数是![贝叶斯网络的极大似然估计](img/B05137_06_252.jpg)给出的参数的子集。因此：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_253.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_253.jpg)'
- en: Here, ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_254.jpg)
    is called the local likelihood function. This becomes very important as the total
    likelihood decomposes into independent terms of local likelihood and is known
    as the global decomposition property of the likelihood function. The idea is that
    these local likelihood functions can be further decomposed for a tabular CPD by
    simply using the count of different outcomes from the training data.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![贝叶斯网络的极大似然估计](img/B05137_06_254.jpg)被称为局部似然函数。当总似然分解为局部似然的独立项时，这成为似然函数的全局分解属性。其思想是，这些局部似然函数可以通过简单地使用训练数据中不同结果的数量来进一步分解为表格CPD。
- en: 'Let *N*[ijk] be the number of times we observe variable or node *i* in the
    state *k*[,] given the parent node configuration *j*:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *N*[ijk] 为在父节点配置 *j* 下观察到的变量或节点 *i* 在状态 *k*[,] 中的次数：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_257.jpg)![Maximum
    likelihood estimation for Bayesian networks](img/B05137_06_258.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_257.jpg)![贝叶斯网络的极大似然估计](img/B05137_06_258.jpg)'
- en: 'For example, we can have a simple entry corresponding to *X*[i] *= a* and *parent*[Xi]
    *= b* by estimating the likelihood function from the training data as:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以通过从训练数据中估计似然函数来得到一个简单的条目，对应于 *X*[i] *= a* 和 *父节点*[Xi] *= b*：
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_261.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络的极大似然估计](img/B05137_06_261.jpg)'
- en: Consider two cases, as an example. In the first, ![Maximum likelihood estimation
    for Bayesian networks](img/B05137_06_262.jpg) is satisfied by 10 instances with
    *parent*[Xi] *= b* =100\. In the second, ![Maximum likelihood estimation for Bayesian
    networks](img/B05137_06_262.jpg) is satisfied by 100 when *parent*[Xi] *= b* =1000\.
    Notice both probabilities come to the same value, whereas the second has 10 times
    more data and is the "more likely" estimate! Similarly, familiarity with domain
    or prior knowledge, or lack of it due to uncertainty, is not captured by MLE.
    Thus, when the number of samples are limited or when the domain experts are aware
    of the priors, then this method suffers from serious issues.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两种情况，例如。在第一种情况下，![贝叶斯网络的极大似然估计](img/B05137_06_262.jpg) 由10个实例满足，其中 *父节点*[Xi]
    *= b* =100。在第二种情况下，当 *父节点*[Xi] *= b* =1000时，![贝叶斯网络的极大似然估计](img/B05137_06_262.jpg)
    满足100。注意，这两个概率值相同，而第二个有10倍更多的数据，是“更可能的”估计！同样，对领域或先验知识的熟悉程度，或者由于不确定性而缺乏这种知识，都没有被MLE所捕捉。因此，当样本数量有限或领域专家了解先验概率时，这种方法会存在严重问题。
- en: Bayesian parameter estimation for Bayesian network
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝叶斯网络参数估计
- en: This technique overcomes the issue of MLE by encoding prior knowledge about
    the parameter *?* with a probability distribution. Thus, we can encode our beliefs
    or prior knowledge about the parameter space as a probability distribution and
    then the joint distribution of variables and parameters are used in estimation.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术通过使用参数 *?* 的先验知识编码来克服MLE的问题。因此，我们可以将我们对参数空间的信念或先验知识编码为概率分布，然后使用变量和参数的联合分布进行估计。
- en: Let us consider single variable parameter learning where we have instances *x*[1],
    *x*[2] … *x*[M] and they all have parameter **?**[X].
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑单变量参数学习的情况，其中我们有一些实例 *x*[1]，*x*[2] … *x*[M]，它们都具有参数 **?**[X]。
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_267.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_267.jpg)'
- en: Figure 13\. Single variable parameter learning
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图13. 单变量参数学习
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_268.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_269.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_268.jpg)![贝叶斯网络参数估计](img/B05137_06_269.jpg)'
- en: 'Thus, the network is a joint probability model over parameters and data. The
    advantage is we can use it for the posterior distribution:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该网络是参数和数据上的联合概率模型。其优点是我们可以用它来表示后验分布：
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_270.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_271.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_270.jpg)![贝叶斯网络参数估计](img/B05137_06_271.jpg)'
- en: ', *P(?) = prior*,'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ，*P(?) = 先验*，
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_273.jpg)
    Thus, the difference between the maximum likelihood and Bayesian estimation is
    the use of the priors.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '![贝叶斯网络参数估计](img/B05137_06_273.jpg) 因此，最大似然估计与贝叶斯估计之间的区别在于使用了先验概率。'
- en: 'Generalizing it to a Bayesian network *G* given the dataset *D*:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 将其推广到给定数据集 *D* 的贝叶斯网络 *G*：
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_274.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_275.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_274.jpg)![贝叶斯网络参数估计](img/B05137_06_275.jpg)'
- en: If we assume global independence of parameters
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设参数的全局独立性
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_276.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_276.jpg)'
- en: Thus, we get
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_277.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络参数估计](img/B05137_06_277.jpg)'
- en: Again, as before, subset **?**[Xi] | *parent*[Xi] of **?** is local and thus
    the entire posterior can be computed in local terms!
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，就像之前一样，**?**[Xi] | *父节点*[Xi] 的子集是局部的，因此整个后验概率可以用局部术语计算！
- en: Prior and posterior using the Dirichlet distribution
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用狄利克雷分布的先验和后验
- en: Often, in practice, a continuous probability distribution known as Dirichlet
    distribution—which is a Beta distribution—is used to represent priors over the
    parameters.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常使用一种称为狄利克雷分布的连续概率分布来表示参数的先验分布——狄利克雷分布是一种贝塔分布。
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_279.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![使用狄利克雷分布的先验和后验](img/B05137_06_279.jpg)'
- en: 'Probability Density Function:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 概率密度函数：
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_280.jpg)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![使用狄利克雷分布的先验和后验](img/B05137_06_280.jpg)'
- en: Here, ![Prior and posterior using the Dirichlet distribution](img/B05137_06_281.jpg),
    ![Prior and posterior using the Dirichlet distribution](img/B05137_06_282.jpg)
    The alpha terms are known as hyperparameters and *a*[ijri] > 0\. The ![Prior and
    posterior using the Dirichlet distribution](img/B05137_06_284.jpg) is the pseudo
    count, also known as equivalent sample size and it gives us a measure of the prior.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![使用狄利克雷分布的先验和后验](img/B05137_06_281.jpg)，![使用狄利克雷分布的先验和后验](img/B05137_06_282.jpg)
    中的 alpha 项被称为超参数，*a*[ijri] > 0。![使用狄利克雷分布的先验和后验](img/B05137_06_284.jpg) 是伪计数，也称为等效样本量，它为我们提供了一个先验的度量。
- en: The Beta function, *B(a*[ij]*)* is normally expressed in terms of gamma function
    as follows
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 贝塔函数，*B(a*[ij]*)* 通常用伽马函数表示如下
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_286.jpg)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![使用狄利克雷分布的先验和后验](img/B05137_06_286.jpg)'
- en: The advantage of using Dirichlet distribution is it is conjugate in nature,
    that is, irrespective of the likelihood, the posterior is also a Dirichlet if
    the prior is Dirichlet!
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用狄利克雷分布的优点在于它本质上是对偶的，也就是说，无论似然函数如何，如果先验是狄利克雷分布，后验也是狄利克雷分布！
- en: It can be shown that the posterior distribution for the parameters *?*[ijk]
    is a Dirichlet with updated hyperparameters and has a closed form solution!
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明参数 *?*[ijk] 的后验分布是一个具有更新超参数的狄利克雷分布，并且有一个封闭形式的解！
- en: '*a*[ijk] = *a*[ijk] + *N*[ijk]'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '*a*[ijk] = *a*[ijk] + *N*[ijk]'
- en: 'If we use maximum a posteriori estimate and posterior means they can be shown
    to be:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用最大后验估计和后验均值，它们可以证明如下：
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_289.jpg)![Prior
    and posterior using the Dirichlet distribution](img/B05137_06_290.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![使用狄利克雷分布的先验和后验](img/B05137_06_289.jpg)![使用狄利克雷分布的先验和后验](img/B05137_06_290.jpg)'
- en: Learning structures
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习结构
- en: Learning Bayesian network without any domain knowledge or understanding of structures
    includes learning the structure and the parameters. We will first discuss some
    measures that are used for evaluating the network structures and then discuss
    a few well-known algorithms for building optimal structures.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何领域知识或对结构理解的情况下学习贝叶斯网络包括学习结构和参数。我们首先将讨论用于评估网络结构的某些度量，然后讨论一些用于构建最优结构的著名算法。
- en: Measures to evaluate structures
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估结构的度量
- en: The measures used to evaluate a Bayes network structure, given the dataset,
    can be broadly divided into the following categories and details of many are available
    here (*References* [14]).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集，用于评估贝叶斯网络结构的度量可以大致分为以下几类，许多细节可以在这里找到（*参考文献* [14]）。
- en: '**Deviance-Threshold Measure**: The two common techniques to measure deviance
    between two variables used in the network and structure are Pearson''s chi-squared
    statistic and the Kullback-Leibler distance.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差阈值度量**：在网络上测量两个变量之间偏差的两种常用技术是皮尔逊卡方统计量和库尔巴克-莱布勒距离。'
- en: Given the dataset *D* of *M* samples, consider two variables *X*[i] and *X*[j],
    the Pearson's chi-squared statistic measuring divergence is
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定包含 *M* 个样本的数据集 *D*，考虑两个变量 *X*[i] 和 *X*[j]，皮尔逊卡方统计量测量发散度是
- en: '![Measures to evaluate structures](img/B05137_06_292.jpg)![Measures to evaluate
    structures](img/B05137_06_424.jpg)![Measures to evaluate structures](img/B05137_06_294.jpg)![Measures
    to evaluate structures](img/B05137_06_295.jpg)'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![评估结构的度量](img/B05137_06_292.jpg)![评估结构的度量](img/B05137_06_424.jpg)![评估结构的度量](img/B05137_06_294.jpg)![评估结构的度量](img/B05137_06_295.jpg)'
- en: '*d*[?2]*(D)* is 0; when the variables are independent and larger values indicate
    there is dependency between the variables.'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*d*[?2]*(D)* 为 0；当变量相互独立时，较大的值表示变量之间存在依赖性。'
- en: 'Kullback-Leibler divergence is:'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 库尔巴克-莱布勒散度是：
- en: '![Measures to evaluate structures](img/B05137_06_297.jpg)'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![评估结构的度量](img/B05137_06_297.jpg)'
- en: '*d*[I]*(D)* is again 0, it shows independence and the larger values indicates
    dependency. Using various statistical hypothesis tests, a threshold can be used
    to determine the significance.'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*d*[I]*(D)* 再次为0，它表示独立性，较大的值表示依赖性。使用各种统计假设检验，可以使用阈值来确定显著性。'
- en: '**Structure Score Measure**: There are various measures to give scores to a
    structure in a Bayes network. We will discuss the most commonly used measures
    here. A log-likelihood score discussed in parameter learning can be used as a
    score for the structure:![Measures to evaluate structures](img/B05137_06_299.jpg)'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构得分度量**：在贝叶斯网络中为结构评分有多种度量方法。在这里，我们将讨论最常用的度量方法。参数学习中讨论的对数似然得分可以用作结构的评分：![评估结构的措施](img/B05137_06_299.jpg)'
- en: '**Bayesian information score** (**BIC**) is also quite a popular scoring technique
    as it avoids overfitting by taking into consideration the penalty for complex
    structures, as shown in the following equation![Measures to evaluate structures](img/B05137_06_300.jpg)![Measures
    to evaluate structures](img/B05137_06_301.jpg)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯信息得分**（**BIC**）也是一种相当流行的评分技术，因为它通过考虑复杂结构的惩罚来避免过拟合，如下面的方程所示：![评估结构的措施](img/B05137_06_300.jpg)![评估结构的措施](img/B05137_06_301.jpg)'
- en: The penalty function is logarithmic in *M*, so, as it increases, the penalty
    is less severe for complex structures.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚函数在 *M* 中是对数形式的，因此，随着其增加，对于复杂结构的惩罚会减轻。
- en: 'The Akaike information score (AIC), similar to BIC, has similar penalty based
    scoring and is:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 阿卡伊克信息得分（AIC），类似于BIC，具有类似的惩罚基础评分，并且是：
- en: '![Measures to evaluate structures](img/B05137_06_302.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![评估结构的措施](img/B05137_06_302.jpg)'
- en: Bayesian scores discussed in parameter learning are also employed as scoring
    measures.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在参数学习中讨论的贝叶斯得分也被用作评分措施。
- en: Methods for learning structures
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学习结构的方法
- en: We will discuss a few algorithms that are used for learning structures in this
    section; details can be found here (*References* [15]).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些用于学习结构的算法；详细信息可以在此处找到（*参考文献* [15]）。
- en: Constraint-based techniques
  id: totrans-402
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于约束的技术
- en: Constraint-based algorithms use independence tests of various variables, trying
    to find different structural dependencies that we discussed in previous sections
    such as the d-separation, v-structure, and so on, by following the step-by-step
    process discussed here.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 基于约束的算法使用各种变量的独立性检验，试图通过遵循此处讨论的逐步过程找到我们在前几节中讨论的不同结构依赖性，例如d-separation、v-structure等。
- en: Inputs and outputs
  id: totrans-404
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 输入和输出
- en: The input is the dataset *D* with all the variables *{X,Y..}* known for every
    instance {1,2, ... *m*}, and no missing values. The output is a Bayesian network
    graph *G* with all edges, directions known in **E** and the CPT table.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是包含所有变量 *{X,Y..}* 的数据集 *D*，对于每个实例 {1,2, ... *m*} 都已知，并且没有缺失值。输出是一个贝叶斯网络图 *G*，其中所有边、方向在
    **E** 中已知，以及CPT表。
- en: How does it work?
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 如何工作？
- en: Create an empty set of undirected edge **E**.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空的无向边集合 **E**。
- en: Test for conditional independence between two variables independent of directions
    to have an edge.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试两个变量之间的条件独立性，这些变量独立于方向以具有边。
- en: If for all subset **S** = *U* – {*X, Y*}, if *X* is independent of *Y*, then
    add it to the set of undirected edge **E***'*.
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果对于所有子集 **S** = *U* – {*X, Y*}，如果 *X* 与 *Y* 独立，则将其添加到无向边集合 **E***' 中。
- en: Once all potential undirected edges are identified, directionality of the edge
    is inferred from the set **E***'*.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦确定了所有潜在的未定向边，边的方向性将从集合 **E***' 中推断出来。
- en: Considering a triplet *{X, Y, Z}*, if there is an edge *X – Z* and *Y – Z*,
    but no edge between *X – Y* using all variables in the set, and further, if *X*
    is not independent of *Y* given all the edges **S** = *U* – {*X, Y, Z*}, this
    implies the direction of ![How does it work?](img/B05137_06_312.jpg) and ![How
    does it work?](img/B05137_06_313.jpg).
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑三元组 *{X, Y, Z}*，如果存在边 *X – Z* 和 *Y – Z*，但使用集合中的所有变量没有 *X – Y* 的边，并且进一步，如果
    *X* 在所有边 **S** = *U* – {*X, Y, Z*} 给定的情况下与 *Y* 不独立，这表明 ![如何工作？](img/B05137_06_312.jpg)
    和 ![如何工作？](img/B05137_06_313.jpg) 的方向。
- en: Add the edges ![How does it work?](img/B05137_06_312.jpg) and ![How does it
    work?](img/B05137_06_313.jpg) to set **E**.
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将边 ![如何工作？](img/B05137_06_312.jpg) 和 ![如何工作？](img/B05137_06_313.jpg) 添加到 **E**
    中设置。
- en: Update the CPT table using local calculations.
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用局部计算更新CPT表。
- en: Return the Bayes network *G*, edges **E**, and the CPT tables.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回贝叶斯网络 *G*、边 **E** 和CPT表。
- en: Advantages and limitations
  id: totrans-415
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Lack of robustness is one of the biggest drawbacks of this method. A small error
    in data can cause a big impact on the structure due to the assumptions of independence
    that will creep into the individual independence tests.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏鲁棒性是这种方法最大的缺点之一。由于独立性假设会渗透到个体独立性测试中，数据中的小错误可能会对结构造成重大影响。
- en: Scalability and computation time is a major concern as every subset of variables
    are tested and is approximately 2^n. As the number of variables increase to the
    100s, this method fails due to computation time.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性和计算时间是主要关注点，因为每个变量的子集都会被测试，大约是 2^n。当变量的数量增加到100多时，由于计算时间，这种方法会失败。
- en: Search and score-based techniques
  id: totrans-418
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 搜索和基于分数的技术
- en: The search and score method can be seen as a heuristic optimization method where
    iteratively, structure is changed through small perturbations, and measures such
    as BIC or MLE are used to give score to the structures to find the optimal score
    and structure. Hill climbing, depth-first search, genetic algorithms, and so on,
    have all been used to search and score.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索和分数方法可以看作是一种启发式优化方法，其中通过迭代，通过小的扰动改变结构，并使用如 BIC 或 MLE 等度量来对结构进行评分，以找到最优的分数和结构。爬山法、深度优先搜索、遗传算法等都被用来进行搜索和评分。
- en: Inputs and outputs
  id: totrans-420
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Input is dataset *D* with all the variables *{X,Y..}* known for every instance
    {1,2, ... *m*} and no missing values. The output is a Bayesian network graph *G*
    with all edges and directions known in **E**.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是数据集 *D*，其中每个实例 {1,2, ... *m*} 的所有变量 *{X,Y..}* 都是已知的，并且没有缺失值。输出是一个贝叶斯网络图 *G*，其中
    **E** 中所有边和方向都是已知的。
- en: How does it work?
  id: totrans-422
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: '![How does it work?](img/B05137_06_316.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_06_316.jpg)'
- en: Figure 14\. Search and Score
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. 搜索和评分
- en: 'Initialize the Graph *G*, either based on domain knowledge or empty or full.
    Initialize the Edge set **E** based on the graph and initialize the CPT tables
    *T* based on the graph *G*, **E**, and the data *D*. Normally terminating conditions
    are also mentioned such as *maxIterations*:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化图 *G*，基于领域知识或空或满。根据图初始化边集 **E**，并根据图 *G*、**E** 和数据 *D* 初始化 CPT 表 *T*。通常还会提到一些终止条件，如
    *maxIterations*：
- en: '*maxScore= -8, score=computeScore(G,***E***, T)*'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*maxScore= -8, score=computeScore(G,***E***, T)*'
- en: Do
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行
- en: '*maxScore=score*'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*maxScore=score*'
- en: For each variable pair *(X, Y)*
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个变量对 *(X, Y)*
- en: For each ![How does it work?](img/B05137_06_324.jpg)
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 ![如何工作？](img/B05137_06_324.jpg)
- en: New Graph G' based on parents and variables with edge changes.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新图 G' 基于父节点和变量，并改变边。
- en: Compute new CPTs *T' ? computeCPT(G',E',D)*.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算新的 CPT 表 *T' ? computeCPT(G',E',D)*。
- en: '*currentScore = computeScore(G'',***E***'',T'')*'
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*currentScore = computeScore(G'',***E***'',T'')*'
- en: 'If *currentScore > score*:'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *currentScore > score*：
- en: '*score = currentScore*'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*score = currentScore*'
- en: '*G'' = G*, **E***''* = **E**'
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*G'' = G*, **E***''* = **E**'
- en: Repeat 3 while (![How does it work?](img/B05137_06_332.jpg)
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 ![如何工作？](img/B05137_06_332.jpg) 时重复 3 次
- en: Advantages and limitations
  id: totrans-438
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Getting stuck in a local optimum, which is the drawback of most of these heuristic
    search methods, is one of the biggest disadvantages.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡在局部最优解，这是大多数启发式搜索方法的缺点之一，是最大的缺点之一。
- en: Convergence or theoretical guarantees are not available in heuristic search,
    so searching for termination is very much by guess work.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启发式搜索中，没有收敛或理论保证，因此寻找终止条件很大程度上是靠猜测。
- en: Markov networks and conditional random fields
  id: totrans-441
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫网络和条件随机场
- en: So far, we have covered directed acyclic graphs in the area of probabilistic
    graph models, including every aspect of representation, inference, and learning.
    When the graphs are undirected, they are known as **Markov networks** (**MN**)
    or **Markov random** **field** (**MRF**). We will discuss some aspects of Markov
    networks in this section covering areas of representation, inference, and learning,
    as before. Markov networks or MRF are very popular in various areas of computer
    vision such as segmentation, de-noising, stereo, recognition, and so on. For further
    reading, see (*References* [10]).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了概率图模型领域的有向无环图，包括表示、推理和学习的各个方面。当图是无向的，它们被称为 **马尔可夫网络**（**MN**）或
    **马尔可夫随机** **场**（**MRF**）。在本节中，我们将讨论马尔可夫网络的一些方面，包括表示、推理和学习，就像之前一样。马尔可夫网络或 MRF
    在计算机视觉的各个领域都非常流行，如分割、去噪、立体、识别等。有关进一步阅读，请参阅（*参考文献* [10]）。
- en: Representation
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示
- en: Even though a Markov network, like Bayesian networks, has undirected edges,
    it still has local interactions and distributions. We will first discuss the concept
    of parameterization, which is a way to capture these interactions, and then the
    independencies in MN.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管马尔可夫网络，像贝叶斯网络一样，有非有向边，但它仍然具有局部交互和分布。我们首先将讨论参数化的概念，这是一种捕捉这些交互的方法，然后讨论MN中的独立性。
- en: Parameterization
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数化
- en: The affinities between the variables in MN are captured through three alternative
    parameterization techniques discussed in the following sections.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在MN中，变量之间的亲和力通过以下章节中讨论的三个替代参数化技术来捕捉。
- en: Gibbs parameterization
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 吉布斯参数化
- en: The probability distribution function is said to be in Gibb's distribution or
    parameterized by Gibb's distribution if
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 如果概率分布函数被称为吉布斯分布或由吉布斯分布参数化，那么：
- en: '![Gibbs parameterization](img/B05137_06_333.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![吉布斯参数化](img/B05137_06_333.jpg)'
- en: '*Z* is called the partitioning function defined as:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '*Z*被称为划分函数，定义为：'
- en: '![Gibbs parameterization](img/B05137_06_334.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![吉布斯参数化](img/B05137_06_334.jpg)'
- en: Note that interaction between variables are captured by factors ![Gibbs parameterization](img/B05137_06_335.jpg)
    and are not the marginal probabilities, but contribute to the joint probability.
    The factors that parameterize a Markov network are called clique potentials. By
    choosing factors over maximal cliques in the graph, the number of parameters are
    reduced substantially.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，变量之间的交互是通过![吉布斯参数化](img/B05137_06_335.jpg)来捕捉的，而不是边缘概率，但它们对联合概率有贡献。参数化马尔可夫网络的因子被称为团势。通过在图中的最大团上选择因子，参数的数量可以大幅减少。
- en: Factor graphs
  id: totrans-453
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 因素图
- en: Graph structure of Markov network does not reveal properties such as whether
    the factors involve maximal cliques or their subsets when using Gibbs parameterization.
    Factor graphs discussed in the section of inferencing in Bayesian networks have
    a step to recognize maximal cliques and thus can capture these parameterizations.
    Please refer to the section on factor graphs in BN.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫网络图的图结构在使用吉布斯参数化时，并不能揭示出因素是否涉及最大团或其子集等性质。在贝叶斯网络推理部分的章节中讨论的因素图有一个识别最大团的步骤，因此可以捕捉这些参数化。请参阅BN中的因素图部分。
- en: Log-linear models
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对数线性模型
- en: Another form of parameterization is to use the energy model representation from
    statistical physics.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种参数化形式是使用来自统计物理的能量模型表示。
- en: The potential is represented as a set of features and a potential table is generally
    represented by features with weights associated with them.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 势可以用一组特征来表示，而势表通常由具有与之相关联的权重的特征表示。
- en: 'If *D* is a set of variables, ![Log-linear models](img/B05137_06_337.jpg) is
    a factor then:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*D*是一组变量，![对数线性模型](img/B05137_06_337.jpg)是一个因子，那么：
- en: '![Log-linear models](img/B05137_06_338.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![对数线性模型](img/B05137_06_338.jpg)'
- en: 'Thus, as the energy increases, the probability decreases and vice versa. The
    logarithmic cell frequencies captured in ![Log-linear models](img/B05137_06_337.jpg)
    are known as log-linear in statistical physics. The joint probability can be represented
    as:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着能量的增加，概率会降低，反之亦然。![对数线性模型](img/B05137_06_337.jpg)中捕捉的对数细胞频率在统计物理中被称为对数线性。联合概率可以表示为：
- en: '![Log-linear models](img/B05137_06_339.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![对数线性模型](img/B05137_06_339.jpg)'
- en: '![Log-linear models](img/B05137_06_340.jpg) is the feature function defined
    over the variables in **D**[i].'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '![对数线性模型](img/B05137_06_340.jpg)是在**D**[i]中的变量上定义的特征函数。'
- en: Independencies
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 独立性
- en: Like Bayesian networks, Markov networks also encode a set of independence assumptions
    governing the flow of influence in undirected graphs.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 与贝叶斯网络一样，马尔可夫网络也编码了一组独立性假设，这些假设控制了无向图中的影响流动。
- en: Global
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全局
- en: 'A set of nodes **Z** separates sets of nodes **X** and **Y**, if there is no
    active path between any node in *X* ? **X** and *Y* ? **Y** given **Z**. Independence
    in graph *G* is:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一组节点**Z**将节点集**X**和**Y**分开，那么在给定**Z**的情况下，**X**中的任何节点到**X**和**Y**之间的**Y**没有活跃路径。图*G*中的独立性是：
- en: '![Global](img/B05137_06_345.jpg)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![全局](img/B05137_06_345.jpg)'
- en: Pairwise Markov
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成对马尔可夫
- en: 'Two nodes, *X* and *Y*, are independent given all other nodes if there is no
    direct edge between them. This property is of local independence and is weakest
    of all:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个节点，*X*和*Y*，之间没有直接边，那么在给定所有其他节点的情况下，这两个节点是独立的。这种性质是局部独立性，也是最弱的：
- en: '![Pairwise Markov](img/B05137_06_346.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![成对马尔可夫](img/B05137_06_346.jpg)'
- en: Markov blanket
  id: totrans-471
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 马尔可夫毯
- en: 'A node is independent of all other nodes in the graph, given its Markov blanket,
    which is an important concept in Markov networks:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定其马尔可夫毯的情况下，节点与图中所有其他节点都是独立的，这是马尔可夫网络中的一个重要概念：
- en: '![Markov blanket](img/B05137_06_347.jpg)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![马尔可夫毯](img/B05137_06_347.jpg)'
- en: Here **U** *= markov blanket of X*.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 **U** *= X的马尔可夫毯*。
- en: '*Figure 15* shows a Markov blanket for variable *X* as its parents, children,
    and children''s parents:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15*显示了变量*X*的马尔可夫毯，包括其父节点、子节点及其子节点的父节点：'
- en: '![Markov blanket](img/B05137_06_349.jpg)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![马尔可夫毯](img/B05137_06_349.jpg)'
- en: Figure 15\. Markov blanket for Node X - its Parents, Children, and Children's
    Parents.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 图15\. 节点X的马尔可夫毯 - 其父节点、子节点及其子节点的父节点。
- en: Inference
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: 'Inference in MNs is similarly #P-complete problem and hence similar approximations
    or heuristics get applied. Most exact and approximate inferencing techniques,
    such as variable elimination method, junction tree method, belief propagation
    method, and so on, which were discussed in Bayes network, are directly applicable
    to Markov networks. The marginals and conditionals remain similar and computed
    over the potential functions over the cliques as'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在MN中进行推理是一个类似的#P完全问题，因此应用了类似的近似或启发式方法。大多数精确和近似推理技术，如变量消除法、交联树法、信念传播法等，这些在贝叶斯网络中讨论过，可以直接应用于马尔可夫网络。边缘和条件概率保持相似，并且是在团上的势函数上计算的
- en: '![Inference](img/B05137_06_350.jpg)![Inference](img/B05137_06_351.jpg)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![推理](img/B05137_06_350.jpg)![推理](img/B05137_06_351.jpg)'
- en: Markov blankets simplify some of the computations.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫毯简化了一些计算。
- en: Learning
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习
- en: Learning the parameters in Markov networks is complex and computationally expensive
    due to the entanglement of all the parameters in the partitioning function. The
    advantageous step of decomposing the computations into local distributions cannot
    be done because of the partitioning function needing the factor coupling of all
    the variables in the network.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 由于马尔可夫网络中分区函数中所有参数的纠缠，学习马尔可夫网络中的参数复杂且计算成本高。由于分区函数需要网络中所有变量的因子耦合，因此无法将计算分解为局部分布的优势步骤。
- en: Maximum likelihood estimation in MN does not have a closed–form solution and
    hence incremental techniques such as gradient descent are used for optimizing
    over the entire parameter space. The optimization function can be shown to be
    a concave function, thus ensuring a global optimum, but each step of the iterations
    in gradient descent requires inferencing over the entire network, making it computationally
    expensive and sometimes intractable.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在MN中进行最大似然估计没有封闭形式的解，因此使用梯度下降等增量技术来在整个参数空间上进行优化。优化函数可以证明是一个凹函数，从而确保全局最优，但梯度下降的每一步迭代都需要在整个网络上进行推理，这使得它计算成本高，有时甚至难以处理。
- en: Bayesian parameter estimation requires integration over the entire space of
    parameters, which again has no closed-form solution and is even harder. Thus,
    most often, approximate learning methods such as **Markov Chain Monte Carlo**
    (**MCMC**) are used for MNs.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯参数估计需要对参数空间进行积分，这同样没有封闭形式的解，甚至更困难。因此，对于MN，通常使用近似学习方法，如**马尔可夫链蒙特卡洛**（**MCMC**）。
- en: Structure learning in the MNs is similar or even harder than parameter learning
    and has been shown to be NP-hard. In the constraint-based approach, for a given
    dataset, conditional independence between the variables is tested. In MNs, each
    pair of variables is tested for conditional independence using mutual information
    between the pair. Then, based on a threshold, an edge is either considered to
    be existing between the pair or not. One disadvantage of this is it requires extremely
    large numbers of samples to refute any noise present in the data. Complexity of
    the network due to occurrence of pairwise edges is another limitation.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在MN中进行结构学习与参数学习相似，甚至更难，并且已被证明是NP难问题。在基于约束的方法中，对于给定的数据集，测试变量之间的条件独立性。在MN中，使用成对变量之间的互信息来测试每对变量之间的条件独立性。然后，基于一个阈值，可以认为成对之间存在边或者不存在。这种方法的缺点之一是需要极大量的样本来反驳数据中存在的任何噪声。由于成对边的出现导致的网络复杂性是另一个限制。
- en: In search and score-based learning, the goal is similar to BNs, where search
    is done for structures and scoring—based on various techniques—is computed to
    help and adjust the search. In the case of MNs, we use features described in the
    log-linear models rather than the potentials. The weighting of the features is
    considered during optimization and scoring.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于搜索和评分的学习中，目标与BNs相似，其中搜索结构，评分（基于各种技术）被计算以帮助和调整搜索。在MNs的情况下，我们使用对数线性模型中描述的特征，而不是势函数。在优化和评分过程中考虑了特征的加权。
- en: Conditional random fields
  id: totrans-488
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件随机字段
- en: '**Conditional** **random fields** (**CRFs**) are a specialized form of Markov
    network where the hidden and observables are mostly modeled for labeled sequence
    prediction problems (*References* [16]). Sequence prediction problems manifest
    in many text mining areas such as next word/letter predictions, **Part of speech**
    (**POS**) tagging, and so on, and in bioinformatics domain for DNA or protein
    sequence predictions.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件** **随机字段** (**CRFs**) 是一种特殊的马尔可夫网络形式，其中隐藏的和可观察的变量主要用于建模标记序列预测问题 (*参考文献*
    [16])。序列预测问题在许多文本挖掘领域都有体现，如下一个词/字母预测、**词性**(**POS**) 标注等，以及在生物信息学领域用于DNA或蛋白质序列预测。'
- en: The idea behind CRFs is the conditional distribution of sequence is modeled
    as feature functions and the labeled data is used to learn using optimization
    the empirical distribution, as shown in the following figure.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: CRFs背后的思想是将序列的条件分布建模为特征函数，并使用标记数据通过优化学习经验分布，如图所示。
- en: The conditional distribution is expressed as follows where *Z*(**x**) is the
    normalizing constant. Maximum likelihood is used for parameter estimation for
    *?* and is generally a convex function in log-linear obtained through iterative
    optimization methods such as gradient descent.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 条件分布如下所示，其中 *Z*(**x**) 是归一化常数。对于 *?* 的参数估计使用最大似然法，通常是通过梯度下降等迭代优化方法获得的对数线性凸函数。
- en: '![Conditional random fields](img/B05137_06_354.jpg)![Conditional random fields](img/B05137_06_355.jpg)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![条件随机字段](img/B05137_06_354.jpg)![条件随机字段](img/B05137_06_355.jpg)'
- en: 'Figure 16: Conditional random fields mapped to the area of sequence prediction
    in the POS tagging domain.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：条件随机字段映射到词性标注领域的序列预测区域。
- en: Specialized networks
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 专用网络
- en: In this section, we will cover some basic specialized probabilistic graph models
    that are very useful in different machine learning applications.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些在机器学习应用中非常有用的基本专用概率图模型。
- en: Tree augmented network
  id: totrans-496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树增强网络
- en: 'In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, we discussed
    the Naïve Bayes network, which makes the simplified assumption that all variables
    are independent of each other and only have dependency on the target or the class
    variable. This is the simplest Bayesian network derived or assumed from the dataset.
    As we saw in the previous sections, learning complex structures and parameters
    in Bayesian networks can be difficult or sometimes intractable. The **tree augmented
    network** or **TAN** (*References* [9]) can be considered somewhere in the middle,
    introducing constraints on how the trees are connected. TAN puts a constraint
    on features or variable relationships. A feature can have only one other feature
    as parent in addition to the target variable, as illustrated in the following
    figure:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html "第2章. 实际应用中的监督学习")《实际应用中的监督学习》中，我们讨论了朴素贝叶斯网络，它简化地假设所有变量相互独立，并且只依赖于目标或类别变量。这是从数据集中推导或假设的最简单的贝叶斯网络。正如我们在前面的章节中看到的，在贝叶斯网络中学习复杂结构和参数可能很困难，有时甚至是不可能的。**树增强网络**或**TAN**
    (*参考文献* [9]) 可以被视为一种折中方案，它引入了对树如何连接的约束。TAN 对特征或变量关系施加约束。一个特征除了目标变量外，只能有一个其他特征作为父节点，如图所示：
- en: '![Tree augmented network](img/B05137_06_356.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![树增强网络](img/B05137_06_356.jpg)'
- en: Figure 17\. Tree augmented network showing comparison with Naïve Bayes and Bayes
    network and the constraint of one parent per node.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：树增强网络与朴素贝叶斯和贝叶斯网络的比较，以及每个节点只有一个父节点的约束。
- en: Input and output
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Inputs are the training dataset *D* with all the features as variables *{X,
    Y..}*. The features have discrete outcomes, if they don't need to be discretized
    as a pre-processing step.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是包含所有特征的变量 *{X, Y..}* 的训练数据集 *D*。如果不需要在预处理步骤中将特征离散化，则特征具有离散结果。
- en: Outputs are TAN as Bayesian network with CPTs.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compute mutual information between every pair of variables from the training
    dataset.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an undirected graph with each node being the variable and edge being the
    mutual information between them.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a maximum weighted spanning tree.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the spanning tree to a directed graph by selecting the outcome or
    the target variable as the root and having all the edges flowing in the outwards
    direction.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is no directed edge between the class variable and other features,
    add it.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the CPTs based on the DAG or TAN constructed previously.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is more accurate than Naïve Bayes in many practical models. It is less complex
    and faster to build and compute than complete Bayes networks.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov chains
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Markov Chains are specialized probabilistic graph models, with directed graphs
    containing loops. Markov chains can be seen as extensions of automata where the
    weights are probabilities of transition. Markov chains are useful to model temporal
    or sequence of changes that are directly observable. See (*References* [12]) for
    further study.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 17* represents a Markov chain (first order) and the general definition
    can be given as a stochastic process consisting of'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: Nodes as states, ![Markov chains](img/B05137_06_357.jpg).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: Edges representing transition probabilities between the states or nodes. It
    is generally represented as a matrix ![Markov chains](img/B05137_06_358.jpg),
    which is a *N* X *N* matrix where *N* is the number of nodes or states. The value
    of ![Markov chains](img/B05137_06_361.jpg) captures the transition probability
    to node *q*[l] given the state *q*[k]. The rows of matrix add to 1 and the values
    of ![Markov chains](img/B05137_06_364.jpg).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: Initial probabilities of being in the state, *p* = {*p*[1], *p*[2], … *p*[N]}.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, it can be written as a triple *M*= (Q, **A**, *p*) and the probability
    of being in any state only depends on the last state (first order):'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov chains](img/B05137_06_367.jpg)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
- en: 'The joint probability:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov chains](img/B05137_06_368.jpg)![Markov chains](img/B05137_06_369.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
- en: Figure 18\. First-order Markov chain
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov models
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many real-world situations, the events we are interested in are not directly
    observable. For example, the words in sentences are observable, but the part-of-speech
    that generated the sentence is not. **Hidden Markov** **models** (**HMM**) help
    us in modeling such states where there are observable events and hidden states
    (*References* [13]). HMM are widely used in various modeling applications for
    speech recognition, language modeling, time series analysis, and bioinformatics
    applications such as DNA/protein sequence predictions, to name a few.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov models](img/B05137_06_370.jpg)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
- en: Figure 19\. Hidden Markov model showing hidden variables and the observables.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden Markov models can be defined again as a triple ![Hidden Markov models](img/B05137_06_371.jpg),
    where:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: is a set of finite states or symbols that are observed. ![Hidden Markov models](img/B05137_06_373.jpg)
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q is a set of finite states that are not observed ![Hidden Markov models](img/B05137_06_375.jpg).
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T are the parameters.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state transition matrix, given as ![Hidden Markov models](img/B05137_06_377.jpg)
    captures the probability of transition from state *q*[k] to *q*[l].
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: Emission probabilities capturing relationships between the hidden and observed
    state, given as ![Hidden Markov models](img/B05137_06_379.jpg) and *b* ? ?. ![Hidden
    Markov models](img/B05137_06_381.jpg).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: Initial state distribution *p* = {*p*[1], *p*[2], … *p*[N]}.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, a path in HMM consisting of a sequence of hidden states Q = {*q*[1],
    *q*[2], … *q*[L]} is a first order Markov chain *M*= (Q, **A**, *p*). This path
    in HMM emits a sequence of symbols, *x*[1], *x*[2], *x*[L], referred to as the
    observations. Thus, knowing both the observations and hidden states the joint
    probability is:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov models](img/B05137_06_384.jpg)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
- en: 'In real-world situations, we only know the observations *x* and do not know
    the hidden states *q*. HMM helps us to answer the following questions:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: What is the most likely path that could have generated the observation *x*?
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of *x*?
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of being in state *q*i *= k* given the observation ?
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most probable path in HMM
  id: totrans-540
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us assume the observations *x* = *x*[1], *x*[2], *x*[L] and we want to
    find the path ![Most probable path in HMM](img/B05137_06_388.jpg) that generated
    the observations. This can be given as:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '![Most probable path in HMM](img/B05137_06_389.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
- en: 'The path *q** need not be unique, but for computation and explanation the assumption
    of the unique path is often made. In a naïve way, we can compute all possible
    paths of length *L* of *q* and chose the one(s) with the highest probability giving
    exponential computing terms or speed. More efficient is using Viterbi''s algorithm
    using the concept of dynamic programming and recursion. It works on the simple
    principle of breaking the equation into simpler terms as:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '![Most probable path in HMM](img/B05137_06_391.jpg)![Most probable path in
    HMM](img/B05137_06_392.jpg)![Most probable path in HMM](img/B05137_06_393.jpg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
- en: Here, ![Most probable path in HMM](img/B05137_06_394.jpg) and ![Most probable
    path in HMM](img/B05137_06_395.jpg) Given the initial condition ![Most probable
    path in HMM](img/B05137_06_396.jpg) and using dynamic programming with keeping
    pointer to the path, we can efficiently compute the answer.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: Posterior decoding in HMM
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The probability of being in a state *q*[i] *= k* given the observation *x*
    can be written using Bayes theorem as:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_397.jpg)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
- en: 'The numerator can be rewritten as:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_398.jpg)![Posterior decoding in
    HMM](img/B05137_06_399.jpg)![Posterior decoding in HMM](img/B05137_06_400.jpg)![Posterior
    decoding in HMM](img/B05137_06_428.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
- en: Where ![Posterior decoding in HMM](img/B05137_06_402.jpg) is called a Forward
    variable and ![Posterior decoding in HMM](img/B05137_06_403.jpg) is called a Backward
    variable.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation of the forward variable is similar to Viterbi''s algorithm
    using dynamic programming and recursion where summation is done instead:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_404.jpg)![Posterior decoding in
    HMM](img/B05137_06_405.jpg)![Posterior decoding in HMM](img/B05137_06_406.jpg)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
- en: The probability of observing *x* can be, then
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_407.jpg)'
  id: totrans-555
  prefs: []
  type: TYPE_IMG
- en: 'The forward variable is the joint probability and the backward variable is
    a conditional probability:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_403.jpg)![Posterior decoding in
    HMM](img/B05137_06_408.jpg)![Posterior decoding in HMM](img/B05137_06_409.jpg)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
- en: 'It is called a backward variable as the dynamic programming table is filled
    starting with the *L*^(th) column to the first in a backward manner. The backward
    probabilities can also be used to compute the probability of observing *x* as:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_411.jpg)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
- en: Tools and usage
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce two tools in Java that are very popular for
    probabilistic graph modeling.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: OpenMarkov
  id: totrans-562
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenMarkov is a Java-based tool for PGMs and here is the description from [www.openmarkov.org](http://www.openmarkov.org):'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-564
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenMarkov is a software tool for probabilistic graphical models (PGMs) developed
    by the Research Centre for Intelligent Decision-Support Systems of the UNED in
    Madrid, Spain.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been designed for: editing and evaluating several types of PGMs, such
    as Bayesian networks, influence diagrams, factored Markov models, and so on, learning
    Bayesian networks from data interactively, and cost-effectiveness analysis.'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: OpenMarkov is very good in performing interactive and automated learning from
    the data. It has capabilities to preprocess the data (discretization using frequency
    and value) and perform structure and parameter learning using a few search algorithms
    such as search-based Hill Climbing and score-based PC. OpenMarkov stores the models
    in a format known as pgmx. To apply the models in most traditional packages there
    may be a need to convert the pgmx models to XMLBIF format. Various open source
    tools provide these conversions.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: Here we have some screenshots illustrating the usage of OpenMarkov to learn
    the structure and parameters from the data.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 20*, we see the screen for interactive learning where you select
    the data file and algorithm to use:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_412.jpg)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
- en: Figure 20\. OpenMarkov GUI – Interactive learning, algorithm selection
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is the **Preprocessing** tab (*Figure 21*) where we can select
    how discretization is done:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_413.jpg)'
  id: totrans-573
  prefs: []
  type: TYPE_IMG
- en: Figure 21\. OpenMarkov GUI – Preprocessing screen
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in *Figure 22*, we see the display of the learned Bayes network structure:'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_414.jpg)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
- en: Figure 22\. OpenMarkov GUI – Structure output
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: Weka Bayesian Network GUI
  id: totrans-578
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weka's Bayes Network editor for interactive and automated learning has a large
    number of options for Bayes network representation, inference and learning as
    compared to OpenMarkov. The advantage in using Weka is the availability of a number
    of well-integrated preprocessing and transformation filters, algorithms, evaluation,
    and experimental metrics.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 23*, we see the Bayes Network Editor where the search algorithm
    is selected and various options can be configured:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka Bayesian Network GUI](img/B05137_06_415.jpg)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
- en: Figure 23\. WEKA Bayes Network – configuring search algorithm
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: 'The learned structure and parameters of the BayesNet are shown in the output
    screen in *Figure 24*:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka Bayesian Network GUI](img/B05137_06_416.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
- en: Figure 24\. WEKA Bayes Network – Learned parameter and structure
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  id: totrans-586
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform a case study with real-world machine learning
    datasets to illustrate some of the concepts from Bayesian networks.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: We will use the UCI Adult dataset, also known as the *Census Income* dataset
    ([http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)).
    This dataset was extracted from the United States Census Bureau's 1994 census
    data. The donors of the data is Ronny Kohavi and Barry Becker, who were with Silicon
    Graphics at the time. The dataset consists of 48,842 instances with 14 attributes,
    with a mix of categorical and continuous types. The target class is binary.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  id: totrans-589
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem consists of predicting the income of members of a population based
    on census data, specifically, whether their income is greater than $50,000.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  id: totrans-591
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a problem of classification and this time around we will be training
    Bayesian graph networks to develop predictive models. We will be using linear,
    non-linear, and ensemble algorithms, as we have done in experiments in previous
    chapters.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  id: totrans-593
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original dataset, there are 3,620 examples with missing values and six
    duplicate or conflicting instances. Here we include only examples with no missing
    values. This set, without unknowns, is divided into 30,162 training instances
    and 15,060 test instances.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis
  id: totrans-595
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The features and their descriptions are given in *Table 3*:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Type information |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
- en: '| age | continuous. |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
- en: '| workclass | Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,
    State-gov, Without-pay, Never-worked. |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
- en: '| fnlwgt | continuous. |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
- en: '| education | Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm,
    Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
    |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
- en: '| education-num | continuous. |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
- en: '| marital-status | Married-civ-spouse, Divorced, Never-married, Separated,
    Widowed, Married-spouse-absent, Married-AF-spouse. |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
- en: '| occupation | Tech-support, Craft-repair, Other-service, Sales, Exec-managerial,
    Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing,
    Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
- en: '| relationship | Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
    |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
- en: '| race | White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
- en: '| sex | Female, Male. |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| capital-gain | continuous. |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
- en: '| capital-loss | continuous. |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
- en: '| hours-per-week | continuous. |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
- en: '| native-country | United-States, Cambodia, England, Puerto-Rico, Canada, Germany,
    Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras,
    Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France,
    Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala,
    Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru,
    Hong, Holand-Netherlands. |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
- en: '*Table 3\. UCI Adult dataset – features*'
  id: totrans-613
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The dataset is split by label as 24.78% (>50K) to 75.22% (<= 50K). Summary
    statistics of key features are given in *Figure 25*:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature analysis](img/B05137_06_417.jpg)![Feature analysis](img/B05137_06_418.jpg)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
- en: Figure 25\. Feature summary statistics
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  id: totrans-617
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will perform detailed analysis on the Adult dataset using different flavors
    of Bayes network structures and with regular linear, non-linear, and ensemble
    algorithms. Weka also has an option to visualize the graph model on the trained
    dataset using the menu item, as shown in *Figure 26*. This is very useful when
    the domain expert wants to understand the assumptions and the structure of the
    graph model. If the domain expert wants to change or alter the network, it can
    be done easily and saved using the Bayes Network editor.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '![Models, results, and evaluation](img/B05137_06_419.jpg)'
  id: totrans-619
  prefs: []
  type: TYPE_IMG
- en: Figure 26\. Weka Explorer – visualization menu
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 27* shows the visualization of the trained Bayes Network model''s graph
    structure:'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: '![Models, results, and evaluation](img/B05137_06_420.jpg)'
  id: totrans-622
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Visualization of learned structure of the Bayesian network.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithms used for experiments are:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian network Classifiers
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes with default Kernel estimation on continuous data
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes with supervised discretization on continuous data
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree augmented network (TAN) with search-score structure parameter learning
    using the K2 algorithm and a choice of three parents per node
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian network with search and score
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching using Hill Climbing and K2
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring using Simple Estimation
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choice of parents changed from two to three to illustrate the effect on metrics
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Bayesian algorithms
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression (default parameters)
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN (IBK with 10 Neighbors)
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Tree (J48, default parameters)
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoostM1 (DecisionStump and default parameters)
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest (default parameters)
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 4* presents the evaluation metrics for all the learners used in the
    experiments, including Bayesian network classifiers as well as the non-Bayesian
    algorithms:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithms | TP Rate | FP Rate | Precision | Recall | F-Measure | MCC | ROC
    Area | PRC Area |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Kernel Estimator) | 0.831 | 0.391 | 0.821 | 0.831 | 0.822 |
    0.494 | 0.891 | 0.906 |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Discretized) | 0.843 | 0.191 | 0.861 | 0.843 | 0.848 | 0.6 |
    0.917 | 0.93 |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
- en: '| TAN (K2, 3 Parents, Simple Estimator) | 0.859 | 0.273 | 0.856 | 0.859 | 0.857
    | 0.6 | 0.916 | 0.931 |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (K2, 3 Parents, Simple Estimator) | 0.863 | 0.283 | 0.858 | 0.863
    | 0.86 | 0.605 | 0.934 | 0.919 |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (K2, 2 Parents, Simple Estimator) | 0.858 | 0.283 | 0.854 | 0.858
    | 0.855 | 0.594 | 0.917 | 0.932 |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (Hill Climbing, 3 Parents, Simple Estimator) | 0.862 | 0.293 | 0.857
    | 0.862 | 0.859 | 0.602 | 0.918 | 0.933 |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 0.851 | 0.332 | 0.844 | 0.851 | 0.845 | 0.561 | 0.903
    | 0.917 |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
- en: '| KNN (10) | 0.834 | 0.375 | 0.824 | 0.834 | 0.826 | 0.506 | 0.867 | 0.874
    |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree (J48) | 0.858 | 0.300 | 0.853 | 0.858 | 0.855 | 0.590 | 0.890
    | 0.904 |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
- en: '| AdaBoostM1 | 0.841 | 0.415 | 0.833 | 0.841 | 0.826 | 0.513 | 0.872 | 0.873
    |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | 0.848 | 0.333 | 0.841 | 0.848 | 0.843 | 0.555 | 0.896 | 0.913
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
- en: '*Table 4\. Classifier performance metrics*'
  id: totrans-653
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Analysis of results
  id: totrans-654
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naïve Bayes with supervised discretization shows relatively better performance
    than kernel estimation. This gives a useful hint that discretization, which is
    needed in most Bayes networks, will play an important role.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: The results in the table show continuous improvement when Bayes network complexity
    is increased. For example, Naïve Bayes with discretization assumes independence
    from all features and shows a TP rate of 84.3, the TAN algorithm where there can
    be one more parent shows a TP rate of 85.9, and BN with three parents shows the
    best TP rate of 86.2\. This clearly indicates that a complex BN with some nodes
    having no more than three parents can capture the domain knowledge and encode
    it well to predict on unseen test data.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: Bayes network where structure is learned using search and score (with K2 search
    with three parents and scoring using Bayes score) and estimation is done using
    simple estimation, performs the best in almost all the metrics of the evaluation,
    as shown in the highlighted values.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: There is a very small difference between Bayes Networks—where structure is learned
    using search and score of Hill Climbing—and K2, showing that even local search
    algorithms can find an optimum.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: Bayes network with a three-parent structure beats most linear, non-linear, and
    ensemble methods such as AdaBoostM1 and Random Forest on almost all the metrics
    on unseen test data. This shows the strength of BNs in not only learning the structure
    and parameters on small datasets with large number of missing values as well as
    predicting well on unseen data, but in beating other sophisticated algorithms
    too.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-660
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PGMs capture domain knowledge as relationships between variables and represent
    joint probabilities. They are used in a range of applications.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: Probability maps an event to a real value between 0 and 1 and can be interpreted
    as a measure of the frequency of occurrence (frequentist view) or as a degree
    of belief in that occurrence (Bayesian view). Concepts of random variables, conditional
    probabilities, Bayes' theorem, chain rule, marginal and conditional independence
    and factors form the foundations to understanding PGMs. MAP and Marginal Map queries
    are ways to ask questions about the variables and relationships in the graph.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: The structure of graphs and their properties such as paths, trails, cycles,
    sub-graphs, and cliques are vital to the understanding of Bayesian networks. Representation,
    Inference, and Learning form the core elements of networks that help us capture,
    extract, and make predictions using these methods. From the representation of
    graphs, we can reason about the flow of influence and detect independencies that
    help reduce the computational load when querying the model. Junction trees, variable
    elimination, and belief propagation methods likewise make inference from queries
    more tractable by reductive steps. Learning from Bayesian networks involves generating
    the structure and model parameters from the data. We discussed several methods
    of learning parameters and structure.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov networks** (**MN**), which have undirected edges, also contain interactions
    that can be captured using parameterization techniques such as Gibbs parameterization,
    Factor Graphs, and Log-Linear Models. Independencies in MN govern flows of influence,
    as in Bayesian networks. Inference techniques are also similar. Learning of parameters
    and structure in MN is hard, and approximate methods are used. Specialized networks
    such as **Tree augmented networks** (**TAN**) make assumptions of independence
    amongst nodes and are very useful in some applications. Markov Chains and hidden
    Markov models are other specialty networks that also find application in a range
    of fields.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: Open Markov and Weka Bayesian Network GUI are introduced as Java-based tools
    for PGMs. The case study in this chapter used Bayesian Networks to learn from
    the UCI Adult census dataset and its performance was compared to other (non-PGM)
    classifiers.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-666
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Daphne Koller and Nir Friedman (2009). *Probabilistic Graphical Models*. MIT
    Press. ISBN 0-262-01319-3.
  id: totrans-667
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Verma and J. Pearl (1988), In proceedings for fourth workshop on Uncertainty
    in Artificial Intelligence, Montana, Pages 352-359\. Causal Networks- Semantics
    and expressiveness.
  id: totrans-668
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dagum, P., and Luby, M. (1993). *Approximating probabilistic inference in Bayesian
    belief networks is NP hard*. Artificial Intelligence 60(1):141–153.
  id: totrans-669
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: U. Bertele and F. Brioschi, *Nonserial Dynamic Programming*, Academic Press.
    New York, 1972.
  id: totrans-670
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shenoy, P. P. and G. Shafer (1990). *Axioms for probability and belief-function
    propagation*, in Uncertainty in Artificial Intelligence, 4, 169-198, North-Holland,
    Amsterdam
  id: totrans-671
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayarri, M.J. and DeGroot, M.H. (1989). *Information in Selection Models*. Probability
    and Bayesian Statistics, (R. Viertl, ed.), Plenum Press, New York.
  id: totrans-672
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spiegelhalter and Lauritzen (1990). *Sequential updating of conditional probabilities
    on directed graphical structures*. Networks 20\. Pages 579-605.
  id: totrans-673
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'David Heckerman, Dan Geiger, David M Chickering (1995). In journal of Machine
    Learning. *Learning Bayesian networks: The combination of knowledge and statistical
    data*.'
  id: totrans-674
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Friedman, N., Geiger, D., & Goldszmidt, M. (1997). *Bayesian network classifiers*.
    Machine Learning, 29, 131– 163.
  id: totrans-675
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isham, V. (1981). *An introduction to spatial point processes and Markov random
    fields*. International Statistical Rewview, 49(1):21–43
  id: totrans-676
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger, *Factor graphs
    and sum-product algorithm*, IEEE Trans. Info. Theory, vol. 47, pp. 498–519, Feb.
    2001\.
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kemeny, J. G. and Snell, J. L. *Finite Markov Chains*. New York: Springer-Verlag,
    1976.'
  id: totrans-678
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Baum, L. E.; Petrie, T. (1966). *Statistical Inference for Probabilistic Functions
    of Finite State Markov Chains*. The Annals of Mathematical Statistics. 37 (6):
    1554–1563\.'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gelman, A., Hwang, J. and Vehtar, A. (2004). *Understanding predictive information
    criteria for Bayesian models*. Statistics and Computing Journal 24: 997\. doi:10.1007/s11222-013-9416-2'
  id: totrans-680
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dimitris. Margaritis (2003). *Learning Bayesian Network Model Structure From
    Data*. Ph.D Thesis Carnegie Mellon University.
  id: totrans-681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'John Lafferty, Andrew McCallum, Fernando C.N. Pereira (2001). *Conditional
    Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data*,
    International Conference on Machine Learning 2001 (ICML 2001), pages 282-289.'
  id: totrans-682
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
