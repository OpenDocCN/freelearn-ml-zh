- en: Chapter 6. Probabilistic Graph Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Probabilistic graph models** (**PGMs**), also known as graph models, capture
    the relationship between different variables and represent the probability distributions.
    PGMs capture joint probability distributions and can be used to answer different
    queries and make inferences that allow us to make predictions on unseen data.
    PGMs have the great advantage of capturing domain knowledge of experts and the
    causal relationship between variables to model systems. PGMs represent the structure
    and they can capture knowledge in a representational framework that makes it easier
    to share and understand the domain and models. PGMs capture the uncertainty or
    the probabilistic nature very well and are thus very useful in applications that
    need scoring or uncertainty-based approaches. PGMs are used in a wide variety
    of applications that use machine learning such as applications to domains of language
    processing, text mining and information extraction, computer vision, disease diagnosis,
    and DNA structure predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Judea Pearl is the pioneer in the area of PGMs and was the first to introduce
    the topic of Bayesian Networks (*References* [2] and [7]). Although covering all
    there is to know about PGMs is beyond the scope of this chapter, our goal is to
    cover the most important aspects of PGMs—Bayes network and directed PGMs—in some
    detail. We will divide the subject into the areas of representation, inference,
    and learning and will discuss specific algorithms and sub-topics in each of these
    areas. We will cover Markov Networks and undirected PGMs, summarizing some differences
    and similarities with PGMs, and addressing related areas such as inference and
    learning. Finally, we will discuss specialized networks such as **tree augmented**
    **networks** (**TAN**), Markov chains and **hidden Markov models** (**HMM**).
    For an in-depth treatment of the subject, see *Probabilistic Graphical Models*,
    by Koller and Friedman (*References* [1]).
  prefs: []
  type: TYPE_NORMAL
- en: Probability revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many basic concepts of probability are detailed in [Appendix B](apb.html "Appendix B. Probability"),
    *Probability*. Some of the key ideas in probability theory form the building blocks
    of probabilistic graph models. A good grasp of the relevant theory can help a
    great deal in understanding PGMs and how they are used to make inferences from
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Concepts in probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will discuss important concepts related to probability theory
    that will be used in the discussion later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The essence of conditional probability, given two related events a and ß, is
    to capture how we assign a value for one of the events when the other is known
    to have occurred. The conditional probability, or the conditional distribution,
    is represented by *P*(*a* | *ß*), that is, the probability of event *a* happening
    given that the event *ß* has occurred (equivalently, given that *ß* is true) and
    is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional probability](img/B05137_06_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The *P*(*a* n *ß*) captures the events where both a and ß occur.
  prefs: []
  type: TYPE_NORMAL
- en: Chain rule and Bayes' theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The conditional probability definition gives rise to the chain rule of conditional
    probabilities that says that when there are multiple events α[1], α[2]….α[n] then:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*α*[1] ∩ *α*[2] ∩….∩ *α*[n] ) = *P*(*α*[1] )*P*(*α*[2] ¦ *α*[1])*P*(*α*[3]
    | *α*[1] ∩ *α*[2])..*P*(*α*[∩] |*α*[1] ∩ *α*[2] ∩….∩ *α*[n-1])'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of several events can be expressed as the probability of the
    first times the probability of the second given the first, and so on. Thus, the
    probability of *α*[n] depends on everything α[1] to α[n] and is independent of
    the order of the events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes rule also follows from the conditional probability rule and can be given
    formally as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chain rule and Bayes'' theorem](img/B05137_06_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Random variables, joint, and marginal distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is natural to map the event spaces and outcomes by considering them as attributes
    and values. Random variables are defined as attributes with different known specific
    values. For example, if *Grade* is an attribute associated with *Student*, and
    has values *{A, B, C}*, then *P(Grade = A)* represents a random variable with
    an outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random variables are generally denoted by capital letters, such as *X*, *Y*,
    and *Z* and values taken by them are denoted by *Val(X) = x*. In this chapter,
    we will primarily discuss values that are categorical in nature, that is, that
    take a fixed number of discrete values. In the real world, the variables can have
    continuous representations too. The distribution of a variable with categories
    {x¹, x² …x^n} can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random variables, joint, and marginal distributions](img/B05137_06_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Such a distribution over many categories is called a **multinomial distribution**.
    In the special case when there are only two categories, the distribution is said
    to be the **Bernoulli distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: Given a random variable, a probability distribution over all the events described
    by that variable is known as the marginal distribution. For example, if Grade
    is the random variable, the marginal distribution can be defined as *(Grade =
    A) = 0.25, P(Grade = b) = 0.37 and P(Grade = C) = 0.38*.
  prefs: []
  type: TYPE_NORMAL
- en: In many real-world models, there are more than one random variables and the
    distribution that considers all of these random variable is called the **joint
    distribution**. For example, if *Intelligence* of student is considered as another
    variable and denoted by *P(Intelligence)* or *P(I)* and has binary outcomes *{low,
    high}*, then the distribution considering *Intelligence* and *Grade* represented
    as *P(Intelligence, Grade)* or *P(I, G)*, is the joint distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Marginal distribution of one random variable can be computed from the joint
    distribution by summing up the values over all of the other variables. The marginal
    distribution over grade can be obtained by summing over all the rows as shown
    in *Table 1* and that over the intelligence can be obtained by summation over
    the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Random variables, joint, and marginal distributions](img/B05137_06_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 1\. Marginal distributions over I and G
  prefs: []
  type: TYPE_NORMAL
- en: Marginal independence and conditional independence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Marginal Independence is defined as follows. Consider two random variables *X*
    and *Y*; then *P(X|Y) = P(X)* means random variable *X* is independent of *Y*.
    It is formally represented as ![Marginal independence and conditional independence](img/B05137_06_042.jpg)
    (*P* satisfies *X* is independent of *Y*).
  prefs: []
  type: TYPE_NORMAL
- en: 'This means the joint distribution can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(X, Y) = P(X)P(Y)*'
  prefs: []
  type: TYPE_NORMAL
- en: If the difficulty level of the exam (*D*) and the intelligence of the student
    (*I*) determine the grade (*G*), we know that the difficulty level of the exam
    is independent of the intelligence of the student and (*D* ⊥ *I*) also implies
    *P(D, I) = P(D)P(I)*.
  prefs: []
  type: TYPE_NORMAL
- en: When two random variables are independent given a third variable, the independence
    is called conditional independence. Given a set of three random variables *X*,
    *Y*, and *Z*, we can say that ![Marginal independence and conditional independence](img/B05137_06_051.jpg);
    that is, variable *X* is independent of *Y* given *Z*. The necessary condition
    for conditional independence is
  prefs: []
  type: TYPE_NORMAL
- en: '![Marginal independence and conditional independence](img/B05137_06_052.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Factors are the basic building blocks for defining the probability distributions
    in high-dimensional (large number of variables) spaces. They give basic operations
    that help in manipulating the probability distributions.
  prefs: []
  type: TYPE_NORMAL
- en: A "factor" is defined as a function that takes as input the random variables
    known as "scope" and gives a real-value output.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, a factor is represented as ![Factors](img/B05137_06_053.jpg) where
    scope is (X[1], *X*[2], ….*X*[k] ).
  prefs: []
  type: TYPE_NORMAL
- en: Factor types
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Different types of factors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Joint distribution**: For every combination of variables, you get a real-valued
    output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unnormalized measure**: When, in a joint distribution, one of the variables
    is constant, the output is also real-valued, but it is unnormalized as it doesn''t
    sum to one. However, it is still a factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional probability distribution**: A probability distribution of the
    form *P(G|I)* is also a factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Various operations are performed on factors, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Factor product**: If two factors *ϕ*[1] (*X*[1], *X*[2]) and *ϕ*[2] (*X*[2],
    *X*[3]) are multiplied, it gives rise to *ϕ*[3] (*X*[1], *X*[2], *X*[3]). In effect,
    it is taking tables corresponding to *ϕ*[1] and multiplying it with *ϕ*[2]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factor marginalization**: This is the same as marginalization, where *ϕ*[1]
    (*X*[1], *X*[2], *X*[3]) can be marginalized over a variable, say *X*[2], to give
    *ϕ*[2] (*X*[1], *X*[3]).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factor reduction**: This is only taking the values of other variables when
    one of the variables is constant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the probability over random variables, many queries can be performed to
    answer certain questions. Some common types of queries are explained in the subsequent
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is one of the most common types of query and it has two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Evidence**: A subset of variables with a well-known outcome or category.
    For example, a random variable **E = e**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query**: A random variable from the rest of the variables. For example, a
    random variable **X**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(**X**|**E** = **e**)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Examples of probabilistic queries are posterior marginal estimations such as
    *P(I = high|L = bad, S = low) = ?* and evidential probability such as *P(L = bad,
    S = low) = ?*.
  prefs: []
  type: TYPE_NORMAL
- en: MAP queries and marginal MAP queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MAP queries are used to find the probability assignment to the subset of variables
    that are most likely and hence are also called **most probable explanation** (**MPE**).
    The difference between these and probabilistic queries is that, instead of getting
    the probability, we get the most likely values for all the variables.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, if we have variables *W= X – E*, where we have *E = e* as the evidence
    and are interested in finding the most likely assignment to the variables in *W*,
  prefs: []
  type: TYPE_NORMAL
- en: '*MAP*(**W**|**e**) = *argmax*[w]*P*(**w**,**e**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A much more general form of marginal query is when we have a subset of variables,
    say given by *Y* that forms our query and with evidence of *E = e*, we are interested
    in finding most likely assignments to the variables in *Y*. Using the MAP definition,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MAP*(**Y**|**e**) = *argmax*[y]*P*(**y**|**e**)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say, *Z= X – Y – E*, then the marginal MAP query is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MAP queries and marginal MAP queries](img/B05137_06_424a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Graph concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we will briefly revisit the concepts from graph theory and some of the
    definitions that we will use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Graph structure and properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A graph is defined as a data structure containing nodes and edges connecting
    these nodes. In the context of this chapter, the random variables are represented
    as nodes, and edges show connections between the random variables.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, if *X = {X*[1]*, X*[2]*,….X*[k]*}* where *X*[1]*, X*[2]*,….X*[k] are
    random variables representing the nodes, then there can either be a directed edge
    belonging to the set e, for example, between the nodes given by ![Graph structure
    and properties](img/B05137_06_079.jpg) or an **undirected edge** ![Graph structure
    and properties](img/B05137_06_079.jpg), and the graph is defined as a data structure
    ![Graph structure and properties](img/B05137_06_081.jpg). A graph is said to be
    a **directed graph** when every edge in the set e between nodes from set **X**
    is directed and similarly an **undirected graph** is one where every edge between
    the nodes is undirected as shown in *Figure 1*. Also, if there is a graph that
    has both directed and undirected edges, the notation of ![Graph structure and
    properties](img/B05137_06_084.jpg) represents an edge that may be directed or
    undirected.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph structure and properties](img/B05137_06_086.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Directed, undirected, and partially-directed graphs
  prefs: []
  type: TYPE_NORMAL
- en: If a directed edge ![Graph structure and properties](img/B05137_06_087.jpg)
    exists in the graph, the node *X*[i] is called the *parent* node and the node
    *X*[j] is called the *child* node.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of an undirected graph, if there is an edge *X*[i] *– X*[j], the
    nodes *X*[i] and *X*[j] are said to be neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: The set of parents of node *X* in a directed graph is called the boundary of
    the node *X* and similarly, adjacent nodes in an undirected graph form each other's
    boundary. The degree of the node *X* is the number of edges it participates in.
    The indegree of the node *X* is the number of edges in the directed graph that
    have a relationship to node *X* such that the edge is between node *Y* and node
    *X* and *X* → *Y*. The degree of the graph is the maximal degree of the node in
    that graph.
  prefs: []
  type: TYPE_NORMAL
- en: Subgraphs and cliques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A subgraph is part of the graph that represents some of the nodes from the entire
    set. A **clique** is a subset of vertices in an undirected graph such that every
    two distinct vertices are adjacent.
  prefs: []
  type: TYPE_NORMAL
- en: Path, trail, and cycles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If there are variables *X*[1], *X*[2], …. *X*[k] in the graph *K = (X, E)*,
    it forms a path if, for every *i* = 1, 2 ... *k* – 1, we have either ![Path, trail,
    and cycles](img/B05137_06_093.jpg) or *X*[i] *–X*[j]; that is, there is either
    a directed edge or an undirected edge between the variables—recall this can be
    depicted as *X*[i] ? *X*[j]. A directed path has at least one directed edge: ![Path,
    trail, and cycles](img/B05137_06_093.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: If there are variables *X*[1], *X*[2], …. *X*[k] in the graph *K = (X, E)* it
    forms a *trail* if for every *i* = 1, 2 ... *k* – 1, we have either ![Path, trail,
    and cycles](img/B05137_06_094.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: A graph is called a **connected** graph, if for every *X*[i], ….*X*[j] there
    is a trail between *X*[i] and *X*[j].
  prefs: []
  type: TYPE_NORMAL
- en: In a graph *K = (X, e)*, if there is a directed path between nodes *X* and *Y*,
    *X* is called the ancestor of *Y* and *Y* is called the *descendant* of *X*.
  prefs: []
  type: TYPE_NORMAL
- en: If a graph *K* has a directed path *X*[1], *X*[2], …. *X*[k] where *X*[1] ?
    *X*[k], the path is called a **cycle**. Conversely, a graph with no cycles is
    called an **acyclic** graph.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generally, all Probabilistic Graphical Models have three basic elements that
    form the important sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Representation**: This answers the question of what does the model mean or
    represent. The idea is how to represent and store the probability distribution
    of *P(X*[1], *X*[2], …. *X*[n]*)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference**: This answers the question: given the model, how do we perform
    queries and get answers. This gives us the ability to infer the values of the
    unknown from the known evidence given the structure of the models. Motivating
    the main discussion points are various forms of inferences involving trade-offs
    between computational and correctness concerns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning**: This answers the question of what model is right given the data.
    Learning is divided into two main parts:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the parameters given the structure and data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the structure with parameters given the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will use the well-known student network as an example of a Bayesian network
    in our discussions to illustrate the concepts and theory. The student network
    has five random variables capturing the relationship between various attributes
    defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficulty of the exam (*D*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intelligence of the student (*I*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grade the student gets (*G*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAT score of the student (*S*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation Letter the student gets based on grade (*L*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these attributes has binary categorical values, for example, the variable
    *Difficulty* (*D*) has two categories (*d*0, *d*1) corresponding to low and high,
    respectively. *Grades* (*G*) has three categorical values corresponding to the
    grades *(A, B, C)*. The arrows as indicated in the section on graphs indicate
    the dependencies encoded from the domain knowledge—for example, *Grade* can be
    determined given that we know the *Difficulty* of the exam and *Intelligence*
    of the student while the *Recommendation Letter* is completely determined if we
    know just the *Grade* (*Figure 2*). It can be further observed that no explicit
    edge between the variables indicates that they are independent of each other—for
    example, the *Difficulty* of the exam and *Intelligence* of the student are independent
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian networks](img/B05137_06_107.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2\. The "Student" network*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A graph compactly represents the complex relationships between random variables,
    allowing fast algorithms to make queries where a full enumeration would be prohibitive.
    In the concepts defined here, we show how directed acyclic graph structures and
    conditional independence make problems involving large numbers of variables tractable.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A Bayesian network is defined as a model of a system with:'
  prefs: []
  type: TYPE_NORMAL
- en: A number of random variables {*X*[1], *X*[2], …. *X*[k]}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Directed** **Acyclic Graph** (**DAG**) with nodes representing random variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A local **conditional probability distribution** (**CPD**) for each node with
    dependence to its parent nodes *P(X*[i] *| parent(X*[i]*))*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A joint probability distribution obtained using the chain rule of distribution
    is a factor given as:![Definition](img/B05137_06_110.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the student network defined, the joint distribution capturing all nodes
    can be represented as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reasoning patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Bayesian networks help in answering various queries given some data and
    facts, and these reasoning patterns are discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Causal or predictive reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If evidence is given as, for example, "low intelligence", then what would be
    the chances of getting a "good letter" as shown in *Figure 3*, in the top right
    quadrant? This is addressed by causal reasoning. As shown in the first quadrant,
    causal reasoning flows from the top down.
  prefs: []
  type: TYPE_NORMAL
- en: Evidential or diagnostic reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If evidence such as a "bad letter" is given, what would be the chances that
    the student got a "good grade"? This question, as shown in *Figure 3* in the top
    left quadrant, is addressed by evidential reasoning. As shown in the second quadrant,
    evidential reasoning flows from the bottom up.
  prefs: []
  type: TYPE_NORMAL
- en: Intercausal reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Obtaining interesting patterns from finding a "related cause" is the objective
    of intercausal reasoning. If evidence of "grade C" and "high intelligence" is
    given, then what would be the chance of course difficulty being "high"? This type
    of reasoning is also called "explaining away" as one cause explains the reason
    for another cause and this is illustrated in the third quadrant, in the bottom-left
    of *Figure 3*.
  prefs: []
  type: TYPE_NORMAL
- en: Combined reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If a student takes an "easy" course and has a "bad letter", what would be the
    chances of him getting a "grade C" ? This is explained by queries with combined
    reasoning patterns. Note that it has mixed information and does not a flow in
    a single fixed direction as in the case of other reasoning patterns and is shown
    in the bottom-right of the figure, in quadrant 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Combined reasoning](img/B05137_06_112.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3\. Reasoning patterns*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Independencies, flow of influence, D-Separation, I-Map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conditional independencies between the nodes can be exploited to reduce
    the computations when performing queries. In this section, we will discuss some
    of the important concepts that are associated with independencies.
  prefs: []
  type: TYPE_NORMAL
- en: Flow of influence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Influence** is the effect of how the condition or outcome of one variable
    changes the value or the belief associated with another variable. We have seen
    this from the reasoning patterns that influence flows from variables in direct
    relationships (parent/child), causal/evidential (parent and child with intermediates)
    and in combined structures.'
  prefs: []
  type: TYPE_NORMAL
- en: The only case where the influence doesn't flow is when there is a "v-structure".
    That is, given edges between three variables ![Flow of influence](img/B05137_06_113.jpg)
    there is a v-structure and no influence flows between *X*[i - 1] and *X*[i + 1].
    For example, no influence flows between the Difficulty of the course and the Intelligence
    of the student.
  prefs: []
  type: TYPE_NORMAL
- en: D-Separation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Random variables *X* and *Y* are said to be d-separated in the graph **G**,
    given there is no active trail between **X** and **Y** in **G** given **Z**. It
    is formally denoted by:'
  prefs: []
  type: TYPE_NORMAL
- en: '*dsep*[G] *(X,Y|Z)*'
  prefs: []
  type: TYPE_NORMAL
- en: The point of d-separation is that it maps perfectly to the conditional independence
    between the points. This gives to an interesting property that in a Bayesian network
    any variable is independent of its non-descendants given the parents of the node.
  prefs: []
  type: TYPE_NORMAL
- en: In the Student network example, the node/variable Letter is d-separated from
    Difficulty, Intelligence, and SAT given the grade.
  prefs: []
  type: TYPE_NORMAL
- en: I-Map
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the d-separation, in graph **G**, we can collect all the independencies
    from the d-separations and these independencies are formally represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![I-Map](img/B05137_06_117.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If *P* satisfies *I*(**G**) then we say the **G** is an independency-map or
    I-Map of *P*.
  prefs: []
  type: TYPE_NORMAL
- en: The main point of I-Map is it can be formally proven that a factorization relationship
    to the independency holds. The converse can also be proved.
  prefs: []
  type: TYPE_NORMAL
- en: In simple terms, one can read in the Bayesian network graph G, all the independencies
    that hold in the distribution P regardless of any parameters!
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the student network—its whole distribution can be shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(D,I,G,S,L) = P(D)P(I|D)P(G¦D,I)P(S¦D,I,G)P(L¦D,I,G,S)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, consider the independence from I-Maps:'
  prefs: []
  type: TYPE_NORMAL
- en: Variables *I* and *D* are non-descendants and not conditional on parents so
    *P(I|D) = P(I)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable *S* is independent of its non-descendants *D* and *G*, given its parent
    *I*. *P(S¦D,I,G)=P(S|I)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable *L* is independent of its non-descendants *D*, *I*, and *S*, given
    its parent *G*. *P(L¦D,I,G,S)=P(L|G)*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(D,I,G,S,L)=P(D)P(I)P(G¦D,I)P(S¦I)P(L¦G)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, we have shown that I-Map helps in factorization given just the graph network!
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The biggest advantage of probabilistic graph models is their ability to answer
    probability queries in the form of conditional or MAP or marginal MAP, given some
    evidence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the probability of evidence **E = e** is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference](img/B05137_06_126.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But the problem has been shown to be NP-Hard (*Reference* [3]) or specifically,
    #P-complete. This means that it is intractable when there are a large number of
    trees or variables. Even for a tree-width (number of variables in the largest
    clique) of 25, the problem seems to be intractable—most real-world models have
    tree-widths larger than this.'
  prefs: []
  type: TYPE_NORMAL
- en: So if the exact inference discussed before is intractable, can some approximations
    be used so that within some bounds of the error, we can make the problem tractable?
    It has been shown that even an approximate algorithm to compute inferences with
    an error *?* < 0.5, so that we find a number *p* such that |*P*(**E** = **e**)
    – *p*|< *?*, is also NP-Hard.
  prefs: []
  type: TYPE_NORMAL
- en: But the good news is that this is among the "worst case" results that show exponential
    time complexity. In the "general case" there can be heuristics applied to reduce
    the computation time both for exact and approximate algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the well-known techniques for performing exact and approximate inferencing
    are depicted in *Figure 4*, which covers most probabilistic graph models in addition
    to Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference](img/B05137_06_130.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Exact and approximate inference techniques
  prefs: []
  type: TYPE_NORMAL
- en: It is beyond the scope of this chapter to discuss each of these in detail. We
    will explain a few of the algorithms in some detail accompanied by references
    to give the reader a better understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Elimination-based inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we will describe two techniques, the variable elimination algorithm and
    the clique-tree or junction-tree algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Variable elimination algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The basics of the **Variable elimination** (**VE**) algorithm lie in the distributive
    property as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(ab+ac+ad)= a (b+c+d)*'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, five arithmetic operations of three multiplications and two
    additions can be reduced to four arithmetic operations of one multiplication and
    three additions by taking a common factor *a* out.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand the reduction of the computations by taking a simple example
    in the student network. If we have to compute a probability query such as the
    difficulty of the exam given the letter was good, that is, *P(D¦L=good)=?*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Bayes theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable elimination algorithm](img/B05137_06_134.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To compute *P(D¦L=good)=?* we can use the chain rule and joint probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable elimination algorithm](img/B05137_06_136.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we rearrange the terms on the right-hand side:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable elimination algorithm](img/B05137_06_137.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If we now replace ![Variable elimination algorithm](img/B05137_06_138.jpg)
    since the factor is independent of the variable *I* that *S* is conditioned on,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variable elimination algorithm](img/B05137_06_140.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, if we proceed carefully, eliminating one variable at a time, we have effectively
    converted *O(2*^n*)* factors to *O(nk*²*)* factors where *n* is the number of
    variables and *k* is the number of observed values for each.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the main idea of the VE algorithm is to impose an order on the variables
    such that the query variable comes last. A list of factors is maintained over
    the ordered list of variables and summation is performed. Generally, we use dynamic
    programming in the implementation of the VE algorithm (*References* [4]).
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: List of Condition Probability Distribution/Table **F**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of query variables **Q**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of observed variables **E** and the observed value **e**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(**Q**|**E** = *e*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The algorithm calls the `eliminate` function in a loop, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*VariableElimination*:'
  prefs: []
  type: TYPE_NORMAL
- en: While *?*, the set of all random variables in the Bayesian network is not empty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove the first variable **Z** from *?*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*eliminate*(*F*, **Z**)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: end loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *?* product of all factors in *F*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate observed variables in *?* to their observed values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: return ![How does it work?](img/B05137_06_155.jpg) (renormalization)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*eliminate* (*F*, **Z**)'
  prefs: []
  type: TYPE_NORMAL
- en: Remove from the *F* all functions, for example, *X*[1], *X*[2], …. *X*[k] that
    involve **Z**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute new function ![How does it work?](img/B05137_06_429.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute new function ![How does it work?](img/B05137_06_425.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add new function *?* to *F*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return *F*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider the same example of the student network with *P(D, L = good)* as the
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pick a variable ordering list: *S*, *I*, *L*, *G*, and *D*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize the active factor list and introduce the evidence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List: *P(S¦I)P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Eliminate the variable SAT or **S** off the list![How does it work?](img/B05137_06_161.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List: *P(I)P(D)P(G¦I,D)P(L¦G)d(L = good)* *?*1 *(I)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Eliminate the variable Intelligence or *I*![How does it work?](img/B05137_06_163.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List: *P(D)P(L¦G)d(L = good)* *?*2 *(G,D)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Eliminate the variable Letter or *L*![How does it work?](img/B05137_06_166.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List: *P(D)* *?*[3] *(G)* *?*[2] *(G,D)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Eliminate the variable Grade or *G*![How does it work?](img/B05137_06_169.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'List: *P(D)* *?*[4] *(D)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus with two values, *P(D=high)* *?*[4] *(D=high)* and *P(D=low)* *?*[4] *(D=low)*,
    we get the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of the VE algorithm is its simplicity and generality that
    can be applied to many networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The computational reduction advantage of VE seems to go away when there are
    many connections in the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The choice of optimal ordering of variables is very important for the computational
    benefit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clique tree or junction tree algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Junction tree or Clique Trees are more efficient forms of variable elimination-based
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: List of Condition Probability Distribution/Table **F**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of query variables **Q**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of observed variables **E** and the observed value **e**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(**Q|**E = *e*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The steps involved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Moralization**: This is a process of converting a directed graph into an
    undirected graph with the following two steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace directed edges with undirected edges between the nodes.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If there are two nodes or vertices that are not connected but have a common
    child, add an edge connecting them. (Note the edge between *V*[4] and *V*[5] and
    *V*[2] and *V*[3] in *Figure* 5):'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_06_178.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 5\. Graph moralization of DAG showing in green how the directional edges
    are changed and red edges showing new additions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Triangulation**: For understanding triangulation, chords must be formed.
    The chord of a cycle is a pair of vertices *V*[i] and *V*[j] of non-consecutive
    vertices that have an edge between them. A graph is called a **chordal or triangulated
    graph** if every cycle of length ≥ 4 has chords. Note the edge between *V*[1]
    and *V*[5] in *Figure 6* forming chords to make the moralized graph a chordal/triangulated
    graph:![How does it work?](img/B05137_06_184.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 6\. Graph triangulation with blue edge addition to convert a moralized
    graph to a chordal graph.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Junction Tree**: From the chordal graphs a junction tree is formed using
    the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the cliques in the graph and make them nodes with the cluster of all
    vertices. A clique is a subgraph where an edge exists between each pair of nodes.
    If two nodes have one or more common vertices create an edge consisting of the
    intersecting vertices as a separator or sepset. For example, the cycle with edges
    *V*[1], *V*[4], *V*[5] and *V*[6], *V*[4], *V*[5] that have a common edge between
    *V*[4], *V*[5] can be reduced to a clique as shown with the common edge as separator.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the preceding graph contains a cycle, all separators in the cycle contain
    the same variable. Remove the cycle in the graph by creating a minimum spanning
    tree, while including maximum separators. The entire transformation process is
    shown in Figure 7:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_06_189.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 7\. Formation of a Junction Tree
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Run the Message Passing algorithm on Junction Tree**: Junction tree can be
    used to compute the joint distribution using factorization of cliques and separators
    as![How does it work?](img/B05137_06_190.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the parameters of Junction Tree**: The junction tree parameters can
    be obtained per node using the parent nodes in the original Bayesian network and
    are called clique potentials, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (*?*[1] (*V*[2],*V*[3],*V*[5]) = *P*(*V*[5] |*V*[2],*V*[3])*P*(*V*[3])(Note
    in the original Bayesian network edge *V*[5] is dependent on *V*[2], *V*[3], whereas
    *V*[3] is independent)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_06_194.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: '![How does it work?](img/B05137_06_196.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: '![How does it work?](img/B05137_06_197.jpg)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
- en: '**Message Passing between nodes/cliques in Junction Tree**: A node in the junction
    tree, represented by clique *C*[i], multiplies all incoming messages from its
    neighbors with its own clique potential, resulting in a factor ![How does it work?](img/B05137_06_199.jpg)
    whose scope is the clique. It then sums out all the variables except the ones
    on sepset or separators *S*[i,j] between *C*[i] and *C*[j] and then sends the
    resulting factor as a message to *C*[j].![How does it work?](img/B05137_06_202.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 8\. Message passing between nodes/cliques in Junction Tree
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_06_203.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Thus, when the message passing reaches the tree root, the joint probability
    distribution is completed.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitaions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm has a theoretical upper bound on the computations that are related
    to the tree width in the junction tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplication of each potential in the cliques can result in numeric overflow
    and underflow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Propagation-based techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we discuss belief propagation, a commonly used message passing algorithm
    for doing inference by introducing factor graphs and the messages that can flow
    in these graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Belief propagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Belief propagation is one of the most practical inference techniques that has
    applicability across most probabilistic graph models including directed, undirected,
    chain-based, and temporal graphs. To understand the belief propagation algorithm,
    we need to first define factor graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Factor graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We know from basic probability theory that the entire joint distribution can
    be represented as a factor over a subset of variables as
  prefs: []
  type: TYPE_NORMAL
- en: '![Factor graph](img/B05137_06_205.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In DAG or Bayesian networks *f*[s](**X**[s]) is a conditional distribution.
    Thus, there is a great advantage in expressing the joint distribution over factors
    over the subset of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Factor graph is a representation of the network where the variables and the
    factors involving the variables are both made into explicit nodes (*References*
    [11]). In a simplified student network from the previous section, the factor graph
    is shown in *Figure 9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Factor graph](img/B05137_06_208.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9\. Factor graph of simplified "Student" network
  prefs: []
  type: TYPE_NORMAL
- en: A factor graph is a bipartite graph, that is, it has two types of nodes, variables
    and factors.
  prefs: []
  type: TYPE_NORMAL
- en: The edges flow between two opposite types, that is, from variables to factors
    and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the Bayesian network to a factor graph is a straightforward procedure
    as shown previously where you start adding variable nodes and conditional probability
    distributions as factor nodes. The relationship between the Bayesian network and
    factor graphs is one-to-many, that is, the same Bayesian network can be represented
    in many factor graphs and is not unique.
  prefs: []
  type: TYPE_NORMAL
- en: Messaging in factor graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are two distinct messages that flow in these factor graphs that form the
    bulk of all computations through communication.
  prefs: []
  type: TYPE_NORMAL
- en: '**Message from factor nodes to variable nodes**: The message that is sent from
    a factor node to the variable node can be mathematically represented as follows:![Messaging
    in factor graph](img/B05137_06_209.jpg)![Messaging in factor graph](img/B05137_06_426.jpg)![Messaging
    in factor graph](img/B05137_06_213.jpg) where ![Messaging in factor graph](img/B05137_06_214.jpg)
    Thus, ![Messaging in factor graph](img/B05137_06_215.jpg) is the message from
    factor node *f*[s] to *x* and the product of all such messages from neighbors
    of *x* to *x* gives the combined probability to *x:*![Messaging in factor graph](img/B05137_06_218.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figure 10\. Message-passing from factor node to variable node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Message from variable nodes to factor nodes**: Similar to the previous example,
    messages from variable to factor can be shown to be![Messaging in factor graph](img/B05137_06_219.jpg)![Messaging
    in factor graph](img/B05137_06_220.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, all the factors coming to the node *x*[m] are multiplied except for the
    factor it is sending to.
  prefs: []
  type: TYPE_NORMAL
- en: '![Messaging in factor graph](img/B05137_06_222.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. Message-passing from variable node to factor node
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: List of Condition Probability Distribution/Table (CPD/CPT) *F*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of query variables **Q**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of observed variables **E** and the observed value **e**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(**Q|**E = *e*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Create a factor graph from the Bayesian network as discussed previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View the node **Q** as the root of the graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Initialize all the leaf nodes, that is: ![How does it work?](img/B05137_06_223.jpg)
    and ![How does it work?](img/B05137_06_224.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply message passing from a leaf to the next node in a recursive manner.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move to the next node, until root is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Marginal at the root node gives the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitaions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This algorithm as discussed is very generic and can be used for most graph models.
    This algorithm gives exact inference in directed trees when there are no cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be easily implemented in parallel and helps in scalability. Based on
    connectivity, the memory requirement can be very high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling-based techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will discuss a simple approach using particles and sampling to illustrate
    the process of generating the distribution *P(X)* from the random variables. The
    idea is to repeatedly sample from the Bayesian network and use the samples with
    counts to approximate the inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Forward sampling with rejection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The key idea is to generate i.i.d. samples iterating over the variables using
    a topological order. In case of some evidence, for example, *P(X|E = e)* that
    contradicts the sample generated, the easiest way is to reject the sample and
    proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: List of Condition Probability Distribution/Table *F*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of query variables **Q**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List of observed variables **E** and the observed value **e**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(**Q|**E = *e*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For *j* = 1 to *m* //number of samples
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a topological order of variables, say **X**[1], **X**[2], **… X**[n].
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For *i* = 1 to *n*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**u**[i] ? **X***(parent*(**X**[i])) //assign *parent*(**X**[i]) to variables'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*sample*(**x**[i], *P*(**X**[i] | **u**[i]) //sample **X**[i] given parent
    assignments'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: if(**x**[i] ?, *P*(**X**[i] | **E** = **e**) reject and go to 1.1.2\. //reject
    sample if it doesn't agree with the evidence.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return (**X**[1], **X**[2],….**X**[n]) as sample.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *P*(**Q** | **E** = e) using counts from the samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example of one sample generated for the student network can be by sampling
    Difficulty and getting Low, next, sampling Intelligence and getting High, next,
    sampling grade using the CPD table for Difficulty=low and Intelligence=High and
    getting Grade=A, sampling SAT using CPD for Intelligence=High and getting SAT=good
    and finally, using Grade=A to sample from Letter and getting Letter=Good. Thus,
    we get first sample (Difficulty=low, Intelligence=High, Grade=A, SAT=good, Letter=Good)
  prefs: []
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The advantages and limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This technique is fairly simple to implement and execute. It requires a large
    number of samples to be approximate within the bounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When evidence set is large, the rejection process becomes costly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea behind learning is to generate either a structure or find parameters
    or both, given the data and the domain experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goals of learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate inference in Bayesian networks. The pre-requisite of inferencing
    is that the structure and parameters are known, which are the output of learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To facilitate prediction using Bayesian networks. Given observed variables **X**,
    predict the target variables **Y**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To facilitate knowledge discovery using Bayesian networks. This means understanding
    causality, relationships, and other features from the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning, in general, can be characterized by *Figure 12*. The assumption is
    that there is a known probability distribution *P^** that may or may not have
    been generated from a Bayesian network *G^**. The observed data samples are assumed
    to be generated or sampled from that known probability distribution *P^**. The
    domain expert may or may not be present to include the knowledge or prior beliefs
    about the structure. Bayesian networks are one of the few techniques where domain
    experts' inputs in terms of relationships in variables or prior probabilities
    can be used directly, in contrast to other machine learning algorithms. At the
    end of the process of knowledge elicitation and learning from data, we get as
    an output a Bayesian network with defined structure and parameters (CPTs).
  prefs: []
  type: TYPE_NORMAL
- en: '![Learning](img/B05137_06_237.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. Elements of learning with Bayesian networks
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on data quality (missing data or complete data) and knowledge of structure
    from the expert (unknown and known), the following are four classes that Learning
    in Bayesian networks fall into, as shown in *Table 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Data | Structure |'
  prefs: []
  type: TYPE_TB
- en: '| Known Structure(Learn Parameters) | Unknown Structure(Learn Structure and
    Parameters) |'
  prefs: []
  type: TYPE_TB
- en: '| Complete Data | Parameter Estimation(Maximum Likelihood, Bayesian Estimation)
    | Optimization(Search and Scoring Techniques) |'
  prefs: []
  type: TYPE_TB
- en: '| Incomplete Data | Non-Linear Parametric Optimization(Expectation Maximization,
    Gradient Descent) | Structure and Parameter Optimization(Structural EM, Mixture
    Models) |'
  prefs: []
  type: TYPE_TB
- en: '*Table 2\. Classes of Bayesian network learning*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learning parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will discuss two broadly used methodologies to estimate
    parameters given the structure. We will discuss only with the complete data and
    readers can refer to the discussion in (*References* [8]) for incomplete data
    parameter estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation for Bayesian networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Maximum** **likelihood estimation** (**MLE**) is a very generic method and
    it can be defined as: given a data set *D*, choose parameters ![Maximum likelihood
    estimation for Bayesian networks](img/B05137_06_239.jpg) that satisfy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_240.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_241.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_242.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: Maximum likelihood is the technique of choosing parameters of the Bayesian network
    given the training data. For a detailed discussion, see (*References* [6]).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the known Bayesian network structure of graph *G* and training data ![Maximum
    likelihood estimation for Bayesian networks](img/B05137_06_243.jpg), we want to
    learn the parameters or CPDs—or CPTs to be precise. This can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_244.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now each example or instance ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_245.jpg)
    can be written in terms of variables. If there are *i* variables represented by
    *x*[i] and the parents of each is given by *parent*[Xi] then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_249.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Interchanging the variables and instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_250.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The term is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_251.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the conditional likelihood of a particular variable *x* *[i]* given
    its parents *parent*[Xi]. Thus, parameters for these conditional likelihoods are
    a subset of parameters given by ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_252.jpg).
    Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_253.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Maximum likelihood estimation for Bayesian networks](img/B05137_06_254.jpg)
    is called the local likelihood function. This becomes very important as the total
    likelihood decomposes into independent terms of local likelihood and is known
    as the global decomposition property of the likelihood function. The idea is that
    these local likelihood functions can be further decomposed for a tabular CPD by
    simply using the count of different outcomes from the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *N*[ijk] be the number of times we observe variable or node *i* in the
    state *k*[,] given the parent node configuration *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_257.jpg)![Maximum
    likelihood estimation for Bayesian networks](img/B05137_06_258.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, we can have a simple entry corresponding to *X*[i] *= a* and *parent*[Xi]
    *= b* by estimating the likelihood function from the training data as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Maximum likelihood estimation for Bayesian networks](img/B05137_06_261.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Consider two cases, as an example. In the first, ![Maximum likelihood estimation
    for Bayesian networks](img/B05137_06_262.jpg) is satisfied by 10 instances with
    *parent*[Xi] *= b* =100\. In the second, ![Maximum likelihood estimation for Bayesian
    networks](img/B05137_06_262.jpg) is satisfied by 100 when *parent*[Xi] *= b* =1000\.
    Notice both probabilities come to the same value, whereas the second has 10 times
    more data and is the "more likely" estimate! Similarly, familiarity with domain
    or prior knowledge, or lack of it due to uncertainty, is not captured by MLE.
    Thus, when the number of samples are limited or when the domain experts are aware
    of the priors, then this method suffers from serious issues.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian parameter estimation for Bayesian network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This technique overcomes the issue of MLE by encoding prior knowledge about
    the parameter *?* with a probability distribution. Thus, we can encode our beliefs
    or prior knowledge about the parameter space as a probability distribution and
    then the joint distribution of variables and parameters are used in estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider single variable parameter learning where we have instances *x*[1],
    *x*[2] … *x*[M] and they all have parameter **?**[X].
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_267.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Single variable parameter learning
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_268.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_269.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the network is a joint probability model over parameters and data. The
    advantage is we can use it for the posterior distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_270.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_271.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ', *P(?) = prior*,'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_273.jpg)
    Thus, the difference between the maximum likelihood and Bayesian estimation is
    the use of the priors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generalizing it to a Bayesian network *G* given the dataset *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_274.jpg)![Bayesian
    parameter estimation for Bayesian network](img/B05137_06_275.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If we assume global independence of parameters
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_276.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![Bayesian parameter estimation for Bayesian network](img/B05137_06_277.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, as before, subset **?**[Xi] | *parent*[Xi] of **?** is local and thus
    the entire posterior can be computed in local terms!
  prefs: []
  type: TYPE_NORMAL
- en: Prior and posterior using the Dirichlet distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Often, in practice, a continuous probability distribution known as Dirichlet
    distribution—which is a Beta distribution—is used to represent priors over the
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_279.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Probability Density Function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_280.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Prior and posterior using the Dirichlet distribution](img/B05137_06_281.jpg),
    ![Prior and posterior using the Dirichlet distribution](img/B05137_06_282.jpg)
    The alpha terms are known as hyperparameters and *a*[ijri] > 0\. The ![Prior and
    posterior using the Dirichlet distribution](img/B05137_06_284.jpg) is the pseudo
    count, also known as equivalent sample size and it gives us a measure of the prior.
  prefs: []
  type: TYPE_NORMAL
- en: The Beta function, *B(a*[ij]*)* is normally expressed in terms of gamma function
    as follows
  prefs: []
  type: TYPE_NORMAL
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_286.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The advantage of using Dirichlet distribution is it is conjugate in nature,
    that is, irrespective of the likelihood, the posterior is also a Dirichlet if
    the prior is Dirichlet!
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown that the posterior distribution for the parameters *?*[ijk]
    is a Dirichlet with updated hyperparameters and has a closed form solution!
  prefs: []
  type: TYPE_NORMAL
- en: '*a*[ijk] = *a*[ijk] + *N*[ijk]'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use maximum a posteriori estimate and posterior means they can be shown
    to be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Prior and posterior using the Dirichlet distribution](img/B05137_06_289.jpg)![Prior
    and posterior using the Dirichlet distribution](img/B05137_06_290.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Learning structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning Bayesian network without any domain knowledge or understanding of structures
    includes learning the structure and the parameters. We will first discuss some
    measures that are used for evaluating the network structures and then discuss
    a few well-known algorithms for building optimal structures.
  prefs: []
  type: TYPE_NORMAL
- en: Measures to evaluate structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The measures used to evaluate a Bayes network structure, given the dataset,
    can be broadly divided into the following categories and details of many are available
    here (*References* [14]).
  prefs: []
  type: TYPE_NORMAL
- en: '**Deviance-Threshold Measure**: The two common techniques to measure deviance
    between two variables used in the network and structure are Pearson''s chi-squared
    statistic and the Kullback-Leibler distance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the dataset *D* of *M* samples, consider two variables *X*[i] and *X*[j],
    the Pearson's chi-squared statistic measuring divergence is
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Measures to evaluate structures](img/B05137_06_292.jpg)![Measures to evaluate
    structures](img/B05137_06_424.jpg)![Measures to evaluate structures](img/B05137_06_294.jpg)![Measures
    to evaluate structures](img/B05137_06_295.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*d*[?2]*(D)* is 0; when the variables are independent and larger values indicate
    there is dependency between the variables.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Kullback-Leibler divergence is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Measures to evaluate structures](img/B05137_06_297.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*d*[I]*(D)* is again 0, it shows independence and the larger values indicates
    dependency. Using various statistical hypothesis tests, a threshold can be used
    to determine the significance.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Structure Score Measure**: There are various measures to give scores to a
    structure in a Bayes network. We will discuss the most commonly used measures
    here. A log-likelihood score discussed in parameter learning can be used as a
    score for the structure:![Measures to evaluate structures](img/B05137_06_299.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian information score** (**BIC**) is also quite a popular scoring technique
    as it avoids overfitting by taking into consideration the penalty for complex
    structures, as shown in the following equation![Measures to evaluate structures](img/B05137_06_300.jpg)![Measures
    to evaluate structures](img/B05137_06_301.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The penalty function is logarithmic in *M*, so, as it increases, the penalty
    is less severe for complex structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Akaike information score (AIC), similar to BIC, has similar penalty based
    scoring and is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measures to evaluate structures](img/B05137_06_302.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian scores discussed in parameter learning are also employed as scoring
    measures.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for learning structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will discuss a few algorithms that are used for learning structures in this
    section; details can be found here (*References* [15]).
  prefs: []
  type: TYPE_NORMAL
- en: Constraint-based techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Constraint-based algorithms use independence tests of various variables, trying
    to find different structural dependencies that we discussed in previous sections
    such as the d-separation, v-structure, and so on, by following the step-by-step
    process discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input is the dataset *D* with all the variables *{X,Y..}* known for every
    instance {1,2, ... *m*}, and no missing values. The output is a Bayesian network
    graph *G* with all edges, directions known in **E** and the CPT table.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Create an empty set of undirected edge **E**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test for conditional independence between two variables independent of directions
    to have an edge.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If for all subset **S** = *U* – {*X, Y*}, if *X* is independent of *Y*, then
    add it to the set of undirected edge **E***'*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all potential undirected edges are identified, directionality of the edge
    is inferred from the set **E***'*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Considering a triplet *{X, Y, Z}*, if there is an edge *X – Z* and *Y – Z*,
    but no edge between *X – Y* using all variables in the set, and further, if *X*
    is not independent of *Y* given all the edges **S** = *U* – {*X, Y, Z*}, this
    implies the direction of ![How does it work?](img/B05137_06_312.jpg) and ![How
    does it work?](img/B05137_06_313.jpg).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the edges ![How does it work?](img/B05137_06_312.jpg) and ![How does it
    work?](img/B05137_06_313.jpg) to set **E**.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the CPT table using local calculations.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the Bayes network *G*, edges **E**, and the CPT tables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Lack of robustness is one of the biggest drawbacks of this method. A small error
    in data can cause a big impact on the structure due to the assumptions of independence
    that will creep into the individual independence tests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability and computation time is a major concern as every subset of variables
    are tested and is approximately 2^n. As the number of variables increase to the
    100s, this method fails due to computation time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search and score-based techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The search and score method can be seen as a heuristic optimization method where
    iteratively, structure is changed through small perturbations, and measures such
    as BIC or MLE are used to give score to the structures to find the optimal score
    and structure. Hill climbing, depth-first search, genetic algorithms, and so on,
    have all been used to search and score.
  prefs: []
  type: TYPE_NORMAL
- en: Inputs and outputs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Input is dataset *D* with all the variables *{X,Y..}* known for every instance
    {1,2, ... *m*} and no missing values. The output is a Bayesian network graph *G*
    with all edges and directions known in **E**.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![How does it work?](img/B05137_06_316.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14\. Search and Score
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the Graph *G*, either based on domain knowledge or empty or full.
    Initialize the Edge set **E** based on the graph and initialize the CPT tables
    *T* based on the graph *G*, **E**, and the data *D*. Normally terminating conditions
    are also mentioned such as *maxIterations*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*maxScore= -8, score=computeScore(G,***E***, T)*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*maxScore=score*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each variable pair *(X, Y)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each ![How does it work?](img/B05137_06_324.jpg)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: New Graph G' based on parents and variables with edge changes.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute new CPTs *T' ? computeCPT(G',E',D)*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*currentScore = computeScore(G'',***E***'',T'')*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *currentScore > score*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*score = currentScore*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*G'' = G*, **E***''* = **E**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat 3 while (![How does it work?](img/B05137_06_332.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Getting stuck in a local optimum, which is the drawback of most of these heuristic
    search methods, is one of the biggest disadvantages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convergence or theoretical guarantees are not available in heuristic search,
    so searching for termination is very much by guess work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov networks and conditional random fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have covered directed acyclic graphs in the area of probabilistic
    graph models, including every aspect of representation, inference, and learning.
    When the graphs are undirected, they are known as **Markov networks** (**MN**)
    or **Markov random** **field** (**MRF**). We will discuss some aspects of Markov
    networks in this section covering areas of representation, inference, and learning,
    as before. Markov networks or MRF are very popular in various areas of computer
    vision such as segmentation, de-noising, stereo, recognition, and so on. For further
    reading, see (*References* [10]).
  prefs: []
  type: TYPE_NORMAL
- en: Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though a Markov network, like Bayesian networks, has undirected edges,
    it still has local interactions and distributions. We will first discuss the concept
    of parameterization, which is a way to capture these interactions, and then the
    independencies in MN.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The affinities between the variables in MN are captured through three alternative
    parameterization techniques discussed in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Gibbs parameterization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The probability distribution function is said to be in Gibb's distribution or
    parameterized by Gibb's distribution if
  prefs: []
  type: TYPE_NORMAL
- en: '![Gibbs parameterization](img/B05137_06_333.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Z* is called the partitioning function defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Gibbs parameterization](img/B05137_06_334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that interaction between variables are captured by factors ![Gibbs parameterization](img/B05137_06_335.jpg)
    and are not the marginal probabilities, but contribute to the joint probability.
    The factors that parameterize a Markov network are called clique potentials. By
    choosing factors over maximal cliques in the graph, the number of parameters are
    reduced substantially.
  prefs: []
  type: TYPE_NORMAL
- en: Factor graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph structure of Markov network does not reveal properties such as whether
    the factors involve maximal cliques or their subsets when using Gibbs parameterization.
    Factor graphs discussed in the section of inferencing in Bayesian networks have
    a step to recognize maximal cliques and thus can capture these parameterizations.
    Please refer to the section on factor graphs in BN.
  prefs: []
  type: TYPE_NORMAL
- en: Log-linear models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another form of parameterization is to use the energy model representation from
    statistical physics.
  prefs: []
  type: TYPE_NORMAL
- en: The potential is represented as a set of features and a potential table is generally
    represented by features with weights associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *D* is a set of variables, ![Log-linear models](img/B05137_06_337.jpg) is
    a factor then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Log-linear models](img/B05137_06_338.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, as the energy increases, the probability decreases and vice versa. The
    logarithmic cell frequencies captured in ![Log-linear models](img/B05137_06_337.jpg)
    are known as log-linear in statistical physics. The joint probability can be represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Log-linear models](img/B05137_06_339.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Log-linear models](img/B05137_06_340.jpg) is the feature function defined
    over the variables in **D**[i].'
  prefs: []
  type: TYPE_NORMAL
- en: Independencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like Bayesian networks, Markov networks also encode a set of independence assumptions
    governing the flow of influence in undirected graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Global
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A set of nodes **Z** separates sets of nodes **X** and **Y**, if there is no
    active path between any node in *X* ? **X** and *Y* ? **Y** given **Z**. Independence
    in graph *G* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Global](img/B05137_06_345.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Pairwise Markov
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two nodes, *X* and *Y*, are independent given all other nodes if there is no
    direct edge between them. This property is of local independence and is weakest
    of all:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pairwise Markov](img/B05137_06_346.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Markov blanket
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'A node is independent of all other nodes in the graph, given its Markov blanket,
    which is an important concept in Markov networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov blanket](img/B05137_06_347.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here **U** *= markov blanket of X*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 15* shows a Markov blanket for variable *X* as its parents, children,
    and children''s parents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov blanket](img/B05137_06_349.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15\. Markov blanket for Node X - its Parents, Children, and Children's
    Parents.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inference in MNs is similarly #P-complete problem and hence similar approximations
    or heuristics get applied. Most exact and approximate inferencing techniques,
    such as variable elimination method, junction tree method, belief propagation
    method, and so on, which were discussed in Bayes network, are directly applicable
    to Markov networks. The marginals and conditionals remain similar and computed
    over the potential functions over the cliques as'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inference](img/B05137_06_350.jpg)![Inference](img/B05137_06_351.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Markov blankets simplify some of the computations.
  prefs: []
  type: TYPE_NORMAL
- en: Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning the parameters in Markov networks is complex and computationally expensive
    due to the entanglement of all the parameters in the partitioning function. The
    advantageous step of decomposing the computations into local distributions cannot
    be done because of the partitioning function needing the factor coupling of all
    the variables in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum likelihood estimation in MN does not have a closed–form solution and
    hence incremental techniques such as gradient descent are used for optimizing
    over the entire parameter space. The optimization function can be shown to be
    a concave function, thus ensuring a global optimum, but each step of the iterations
    in gradient descent requires inferencing over the entire network, making it computationally
    expensive and sometimes intractable.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian parameter estimation requires integration over the entire space of
    parameters, which again has no closed-form solution and is even harder. Thus,
    most often, approximate learning methods such as **Markov Chain Monte Carlo**
    (**MCMC**) are used for MNs.
  prefs: []
  type: TYPE_NORMAL
- en: Structure learning in the MNs is similar or even harder than parameter learning
    and has been shown to be NP-hard. In the constraint-based approach, for a given
    dataset, conditional independence between the variables is tested. In MNs, each
    pair of variables is tested for conditional independence using mutual information
    between the pair. Then, based on a threshold, an edge is either considered to
    be existing between the pair or not. One disadvantage of this is it requires extremely
    large numbers of samples to refute any noise present in the data. Complexity of
    the network due to occurrence of pairwise edges is another limitation.
  prefs: []
  type: TYPE_NORMAL
- en: In search and score-based learning, the goal is similar to BNs, where search
    is done for structures and scoring—based on various techniques—is computed to
    help and adjust the search. In the case of MNs, we use features described in the
    log-linear models rather than the potentials. The weighting of the features is
    considered during optimization and scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional random fields
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Conditional** **random fields** (**CRFs**) are a specialized form of Markov
    network where the hidden and observables are mostly modeled for labeled sequence
    prediction problems (*References* [16]). Sequence prediction problems manifest
    in many text mining areas such as next word/letter predictions, **Part of speech**
    (**POS**) tagging, and so on, and in bioinformatics domain for DNA or protein
    sequence predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind CRFs is the conditional distribution of sequence is modeled
    as feature functions and the labeled data is used to learn using optimization
    the empirical distribution, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: The conditional distribution is expressed as follows where *Z*(**x**) is the
    normalizing constant. Maximum likelihood is used for parameter estimation for
    *?* and is generally a convex function in log-linear obtained through iterative
    optimization methods such as gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: '![Conditional random fields](img/B05137_06_354.jpg)![Conditional random fields](img/B05137_06_355.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Conditional random fields mapped to the area of sequence prediction
    in the POS tagging domain.'
  prefs: []
  type: TYPE_NORMAL
- en: Specialized networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover some basic specialized probabilistic graph models
    that are very useful in different machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: Tree augmented network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning*, we discussed
    the Naïve Bayes network, which makes the simplified assumption that all variables
    are independent of each other and only have dependency on the target or the class
    variable. This is the simplest Bayesian network derived or assumed from the dataset.
    As we saw in the previous sections, learning complex structures and parameters
    in Bayesian networks can be difficult or sometimes intractable. The **tree augmented
    network** or **TAN** (*References* [9]) can be considered somewhere in the middle,
    introducing constraints on how the trees are connected. TAN puts a constraint
    on features or variable relationships. A feature can have only one other feature
    as parent in addition to the target variable, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tree augmented network](img/B05137_06_356.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17\. Tree augmented network showing comparison with Naïve Bayes and Bayes
    network and the constraint of one parent per node.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Inputs are the training dataset *D* with all the features as variables *{X,
    Y..}*. The features have discrete outcomes, if they don't need to be discretized
    as a pre-processing step.
  prefs: []
  type: TYPE_NORMAL
- en: Outputs are TAN as Bayesian network with CPTs.
  prefs: []
  type: TYPE_NORMAL
- en: How does it work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compute mutual information between every pair of variables from the training
    dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an undirected graph with each node being the variable and edge being the
    mutual information between them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a maximum weighted spanning tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the spanning tree to a directed graph by selecting the outcome or
    the target variable as the root and having all the edges flowing in the outwards
    direction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If there is no directed edge between the class variable and other features,
    add it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the CPTs based on the DAG or TAN constructed previously.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advantages and limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is more accurate than Naïve Bayes in many practical models. It is less complex
    and faster to build and compute than complete Bayes networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Markov chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Markov Chains are specialized probabilistic graph models, with directed graphs
    containing loops. Markov chains can be seen as extensions of automata where the
    weights are probabilities of transition. Markov chains are useful to model temporal
    or sequence of changes that are directly observable. See (*References* [12]) for
    further study.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 17* represents a Markov chain (first order) and the general definition
    can be given as a stochastic process consisting of'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes as states, ![Markov chains](img/B05137_06_357.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Edges representing transition probabilities between the states or nodes. It
    is generally represented as a matrix ![Markov chains](img/B05137_06_358.jpg),
    which is a *N* X *N* matrix where *N* is the number of nodes or states. The value
    of ![Markov chains](img/B05137_06_361.jpg) captures the transition probability
    to node *q*[l] given the state *q*[k]. The rows of matrix add to 1 and the values
    of ![Markov chains](img/B05137_06_364.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Initial probabilities of being in the state, *p* = {*p*[1], *p*[2], … *p*[N]}.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, it can be written as a triple *M*= (Q, **A**, *p*) and the probability
    of being in any state only depends on the last state (first order):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov chains](img/B05137_06_367.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The joint probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Markov chains](img/B05137_06_368.jpg)![Markov chains](img/B05137_06_369.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18\. First-order Markov chain
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many real-world situations, the events we are interested in are not directly
    observable. For example, the words in sentences are observable, but the part-of-speech
    that generated the sentence is not. **Hidden Markov** **models** (**HMM**) help
    us in modeling such states where there are observable events and hidden states
    (*References* [13]). HMM are widely used in various modeling applications for
    speech recognition, language modeling, time series analysis, and bioinformatics
    applications such as DNA/protein sequence predictions, to name a few.
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov models](img/B05137_06_370.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19\. Hidden Markov model showing hidden variables and the observables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden Markov models can be defined again as a triple ![Hidden Markov models](img/B05137_06_371.jpg),
    where:'
  prefs: []
  type: TYPE_NORMAL
- en: is a set of finite states or symbols that are observed. ![Hidden Markov models](img/B05137_06_373.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q is a set of finite states that are not observed ![Hidden Markov models](img/B05137_06_375.jpg).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: T are the parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state transition matrix, given as ![Hidden Markov models](img/B05137_06_377.jpg)
    captures the probability of transition from state *q*[k] to *q*[l].
  prefs: []
  type: TYPE_NORMAL
- en: Emission probabilities capturing relationships between the hidden and observed
    state, given as ![Hidden Markov models](img/B05137_06_379.jpg) and *b* ? ?. ![Hidden
    Markov models](img/B05137_06_381.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: Initial state distribution *p* = {*p*[1], *p*[2], … *p*[N]}.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, a path in HMM consisting of a sequence of hidden states Q = {*q*[1],
    *q*[2], … *q*[L]} is a first order Markov chain *M*= (Q, **A**, *p*). This path
    in HMM emits a sequence of symbols, *x*[1], *x*[2], *x*[L], referred to as the
    observations. Thus, knowing both the observations and hidden states the joint
    probability is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hidden Markov models](img/B05137_06_384.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In real-world situations, we only know the observations *x* and do not know
    the hidden states *q*. HMM helps us to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the most likely path that could have generated the observation *x*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of *x*?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of being in state *q*i *= k* given the observation ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most probable path in HMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let us assume the observations *x* = *x*[1], *x*[2], *x*[L] and we want to
    find the path ![Most probable path in HMM](img/B05137_06_388.jpg) that generated
    the observations. This can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Most probable path in HMM](img/B05137_06_389.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The path *q** need not be unique, but for computation and explanation the assumption
    of the unique path is often made. In a naïve way, we can compute all possible
    paths of length *L* of *q* and chose the one(s) with the highest probability giving
    exponential computing terms or speed. More efficient is using Viterbi''s algorithm
    using the concept of dynamic programming and recursion. It works on the simple
    principle of breaking the equation into simpler terms as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Most probable path in HMM](img/B05137_06_391.jpg)![Most probable path in
    HMM](img/B05137_06_392.jpg)![Most probable path in HMM](img/B05137_06_393.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Most probable path in HMM](img/B05137_06_394.jpg) and ![Most probable
    path in HMM](img/B05137_06_395.jpg) Given the initial condition ![Most probable
    path in HMM](img/B05137_06_396.jpg) and using dynamic programming with keeping
    pointer to the path, we can efficiently compute the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Posterior decoding in HMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The probability of being in a state *q*[i] *= k* given the observation *x*
    can be written using Bayes theorem as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_397.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The numerator can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_398.jpg)![Posterior decoding in
    HMM](img/B05137_06_399.jpg)![Posterior decoding in HMM](img/B05137_06_400.jpg)![Posterior
    decoding in HMM](img/B05137_06_428.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![Posterior decoding in HMM](img/B05137_06_402.jpg) is called a Forward
    variable and ![Posterior decoding in HMM](img/B05137_06_403.jpg) is called a Backward
    variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The computation of the forward variable is similar to Viterbi''s algorithm
    using dynamic programming and recursion where summation is done instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_404.jpg)![Posterior decoding in
    HMM](img/B05137_06_405.jpg)![Posterior decoding in HMM](img/B05137_06_406.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The probability of observing *x* can be, then
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_407.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The forward variable is the joint probability and the backward variable is
    a conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_403.jpg)![Posterior decoding in
    HMM](img/B05137_06_408.jpg)![Posterior decoding in HMM](img/B05137_06_409.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It is called a backward variable as the dynamic programming table is filled
    starting with the *L*^(th) column to the first in a backward manner. The backward
    probabilities can also be used to compute the probability of observing *x* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Posterior decoding in HMM](img/B05137_06_411.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tools and usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce two tools in Java that are very popular for
    probabilistic graph modeling.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMarkov
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenMarkov is a Java-based tool for PGMs and here is the description from [www.openmarkov.org](http://www.openmarkov.org):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenMarkov is a software tool for probabilistic graphical models (PGMs) developed
    by the Research Centre for Intelligent Decision-Support Systems of the UNED in
    Madrid, Spain.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been designed for: editing and evaluating several types of PGMs, such
    as Bayesian networks, influence diagrams, factored Markov models, and so on, learning
    Bayesian networks from data interactively, and cost-effectiveness analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenMarkov is very good in performing interactive and automated learning from
    the data. It has capabilities to preprocess the data (discretization using frequency
    and value) and perform structure and parameter learning using a few search algorithms
    such as search-based Hill Climbing and score-based PC. OpenMarkov stores the models
    in a format known as pgmx. To apply the models in most traditional packages there
    may be a need to convert the pgmx models to XMLBIF format. Various open source
    tools provide these conversions.
  prefs: []
  type: TYPE_NORMAL
- en: Here we have some screenshots illustrating the usage of OpenMarkov to learn
    the structure and parameters from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 20*, we see the screen for interactive learning where you select
    the data file and algorithm to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_412.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 20\. OpenMarkov GUI – Interactive learning, algorithm selection
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is the **Preprocessing** tab (*Figure 21*) where we can select
    how discretization is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_413.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 21\. OpenMarkov GUI – Preprocessing screen
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in *Figure 22*, we see the display of the learned Bayes network structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![OpenMarkov](img/B05137_06_414.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 22\. OpenMarkov GUI – Structure output
  prefs: []
  type: TYPE_NORMAL
- en: Weka Bayesian Network GUI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weka's Bayes Network editor for interactive and automated learning has a large
    number of options for Bayes network representation, inference and learning as
    compared to OpenMarkov. The advantage in using Weka is the availability of a number
    of well-integrated preprocessing and transformation filters, algorithms, evaluation,
    and experimental metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 23*, we see the Bayes Network Editor where the search algorithm
    is selected and various options can be configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka Bayesian Network GUI](img/B05137_06_415.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 23\. WEKA Bayes Network – configuring search algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The learned structure and parameters of the BayesNet are shown in the output
    screen in *Figure 24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Weka Bayesian Network GUI](img/B05137_06_416.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 24\. WEKA Bayes Network – Learned parameter and structure
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform a case study with real-world machine learning
    datasets to illustrate some of the concepts from Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the UCI Adult dataset, also known as the *Census Income* dataset
    ([http://archive.ics.uci.edu/ml/datasets/Census+Income](http://archive.ics.uci.edu/ml/datasets/Census+Income)).
    This dataset was extracted from the United States Census Bureau's 1994 census
    data. The donors of the data is Ronny Kohavi and Barry Becker, who were with Silicon
    Graphics at the time. The dataset consists of 48,842 instances with 14 attributes,
    with a mix of categorical and continuous types. The target class is binary.
  prefs: []
  type: TYPE_NORMAL
- en: Business problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem consists of predicting the income of members of a population based
    on census data, specifically, whether their income is greater than $50,000.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning mapping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a problem of classification and this time around we will be training
    Bayesian graph networks to develop predictive models. We will be using linear,
    non-linear, and ensemble algorithms, as we have done in experiments in previous
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Data sampling and transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original dataset, there are 3,620 examples with missing values and six
    duplicate or conflicting instances. Here we include only examples with no missing
    values. This set, without unknowns, is divided into 30,162 training instances
    and 15,060 test instances.
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The features and their descriptions are given in *Table 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Type information |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| age | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| workclass | Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov,
    State-gov, Without-pay, Never-worked. |'
  prefs: []
  type: TYPE_TB
- en: '| fnlwgt | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| education | Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm,
    Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
    |'
  prefs: []
  type: TYPE_TB
- en: '| education-num | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| marital-status | Married-civ-spouse, Divorced, Never-married, Separated,
    Widowed, Married-spouse-absent, Married-AF-spouse. |'
  prefs: []
  type: TYPE_TB
- en: '| occupation | Tech-support, Craft-repair, Other-service, Sales, Exec-managerial,
    Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing,
    Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces. |'
  prefs: []
  type: TYPE_TB
- en: '| relationship | Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.
    |'
  prefs: []
  type: TYPE_TB
- en: '| race | White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black. |'
  prefs: []
  type: TYPE_TB
- en: '| sex | Female, Male. |'
  prefs: []
  type: TYPE_TB
- en: '| capital-gain | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| capital-loss | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| hours-per-week | continuous. |'
  prefs: []
  type: TYPE_TB
- en: '| native-country | United-States, Cambodia, England, Puerto-Rico, Canada, Germany,
    Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras,
    Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France,
    Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala,
    Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru,
    Hong, Holand-Netherlands. |'
  prefs: []
  type: TYPE_TB
- en: '*Table 3\. UCI Adult dataset – features*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The dataset is split by label as 24.78% (>50K) to 75.22% (<= 50K). Summary
    statistics of key features are given in *Figure 25*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature analysis](img/B05137_06_417.jpg)![Feature analysis](img/B05137_06_418.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 25\. Feature summary statistics
  prefs: []
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will perform detailed analysis on the Adult dataset using different flavors
    of Bayes network structures and with regular linear, non-linear, and ensemble
    algorithms. Weka also has an option to visualize the graph model on the trained
    dataset using the menu item, as shown in *Figure 26*. This is very useful when
    the domain expert wants to understand the assumptions and the structure of the
    graph model. If the domain expert wants to change or alter the network, it can
    be done easily and saved using the Bayes Network editor.
  prefs: []
  type: TYPE_NORMAL
- en: '![Models, results, and evaluation](img/B05137_06_419.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 26\. Weka Explorer – visualization menu
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 27* shows the visualization of the trained Bayes Network model''s graph
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Models, results, and evaluation](img/B05137_06_420.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: Visualization of learned structure of the Bayesian network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithms used for experiments are:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian network Classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes with default Kernel estimation on continuous data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes with supervised discretization on continuous data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree augmented network (TAN) with search-score structure parameter learning
    using the K2 algorithm and a choice of three parents per node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian network with search and score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Searching using Hill Climbing and K2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scoring using Simple Estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choice of parents changed from two to three to illustrate the effect on metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Bayesian algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression (default parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KNN (IBK with 10 Neighbors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision Tree (J48, default parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoostM1 (DecisionStump and default parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest (default parameters)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Table 4* presents the evaluation metrics for all the learners used in the
    experiments, including Bayesian network classifiers as well as the non-Bayesian
    algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithms | TP Rate | FP Rate | Precision | Recall | F-Measure | MCC | ROC
    Area | PRC Area |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Kernel Estimator) | 0.831 | 0.391 | 0.821 | 0.831 | 0.822 |
    0.494 | 0.891 | 0.906 |'
  prefs: []
  type: TYPE_TB
- en: '| Naïve Bayes (Discretized) | 0.843 | 0.191 | 0.861 | 0.843 | 0.848 | 0.6 |
    0.917 | 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| TAN (K2, 3 Parents, Simple Estimator) | 0.859 | 0.273 | 0.856 | 0.859 | 0.857
    | 0.6 | 0.916 | 0.931 |'
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (K2, 3 Parents, Simple Estimator) | 0.863 | 0.283 | 0.858 | 0.863
    | 0.86 | 0.605 | 0.934 | 0.919 |'
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (K2, 2 Parents, Simple Estimator) | 0.858 | 0.283 | 0.854 | 0.858
    | 0.855 | 0.594 | 0.917 | 0.932 |'
  prefs: []
  type: TYPE_TB
- en: '| BayesNet (Hill Climbing, 3 Parents, Simple Estimator) | 0.862 | 0.293 | 0.857
    | 0.862 | 0.859 | 0.602 | 0.918 | 0.933 |'
  prefs: []
  type: TYPE_TB
- en: '| Logistic Regression | 0.851 | 0.332 | 0.844 | 0.851 | 0.845 | 0.561 | 0.903
    | 0.917 |'
  prefs: []
  type: TYPE_TB
- en: '| KNN (10) | 0.834 | 0.375 | 0.824 | 0.834 | 0.826 | 0.506 | 0.867 | 0.874
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision Tree (J48) | 0.858 | 0.300 | 0.853 | 0.858 | 0.855 | 0.590 | 0.890
    | 0.904 |'
  prefs: []
  type: TYPE_TB
- en: '| AdaBoostM1 | 0.841 | 0.415 | 0.833 | 0.841 | 0.826 | 0.513 | 0.872 | 0.873
    |'
  prefs: []
  type: TYPE_TB
- en: '| Random Forest | 0.848 | 0.333 | 0.841 | 0.848 | 0.843 | 0.555 | 0.896 | 0.913
    |'
  prefs: []
  type: TYPE_TB
- en: '*Table 4\. Classifier performance metrics*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Analysis of results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Naïve Bayes with supervised discretization shows relatively better performance
    than kernel estimation. This gives a useful hint that discretization, which is
    needed in most Bayes networks, will play an important role.
  prefs: []
  type: TYPE_NORMAL
- en: The results in the table show continuous improvement when Bayes network complexity
    is increased. For example, Naïve Bayes with discretization assumes independence
    from all features and shows a TP rate of 84.3, the TAN algorithm where there can
    be one more parent shows a TP rate of 85.9, and BN with three parents shows the
    best TP rate of 86.2\. This clearly indicates that a complex BN with some nodes
    having no more than three parents can capture the domain knowledge and encode
    it well to predict on unseen test data.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes network where structure is learned using search and score (with K2 search
    with three parents and scoring using Bayes score) and estimation is done using
    simple estimation, performs the best in almost all the metrics of the evaluation,
    as shown in the highlighted values.
  prefs: []
  type: TYPE_NORMAL
- en: There is a very small difference between Bayes Networks—where structure is learned
    using search and score of Hill Climbing—and K2, showing that even local search
    algorithms can find an optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes network with a three-parent structure beats most linear, non-linear, and
    ensemble methods such as AdaBoostM1 and Random Forest on almost all the metrics
    on unseen test data. This shows the strength of BNs in not only learning the structure
    and parameters on small datasets with large number of missing values as well as
    predicting well on unseen data, but in beating other sophisticated algorithms
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PGMs capture domain knowledge as relationships between variables and represent
    joint probabilities. They are used in a range of applications.
  prefs: []
  type: TYPE_NORMAL
- en: Probability maps an event to a real value between 0 and 1 and can be interpreted
    as a measure of the frequency of occurrence (frequentist view) or as a degree
    of belief in that occurrence (Bayesian view). Concepts of random variables, conditional
    probabilities, Bayes' theorem, chain rule, marginal and conditional independence
    and factors form the foundations to understanding PGMs. MAP and Marginal Map queries
    are ways to ask questions about the variables and relationships in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of graphs and their properties such as paths, trails, cycles,
    sub-graphs, and cliques are vital to the understanding of Bayesian networks. Representation,
    Inference, and Learning form the core elements of networks that help us capture,
    extract, and make predictions using these methods. From the representation of
    graphs, we can reason about the flow of influence and detect independencies that
    help reduce the computational load when querying the model. Junction trees, variable
    elimination, and belief propagation methods likewise make inference from queries
    more tractable by reductive steps. Learning from Bayesian networks involves generating
    the structure and model parameters from the data. We discussed several methods
    of learning parameters and structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov networks** (**MN**), which have undirected edges, also contain interactions
    that can be captured using parameterization techniques such as Gibbs parameterization,
    Factor Graphs, and Log-Linear Models. Independencies in MN govern flows of influence,
    as in Bayesian networks. Inference techniques are also similar. Learning of parameters
    and structure in MN is hard, and approximate methods are used. Specialized networks
    such as **Tree augmented networks** (**TAN**) make assumptions of independence
    amongst nodes and are very useful in some applications. Markov Chains and hidden
    Markov models are other specialty networks that also find application in a range
    of fields.'
  prefs: []
  type: TYPE_NORMAL
- en: Open Markov and Weka Bayesian Network GUI are introduced as Java-based tools
    for PGMs. The case study in this chapter used Bayesian Networks to learn from
    the UCI Adult census dataset and its performance was compared to other (non-PGM)
    classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Daphne Koller and Nir Friedman (2009). *Probabilistic Graphical Models*. MIT
    Press. ISBN 0-262-01319-3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: T. Verma and J. Pearl (1988), In proceedings for fourth workshop on Uncertainty
    in Artificial Intelligence, Montana, Pages 352-359\. Causal Networks- Semantics
    and expressiveness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dagum, P., and Luby, M. (1993). *Approximating probabilistic inference in Bayesian
    belief networks is NP hard*. Artificial Intelligence 60(1):141–153.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: U. Bertele and F. Brioschi, *Nonserial Dynamic Programming*, Academic Press.
    New York, 1972.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shenoy, P. P. and G. Shafer (1990). *Axioms for probability and belief-function
    propagation*, in Uncertainty in Artificial Intelligence, 4, 169-198, North-Holland,
    Amsterdam
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayarri, M.J. and DeGroot, M.H. (1989). *Information in Selection Models*. Probability
    and Bayesian Statistics, (R. Viertl, ed.), Plenum Press, New York.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Spiegelhalter and Lauritzen (1990). *Sequential updating of conditional probabilities
    on directed graphical structures*. Networks 20\. Pages 579-605.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'David Heckerman, Dan Geiger, David M Chickering (1995). In journal of Machine
    Learning. *Learning Bayesian networks: The combination of knowledge and statistical
    data*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Friedman, N., Geiger, D., & Goldszmidt, M. (1997). *Bayesian network classifiers*.
    Machine Learning, 29, 131– 163.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Isham, V. (1981). *An introduction to spatial point processes and Markov random
    fields*. International Statistical Rewview, 49(1):21–43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger, *Factor graphs
    and sum-product algorithm*, IEEE Trans. Info. Theory, vol. 47, pp. 498–519, Feb.
    2001\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kemeny, J. G. and Snell, J. L. *Finite Markov Chains*. New York: Springer-Verlag,
    1976.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Baum, L. E.; Petrie, T. (1966). *Statistical Inference for Probabilistic Functions
    of Finite State Markov Chains*. The Annals of Mathematical Statistics. 37 (6):
    1554–1563\.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gelman, A., Hwang, J. and Vehtar, A. (2004). *Understanding predictive information
    criteria for Bayesian models*. Statistics and Computing Journal 24: 997\. doi:10.1007/s11222-013-9416-2'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dimitris. Margaritis (2003). *Learning Bayesian Network Model Structure From
    Data*. Ph.D Thesis Carnegie Mellon University.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'John Lafferty, Andrew McCallum, Fernando C.N. Pereira (2001). *Conditional
    Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data*,
    International Conference on Machine Learning 2001 (ICML 2001), pages 282-289.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
