<html><head></head><body>
		<div id="_idContainer084">
			<h1 id="_idParaDest-120" class="chapter-number"><a id="_idTextAnchor020"/>8</h1>
			<h1 id="_idParaDest-121">Detecting Fake News with Graph Neural Networks</h1>
			<p>In the previous chapters, we looked at tabular data, which was comprised of individual data points with their own features. While modeling and running our experiments, we did not consider any features of the relationship among the data points. Much real-world data, particularly that in the domain of cybersecurity, can naturally occur as graphs and be represented as a set of nodes, some of which are connected using edges. Examples include social networks, where users, photos, and posts can be connected using edges. Another example is the internet, which is a large graph of computers connected to <span class="No-Break">each other.</span></p>
			<p>Traditional machine learning algorithms cannot directly learn from graphs. Algorithms such as regression, neural networks, and trees, and optimization techniques such as gradient descent are designed to operate on Euclidean (flat) data structures. This has led to the development of <strong class="bold">Graph Neural Networks</strong> (<strong class="bold">GNNs</strong>), an upcoming area of research in the field of machine learning. This has found tremendous applications in cybersecurity, particularly in areas such as botnets, fake news detection, and fraud analytics. This chapter will focus on detecting fake news using GNNs. We will first cover the basics of graph theory, followed by how graph machine learning can be used as a tool to frame a security problem. Although we will learn how to use GNN models to detect fake news, the techniques we will introduce are generic and can be applied to <span class="No-Break">multiple problems.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>An introduction <span class="No-Break">to graphs</span></li>
				<li>Machine learning <span class="No-Break">on graphs</span></li>
				<li>Fake news detection <span class="No-Break">with GNNs</span></li>
			</ul>
			<p>By the end of this chapter, you will have an understanding of how certain data can be modeled as graphs, and how to apply graph machine learning for <span class="No-Break">effective classification.</span></p>
			<h1 id="_idParaDest-122">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208"><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%208</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-123">An introduction to graphs</h1>
			<p>First, let us understand what graphs are and the key terms related <span class="No-Break">to graphs.</span></p>
			<h2 id="_idParaDest-124">What is a graph?</h2>
			<p>A graph is a data<a id="_idIndexMarker587"/> structure that is represented as a set of nodes connected by a set of edges. Mathematically, we specify a graph <em class="italic">G</em> as (<em class="italic">V, E</em>), where <em class="italic">V</em> represents the nodes or vertices and <em class="italic">E</em> represents the edges between them, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B19327_08_01.jpg" alt="Figure 8.1 – A simple graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A simple graph</p>
			<p>In the previous graph, we have <span class="No-Break">the following:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">5</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">6</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1,3</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">2,3</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">2,5</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">3,6</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="No-Break"><span class="_-----MathTools-_Math_Number">4,6</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">5.6</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
			<p>Note that the order in which the nodes and edges are mentioned does not matter. The graph shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> is an undirected graph, which means that the direction of the edges does not matter. There <a id="_idIndexMarker588"/>can also be directed graphs in which the definition of the edge has some meaning, which gives importance to the direction of the edge. For example, a graph depicting the water flow of from various cities would have directed edges, as water flowing from city A to city B does not imply that water flows from B <span class="No-Break">to A.</span></p>
			<p>An example of a directed graph is <span class="No-Break">shown here:</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B19327_08_02.jpg" alt="Figure 8.2 – An example of a directed graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – An example of a directed graph</p>
			<p>Here, we would define the graph <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">3</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">4</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">5</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">7</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Space"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">8</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
			<p><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">{</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1,4</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">4,1</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">1,3</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">2,3</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">3,5</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">5,7</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">6,2</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="No-Break"><span class="_-----MathTools-_Math_Number">7,3</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Operator">,</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">(</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">8,3</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">}</span></span></p>
			<p>Note that here, as it is a directed graph, the order in which nodes in each edge tuple are specified matters. We have added both <strong class="source-inline">(1,4)</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">(4,1)</strong></span><span class="No-Break">.</span></p>
			<p>In this graph, the edges are unmarked (that is, there<a id="_idIndexMarker589"/> is no weight associated with an edge). Such a graph is <a id="_idIndexMarker590"/>called an unweighted graph. A graph in which there are weights associated with edges is called a weighted graph. Edge <a id="_idIndexMarker591"/>weights can represent properties of relationships between nodes, or the intensity of relationships. For example, in a highway transport network, where nodes represent cities, the edge weights can depict the distance on the highway or the time taken to traverse <span class="No-Break">on average.</span></p>
			<h2 id="_idParaDest-125">Representing graphs</h2>
			<p>As discussed, a<a id="_idIndexMarker592"/> graph can be represented by a set of vertices and edges between vertices. On a computer, or inside a program, a graph can be represented in one of two ways – an adjacency matrix or an <span class="No-Break">adjacency list.</span></p>
			<p>An adjacency <a id="_idIndexMarker593"/>matrix is an <em class="italic">N</em> x <em class="italic">N</em> matrix, where <em class="italic">N</em> is the total number of nodes in the graph. Entries in the matrix denote edge relations between the nodes. If <em class="italic">A</em> is the adjacency matrix, then <em class="italic">A</em><span class="subscript">ij</span> = 1 if there is an edge between the nodes <em class="italic">i</em> and <em class="italic">j</em>. If the graph is undirected, then we have <em class="italic">A</em><span class="subscript">ij</span> = <em class="italic">A</em><span class="subscript">ji</span>; if the graph is directed, then <em class="italic">A</em><span class="subscript">ij</span> = 1 means that there is an edge from node <em class="italic">i</em> to <span class="No-Break">node </span><span class="No-Break"><em class="italic">j</em></span><span class="No-Break">.</span></p>
			<p>Consider the following <span class="No-Break">directed graph:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B19327_08_03.jpg" alt="Figure 8.3 – A sample-directed graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – A sample-directed graph</p>
			<p>For this directed <a id="_idIndexMarker594"/>graph, the adjacency matrix would be defined <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker595"/></span><span class="No-Break">follows:</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – An adjacency matrix corresponding to the graph in Figure 8.3</p>
			<p>Alternatively, if the graph is weighted, then the entries in the adjacency matrix can be modified to represent the <span class="No-Break">weight instead.</span></p>
			<p>While adjacency matrices are interpretable and straightforward to understand, they are computationally expensive. As the number of nodes in the graph increases, the size of the adjacency matrix also increases. In technical terms, the space complexity is <em class="italic">O(N</em><span class="superscript">2</span><em class="italic">)</em>, where <em class="italic">N</em> is the number<a id="_idIndexMarker596"/> of nodes. Larger adjacency matrices are difficult to store and also require a significant <span class="No-Break">processing overhead.</span></p>
			<p>An alternative way to represent a graph is an adjacency list. In this form of representation, the graph is stored as a dictionary or hash table. The keys represent the nodes, and the value is a list of nodes that the key node has an edge to. The list of nodes is represented in memory as a linked list. For the graph in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em>, the adjacency list representation is <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B19327_08_04.jpg" alt="Figure 8.4 – An adjacency list representation of the graph shown in Figure 8.3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – An adjacency list representation of the graph shown in Figure 8.3</p>
			<p>Here, we can clearly see the nodes that a given node has an edge to. Even when the number of nodes is large<a id="_idIndexMarker597"/> and the edges are sparse, the adjacency list turns out to be an <span class="No-Break">efficient representation.</span></p>
			<h2 id="_idParaDest-126">Graphs in the real world</h2>
			<p>Much real-world data in <a id="_idIndexMarker598"/>nearly all domains of science can be naturally represented as networks using graphs. In the chemical and medical sciences, drugs can be represented as a network of molecules, which are themselves networks of atoms. In information systems and the internet, a graph can be constructed that represents machines and the connections among them. In the field of engineering, a graph can be constructed to represent cities, countries, and the transport networks <span class="No-Break">within them.</span></p>
			<p>Graphs provide us with an aggregate and feature-rich view of data. Whereas, traditionally, every data point would be examined independently, graphs now allow us to examine the relationships between various data points. Relationships and neighborhoods contain valuable information that can be leveraged in recommendations, clustering, fraud detection, and other <span class="No-Break">analytic tasks.</span></p>
			<p>Graphs are rapidly emerging as important analytic tools in the cybersecurity domain. Similar to the other fields, several security applications can be represented as graphs, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>A social media<a id="_idIndexMarker599"/> network can be represented as a graph with users as nodes. Edges between nodes identify friendships and <span class="No-Break">family relationships.</span></li>
				<li>Network traffic can be modeled as a graph, with nodes as IP addresses and edges as <span class="No-Break">communication messages.</span></li>
				<li>Domains, URLs, or websites can be represented as a graph, where edges indicate the presence of links between two websites <span class="No-Break">or domains.</span></li>
				<li>Malware files can be represented as a graph, where nodes are functions and edges represent calls <span class="No-Break">between them.</span></li>
			</ul>
			<p>Nodes represent entities of interest, and edges represent the relationships between them. Not all nodes have to be of the same category – for example, a social network graph can have both posts and users as nodes. Similarly, there can also be heterogeneity in edges. One kind of edge can connect users who are friends, while another one can connect users who have at least 10 mutual friends. A third one can connect users who have commented on the <span class="No-Break">same post.</span></p>
			<p>Every node in a graph can have features associated with it. These represent characteristics of the entity that the node represents. Features associated with a user can be their age, the age of the account, the number of friends, the number of posts shared, and so on. Similarly, edges can also have features. For example, if an edge represents two users being friends, the edge features can be the number of mutual friends, the age of the<a id="_idIndexMarker600"/> friendship, the common pages followed, and <span class="No-Break">so on.</span></p>
			<p>Consider the sample social network graph shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B19327_08_05.jpg" alt="Figure 8.5 – A social network graph"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – A social network graph</p>
			<p>In this graph, the nodes represent users. Every user has features associated with them – the number of friends, followers, and posts per week, as well as a list of pages liked. These become <a id="_idIndexMarker601"/>node attributes. There are<a id="_idIndexMarker602"/> two kinds of edges, marked by different colors. A green edge is constructed between two users if they have communicated over messages. A gray edge<a id="_idIndexMarker603"/> is added if the two have more than five mutual friends. Note that edges can also have features. In this case, the green edge (which denotes the fact that two users talked over messages) has properties related to that relationship, such as message timestamps, the text, the average duration between messages, and the number of images exchanged. These become the <span class="No-Break">edge attributes.</span></p>
			<p>This concludes our discussion on the fundamentals of graphs and how real-world data can naturally be represented in graph form. In the next section, we will look at how machine learning can be applied <span class="No-Break">to graphs.</span></p>
			<h1 id="_idParaDest-127">Machine learning on graphs</h1>
			<p>Machine learning<a id="_idIndexMarker604"/> techniques (such as classification or clustering) can nowadays be applied to nodes, edges, or entire graphs. The concepts remain the same, but we apply the algorithms to graph entities, and therefore, some of the tasks can be framed as a node, link, or subgraph classification. For example, in a network of users on social media, identifying abusive or bot users would be a node classification task. Identifying malicious messages or transactions would be an edge classification problem. Detecting groups of hate speech disseminators would be a graph <span class="No-Break">classification problem.</span></p>
			<p>In graph machine learning, the challenge lies in extracting features from a graph. A possible approach would be using the adjacency matrix and node features as an attribute vector and feeding it to a traditional machine learning algorithm. However, the model produced will not be permutation-invariant, as there is no inherent order within the nodes in a graph; models based on a graph should be permutation-invariant, as the order of nodes should not really matter. There have been several approaches proposed to handle these challenges, which we will take a brief <span class="No-Break">look at.</span></p>
			<h2 id="_idParaDest-128">Traditional graph learning</h2>
			<p>In a naïve approach to machine <a id="_idIndexMarker605"/>learning on graphs, we will parse the graph and extract features for the entity we’re interested in. If the task is at the node level, we extract a set of features for each node. If it is at the edge level, we extract a set of features for each edge. However, prior research has shown that this traditional approach is most suited for node-level tasks, such as node classification or clustering. These features are typically based on standard <span class="No-Break">graph-based metrics.</span></p>
			<p>For example, consider a social media network where nodes are users, and edges between users indicate some sort of relationship (e.g., the users are friends, they share a certain number of mutual connections, they have a common or similar activity, or they have interacted via comments <span class="No-Break">or messages).</span></p>
			<p>First, you can extract <a id="_idIndexMarker606"/>common graph metrics for every user, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>The node in-degree (the number of incoming edges to <span class="No-Break">a node)</span></li>
				<li>The node out-degree (the number of outgoing edges from <span class="No-Break">a node)</span></li>
				<li>The sum of outgoing <span class="No-Break">edge weights</span></li>
				<li>The sum of incoming <span class="No-Break">edge weights</span></li>
				<li>The distance to the nearest neighbor (as defined by the <span class="No-Break">edge weight)</span></li>
				<li>The in-degree of the <span class="No-Break">nearest neighbor</span></li>
				<li>The distance to the root (some <span class="No-Break">predefined neighbor)</span></li>
				<li>Whether the node is part of a cyclic subgraph (that is, forming <span class="No-Break">a loop)</span></li>
			</ul>
			<p>Additionally, we can use node-specific features that we would have used in traditional machine learning, such as the number of friends, the number of pages liked, the number of logins, the number of posts, the key topics the user writes about, and the average likes per post. Together, these feature sets will form our <span class="No-Break">feature vector.</span></p>
			<p>Once the feature vector has been created, any standard machine learning model (logistic regression, random forest, SVM, or a deep neural network) can be trained <span class="No-Break">for classification.</span></p>
			<p>While this approach is straightforward, it has a major disadvantage – it examines each node individually and does not consider the interrelationships within nodes. Additionally, features have to be handcrafted. Not all graph metrics will make sense in all use cases, so you may have to come up with more <span class="No-Break">creative measurements.</span></p>
			<h2 id="_idParaDest-129">Graph embeddings</h2>
			<p>The previous section described traditional feature extraction on graphs and the disadvantages that come <a id="_idIndexMarker607"/>with it. In this section, we will look at a slightly advanced technique – node embeddings, which are analogous to <span class="No-Break">word embeddings.</span></p>
			<h3>Node embeddings</h3>
			<p>In the previous chapter, we looked <a id="_idIndexMarker608"/>at word embeddings using the Word2Vec<a id="_idIndexMarker609"/> algorithm. An embedding is a mathematical representation of a word, such that words with a similar meaning or closer in semantics are closer to each other in the embedding space. For example, the words <em class="italic">king</em> and <em class="italic">queen</em> will have embeddings that are very similar, as will the words <em class="italic">banana</em> and <em class="italic">apple</em>. Node embeddings operate under a similar concept; embeddings are generated for each node such that similar nodes have <span class="No-Break">similar embeddings.</span></p>
			<p>As an example, see the following diagram, taken from the Stanford Course on Graph Neural <span class="No-Break">Networks (CS224W):</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B19327_08_06.jpg" alt="Figure 8.6 – A graph and its corresponding node embeddings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – A graph and its corresponding node embeddings</p>
			<p>On the left, you can see a graph where nodes are colored, and on the right, you can see the same nodes plotted in an embedding space (a high-dimensional embedding was generated, followed by principal component analysis to reduce it to two dimensions). We can clearly see that nodes of a similar color cluster together in the <span class="No-Break">embedding space.</span></p>
			<p>Node embeddings are generated based on random walks from a node. A random walk is basically the <a id="_idIndexMarker610"/>sequence of nodes traversed, starting at a source node. Given a source node, we select a neighbor at random and move to it. At this neighbor, we <a id="_idIndexMarker611"/>again select another neighbor (of this new node) and move to it. We do this for a fixed number of steps. The sequence of nodes visited forms a <span class="No-Break">random walk.</span></p>
			<p>We use random walks to generate node embeddings. The underlying rationale is that if a random walk starting at <em class="italic">u</em> visits <em class="italic">v</em> with a high probability, then the nodes <em class="italic">u</em> and <em class="italic">v</em> are likely to be similar. Nodes visited in the same random walk will be close to each other in the <span class="No-Break">embedding space.</span></p>
			<p>To generate our embeddings, we estimate the probability of visiting node <em class="italic">v</em> on a random walk that started at node <em class="italic">u</em>. We learn the mapping from the node to the embedding space. We want to learn feature representations that are predictive of the nodes in the random walk. We first initialize at random an embedding for each node. We then run fixed-length, short random walks and calculate a loss for each walk. The loss function generally used is the log-likelihood. If <em class="italic">u</em> is the node of interest, the loss function is defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">L</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Variable">V</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">∑</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">N</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">u</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Function_v-normal">log</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space">|</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">z</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">u</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">)</span></span></p>
			<p>Here, <em class="italic">z</em> denotes the embedding representation. <em class="italic">N</em><span class="subscript">R(u)</span> represents the neighborhood of node <em class="italic">u</em>. We estimate the probability <em class="italic">P</em> of two nodes co-occurring on the same random walk as the softmax of the dot product of their embeddings. The neighborhood set can be formed by any criterion (such as neighbors at one hop, two hops, or satisfying some criterion <span class="No-Break">for similarity).</span></p>
			<p>We optimize for the loss function using gradient descent, just like we would optimize for parameters in any other algorithm, such as a neural network or a linear regression. The embedding <em class="italic">z</em> is first initialized to some random value. For each node, a loss is calculated as well as the partial derivative of the loss with respect to <em class="italic">z</em>. Finally, <em class="italic">z</em> is updated by making adjustments in the right direction as suggested by the gradient. After this has been completed for multiple iterations, <em class="italic">z</em> will have the embeddings that result in the smallest <span class="No-Break">possible loss.</span></p>
			<p>The approach <a id="_idIndexMarker612"/>described so far was based on random walks. However, a popular algorithm, Node2Vec, operates on the principle of biased walks. Instead of picking<a id="_idIndexMarker613"/> the next node to jump to randomly, it operates based on two parameters – <em class="italic">p</em>, which denotes the probability of returning to the previous node jumped from, and <em class="italic">q</em>, which denotes the probability of moving outward to another node. When the value of <em class="italic">p</em> is low, the random<a id="_idIndexMarker614"/> walk functions as a <strong class="bold">breadth-first search</strong> (<strong class="bold">BFS</strong>) (a graph traversal algorithm that explores all the vertices at the same distance from the starting vertex, before moving to the vertices at the next level). On the other hand, when <em class="italic">q</em> is low, it<a id="_idIndexMarker615"/> functions as a <strong class="bold">depth-first search</strong> (<strong class="bold">DFS</strong>) (a graph traversal algorithm that explores as far as possible along each branch <span class="No-Break">before backtracking).</span></p>
			<h3>From node embeddings to graph embeddings</h3>
			<p>The random walk<a id="_idIndexMarker616"/> and Node2Vec methods describe <a id="_idIndexMarker617"/>how to obtain an embedding representation for a single node. However, oftentimes, we want to solve tasks at the graph level, such as classifying subgraphs or entire graphs. There are two approaches that can compute embeddings <span class="No-Break">for graphs.</span></p>
			<p>The first approach is calculating embeddings for each node separately and then aggregating them to obtain a graph-level representation. The aggregation can be as simple as a sum or average. The sum or average can be weighted by node importance, label, or some other feature. While this approach is fairly straightforward, it obfuscates the embeddings of each individual node; it does not take the structure and connections between nodes <span class="No-Break">into account.</span></p>
			<p>Another popular approach is to introduce a dummy node into the graph or subgraph. This node is thought of as having edges to all of the other nodes in the graph. We then use the learned models to calculate the embedding for this dummy node. As this node is connected to all the other nodes, it captures the structural relationships between them and can represent the graph as <span class="No-Break">a whole.</span></p>
			<h2 id="_idParaDest-130">GNNs</h2>
			<p>Existing methods for machine<a id="_idIndexMarker618"/> learning on graphs face the following <span class="No-Break">two issues:</span></p>
			<ul>
				<li>If we use traditional methods of extracting features based on graph analytics, we lose out on incorporating the node-level features and their relationships. Aggregating metrics at a graph or subgraph level causes information loss, due to noisy data in <span class="No-Break">the model.</span></li>
				<li>If we use node embeddings, we use only the interconnections between nodes and not the node features. Valuable signals that may have been meaningful for a classification model present in node features will <span class="No-Break">be lost.</span></li>
			</ul>
			<p>This led to the development of GNNs. Using GNN models, it is possible to apply deep learning algorithms (such as convolution, backpropagation, autoencoders, and attention-based transformers) directly to graphs. They accept entire graphs as input (instead of vectors) and learn embedding representations for each node. Every node has an internal state (initially set to a null vector, or based on the features of a node). A node aggregates information from its neighbors, followed by neural message passing, which propagates <span class="No-Break">this information.</span></p>
			<p>Neural message passing is the fundamental principle on which GNNs operate. Consider the following figure, illustrating the operation of message passing and a GNN in a node <span class="No-Break">classification context:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B19327_08_07.jpg" alt="Figure 8.7 – Message passing in GNNs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Message passing in GNNs</p>
			<p>Consider the graph with nodes and edges, shown in <strong class="bold">(i)</strong>. Let <strong class="bold">D</strong> be the initial node under consideration. This is an arbitrary choice, and any node can be chosen to start with. Each node has an internal state representation. At first, the internal state is set to the feature vector for <span class="No-Break">the node.</span></p>
			<p>Each neighbor of <strong class="bold">D</strong> <a id="_idIndexMarker619"/>now passes messages to <strong class="bold">D</strong>, as shown in <strong class="bold">(ii)</strong>. The messages are simply the internal states, or some function of the internal state. After <strong class="bold">D</strong> receives the messages, it does two things, as shown <span class="No-Break">in </span><span class="No-Break"><strong class="bold">(iii)</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li>Aggregates the messages to come up with a unified representation. Aggregation can be a sum, mean, or min/max pooling of <span class="No-Break">the states.</span></li>
				<li>Updates its own internal state by applying an aggregation function <em class="italic">f</em> over the original state and the aggregated state produced in the <span class="No-Break">previous step.</span></li>
			</ul>
			<p>Finally, apply a function <em class="italic">g</em> such as a sigmoid or softmax to obtain a prediction for node <strong class="bold">D</strong>. Comparing this with the ground truth label for the node, we can calculate a loss, as shown in <strong class="bold">(iv)</strong>, which can <span class="No-Break">be backpropagated.</span></p>
			<p>All of these steps (<strong class="bold">(i)</strong>–<strong class="bold">(iv)</strong>) occur for each node at the same instant. The message passing happens first, and the updates of the state occur together. This process can be repeated for <span class="No-Break">multiple iterations.</span></p>
			<p>Note an interesting consequence of message passing. At <em class="italic">t = 0</em> (the very first step), a node has information only about itself (based on its own features). At <em class="italic">t = 1</em> (after the first step of message passing has completed), a node has received feature information from its neighbors, so it has knowledge about its first neighbors. At <em class="italic">t = 2</em>, the node will receive messages from its neighbors, but these messages will already encode information from <em class="italic">their</em> neighbors; thus, a node will have knowledge of its two-hop neighbors as well. As the steps repeat, information spreads farther <span class="No-Break">and farther.</span></p>
			<p>Now that we have described graphs sufficiently, and grasped the concepts behind GNNs, it is time to put them <span class="No-Break">to use.</span></p>
			<h1 id="_idParaDest-131">Fake news detection with GNN</h1>
			<p>In this<a id="_idIndexMarker620"/> section, we will <a id="_idIndexMarker621"/>learn how fake news can be detected using <span class="No-Break">a GNN.</span></p>
			<h2 id="_idParaDest-132">Modeling a GNN</h2>
			<p>While some problems can naturally be thought of as graphs, as data scientists, you need to conceptualize and build a <a id="_idIndexMarker622"/>graph. Data may still be available to you in tabular form, but it will be up to you to build a meaningful graph <span class="No-Break">from it.</span></p>
			<p>Solving any task with a GNN involves the following <span class="No-Break">high-level steps:</span></p>
			<ol>
				<li>Identifying the entities that will be <span class="No-Break">your nodes.</span></li>
				<li>Defining a rule or metric to connect nodes <span class="No-Break">via edges.</span></li>
				<li>Defining a set of features for nodes <span class="No-Break">and edges.</span></li>
				<li>Determining the kind of graph task the given problem can translate into (node classification, edge classification, or <span class="No-Break">subgraph classification).</span></li>
			</ol>
			<p>In social media-related domains, such as friend recommendation, post virality, and fake news detection, we have multiple choices for nodes, their features, and the methodology for edges between them, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Nodes can be users, their posts, <span class="No-Break">or comments</span></li>
				<li>Features can be user-historical behavior or text-related features <span class="No-Break">from content</span></li>
				<li>Edges can be based on whether they have mutual friends or follow the <span class="No-Break">same page</span></li>
			</ul>
			<p>There is a growing body of research that explores these methodologies and applications of GNNs for <span class="No-Break">internet security.</span></p>
			<h2 id="_idParaDest-133">The UPFD framework</h2>
			<p>For our experiment, we will <a id="_idIndexMarker623"/>be following the <strong class="bold">User Preference-aware Fake News Detection</strong> (<strong class="bold">UPFD</strong>) framework ([2104.12259] <em class="italic">User Preference-aware Fake News Detection</em> – <a href="http://arxiv.org">arxiv.org</a>). In this UPFD framework, every news article is transformed into a graph. The task is to detect whether a <a id="_idIndexMarker624"/>news article is fake news or not. As a news article is a graph, this is essentially a graph <span class="No-Break">classification problem.</span></p>
			<p>For every news article, we obtain an information diffusion graph between users. In short, the process works as follows. For every news article, we do <span class="No-Break">the following:</span></p>
			<ol>
				<li>Identify the set of users who are engaged in propagating the news article (via likes or retweets). These become the nodes of <span class="No-Break">the graph.</span></li>
				<li>Crawl the recent 200 tweets of each user (this is a design choice; you may decide to crawl more or less depending on your <span class="No-Break">use case!).</span></li>
				<li>Using pre-trained BERT embeddings and word vectors, encode the news article into a feature<a id="_idIndexMarker625"/> vector. This feature vector per user represents the <span class="No-Break">node features.</span></li>
				<li>Based on the order of retweeting the news article, build an information diffusion path that indicates how the news article spread from one user to another. These form the edges of <span class="No-Break">the graph.</span></li>
				<li>Finally, using the GNN, train a classification model to classify each graph (that is, each article) as fake news or not, based on the graph structure as well as <span class="No-Break">user representations.</span></li>
			</ol>
			<p>The dataset used for UPFD has been made available publicly. This dataset contains graph representations for news articles, after all the pre-processing, feature extraction, and information diffusion has been done. Using this dataset directly saves us the trouble of implementing data <span class="No-Break">preparation pipelines.</span></p>
			<h2 id="_idParaDest-134">Dataset and setup</h2>
			<p>We will first install the required libraries. As discussed in previous chapters, PyTorch is a deep learning framework developed by Facebook that can help us flexibly and easily implement most machine learning<a id="_idIndexMarker626"/> models, including neural networks. PyTorch Geometric is the graph counterpart of PyTorch that can be used to implement GNNs. It is built upon PyTorch and contains several methods for deep learning on graphs. The following commands will install PyTorch and <span class="No-Break">PyTorch Geometric:</span></p>
			<pre class="source-code">
pip install torch
pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html
pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-${TORCH}+${CUDA}.html
pip install -q git+https://github.com/rusty1s/pytorch_geometric.git</pre>
			<p>We can check that<a id="_idIndexMarker627"/> the installation <span class="No-Break">was successful:</span></p>
			<pre class="source-code">
import torch
import torch_geometric
version = torch.__version__
print("Torch version is {}".format(version))</pre>
			<p>This will show you the version of PyTorch you <span class="No-Break">are using.</span></p>
			<p>We will now download the fake news dataset from the UPFD paper. This has been integrated with the PyTorch <span class="No-Break">Geometric datasets:</span></p>
			<pre class="source-code">
from torch_geometric.datasets import UPFD
DATA_ROOT = "/content/FakeNewsNet/dataset"
train_data = UPFD(root = DATA_ROOT,
                  name="gossipcop", feature="content",
                  split="train")
test_data = UPFD(root = DATA_ROOT,
                 name="gossipcop", feature="content",
                 split="test")</pre>
			<p>We can examine the size of our training and <span class="No-Break">test sets:</span></p>
			<pre class="source-code">
print("# Training Examples: {}".format(len(train_data)))
print("# Test Examples: {}".format(len(test_data)))</pre>
			<p>This should give you the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
# Training Examples: 1092
# Test Examples: 3826</pre>
			<p>What does the training data look like? Let us look at the <span class="No-Break">first element:</span></p>
			<pre class="source-code">
train_data[0]
Data(x=[76, 310], edge_index=[2, 75], y=[1])</pre>
			<p>This says that the first <a id="_idIndexMarker628"/>element is an object of the <strong class="source-inline">Data</strong> class. The <strong class="source-inline">x</strong> attribute represents the features of <span class="No-Break">the nodes.</span></p>
			<p>We can visualize one of the graphs using the <strong class="source-inline">networkx</strong> library. First, install the library using the <span class="No-Break">package manager:</span></p>
			<pre class="source-code">
pip install networkx</pre>
			<p>Now, we can visualize the graphs using <span class="No-Break">this library:</span></p>
			<pre class="source-code">
nx.draw(to_networkx(train_data[1]))</pre>
			<p>It will produce the <span class="No-Break">following result:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B19327_08_08.jpg" alt="Figure 8.8 – A sample graph from the UPFD dataset"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – A sample graph from the UPFD dataset</p>
			<p>On some systems, or if you are using Google Colab, the <strong class="source-inline">networkx</strong> dependencies do not load properly, and <a id="_idIndexMarker629"/>you might get an error when visualizing the graph that says that the function is not defined. If that happens, you need to manually copy the code snippet of that function. For convenience, the function definition is <span class="No-Break">given here:</span></p>
			<pre class="source-code">
def to_networkx(data, node_attrs=None, edge_attrs=None, to_undirected=False,
                remove_self_loops=False):
    if to_undirected:
        G = nx.Graph()
    else:
        G = nx.DiGraph()
    G.add_nodes_from(range(data.num_nodes))
    node_attrs, edge_attrs = node_attrs or [], edge_attrs or []
    values = {}
    for key, item in data(*(node_attrs + edge_attrs)):
        if torch.is_tensor(item):
            values[key] = item.squeeze().tolist()
        else:
            values[key] = item
        if isinstance(values[key], (list, tuple)) and len(values[key]) == 1:
            values[key] = item[0]
    for i, (u, v) in enumerate(data.edge_index.t().tolist()):
        if to_undirected and v &gt; u:
            continue
        if remove_self_loops and u == v:
            continue
        G.add_edge(u, v)
        for key in edge_attrs:
            G[u][v][key] = values[key][i]
    for key in node_attrs:
        for i, feat_dict in G.nodes(data=True):
            feat_dict.update({key: values[key][i]})
    return G</pre>
			<p>Copying this into your <a id="_idIndexMarker630"/>script should resolve <span class="No-Break">the error.</span></p>
			<p>Now that we are familiar with what the data looks like internally, we can begin <span class="No-Break">our experiment.</span></p>
			<h2 id="_idParaDest-135">Implementing GNN-based fake news detection</h2>
			<p>First, we read the <a id="_idIndexMarker631"/>data into the <strong class="source-inline">DataLoader</strong> object so that it can easily be consumed by our model. The <strong class="source-inline">batch_size</strong> parameter depicts the number of examples that will be processed in the <span class="No-Break">same batch:</span></p>
			<pre class="source-code">
from torch_geometric.loader import DataLoader
train_loader = DataLoader(train_data, batch_size=128, shuffle=True)
test_loader = DataLoader(test_data, batch_size=128, shuffle=False)</pre>
			<p>Now, we will define the <span class="No-Break">actual GNN:</span></p>
			<pre class="source-code">
from torch_geometric.nn import global_max_pool as gmp
from torch_geometric.nn import GATConv
from torch.nn import Linear
class GNN(torch.nn.Module):
    def __init__(self,
                 n_in, n_hidden, n_out):
        super().__init__()
        # Graph Convolutions
        self.convolution_1 = GATConv(n_in, n_hidden)
        self.convolution_2 = GATConv(n_hidden, n_hidden)
        self.convolution_3 = GATConv(n_hidden, n_hidden)
        # Readout Layers
        # For news features
        self.lin_news = Linear(n_in, n_hidden)
        # For processing graph features
        self.lin0 = Linear(n_hidden, n_hidden)
        # For pre-final layer for softmax
        self.lin1 = Linear(2*n_hidden, n_out)
    def forward(self, x, edge_index, batch):
        # Graph Convolutions
        h = self.conv1(x, edge_index).relu()
        h = self.conv2(h, edge_index).relu()
        h = self.conv3(h, edge_index).relu()
        # Pooling
        h = gmp(h, batch)
        # Readout
        h = self.lin0(h).relu()
        # Following the UPFD paper, we include raw word2vec embeddings of news
        root = (batch[1:] - batch[:-1]).nonzero(as_tuple=False).view(-1)
        root = torch.cat([root.new_zeros(1), root + 1], dim=0)
        news = x[root]
        news = self.lin_news(news).relu()
        out = self.lin1(torch.cat([h, news], dim=-1))
        return torch.sigmoid(out)</pre>
			<p>Let us deconstruct this<a id="_idIndexMarker632"/> code a little bit. At a high level, we write a class to define our GNN. The class has two functions – a constructor that creates objects and sets initial parameters (<strong class="source-inline">__init__</strong>), and a method that defines the structure of the network by laying out, step by step, the transformations that happen in the forward pass of the <span class="No-Break">neural network.</span></p>
			<p>The constructor function takes in three parameters. These are hyperparameters and are chosen for optimal performance. The first parameter is the number of input features. The second parameter defines the size of the hidden layer. And the final parameter defines the number of neurons in the output layer (typically, either 1, in the case of binary classification, or equal to the number of classes in <span class="No-Break">multi-class classification).</span></p>
			<p>The constructor defines the<a id="_idIndexMarker633"/> layers of the neural network we will need to process our data. First, we define three convolutional layers. Then, we define our readout layers. We will need three separate <span class="No-Break">readout layers:</span></p>
			<ul>
				<li>One for the <span class="No-Break">news features</span></li>
				<li>One for the <span class="No-Break">graph features</span></li>
				<li>One to obtain softmax <span class="No-Break">probability distributions</span></li>
			</ul>
			<p>The <strong class="source-inline">forward()</strong> function defines the steps taken in the forward pass of the neural network. In other words, it describes how the input features are transformed into the <span class="No-Break">output label.</span></p>
			<p>First, we see that the features are processed through the first convolution layer. The output is passed to a second convolution, and the output of this is passed to a third. Then, a pooling layer aggregates the output of this final convolution. The first readout layer will process the pooled output. This completes the processing of <span class="No-Break">graph features.</span></p>
			<p>We will follow the same philosophy outlined in the UPFD paper. We will extract news features as well. Going by the design of the dataset, the first node is the root node in the graph. We extract features from this node and pass them through the readout layer we defined for the <span class="No-Break">news features.</span></p>
			<p>Finally, we concatenate the two – the readout output of the graph features and the readout output of the news features. This concatenated vector is passed through a final readout layer. We transform the output of this using a sigmoid function (as it is a binary classification) to obtain the output class <span class="No-Break">label probability.</span></p>
			<p>Now that we have defined the class and the structure of the GNN, we can actually train the model! We begin by defining some important terms <span class="No-Break">we need:</span></p>
			<pre class="source-code">
if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
model = GNN(train_data.num_features, 128, 1).to(device)
optimizer = torch.optim.Adam(model.parameters(),
                            lr=0.01, weight_decay=0.01)
loss_fnc = torch.nn.BCELoss()</pre>
			<p>First, we check whether a GPU is available via CUDA. If it is, we set <strong class="source-inline">'cuda'</strong> as the device, as opposed to the usual <strong class="source-inline">'cpu'</strong>. Using CUDA or any GPU significantly speeds up the model training, as matrix multiplication, gradient calculation, and backpropagation can be decoupled <span class="No-Break">and parallelized.</span></p>
			<p>Then, we define three<a id="_idIndexMarker634"/> key objects <span class="No-Break">we need:</span></p>
			<ul>
				<li><strong class="bold">Model</strong>: We defined the<a id="_idIndexMarker635"/> GNN class that describes the model structure and operations in the forward pass. We use this class to create a <span class="No-Break">GNN object.</span></li>
				<li><strong class="bold">Optimizer</strong>: We will calculate<a id="_idIndexMarker636"/> the output in the forward pass, compute a loss, and backpropagate with the gradients. The optimizer will help us perform gradient descent to <span class="No-Break">minimize loss.</span></li>
				<li><strong class="bold">Loss</strong>: The core of a <a id="_idIndexMarker637"/>neural network (or any machine learning model) is to minimize a loss so that the predicted values are as close to the actual values as possible. Here, for the loss function, we will use the binary cross-entropy loss, as it is a binary <span class="No-Break">classification problem.</span></li>
			</ul>
			<p>Now, we write the actual <span class="No-Break">training function:</span></p>
			<pre class="source-code">
def train(epoch):
    model.train()
    total_loss = 0
    for data in train_loader:
        data = data.to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index, data.batch)
        loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
        loss.backward()
        optimizer.step()
        total_loss += float(loss)   data.num_graphs
    return total_loss / len(train_loader.dataset)</pre>
			<p>Let us deconstruct <a id="_idIndexMarker638"/>what we have written in the training function. Recall that the basic steps in a neural network are <span class="No-Break">as follows:</span></p>
			<ol>
				<li>Pass the features input to the <span class="No-Break">neural network.</span></li>
				<li>Process the features through the various layers till you reach <span class="No-Break">the output.</span></li>
				<li>Once the output is derived, compute the loss by comparing it with the ground <span class="No-Break">truth label.</span></li>
				<li>Backpropagate the loss and adjust each of the parameters in the layers using <span class="No-Break">gradient descent.</span></li>
				<li>Repeat <em class="italic">steps 1–4</em> for each example to complete <span class="No-Break">one epoch.</span></li>
				<li>Repeat <em class="italic">steps 1–5</em> for multiple epochs to minimize <span class="No-Break">the loss.</span></li>
			</ol>
			<p>The previous training code merely replicates these steps. We read the input features and copy them over to the GPU if needed. When we run them through the model, we basically do the forward pass. Based on the output, we calculate the loss and then initiate the <span class="No-Break">backward propagation.</span></p>
			<p>We will also define a function that calculates metrics for us. Recall that we have previously used the confusion matrix to compute certain metrics, such as accuracy, precision, recall, and the F-1 score. In this example, we will use the built-in modules provided by scikit-learn. These functions will take in the list of predicted and actual values, and compare them to calculate a <span class="No-Break">given metric.</span></p>
			<p>Our metric function is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from sklearn.metrics import accuracy_score, f1_score
def metrics(predicted, actuals):
    preds = torch.round(torch.cat(predicted))
    acts = torch.cat(actuals)
    acc = accuracy_score(preds, acts)
    f1 = f1_score(preds, acts)
    return acc, f1</pre>
			<p>We have already written a function that will train the model. We will now write a similar function but just for inference. We first set the model in the evaluation mode using the <strong class="source-inline">eval()</strong> function. In<a id="_idIndexMarker639"/> the evaluation mode, there is no gradient calculation (as there is no training involved, we do not need the gradients for backpropagation; this saves computation time as well <span class="No-Break">as memory).</span></p>
			<p>Similar to the training function, the evaluation function reads a batch of data and moves it to the GPU if needed and available. It runs the data through the model and calculates the predicted output class. The predicted labels from the model output and actual labels from the ground truth are appended to two separate lists. Based on the model output and the ground truth, we calculate the loss. We repeat this for each batch and sum up the <span class="No-Break">loss values.</span></p>
			<p>At the end, when all the data has been processed, we have the average loss (the total loss divided by the number of data points). We use the actual and predicted lists to calculate the metrics using the metric function <span class="No-Break">defined earlier:</span></p>
			<pre class="source-code">
@torch.no_grad()
def test(epoch):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    for data in test_loader:
        data = data.to(device)
        out = model(data.x, data.edge_index, data.batch)
        loss = loss_fnc(torch.reshape(out, (-1,)), data.y.float())
        total_loss += float(loss)   data.num_graphs
        all_preds.append(torch.reshape(out, (-1,)))
        all_labels.append(data.y.float())
    # Calculate Metrics
    accuracy, f1 = metrics(all_preds, all_labels)
    avg_loss = total_loss/len(test_loader.dataset)
    return avg_loss, accuracy, f1</pre>
			<p>Finally, it is time to<a id="_idIndexMarker640"/> put all these functions into action! So far, we have <span class="No-Break">the following:</span></p>
			<ul>
				<li>A class that defines our <span class="No-Break">GNN structure</span></li>
				<li>A function that will run a training iteration over the <span class="No-Break">GNN model</span></li>
				<li>A function that will run an evaluation/inference iteration on <span class="No-Break">the model</span></li>
				<li>A function that will compute <span class="No-Break">common metrics</span></li>
			</ul>
			<p>We will now use all of these to run our GNN experiment. We will first run the training function, and obtain a loss. We will run the test function and obtain the test loss. We repeat this for multiple epochs. As the epochs progress, we can observe how the loss changes in both the training and <span class="No-Break">test data:</span></p>
			<pre class="source-code">
NUM_EPOCHS = 50
train_losses = []
test_losses = []
for epoch in range(NUM_EPOCHS):
    train_loss = train(epoch)
    test_loss, test_acc, test_f1 = test(epoch)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    print(f'Epoch: {epoch:04d}  ==  Training Loss: {train_loss:.4f}  ==  '
          f'TestLoss: {test_loss:.4f}  ==  TestAcc: {test_acc:.4f}  ==  TestF1: {test_f1:.4f}')</pre>
			<p>This should produce <a id="_idIndexMarker641"/>something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B19327_08_09.jpg" alt="Figure 8.9 – The training loop of the GNN"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The training loop of the GNN</p>
			<p>We can manually observe<a id="_idIndexMarker642"/> the loss trends. However, it would be easier if we could visualize it as a time series <span class="No-Break">over epochs:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
plt.plot(list(range(NUM_EPOCHS)),
      train_losses,
      color = 'blue', label = 'Training Loss')
plt.plot(list(range(NUM_EPOCHS)),
      test_losses,
      color = 'red', label = 'Test Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()</pre>
			<p>You will get the <a id="_idIndexMarker643"/><span class="No-Break">following result:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B19327_08_10.jpg" alt="Figure 8.10 – Loss trends across epochs"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Loss trends across epochs</p>
			<p>We see that as the epochs progressed, the loss steadily decreased for the training data. The test loss has also<a id="_idIndexMarker644"/> decreased, but there are some spikes, probably caused <span class="No-Break">by overfitting.</span></p>
			<h2 id="_idParaDest-136">Playing around with the model</h2>
			<p>As in the previous<a id="_idIndexMarker645"/> chapters, this model also makes several design choices that affect its performance. We will leave it up to you to try out some changes and observe model performance. Some things to be considered are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Hyperparameters</strong>: The learning rate in the training function and the number of epochs are simply design choices. What happens if they are changed? How do the training and test loss trends change? How do the accuracy and F-1 <span class="No-Break">score change?</span></li>
				<li><strong class="bold">Graph layers</strong>: The previous example uses the <strong class="bold">GATConv</strong> layers; however, we can just as easily use other types of layers available in the PyTorch Geometric library. Read up on these layers – which other layers are suitable? How does using a different layer affect <span class="No-Break">the performance?</span></li>
				<li><strong class="bold">Architecture</strong>: We used three convolutional layers and three graph layers in our model. However, we can go as high as we want. What happens if we use five layers? Ten layers? Twenty layers? What if we increase only the convolution layers and not the graph layers, or <span class="No-Break">vice versa?</span></li>
			</ul>
			<p>This completes our discussion of how to model data as graphs, and how GNNs can be used to detect <span class="No-Break">fake news.</span></p>
			<h1 id="_idParaDest-137">Summary</h1>
			<p>A lot of real-world data can be naturally represented as graphs. Graphs are especially important in a social network context where multiple entities (users, posts, or media) are linked together, forming natural graphs. In recent times, the spread of misinformation and fake news is a problem of growing concern. This chapter focused on detecting fake news <span class="No-Break">using GNNs.</span></p>
			<p>We began by first learning some basic concepts about graphs and techniques to learn on graphs. This included using static features extracted from graph analytics (such as degrees and path lengths), node and graph embeddings, and finally, neural message passing, using GNNs. We looked at the UPFD framework and how a graph can be built for a news article, complete with node features that incorporate historical user behavior. Finally, we trained a GNN model to build a graph classifier that detects whether a news article is fake <span class="No-Break">or not.</span></p>
			<p>In the field of cybersecurity, graphs are especially important. This is because attacks are often coordinated from different entities, or directed toward a set of similar targets. Designing a graph and implementing a GNN-based classifier is a valuable skill for data scientists in the <span class="No-Break">security domain.</span></p>
			<p>In the next chapter, we will look at adversarial machine learning and how it can be used to fool or attack machine <span class="No-Break">learning models.</span></p>
		</div>
	</body></html>