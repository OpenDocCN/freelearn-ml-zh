["```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras import regularizers\n```", "```py\nSetSeed = 1\n```", "```py\nCreditCardData = pd.read_csv(\"creditcard.csv\")\n```", "```py\nCountClasses = pd.value_counts(CreditCardData['Class'], sort = True)\nprint(CountClasses)\n```", "```py\n0 284315\n1 492\n```", "```py\nprint(CreditCardData.Amount.describe())\n```", "```py\ncount 284807.000000\nmean      88.349619\nstd      250.120109\nmin        0.000000\n25%        5.600000\n50%       22.000000\n75%       77.165000\nmax    25691.160000\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\n\nData = CreditCardData.drop(['Time'], axis=1)\nData['Amount'] = StandardScaler().fit_transform(Data['Amount'].values.reshape(-1, 1))\n\nprint(Data.Amount.describe())\n```", "```py\ncount   2.848070e+05\nmean    2.913952e-17\nstd     1.000002e+00\nmin    -3.532294e-01\n25%    -3.308401e-01\n50%    -2.652715e-01\n75%    -4.471707e-02\nmax     1.023622e+02\n```", "```py\nXTrain, XTest = train_test_split(Data, test_size=0.3, random_state=SetSeed)\nXTrain = XTrain[XTrain.Class == 0]\nXTrain = XTrain.drop(['Class'], axis=1)\n\nYTest = XTest['Class']\nXTest = XTest.drop(['Class'], axis=1)\n\nXTrain = XTrain.values\nXTest = XTest.values \n```", "```py\nInputDim = XTrain.shape[1]\n\nInputModel = Input(shape=(InputDim,))\nEncodedLayer = Dense(16, activation='relu')(InputModel)\nDecodedLayer = Dense(InputDim, activation='sigmoid')(EncodedLayer)\nAutoencoderModel = Model(InputModel, DecodedLayer)\nAutoencoderModel.summary()\n```", "```py\nNumEpoch = 100\nBatchSize = 32\nAutoencoderModel.compile(optimizer='adam', \n                        loss='mean_squared_error', \n                             metrics=['accuracy'])\n```", "```py\nhistory = AutoencoderModel.fit(XTrain, XTrain,\n                    epochs=NumEpoch,\n                    batch_size=BatchSize,\n                    shuffle=True,\n                    validation_data=(XTest, XTest),\n                    verbose=1,\n                    ).history\n```", "```py\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right');\n```", "```py\nPredData = AutoencoderModel.predict(XTest)\nmse = np.mean(np.power(XTest - PredData, 2), axis=1)\nErrorCreditCardData = pd.DataFrame({'Error': mse,\n                        'TrueClass': YTest})\nErrorCreditCardData.describe()\n```", "```py\nfrom sklearn.metrics import confusion_matrix\n\nthreshold = 3.\nYPred = [1 if e > threshold else 0 for e in ErrorCreditCardData.Error.values]\nConfMatrix = confusion_matrix(ErrorCreditCardData.TrueClass, YPred)\nprint(ConfMatrix)\n```", "```py\n[[83641  1667]\n [ 28     107]]\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\nprint(accuracy_score(ErrorCreditCardData.TrueClass, YPred))\n```", "```py\n0.9801622134054282\n```", "```py\nimport gensim\n```", "```py\nsentences = [['my', 'first', 'book', 'with', 'Packt', 'is', 'on','Matlab'],\n      ['my', 'second', 'book', 'with', 'Packt', 'is', 'on','R'],\n      ['my', 'third', 'book', 'with', 'Packt', 'is', 'on','Python'],\n      ['one', 'more', 'book'],\n      ['is', 'on', 'Python', 'too']]\n```", "```py\nModel1 = gensim.models.Word2Vec(sentences, min_count=1, sg=0)\n```", "```py\nprint(Model1)\n```", "```py\nWord2Vec(vocab=15, size=100, alpha=0.025)\n```", "```py\nwordsM1 = list(Model1.wv.vocab)\nprint(wordsM1)\n```", "```py\n['my', 'first', 'book', 'with', 'Packt', 'is', 'on', 'Matlab', 'second', 'R', 'third', 'Python', 'one', 'more', 'too']\n```", "```py\nprint(Model1.wv['book'])\n```", "```py\nModel2 = gensim.models.Word2Vec(sentences, min_count=1, sg=1)\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist\n```", "```py\n(XTrain, YTrain), (XTest, YTest) = mnist.load_data()\n```", "```py\nXTrain = XTrain.reshape((len(XTrain), np.prod(XTrain.shape[1:])))\nXTest = XTest.reshape((len(XTest), np.prod(XTest.shape[1:])))\n```", "```py\nfrom sklearn.utils import shuffle\nXTrain, YTrain = shuffle(XTrain, YTrain)\nXTrain, YTrain = XTrain[:1000], YTrain[:1000] \n```", "```py\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nXPCATransformed = pca.fit_transform(XTrain)\n```", "```py\nfig, plot = plt.subplots()\nfig.set_size_inches(70, 50)\nplt.prism()\nplot.scatter(XPCATransformed[:, 0], XPCATransformed[:, 1], c=YTrain)\nplot.legend()\nplot.set_xticks(())\nplot.set_yticks(())\nplt.tight_layout()\n```", "```py\nfrom sklearn.manifold import TSNE\nTSNEModel = TSNE(n_components=2)\nXTSNETransformed = TSNEModel.fit_transform(XTrain)\n```", "```py\nfig, plot = plt.subplots()\nfig.set_size_inches(70, 50)\nplt.prism()\nplot.scatter(XTSNETransformed[:, 0], XTSNETransformed[:, 1], c=YTrain)\nplot.set_xticks(())\nplot.set_yticks(())\nplt.tight_layout()\nplt.show()\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras import models\nfrom keras import layers\n```", "```py\nTweetData = pd.read_csv('Tweets.csv')\nTweetData = TweetData.reindex(np.random.permutation(TweetData.index))\nTweetData = TweetData[['text', 'airline_sentiment']]\n```", "```py\nXTrain, XTest, YTrain, YTest = train_test_split(TweetData.text, TweetData.airline_sentiment, test_size=0.3, random_state=11)\n```", "```py\nTkData = Tokenizer(num_words=1000,\n                 filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{\"}~\\t\\n',lower=True, split=\" \")\nTkData.fit_on_texts(XTrain)\nXTrainSeq = TkData.texts_to_sequences(XTrain)\nXTestSeq = TkData.texts_to_sequences(XTest)\n```", "```py\nXTrainSeqTrunc = pad_sequences(XTrainSeq, maxlen=24)\nXTestSeqTrunc = pad_sequences(XTestSeq, maxlen=24)\n```", "```py\nLabelEnc = LabelEncoder()\nYTrainLabelEnc = LabelEnc.fit_transform(YTrain)\nYTestLabelEnc = LabelEnc.transform(YTest)\nYTrainLabelEncCat = to_categorical(YTrainLabelEnc)\nYTestLabelEncCat = to_categorical(YTestLabelEnc)\n```", "```py\nEmbModel = models.Sequential()\nEmbModel.add(layers.Embedding(1000, 8, input_length=24))\nEmbModel.add(layers.Flatten())\nEmbModel.add(layers.Dense(3, activation='softmax'))\n```", "```py\nEmbModel.compile(optimizer='rmsprop'\n                 , loss='categorical_crossentropy'\n                 , metrics=['accuracy'])\n\nEmbHistory = EmbModel.fit(XTrainSeqTrunc\n                 , YTrainLabelEncCat\n                 , epochs=100\n                 , batch_size=512\n                 , validation_data=(XTestSeqTrunc, YTestLabelEncCat)\n                 , verbose=1)\n```", "```py\nprint('Train Accuracy: ', EmbHistory.history['acc'][-1])\nprint('Validation Accuracy: ', EmbHistory.history['val_acc'][-1])\n```", "```py\nTrain Accuracy: 0.9295472287275566\nValidation Accuracy: 0.7625227688874486\n```", "```py\nplt.plot(EmbHistory.history['acc'])\nplt.plot(EmbHistory.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'Validation'], loc='upper left')\nplt.show()\n```", "```py\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.datasets import make_multilabel_classification\n```", "```py\nX, Y = make_multilabel_classification(n_samples=100, n_features=20, n_classes=5, n_labels=2, random_state=1)\n```", "```py\nLDAModel = LatentDirichletAllocation(n_components=5, random_state=1)\n```", "```py\nLDAModel.fit(X) \n```", "```py\nprint(LDAModel.transform(X[-10:]))\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\n```", "```py\nNGData = fetch_20newsgroups(shuffle=True, random_state=7,\n                             remove=('headers', 'footers', 'quotes'))\n```", "```py\nprint(list(NGData.target_names))\n```", "```py\n['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n```", "```py\nNGData = NGData.data[:2000]\n```", "```py\nNGDataVect = CountVectorizer(max_df=0.93, min_df=2,\n                                max_features=1000,\n                                stop_words='english')\n\nNGDataVectModel = NGDataVect.fit_transform(NGData)\n```", "```py\nLDAModel = LatentDirichletAllocation(n_components=10, max_iter=5,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\n```", "```py\nLDAModel.fit(NGDataVectModel)\n```", "```py\nNGDataVectModelFeatureNames = NGDataVect.get_feature_names()\n\nfor topic_idx, topic in enumerate(LDAModel.components_):\n     message = \"Topic #%d: \" % topic_idx\n     message += \" \".join([NGDataVectModelFeatureNames[i]\n     for i in topic.argsort()[:-20 - 1:-1]])\n     print(message)\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim import corpora, models\n```", "```py\nDoc1 = \"Some doctors say that pizza is good for your health.\"\nDoc2 = \"The pizza is good to eat, my sister likes to eat a good pizza, but not to my brother.\"\nDoc3 = \"Doctors suggest that walking can cause a decrease in blood pressure.\"\nDoc4 = \"My brother likes to walk, but my sister don't like to walk.\"\nDoc5 = \"When my sister is forced to walk for a long time she feels an increase in blood pressure.\"\nDoc6 = \"When my brother eats pizza, he has health problems.\"\n```", "```py\nDocList = [Doc1, Doc2, Doc3, Doc4, Doc5, Doc6]\n```", "```py\nTokenizer = RegexpTokenizer(r'\\w+')\nEnStop = get_stop_words('en')\nPStemmer = PorterStemmer()\nTexts = []\n```", "```py\nfor i in DocList:\n```", "```py\n    raw = i.lower()\n    Tokens = Tokenizer.tokenize(raw)\n```", "```py\n    StoppedTokens = [i for i in Tokens if not i in EnStop]\n```", "```py\nStemmedTokens = [PStemmer.stem(i) for i in StoppedTokens]\n```", "```py\nTexts.append(StemmedTokens)\n```", "```py\nDictionary = corpora.Dictionary(Texts)\n```", "```py\nCorpusMat = [Dictionary.doc2bow(text) for text in Texts]\n```", "```py\nLDAModel = models.ldamodel.LdaModel(CorpusMat, num_topics=3, id2word = Dictionary, passes=20)\nprint(LDAModel.print_topics(num_topics=3, num_words=3))\n```", "```py\n[(0, '0.079*\"walk\" + 0.079*\"blood\" + 0.079*\"pressur\"'), \n (1, '0.120*\"like\" + 0.119*\"eat\" + 0.119*\"brother\"'), \n (2, '0.101*\"doctor\" + 0.099*\"health\" + 0.070*\"pizza\"')]\n```"]