<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">A Brief Introduction to Deep Learning and TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to briefly introduce deep learning with some examples based on TensorFlow. This topic is quite complex and needs dedicated books; however, our goal is to allow the reader to understand some basic concepts that can be useful before starting a complete course. In the first section, we're presenting the structure of artificial neural networks and how they can be transformed in a complex computational graph with several different layers. In the second one, instead, we're going to introduce the basic concepts concerning TensorFlow and we'll show some examples based on algorithms already discussed in previous chapters. In the last section, we briefly present Keras, a high-level deep learning framework and we build an example of image classification using a convolutional neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep learning at a glance</h1>
                </header>
            
            <article>
                
<p>Deep learning has become very famous in the last few decades, thanks to hundreds of applications that are changing the way we interact with many electronic (and non-electronic) systems. Speech, text, and image recognition; autonomous vehicles; and intelligent bots (just to name a few) are common applications normally based on deep learning models and have outperformed any previous classical approach.</p>
<p>To better understand what a deep architecture is (considering that this is only a brief introduction), we need to step back and talk about standard artificial neural networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Artificial neural networks</h1>
                </header>
            
            <article>
                
<p>An <strong>artificial neural network</strong> (<strong>ANN</strong>) or simply neural network is a directed structure that connects an input layer with an output one. Normally, all operations are differentiable and the overall vectorial function can be easily written as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="73" src="assets/e7c98e70-61e6-42ad-b597-10f44309e086.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="289" src="assets/f927c9b8-7a07-41c1-82c4-c9c3f783da73.png"/></div>
<p class="CDPAlignLeft CDPAlign">The adjective "neural" comes from two important elements: the internal structure of a basic computational unit and the interconnections among them. Let's start with the former. In the following figure, there's a schematic representation of an artificial neuron:</p>
<div class="CDPAlignCenter CDPAlign"><img height="356" width="438" class="image-border" src="assets/36f956de-358a-4cd5-97e8-d62ac8b2a1af.png"/></div>
<p class="CDPAlignLeft CDPAlign">A neuron core is connected with <em>n</em> input channels, each of them characterized by a synaptic weight <em>w<sub>i</sub></em>. The input is split into its components and they are multiplied by the corresponding weight and summed. An optional bias can be added to this sum (it works like another weight connected to a unitary input). The resulting sum is filtered by an activation function <em>f<sub>a</sub></em> (for example a sigmoid, if you recall how a logistic regression works) and the output is therefore produced. In <a href="9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml" target="_blank">Chapter 5</a>, <em>Logistic Regression</em>, we also discussed perceptrons (the first artificial neural networks), which correspond exactly to this architecture with a binary-step activation function. On the other hand, even a logistic regression can be represented as a single neuron neural network, where <em>f<sub>a</sub>(x)</em> is a sigmoid. The main problem with this architecture is that it's intrinsically linear because the output is always a function of the dot product between the input vector and the weight one. You already know all the limitations that such a system has; therefore it's necessary to step forward and create the first <strong>Multi-layer Perceptron</strong> (<strong>MLP</strong>). In the following figure, there's a schematic representation of an MLP with an n-dimensional input, <em>p</em> hidden neurons, and a <em>k</em>-dimensional output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="375" width="397" class="image-border" src="assets/535b5780-b621-46d8-b69f-e5175b4e8b4f.png"/></div>
<p class="CDPAlignLeft CDPAlign">There are three layers (even though the number can be larger): the input layer, which receives the input vectors; a hidden layer; and the output one, which is responsible for producing the output. As you can see, every neuron is connected to all the neurons belonging the next layer and now we have two weight matrices, <em>W = (w<sub>ij</sub>)</em><strong> </strong>and <em>H = (h<sub>jk</sub>)</em>, using the convention that the first index is referred to the previous layer and the second to the following one.</p>
<p class="CDPAlignLeft CDPAlign">Therefore, the net input to each hidden neuron and the corresponding output is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="100" width="387" src="assets/b2ac7fde-a8f3-4379-a6d2-16798579d9bb.png"/></div>
<p class="CDPAlignLeft CDPAlign">In the same way, we can compute the network output:</p>
<div class="CDPAlignCenter CDPAlign"><img height="93" width="494" src="assets/5cc3888c-04f9-4496-a850-bf4db9162ca7.png"/></div>
<p class="CDPAlignLeft CDPAlign">As you can see, the network has become highly non-linear and this feature allows us to model complex scenarios that were impossible to manage with linear methods. But how can we determine the values for all synaptic weights and biases? The most famous algorithm is called <strong>back-propagation</strong> and it works in a very simple way (the only important assumption is that both <em>f<sub>a</sub>(x)</em> must be differentiable).</p>
<p class="CDPAlignLeft CDPAlign">First of all, we need to define an error (loss) function; for many classification tasks, it can be the total squared error:</p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="257" src="assets/f8837b97-25d9-4e7d-8fa8-e72caba6d0ae.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here we have assumed to have <em>N</em> input samples. Expanding it, we obtain:</p>
<div class="CDPAlignCenter CDPAlign"><img height="64" width="420" src="assets/06d71310-d0c2-4823-be4a-728e2ddbcf32.png"/></div>
<p class="CDPAlignLeft CDPAlign">This function depends on all variables (weights and biases), but we can start from the bottom and consider first only <em>h</em><sub><em>jk</em> </sub>(for simplicity I'm not considering the biases as normal weights); therefore we can compute the gradients and update the weights:</p>
<div class="CDPAlignCenter CDPAlign"><strong><sub><img height="54" width="444" src="assets/ac32ae69-659c-462f-abd5-92a88181bd25.png"/> </sub></strong></div>
<p class="CDPAlignLeft CDPAlign">In the same way, we can derive the gradient with respect to <em>w<sub>ij</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="113" width="513" src="assets/4d120e71-d46d-4944-ae88-f7eb430334c6.png"/></div>
<p class="CDPAlignLeft CDPAlign">As you can see, the term alpha (which is proportional to the error delta) is back-propagated from the output layer to the hidden one. If there are many hidden layers, this procedure should be repeated recursively until the first layer. The algorithm adopts the gradient descent method; therefore it updates the weights iteratively until convergence:</p>
<div class="CDPAlignCenter CDPAlign"><img height="90" width="167" src="assets/5173acce-f8f1-41f0-8077-137731ad6016.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here, the parameter <kbd>eta</kbd> (Greek letter in the formula) is the learning rate.</p>
<p class="CDPAlignLeft CDPAlign">In many real problems, the stochastic gradient descent method is adopted (read <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">https://en.wikipedia.org/wiki/Stochastic_gradient_descent</a>,<a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent"/> for further information), which works with batches of input samples, instead of considering the entire dataset. Moreover, many optimizations can be employed to speed up the convergence, but they are beyond the scope of this book. In Goodfellow I., Bengio Y., Courville A., <em>Deep Learning</em>, MIT Press<em>,</em> the reader can find all the details about the majority of them. For our purposes, it's important to know that we can build a complex network and, after defining a global loss function, optimize all the weights with a standard procedure. In the section dedicated to TensorFlow, we're going to show an example of MLP, but we're not implementing the learning algorithm because, luckily, all optimizers have already been built and can be applied to every architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep architectures</h1>
                </header>
            
            <article>
                
<p>MLPs are powerful, but their expressiveness is limited by the number and the nature of the layers. Deep learning architectures, on the other side, are based on a sequence of heterogeneous layers which perform different operations organized in a computational graph. The output of a layer, correctly reshaped, is fed into the following one, until the output, which is normally associated with a loss function to optimize. The most interesting applications have been possible thanks to this stacking strategy, where the number of variable elements (weights and biases) can easily reach over 10 million; therefore, the ability to capture small details and generalize them exceeds any expectations. In the following section, I'm going to introduce briefly the most important layer types.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layers</h1>
                </header>
            
            <article>
                
<p>A fully connected (sometimes called dense) layer is made up of n neurons and each of them receives all the output values coming from the previous layer (like the hidden layer in a MLP). It can be characterized by a weight matrix, a bias vector, and an activation function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="117" src="assets/647ecc9f-9608-4cee-bfd3-5b1c2f8ec657.png"/></div>
<p>They are normally used as intermediate or output layers, in particular when it's necessary to represent a probability distribution. For example, a deep architecture could be employed for an image classification with <em>m</em> output classes. In this case, the <em>softmax</em> activation function allows having an output vector where each element is the probability of a class (and the sum of all outputs is always normalized to 1.0). In this case, the argument is considered as a <strong>logit</strong> or the logarithm of a probability:</p>
<div class="CDPAlignCenter CDPAlign"><img height="33" width="127" src="assets/7898100d-7f45-46c5-8c3e-aa550424f86b.png"/></div>
<p><em>W<sub>i</sub></em> is the i-th row of <em>W</em><strong>. </strong>The probability of a class <em>y<strong><sub>i</sub></strong></em> is obtained by applying the <em>softmax</em> function to each <em>logit</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="51" width="241" src="assets/76e5ef7e-e437-44d0-8258-7215cffb1605.png"/></div>
<p>This type of output can <span>easily</span><span> </span><span>be trained using a cross-entropy loss function, as already discussed for logistic regression.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional layers</h1>
                </header>
            
            <article>
                
<p>Convolutional layers are normally applied to bidimensional inputs (even though they can be used for vectors and 3D matrices) and they became particularly famous thanks to their extraordinary performance in image classification tasks. They are based on the discrete convolution of a small kernel <em>k</em> with a bidimensional input (which can be the output of another convolutional layer):</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="320" src="assets/3d8eb5e0-ef8d-42c1-bdba-0342cd94c16f.png"/></div>
<p>A layer is normally made up of n fixed-size kernels, and their values are considered as weights to learn using a back-propagation algorithm. A convolutional architecture, in most cases, starts with layers with few larger kernels (for example, 16 (8 x 8) matrices) and feeds their output to other layers with a higher number of smaller kernels (32 (5 x 5), 128 (4 x 4), and 256 (3 x 3)). In this way, the first layers should learn to capture more generic features (such as orientation), while the following ones will be trained to capture smaller and smaller elements (such as the position of eyes, nose, and mouth in a face). The output of the last convolutional layer is normally flattened (transformed into a 1D vector) and used as input for one or more fully connected layers.</p>
<p>In the following figure, there's a schematic representation of a convolution over a picture:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/3f6d3092-71f1-456f-989a-500996f968b2.png"/></div>
<p>Each square set of 3 x 3 pixels is convoluted with a Laplacian kernel and transformed into a single value, which corresponds to the sum of upper, lower, left, and right pixels (considering the centre) minus four times the central one. We're going to see a complete example using this kernel in the following section. </p>
<p>To reduce the complexity when the number of convolutions is very high, one or more <strong>pooling layers</strong> can be employed. Their task is to transform each group of input points (pixels in an image) into a single value using a predefined strategy. The most common pooling layers are:</p>
<ul>
<li><strong>Max pooling</strong>: Every bidimensional group of (<em>m</em> x <em>n</em>) pixels is transformed into a single pixel whose value is the greatest in the group.</li>
<li><strong>Average pooling</strong>: Every bidimensional group of (<em>m</em> x <em>n</em>) pixels is transformed into a single pixel whose value is the average of the group.</li>
</ul>
<p>In this way, the dimensionality of the original matrix can be reduced with a loss of information, but that can often be discarded (in particular in the first layers where the granularity of the features is coarse). Another important category of layers are the <strong>zero-padding</strong> ones. They work by adding null values (0) before and after the input (1D) or at the left, right, top and bottom side of 2D input. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropout layers</h1>
                </header>
            
            <article>
                
<p>A dropout layer is used to prevent overfitting of the network by <span>randomly</span><span> </span><span>setting a fixed number of input elements to 0. This layer is adopted during the training phase, but it's normally deactivated during test, validation, and production phases. Dropout networks can exploit higher learning rates, moving in different directions on the loss surface (setting to zero a few random input values in the hidden layers is equivalent to training different sub-models) and excluding all the error-surface areas that don't lead to a consistent optimization. Dropout is very useful in very big models, where it increases the overall performance and reduces the risk of freezing some weights and overfitting the model. </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks</h1>
                </header>
            
            <article>
                
<p>A recurrent layer is made up of particular neurons that present recurrent connections so as to bind the state at time <em>t</em> to its previous values (in general, only one). This category of computational cells is particularly useful when it's necessary to capture the temporal dynamics of an input sequence. In many situations, in fact, we expect an output value that must be correlated with the history of the corresponding inputs. But an MLP, as well as the other models that we've discussed, are stateless. Therefore, their output is determined only by the current input. RNNs overcome this problem by providing an internal memory which can capture short-term and long-term dependencies.</p>
<p>The most common cells are <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) and <strong>Gated Recurrent Unit</strong> (<strong>GRU</strong>) and they can both be trained using a standard back-propagation approach. As this is only an introduction, I cannot go deeper (RNN mathematical complexity is non-trivial); however, it's useful to remember that whenever a temporal dimension must be included in a deep model, RNNs offer stable and powerful support.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a computational framework created by Google and has become one of the most diffused deep-learning toolkits. It can work with both CPUs and GPUs and already implements most of the operations and structures required to build and train a complex model. TensorFlow can be installed as a Python package on Linux, Mac, and Windows (with or without GPU support); however, I suggest you follow the instructions provided on the website (the link can be found in the infobox at the end of this chapter) to avoid common mistakes.</p>
<p>The main concept behind TensorFlow is the computational graph, or a set of subsequent operations that transform an input batch into the desired output. In the following figure, there's a schematic representation of a graph:</p>
<div class="CDPAlignCenter CDPAlign"><img height="359" width="419" class="image-border" src="assets/ec9a5336-23d5-4470-acb6-735806039a53.png"/></div>
<p>Starting from the bottom, we have two input nodes (<strong>a</strong> and <strong>b</strong>), a transpose operation (that works on <strong>b</strong>), a matrix multiplication and a mean reduction. The <strong>init</strong> block is a separate operation, which is formally part of the graph, but it's not directly connected to any other node; therefore it's autonomous (indeed, it's a global initializer).  </p>
<p>As this one is only a brief introduction, it's useful to list all of the most important strategic elements needed to work with TensorFlow so as to be able to build a few simple examples that can show the enormous potential of this framework:</p>
<ul>
<li><strong>Graph</strong>: This represents the computational structure that connects a generic input batch with the output tensors through a directed network made of operations. It's defined as a <kbd>tf.Graph()</kbd> instance and normally used with a Python context manager.</li>
<li><strong>Placeholder</strong>: This is a reference to an external variable, which must be explicitly supplied when it's requested for the output of an operation that uses it directly or indirectly. For example, a placeholder can represent a variable <kbd>x</kbd>, which is first transformed into its squared value and then summed to a constant value. The output is then <kbd>x<sup>2</sup>+c</kbd>, which is materialized by passing a concrete value for <kbd>x</kbd>. It's defined as a <kbd>tf.placeholder()</kbd> instance.</li>
<li><strong>Variable</strong>: An internal variable used to store values which are updated by the algorithm. For example, a variable can be a vector containing the weights of a logistic regression. It's normally initialized before a training process and automatically modified by the built-in optimizers. It's defined as a <kbd>tf.Variable()</kbd> instance. A variable can also be used to store elements which must not be considered during training processes; in this case, it must be declared with the parameter <kbd>trainable=False</kbd>.</li>
<li><strong>Constant</strong>: A constant value defined as a <kbd>tf.constant()</kbd> instance.</li>
<li><strong>Operation</strong>: A mathematical operation that can work with placeholders, variables, and constants. For example, the multiplication of two matrices is an operation defined as <kbd>tf.matmul(A, B)</kbd>. Among all operations, gradient calculation is one of the most important. TensorFlow allows determining the gradients starting from a determined point in the computational graph, until the origin or another point that must be logically before it. We're going to see an example of this operation.</li>
<li><strong>Session</strong>: This is a sort of wrapper-interface between TensorFlow and our working environment (for example, Python or C++). When the evaluation of a graph is needed, this macro-operation will be managed by a session, which must be fed with all placeholder values and will produce the required outputs using the requested devices. For our purposes, it's not necessary to go deeper into this concept; however, I invite the reader to retrieve further information from the website or from one of the resources listed at the end of this chapter. It's declared as an instance of <kbd>tf.Session()</kbd> or, as we're going to do, an instance of <kbd>tf.InteractiveSession()</kbd>. This type of session is particularly useful when working with notebooks or shell commands, because it places itself automatically as the default one.</li>
<li><strong>Device</strong>: A physical computational device, such as a CPU or a GPU. It's declared explicitly through an instance of the class <kbd>tf.device()</kbd> and used with a context manager. When the architecture contains more computational devices, it's possible to split the jobs so as to parallelize many operations. If no device is specified, TensorFlow will use the default one (which is the main CPU or a suitable GPU if all the necessary components are installed).</li>
</ul>
<p>We can now analyze some simple examples using these concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing gradients</h1>
                </header>
            
            <article>
                
<p>The option to compute the gradients of all output tensors with respect to any connected input or node is one of the most interesting features of TensorFlow, because it allows us to create learning algorithms without worrying about the complexity of all transformations. In this example, we first define a linear dataset representing the function <em>f(x) = x</em> in the range (-100, 100):</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; nb_points = 100</strong><br/><strong>&gt;&gt;&gt; X = np.linspace(-nb_points, nb_points, 200, dtype=np.float32)</strong></pre>
<p>The corresponding plot is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="285" width="386" class="image-border" src="assets/f6bdb853-a204-4bfa-87e6-ad1790513c93.png"/></div>
<p class="CDPAlignLeft CDPAlign">Now we want to use TensorFlow to compute:</p>
<div class="CDPAlignCenter CDPAlign"><img height="109" width="96" src="assets/e7d221f9-5369-4dd5-b9b2-d1f40ae217d2.png"/></div>
<p class="CDPAlignLeft CDPAlign">The first step is defining a graph:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>import tensorflow as tf</strong><br/><br/><strong>&gt;&gt;&gt; graph = tf.Graph()</strong></pre>
<p>Within the context of this graph, we can define our input placeholder and other operations:</p>
<pre><strong>&gt;&gt;&gt; with graph.as_default():</strong><br/><strong>&gt;&gt;&gt;    Xt = tf.placeholder(tf.float32, shape=(None, 1), name='x')</strong><br/><strong>&gt;&gt;&gt;    Y = tf.pow(Xt, 3.0, name='x_3')</strong><br/><strong>&gt;&gt;&gt;    Yd = tf.gradients(Y, Xt, name='dx')</strong><br/><strong>&gt;&gt;&gt;    Yd2 = tf.gradients(Yd, Xt, name='d2x')</strong></pre>
<p>A placeholder is generally defined with a type (first parameter), a shape, and an optional name. We've decided to use a <kbd>tf.float32</kbd> type because this is the only type also supported by GPUs. Selecting <kbd>shape=(None, 1)</kbd> means that it's possible to use any bidimensional vectors with the second dimension equal to 1.</p>
<p>The first operation computes the third power if <kbd>Xt</kbd> is working on all elements. The second operation computes all the gradients of <kbd>Y</kbd> with respect to the input placeholder <kbd>Xt</kbd>. The last operation will repeat the gradient computation, but in this case, it uses <kbd>Yd</kbd>, which is the output of the first gradient operation.</p>
<p>We can now pass some concrete data to see the results. The first thing to do is create a session connected with this graph:</p>
<pre><strong>&gt;&gt;&gt; session = tf.InteractiveSession(graph=graph)</strong></pre>
<p>By using this session, we ask any computation using the method <kbd>run()</kbd>. All the input parameters must be supplied through a feed-dictionary, where the key is the placeholder, while the value is the actual array:</p>
<pre><strong>&gt;&gt;&gt; X2, dX, d2X = session.run([Y, Yd, Yd2], feed_dict={Xt: X.reshape((nb_points*2, 1))})</strong></pre>
<p>We needed to reshape our array to be compliant with the placeholder. The first argument of <kbd>run()</kbd> is a list of tensors that we want to be computed. In this case, we need all operation outputs. The plot of each of them is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/5184642f-a567-4058-8295-14625558a3dd.png"/></div>
<p class="CDPAlignLeft CDPAlign">As expected, they represent respectively: <em>x<sup>3</sup></em>, <em>3x<sup>2</sup></em>, and <em>6x</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Now we can try a more complex example implementing a logistic regression algorithm. The first step, as usual, is creating a dummy dataset:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 500</strong><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=2, n_redundant=0, n_classes=2)</strong></pre>
<p>The dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="381" width="484" class="image-border" src="assets/a090ec57-21aa-417d-8bee-4aeee20ed613.png"/></div>
<p class="CDPAlignLeft CDPAlign">At this point, we can create the graph and all placeholders, variables, and operations:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>import tensorflow as tf</strong><br/><br/><strong>&gt;&gt;&gt; graph = tf.Graph()</strong><br/><br/><strong>&gt;&gt;&gt; with graph.as_default():</strong><br/><strong>&gt;&gt;&gt;    Xt = tf.placeholder(tf.float32, shape=(None, 2), name='points')</strong><br/><strong>&gt;&gt;&gt;    Yt = tf.placeholder(tf.float32, shape=(None, 1), name='classes')</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    W = tf.Variable(tf.zeros((2, 1)), name='weights')</strong><br/><strong>&gt;&gt;&gt;    bias = tf.Variable(tf.zeros((1, 1)), name='bias')</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    Ye = tf.matmul(Xt, W) + bias</strong><br/><strong>&gt;&gt;&gt;    Yc = tf.round(tf.sigmoid(Ye))</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Ye, labels=Yt))</strong><br/><strong>&gt;&gt;&gt;    training_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)</strong></pre>
<p>The placeholder <kbd>Xt</kbd> is needed for the points, while <kbd>Yt</kbd> represents the labels. At this point, we need to involve a couple of variables: if you remember, they store values that are updated by the training algorithm. In this case, we need a weight vector <kbd>W</kbd> (with two elements) and a single <kbd>bias</kbd>. When a variable is declared, its initial value must be provided; we've decided to set both to zero using the function <kbd>tf.zeros()</kbd>, which accepts as argument the shape of the desired tensor. </p>
<p>Now we can compute the output (if you don't remember how logistic regression works, please step back to <a href="9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml" target="_blank">Chapter 5</a>, <em>Logistic Regression</em>) in two steps: first the sigmoid exponent <kbd>Ye</kbd> and then the actual binary output <kbd>Yc</kbd>, which is obtained by rounding the sigmoid value. The training algorithm for a logistic regression minimizes the negative log-likelihood, which corresponds to the cross-entropy between the real distribution <kbd>Y</kbd> and <kbd>Yc</kbd>. It's easy to implement this loss function; however, the function <kbd>tf.log()</kbd> is numerically unstable (when its value becomes close to zero, it tends to negative infinity and yields a <kbd>NaN</kbd> value); therefore, TensorFlow has implemented a more robust function, <kbd>tf.nn.sigmoid_cross_entropy_with_logits()</kbd>, which computes the cross-entropy assuming the output is produced by a sigmoid. It takes two parameters, the <kbd>logits</kbd> (which corresponds to the exponent <kbd>Ye</kbd>) and the target <kbd>labels</kbd>, that are stored in <kbd>Yt</kbd>.</p>
<p>Now we can work with one of the most powerful TensorFlow features: the training optimizers. After defining a loss function, it will be dependent on placeholders, constants, and variables. A training optimizer (such as <kbd>tf.train.GradientDescentOptimizer()</kbd>), through its method <kbd>minimize()</kbd>, accepts the loss function to optimize. Internally, according to every specific algorithm, it will compute the gradients of the loss function with respect to all trainable variables and will apply the corresponding corrections to the values. The parameter passed to the optimizer is the learning rate.</p>
<p>Therefore, we have defined an extra operation called <kbd>training_step</kbd>, which corresponds to a single stateful update step. It doesn't matter how complex the graph is; all trainable variables involved in a loss function will be optimized with a single instruction.</p>
<p>Now it's time to train our logistic regression. The first thing to do is to ask TensorFlow to initialize all variables so that they are ready when the operations have to work with them:</p>
<pre><strong>&gt;&gt;&gt; session = tf.InteractiveSession(graph=graph)</strong><br/><strong>&gt;&gt;&gt; tf.global_variables_initializer().run()</strong></pre>
<p>At this point, we can create a simple training loop (it should be stopped when the loss stops decreasing; however, we have a fixed number of iterations):</p>
<pre><strong>&gt;&gt;&gt; feed_dict = {</strong><br/><strong>&gt;&gt;&gt;    Xt: X,</strong><br/><strong>&gt;&gt;&gt;    Yt: Y.reshape((nb_samples, 1))</strong><br/><strong>&gt;&gt;&gt; }</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(5000):</strong><br/><strong>&gt;&gt;&gt;    loss_value, _ = session.run([loss, training_step], feed_dict=feed_dict)</strong><br/><strong>&gt;&gt;&gt;    if i % 100 == 0:</strong><br/><strong>&gt;&gt;&gt;    print('Step %d, Loss: %.3f' % (i, loss_value))<br/>Step 0, Loss: 0.269<br/>Step 100, Loss: 0.267<br/>Step 200, Loss: 0.265<br/>Step 300, Loss: 0.264<br/>Step 400, Loss: 0.263<br/>Step 500, Loss: 0.262<br/>Step 600, Loss: 0.261<br/>Step 700, Loss: 0.260<br/>Step 800, Loss: 0.260<br/>Step 900, Loss: 0.259</strong><br/><strong>...</strong></pre>
<p>As you can see, at each iteration we ask TensorFlow to compute the loss function and a training step, and we always pass the same dictionary containing <kbd>X</kbd> and <kbd>Y</kbd>. At the end of this loop, the loss function is stable and we can check the quality of this logistic regression by plotting the separating hyperplane:</p>
<div class="CDPAlignCenter CDPAlign"><img height="500" width="509" class="image-border" src="assets/61e3843b-4a4d-418f-9db2-7ae07aa0b78d.png"/></div>
<p class="CDPAlignLeft CDPAlign">The result is approximately equivalent to the one obtained with the scikit-learn implementation. If we want to know the values of both coefficients (weights) and intercept (bias), we can ask TensorFlow to retrieve them by calling the method <kbd>eval()</kbd> on each variable:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; Wc, Wb = W.eval(), bias.eval()</strong><br/><br/><strong>&gt;&gt;&gt; print(Wc)</strong><br/><strong>[[-1.16501403]<br/> [ 3.10014033]]</strong><br/><br/><strong>&gt;&gt;&gt; print(Wb)</strong><br/><strong>[[-0.12583369]]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classification with a multi-layer perceptron</h1>
                </header>
            
            <article>
                
<p>We can now build an architecture with two dense layers and train a classifier for a more complex dataset. Let's start by creating it:</p>
<pre><strong>from sklearn.datasets import make_classification</strong><br/><br/><strong>&gt;&gt;&gt; nb_samples = 1000</strong><br/><strong>&gt;&gt;&gt; nb_features = 3</strong><br/><br/><strong>&gt;&gt;&gt; X, Y = make_classification(n_samples=nb_samples, n_features=nb_features, </strong><br/><strong>&gt;&gt;&gt; n_informative=3, n_redundant=0, n_classes=2, n_clusters_per_class=3)</strong></pre>
<p>Even if we have only two classes, the dataset has three features and three clusters per class; therefore it's almost impossible that a linear classifier can separate it with very high accuracy. A plot of the dataset is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/76c390a4-fc1c-4d6b-95ad-be58f677d82b.png"/></div>
<p class="CDPAlignLeft CDPAlign">For benchmarking purposes, it's useful to test a logistic regression:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from sklearn.model_selection import train_test_split</strong><br/><strong>from sklearn.linear_model import LogisticRegression</strong><br/><br/><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)</strong><br/><br/><strong>&gt;&gt;&gt; lr = LogisticRegression()</strong><br/><strong>&gt;&gt;&gt; lr.fit(X_train, Y_train)</strong><br/><strong>&gt;&gt;&gt; print('Score: %.3f' % lr.score(X_test, Y_test))</strong><br/><strong>Score: 0.715</strong></pre>
<p>The score computed on the test set is about 71%, which is not really bad but below an acceptable threshold. Let's try with an MLP with 50 hidden neurons (with hyperbolic tangent activation) and 1 sigmoid output neuron. The hyperbolic tangent is:</p>
<div class="CDPAlignCenter CDPAlign"><img height="66" width="150" src="assets/301cace9-c3c1-40e4-acb8-315d536b31f9.png"/></div>
<p>And it's bounded asymptotically between -1.0 and 1.0.</p>
<p>We are not going to implement each layer manually, but we're using the built-in class <kbd>tf.contrib.layers.fully_connected()</kbd>. It accepts the input tensor or placeholder <span>as the first argument</span><span> </span><span>and the number of layer-output neurons </span><span>as the second one</span><span>. The activation function can be specified using the attribute</span> <kbd>activation_fn</kbd><span>:</span></p>
<pre><strong>import tensorflow as tf</strong><br/><strong>import tensorflow.contrib.layers as tfl</strong><br/><br/><strong>&gt;&gt;&gt; graph = tf.Graph()</strong><br/><br/><strong>&gt;&gt;&gt; with graph.as_default():</strong><br/><strong>&gt;&gt;&gt;    Xt = tf.placeholder(tf.float32, shape=(None, nb_features), name='X')</strong><br/><strong>&gt;&gt;&gt;    Yt = tf.placeholder(tf.float32, shape=(None, 1), name='Y')</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    layer_1 = tfl.fully_connected(Xt, num_outputs=50, activation_fn=tf.tanh)</strong><br/><strong>&gt;&gt;&gt;    layer_2 = tfl.fully_connected(layer_1, num_outputs=1,</strong><br/><strong>&gt;&gt;&gt;                                  activation_fn=tf.sigmoid)</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    Yo = tf.round(layer_2)</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    loss = tf.nn.l2_loss(layer_2 - Yt)</strong><br/><strong>&gt;&gt;&gt;    training_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)</strong></pre>
<p>As in the previous example, we have defined two placeholders, <kbd>Xt</kbd> and <kbd>Yt</kbd>, and two fully connected layers. The first one accepts as input <kbd>Xt</kbd> and has 50 output neurons (with <kbd>tanh</kbd> activation), while the second accepts as input the output of the previous layer (<kbd>layer_1</kbd>) and has only one sigmoid neuron, representing the class. The rounded output is provided by <kbd>Yo</kbd>, while the loss function is the total squared error, and it's implemented using the function <kbd>tf.nn.l2_loss()</kbd> computed on the difference between the output of the network (<kbd>layer_2</kbd>) and the target class placeholder <kbd>Yt</kbd>. The training step is implemented using a standard gradient descent optimizer, as for the logistic regression example.</p>
<p>We can now implement a training loop, splitting our dataset into a fixed number of batches (the number of samples is defined in the variable <kbd>batch_size</kbd>) and repeating a complete cycle for <kbd>nb_epochs</kbd> epochs:</p>
<pre><strong>&gt;&gt;&gt; session = tf.InteractiveSession(graph=graph)</strong><br/><strong>&gt;&gt;&gt; tf.global_variables_initializer().run()</strong><br/><br/><strong>&gt;&gt;&gt; nb_epochs = 200</strong><br/><strong>&gt;&gt;&gt; batch_size = 50</strong><br/><br/><strong>&gt;&gt;&gt; for e in range(nb_epochs):</strong><br/><strong>&gt;&gt;&gt;    total_loss = 0.0</strong><br/><strong>&gt;&gt;&gt;    Xb = np.ndarray(shape=(batch_size, nb_features), dtype=np.float32)</strong><br/><strong>&gt;&gt;&gt;    Yb = np.ndarray(shape=(batch_size, 1), dtype=np.float32)</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;    for i in range(0, X_train.shape[0]-batch_size, batch_size):</strong><br/><strong>&gt;&gt;&gt;       Xb[:, :] = X_train[i:i+batch_size, :]</strong><br/><strong>&gt;&gt;&gt;       Yb[:, 0] = Y_train[i:i+batch_size]</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;       loss_value, _ = session.run([loss, training_step], </strong><br/><strong>&gt;&gt;&gt;                                   feed_dict={Xt: Xb, Yt: Yb})</strong><br/><strong>&gt;&gt;&gt;       total_loss += loss_value</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;        Y_predicted = session.run([Yo], </strong><br/><strong>&gt;&gt;&gt;               feed_dict={Xt: X_test.reshape((X_test.shape[0], nb_features))})</strong><br/><strong>&gt;&gt;&gt;        accuracy = 1.0 -</strong><br/><strong>&gt;&gt;&gt;            (np.sum(np.abs(np.array(Y_predicted[0]).squeeze(axis=1) -Y_test)) /</strong><br/><strong>&gt;&gt;&gt;            float(Y_test.shape[0]))</strong><br/><strong>&gt;&gt;&gt; </strong><br/><strong>&gt;&gt;&gt;        print('Epoch %d) Total loss: %.2f - Accuracy: %.2f' % </strong><br/><strong>&gt;&gt;&gt;              (e, total_loss, accuracy))</strong><br/><br/><strong>Epoch 0) Total loss: 78.19 - Accuracy: 0.66<br/>Epoch 1) Total loss: 75.02 - Accuracy: 0.67<br/>Epoch 2) Total loss: 72.28 - Accuracy: 0.68<br/>Epoch 3) Total loss: 68.52 - Accuracy: 0.71<br/>Epoch 4) Total loss: 63.50 - Accuracy: 0.79<br/>Epoch 5) Total loss: 57.51 - Accuracy: 0.84</strong><br/><strong>...</strong><br/><strong>Epoch 195) Total loss: 15.34 - Accuracy: 0.94<br/>Epoch 196) Total loss: 15.32 - Accuracy: 0.94<br/>Epoch 197) Total loss: 15.31 - Accuracy: 0.94<br/>Epoch 198) Total loss: 15.29 - Accuracy: 0.94<br/>Epoch 199) Total loss: 15.28 - Accuracy: 0.94</strong></pre>
<p class="CDPAlignLeft CDPAlign">As it's possible to see, without particular attention to all details, the accuracy computed on the test set is 94%. This is an acceptable value, considering the structure of the dataset. In Goodfellow I., Bengio Y., Courville A., <em>Deep Learning</em>, MIT Press<em>, </em>the reader will find details of many important concepts that can still improve the performance and speed up the convergence process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image convolution</h1>
                </header>
            
            <article>
                
<p>Even if we're not building a complete deep learning model, we can test how convolution works with a simple example. The input image we're using is already provided by <kbd>SciPy</kbd>:</p>
<pre><strong>from scipy.misc import face</strong><br/><br/><strong>&gt;&gt;&gt; img = face(gray=True)</strong></pre>
<p>The original picture is shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="345" class="image-border" src="assets/22e0ad00-5f44-4708-acc4-f8e0addbcdde.png"/></div>
<p>We're going to apply a Laplacian filter, which emphasizes the boundary of each shape:</p>
<pre class="mce-root"><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; kernel = np.array(</strong><br/><strong>&gt;&gt;&gt;    [[0, 1, 0],</strong><br/><strong>&gt;&gt;&gt;     [1, -4, 0],</strong><br/><strong>&gt;&gt;&gt;     [0, 1, 0]], </strong><br/><strong>&gt;&gt;&gt;    dtype=np.float32)</strong><br/><br/><strong>&gt;&gt;&gt; cfilter = np.zeros((3, 3, 1, 1), dtype=np.float32)</strong><br/><strong>&gt;&gt;&gt; cfilter[:, :, 0, 0] = kernel<br/></strong></pre>
<p>The kernel must be repeated twice because the TensorFlow convolution function <kbd>tf.nn.conv2d</kbd> expects an input and an output filter. We can now build the graph and test it:</p>
<pre><strong>import tensorflow as tf</strong><br/><br/><strong>&gt;&gt;&gt; graph = tf.Graph()</strong><br/><br/><strong>&gt;&gt;&gt; with graph.as_default():</strong><br/><strong>&gt;&gt;&gt;    x = tf.placeholder(tf.float32, shape=(None, 768, 1024, 1), name='image')</strong><br/><strong>&gt;&gt;&gt;    f = tf.constant(cfilter)</strong><br/><strong>    </strong><br/><strong>&gt;&gt;&gt;    y = tf.nn.conv2d(x, f, strides=[1, 1, 1, 1], padding='SAME')</strong><br/><br/><strong>&gt;&gt;&gt; session = tf.InteractiveSession(graph=graph)</strong><br/><br/><strong>&gt;&gt;&gt; c_img = session.run([y], feed_dict={x: img.reshape((1, 768, 1024, 1))})</strong><br/><strong>&gt;&gt;&gt; n_img = np.array(c_img).reshape((768, 1024))</strong></pre>
<p>The parameters <kbd>strides</kbd> is a four-dimensional vector (each value corresponds to the input dimensions, so the first is the batch and the last one is the number of channels) that specifies how many pixels the sliding window must shift. In this case, we want to cover all the image shifting pixel to pixel. The parameter <kbd>padding</kbd> determines how the new dimensions must be computed and whether it's necessary to apply a zero padding. In our case, we're using the value <kbd>SAME</kbd>, which computes the dimensions by rounding off to the next integer the original dimensions divided by the corresponding strides value (as the latter are both 1.0, the resulting image size will be exactly like the original one).</p>
<p>The output image is shown here:</p>
<div class="CDPAlignCenter CDPAlign"><img height="306" width="384" class="image-border" src="assets/dfb8bb16-70eb-4c94-acb1-1e047180c38d.png"/></div>
<div class="packt_infobox">The installation instructions for every operating system can be found on <a href="https://www.tensorflow.org/install/">https://www.tensorflow.org/install/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A quick glimpse inside Keras</h1>
                </header>
            
            <article>
                
<p>Keras (<a href="https://keras.io">https://keras.io</a>) is a high-level deep learning framework that works seamlessly with low-level backends like TensorFlow, Theano or CNTK. In Keras a model is like a sequence of layers where each output is fed into the following computational block until the final layer is reached. The generic structure of a model is:</p>
<pre><strong>from keras.models import Sequential</strong><br/><br/><strong>&gt;&gt;&gt; model = Sequential()</strong><br/><br/><strong>&gt;&gt;&gt; model.add(...)</strong><br/><strong>&gt;&gt;&gt; model.add(...)</strong><br/><strong>...</strong><br/><strong>&gt;&gt;&gt; model.add(...)</strong></pre>
<p>The class <kbd>Sequential</kbd> defines a generic empty model, that already implements all the methods needed to <kbd>add</kbd> layers, <kbd>compile</kbd> the model according to the underlying framework, to <kbd>fit</kbd> and <kbd>evaluate</kbd> the model and to <kbd>predict</kbd> the output given an input. All the most common layers are already implemented, including:</p>
<ul>
<li>Dense, Dropout and Flattening layers</li>
<li>Convolutional (1D, 2D and 3D) layers</li>
<li>Pooling layers</li>
<li>Zero padding layers</li>
<li>RNN layers</li>
</ul>
<p>A model can be compiled using several loss functions (like MSE or cross-entropy) and all the most diffused Stochastic Gradient Descent optimization algorithms (like RMSProp or Adam). For further details about the mathematical foundation of these methods, please refer to Goodfellow I., Bengio Y., Courville A., <em>Deep Learning</em>, MIT Press. As it's impossible to discuss all important elements in such a short space, I prefer to create a complete example of image classification based on a convolutional network. The dataset we're going to use is the CIFAR-10 (<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>) which is made up of 60000 small RGB images (32 x 32) belonging to 10 different categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). In the following figure, a subset of images is shown:</p>
<div class="CDPAlignCenter CDPAlign"><img height="534" width="544" class="image-border" src="assets/e758d067-bd32-4d83-86aa-eaa275fed875.png"/></div>
<p>Since the last release, Keras allows us to download this dataset using a built-in function; therefore, no further actions are required to use it.</p>
<p>The first step is loading the dataset and splitting it into training and test subsets:</p>
<pre><strong>from keras.datasets import cifar10</strong><br/><br/><strong>&gt;&gt;&gt; (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()</strong></pre>
<p>The training dataset contains 50000 images, while the test set 10000. Now it's possible to build the model. We want to use a few convolutional layers to capture the specific elements of each category. As explained in the previous section, these particular layers can learn to identify specific geometric properties and generalize in an excellent way. In our small architecture, we start with a (5 x 5) filter size to capture all the low-level features (like the orientation) and proceed by increasing the number of filters and reducing their size. In this way, the high-level features (like the shape of a wheel or the relative position of eyes, nose, and mouth) can also be captured.   </p>
<pre class="mce-root"><strong>from keras.models import Sequential<br/>from keras.layers.convolutional import Conv2D, ZeroPadding2D<br/>from keras.layers.pooling import MaxPooling2D<br/><br/>&gt;&gt;&gt; model = Sequential()</strong><br/><strong> </strong><br/><strong>&gt;&gt;&gt; model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=(32 ,32, 3)))</strong><br/><strong>&gt;&gt;&gt; model.add(MaxPooling2D(pool_size=(2, 2)))</strong><br/><br/><strong>&gt;&gt;&gt; model.add(Conv2D(64, kernel_size=(4, 4), activation='relu'))</strong><br/><strong>&gt;&gt;&gt; model.add(ZeroPadding2D((1, 1)))</strong><br/><strong> </strong><br/><strong>&gt;&gt;&gt; model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))</strong><br/><strong>&gt;&gt;&gt; model.add(MaxPooling2D(pool_size=(2, 2)))</strong><br/><strong>&gt;&gt;&gt; model.add(ZeroPadding2D((1, 1)))</strong></pre>
<p>The first instruction creates a new empty model. At this point, we can all the layers we want to include in the computational graph. The most common parameters of a convolutional layer are:</p>
<ul>
<li><strong>The number of filters</strong></li>
<li><strong>Kernel size</strong> (as tuple)</li>
<li><strong>Strides</strong> (the default value is [1, 1]). This parameter specifies how many pixels the sliding window must consider when shifting on the image. [1, 1] means that no pixels are discarded. [2, 2] means that every horizontal and vertical shift will have a width of 2 pixels and so forth.</li>
<li><strong>Activation</strong> (the default value is None, meaning that the identity function will be used)</li>
<li><strong>Input shape</strong> (only for the first layer is this parameter mandatory)</li>
</ul>
<p>Our first layer has 32 (5 x 5) filters with a <strong>ReLU</strong> (<strong>Rectified Linear Unit</strong>) activation. This function is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="170" src="assets/09fbf182-d47a-4c52-ac17-d45f21dc2afd.png"/></div>
<p>The second layer reduces the dimensionality with a max pooling considering (2 x 2) blocks. Then we apply another convolution with 64 (4 x 4) filters followed by a zero padding (1 pixel at the top, bottom, left and right side of the input) and finally, we have the third convolutional layer with 128 (3 x 3) filters followed by a max pooling and a zero padding.</p>
<p>At this point, we need to flatten the output of the last layer, so to work like in a MLP:</p>
<pre><strong>from keras.layers.core import Dense, Dropout, Flatten<br/><br/>&gt;&gt;&gt; model.add(Dropout(0.2))<br/>&gt;&gt;&gt; model.add(Flatten())<br/>&gt;&gt;&gt; model.add(Dense(128, activation='relu'))<br/>&gt;&gt;&gt; model.add(Dropout(0.2))<br/>&gt;&gt;&gt; model.add(Dense(10, activation='softmax'))<br/></strong></pre>
<p>A dropout (with a probability of 0.2) is applied to the output of the last zero-padding layer; then this multidimensional value is flattened and transformed in a vector. This value is fed into a fully-connected layer with 128 neurons and ReLU activation. Another dropout is applied to the output (to prevent the overfitting) and, finally, this vector is fed into another fully connected layer with 10 neurons with a <em>softmax</em> activation:</p>
<div class="CDPAlignCenter CDPAlign"><img height="46" width="151" src="assets/c01a96a0-0c0e-400d-a23e-003f70d8b2e5.png"/></div>
<p>In this way, the output of the model represents a discrete probability distribution (each value is the probability of the corresponding class).</p>
<p>The last step before training the model is compiling it:</p>
<pre><strong>&gt;&gt;&gt; model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</strong></pre>
<p>Keras will transform the high-level description into low-level operations (like the ones we have discussed in the previous section) with a categorical cross-entropy loss function (see the example of TensorFlow logistic regression) and the Adam optimizer. Moreover, it will apply an accuracy metric to <span>dynamically</span><span> </span><span>evaluate the performance.</span></p>
<p>At this point, the model can be trained. We need only two preliminary operations:</p>
<ul>
<li>Normalizing the images so they have values between 0 and 1</li>
<li>Applying the one-hot encoding to the integer label</li>
</ul>
<p>The first operation can be simply performed by dividing the dataset by 255, while the second can be easily carried out using the built-in function <kbd>to_categorical()</kbd>:</p>
<pre><strong>from keras.utils import to_categorical<br/><br/>&gt;&gt;&gt; model.fit(X_train / 255.0, to_categorical(Y_train), batch_size=32, epochs=15)</strong></pre>
<p>We want to train with batches made up of 32 images and for a period of 15 epochs. The reader is free to change all these values to compare the results. The output provided by Keras shows the progress in the learning phase:</p>
<pre><strong>Epoch 1/15</strong><br/><strong>50000/50000 [==============================] - 25s - loss: 1.5845 - acc: 0.4199 </strong><br/><strong>Epoch 2/15</strong><br/><strong>50000/50000 [==============================] - 24s - loss: 1.2368 - acc: 0.5602 </strong><br/><strong>Epoch 3/15</strong><br/><strong>50000/50000 [==============================] - 26s - loss: 1.0678 - acc: 0.6247 </strong><br/><strong>Epoch 4/15</strong><br/><strong>50000/50000 [==============================] - 25s - loss: 0.9495 - acc: 0.6658 </strong><br/><strong>Epoch 5/15</strong><br/><strong>50000/50000 [==============================] - 26s - loss: 0.8598 - acc: 0.6963 </strong><br/><strong>Epoch 6/15</strong><br/><strong>50000/50000 [==============================] - 26s - loss: 0.7829 - acc: 0.7220 </strong><br/><strong>Epoch 7/15</strong><br/><strong>50000/50000 [==============================] - 26s - loss: 0.7204 - acc: 0.7452 </strong><br/><strong>Epoch 8/15</strong><br/><strong>50000/50000 [==============================] - 26s - loss: 0.6712 - acc: 0.7629 </strong><br/><strong>Epoch 9/15</strong><br/><strong>50000/50000 [==============================] - 27s - loss: 0.6286 - acc: 0.7779 </strong><br/><strong>Epoch 10/15</strong><br/><strong>50000/50000 [==============================] - 27s - loss: 0.5753 - acc: 0.7952 </strong><br/><strong>Epoch 11/15</strong><br/><strong>50000/50000 [==============================] - 27s - loss: 0.5433 - acc: 0.8049 </strong><br/><strong>Epoch 12/15</strong><br/><strong>50000/50000 [==============================] - 27s - loss: 0.5112 - acc: 0.8170 </strong><br/><strong>Epoch 13/15</strong><br/><strong>50000/50000 [==============================] - 27s - loss: 0.4806 - acc: 0.8293 </strong><br/><strong>Epoch 14/15</strong><br/><strong>50000/50000 [==============================] - 28s - loss: 0.4551 - acc: 0.8365 </strong><br/><strong>Epoch 15/15</strong><br/><strong>50000/50000 [==============================] - 28s - loss: 0.4342 - acc: 0.8444</strong></pre>
<p>At the end of the 15th epoch, the accuracy on the training set is about 84% (a very good result). The final operation is evaluating the model with the test set:</p>
<pre><strong>&gt;&gt;&gt; scores = model.evaluate(X_test / 255.0, to_categorical(Y_test))</strong><br/><strong>&gt;&gt;&gt; print('Loss: %.3f' % scores[0])</strong><br/><strong>&gt;&gt;&gt; print('Accuracy: %.3f' % scores[1])</strong><br/><strong>Loss: 0.972</strong><br/><strong>Accuracy: 0.719</strong></pre>
<p>The final validation accuracy is lower (about 72%) than the one achieved during the training phase. This is a normal behavior for deep models, therefore, when optimizing the algorithm, it's always a good practice to use the cross validation or a well-defined test set (with the same distribution of the training set and 25-30% of total samples).</p>
<p>Of course, we have presented a very simple architecture, but the reader can go deeper into these topics and create more complex models (Keras <span>also</span><span> </span><span>contains some very famous pre-trained architectures like VGG16/19 and Inception V3 that can </span><span>also</span><span> </span><span>be used to perform image</span> classifications <span>with 1000 categories).</span></p>
<div class="packt_infobox">All the information needed to install Keras with different backends, and the official documentation can be found on the website: <br/>
<a href="https://keras.io">https://keras.io</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Goodfellow I., Bengio Y., Courville A., <em>Deep Learning</em>, MIT Press </li>
<li><span class="a-size-small a-color-secondary">Abrahams S.,</span><span class="a-size-small a-color-secondary"> </span><span class="a-size-small a-color-secondary"><span class="a-size-small a-color-secondary">Hafner D., <em>TensorFlow for Machine Intelligence: A Hands-On Introduction to Learning Algorithms</em>, Bleeding Edge Press</span></span></li>
<li>Bonaccorso G., <em>Neural Artistic Style Transfer with Keras</em>, <a href="https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer" target="_blank">https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer</a></li>
<li><span>Krizhevsky A, Learning Multiple Layers of Features from Tiny Images, 2009 (<a href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>)</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have briefly discussed some basic deep learning concepts, and the reader should now understand what a computational graph is and how it can be modeled using TensorFlow. A deep architecture, in fact, can be seen as a sequence of layers connected to each other. They can have different characteristics and purposes, but the overall graph is always a directed structure that associates input values with a final output layer. Therefore, it's possible to derive a global loss function that will be optimized by a training algorithm. We also saw how TensorFlow computes the gradients of an output tensor with respect to any previous connected layer and therefore how it's possible to implement the standard back-propagation strategy seamlessly to deep architectures. We did not discuss actual deep learning problems and methods because they require much more space; however, the reader can easily find many valid resources to continue his/her exploration in this fascinating field. </p>
<p>In the next chapter, we're going to summarize many of the concepts previously discussed in order to create complex machine learning architectures.</p>


            </article>

            
        </section>
    </body></html>