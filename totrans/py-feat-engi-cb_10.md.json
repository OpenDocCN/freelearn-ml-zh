["```py\n     import matplotlib.pyplot as plt\n    import pandas as pd\n    ```", "```py\n     X = pd.read_csv(\n        \"occupancy.csv\", parse_dates=[\"date\"])\n    X.head()\n    ```", "```py\n     def plot_timeseries(n_id):\n        fig, axes = plt.subplots(nrows=2, ncols=3,\n        figsize=(20, 10))\n        X[X[«id»] == n_id][\"temperature\"].plot(\n            ax=axes[0, 0], title=\"temperature\")\n        X[X[«id»] == n_id][\"humidity\"].plot(\n            ax=axes[0, 1], title=\"humidity\")\n        X[X[«id»] == n_id][\"light\"].plot(\n            ax=axes[0, 2], title=\"light\")\n        X[X[«id»] == n_id][\"co2\"].plot(\n        ax=axes[1, 0], title=\"co2\")\n        X[X[«id»] == n_id][\"humidity_ratio\"].plot(\n            ax=axes[1,1], title=\"humidity_ratio\")\n        plt.show()\n    ```", "```py\n     plot_timeseries(2)\n    ```", "```py\n     plot_timeseries(15)\n    ```", "```py\n     import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from sklearn.model_selection import train_test_split\n    from tsfresh import extract_features\n    from tsfresh.utilities.dataframe_functions import (\n        impute\n    )\n    ```", "```py\n     X = pd.read_csv(\"occupancy.csv\", parse_dates=[\"date\"])\n    ```", "```py\n     y = pd.read_csv(\"occupancy_target.csv\",\n        index_col=\"id\")[\"occupancy\"]\n    ```", "```py\n     features = extract_features(\n        X[[«id», «light»]], column_id=\"id\")\n    ```", "```py\n     feats = features.columns[10:15]\n    ```", "```py\n    <st c=\"14413\">Index(['light__mean', 'light__length',</st> <st c=\"14453\">'light__standard_deviation',</st> <st c=\"14482\">'light__variation_coefficient',</st> <st c=\"14514\">'light__variance'], dtype='object')</st>\n    ```", "```py\n     features[feats].head()\n    ```", "```py\n     impute(features)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        features,\n        y,\n        test_size=0.1,\n        random_state=42,\n    )\n    ```", "```py\n     cls = LogisticRegression(random_state=10, C=0.01)\n    cls.fit(X_train, y_train)\n    print(classification_report(\n         y_test, cls.predict(X_test)))\n    ```", "```py\n     <st c=\"16959\">precision     recall</st> <st c=\"16976\">f1-score   support</st>\n     <st c=\"16993\">0         1.00        1.00        1.00           11</st>\n     <st c=\"17013\">1         1.00</st> <st c=\"17021\">1.00        1.00            3</st>\n     <st c=\"17032\">accuracy                                       1.00           14</st>\n     <st c=\"17049\">macro avg         1.00        1.00        1.00</st> <st c=\"17074\">14</st>\n    <st c=\"17077\">weighted avg         1.00        1.00        1.00           14</st>\n    ```", "```py\n     features = extract_features(\n        X,\n        column_id=\"id\",\n        impute_function=impute,\n        column_sort=\"date\",\n    )\n    ```", "```py\n     import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from sklearn.model_selection import train_test_split\n    from tsfresh import (\n        extract_features,\n        extract_relevant_features,\n        select_features,\n    )\n    from tsfresh.utilities.dataframe_functions import impute\n    ```", "```py\n     X = pd.read_csv(\"occupancy.csv\", parse_dates=[\"date\"])\n    y = pd.read_csv(\"occupancy_target.csv\",\n        index_col=\"id\")[\"occupancy\"]\n    ```", "```py\n     features = extract_features(\n        X[[«id», «light»]],\n        column_id=\"id\",\n        impute_function=impute,\n    )\n    ```", "```py\n     features = select_features(features, y)\n    ```", "```py\n     feats = features.columns[0:5]\n    features[feats].head()\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        features,\n        y,\n        test_size=0.1,\n        random_state=42,\n    )\n    ```", "```py\n     cls = LogisticRegression(\n        random_state=10, C=0.1, max_iter=1000)\n    cls.fit(X_train, y_train)\n    print(classification_report(\n        y_test, cls.predict(X_test)))\n    ```", "```py\n     <st c=\"29491\">precision     recall  f1-score   support</st>\n     <st c=\"29525\">0</st> <st c=\"29527\">1.00        0.91        0.95           11</st>\n     <st c=\"29545\">1         0.75        1.00        0.86            3</st>\n    **<st c=\"29564\">accuracy                                       0.93           14</st>**\n     **<st c=\"29581\">macro avg         0.88        0.95        0.90           14</st>**\n    `<st c=\"29980\">extract_relevant_features</st>`<st c=\"30005\">, and, like this, combine</st> *<st c=\"30031\">steps 3</st>* <st c=\"30038\">and</st> *<st c=\"30043\">4</st>*<st c=\"30044\">. We’ll do that to create and select features automatically for the five time series in</st> <st c=\"30132\">our dataset:</st>\n\n    ```", "```py \n    ```", "```py\n     import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n    from sklearn.model_selection import train_test_split\n    from tsfresh.feature_extraction import (\n        extract_features\n    )\n    from tsfresh.feature_extraction import settings\n    ```", "```py\n     X = pd.read_csv(\"occupancy.csv\", parse_dates=[\"date\"])\n    y = pd.read_csv(\"occupancy_target.csv\",\n        index_col=\"id\")[\"occupancy\"]\n    ```", "```py\n     minimal_feat = settings.MinimalFCParameters()\n    minimal_feat.items()\n    ```", "```py\n    <st c=\"36433\">ItemsView({'sum_values': None, 'median': None, 'mean': None, 'length': None, 'standard_deviation': None, 'variance': None, 'root_mean_square': None, 'maximum': None, 'absolute_maximum': None, 'minimum': None})</st>\n    ```", "```py\n     features = extract_features(\n        X[[«id», «light»]],\n        column_id=\"id\", <st c=\"37035\">default_fc_parameters=minimal_feat,</st> )\n    features.shape\n    ```", "```py\n     features.head()\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        features,\n        y,\n        test_size=0.1,\n        random_state=42,\n    )\n    ```", "```py\n     cls = LogisticRegression(random_state=10, C=0.01)\n    cls.fit(X_train, y_train)\n    print(classification_report(\n        y_test, cls.predict(X_test)))\n    ```", "```py\n     <st c=\"38730\">precision     recall  f1-score   support</st>\n    **<st c=\"38764\">0         1.00        0.91        0.95           11</st>**\n     **<st c=\"38784\">1         0.75        1.00        0.86            3</st>**\n     **<st c=\"38803\">accuracy                                       0.93           14</st>**\n     **<st c=\"38820\">macro avg         0.88        0.95        0.90           14</st>**\n    **<st c=\"38848\">weighted avg         0.95        0.93</st>** **<st c=\"38871\">0.93           14</st>**\n    ```", "```py\n     light_feat = {\n        «sum_values\": None,\n        \"median\": None,\n        «standard_deviation\": None,\n        \"quantile\": [{\"q\": 0.2}, {\"q\": 0.7}],\n    }\n    ```", "```py\n     co2_feat = {\n        «root_mean_square\": None,\n        «number_peaks\": [{\"n\": 1}, {\"n\": 2}],\n    }\n    ```", "```py\n     kind_to_fc_parameters = {\n        «light»: light_feat,\n        \"co2\": co2_feat,\n    }\n    ```", "```py\n     features = extract_features(\n        X[[«id», «light», «co2»]],\n        column_id=\"id\",\n        kind_to_fc_parameters=kind_to_fc_parameters,\n    )\n    ```", "```py\n    <st c=\"40218\">Index(['light__sum_values', 'light__median',</st>\n     <st c=\"40263\">'light__standard_deviation',</st>\n     <st c=\"40292\">'light__quantile__q_0.2',</st>\n     <st c=\"40318\">'light__quantile__q_0.7',</st>\n     <st c=\"40344\">'co2__root_mean_square',</st>\n     <st c=\"40369\">'co2__number_peaks__n_1',</st>\n     <st c=\"40395\">'co2__numb</st><st c=\"40406\">er_peaks__n_2'],</st>\n     <st c=\"40423\">dtype='object')</st>\n    ```", "```py\n     import pandas as pd\n    from sklearn.feature_selection import SelectFromModel\n    from sklearn.linear_model import LogisticRegression\n    from tsfresh import (\n        extract_features,\n        extract_relevant_features,\n    )\n    from tsfresh.feature_extraction import settings\n    ```", "```py\n     X = pd.read_csv(\"occupancy.csv\", parse_dates=[\"date\"])\n    y = pd.read_csv(\n        \"occupancy_target.csv\",\n        index_col=\"id\")[\"occupancy\"]\n    ```", "```py\n     features = extract_relevant_features(\n        X,\n        y,\n        column_id=\"id\",\n        column_sort=\"date\",\n    )\n    features.shape\n    ```", "```py\n     cls = LogisticRegression(\n        penalty=\"l1\",\n        solver=»liblinear\",\n        random_state=10,\n        C=0.05,\n        max_iter=1000,\n    )\n    ```", "```py\n     selector = SelectFromModel(cls)\n    ```", "```py\n     selector.fit(features, y)\n    ```", "```py\n     features = selector.get_feature_names_out()\n    ```", "```py\n    array([\n    'light__sum_of_reoccurring_data_points',\n    'co2__fft_coefficient__attr_\"abs\"__coeff_0',\n    'co2__spkt_welch_density__coeff_2', 'co2__variance',\n    'temperature__c3__lag_1', 'temperature__abs_energy',\n    'temperature__c3__lag_2', 'temperature__c3__lag_3',\n    'co2__sum_of_reoccurring_data_points',\n    'light__spkt_welch_density__coeff_8',\n    'light__agg_linear_trend__attr_\"intercept\"__chunk_len_50__f_agg_\"var\"',\n             'light__agg_linear_trend__attr_\"slope\"__chunk_len_50__f_agg_\"var\"',  'light__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"var\"'],\n    dtype=object)\n    ```", "```py\n     kind_to_fc_parameters = settings.from_columns(\n        selector.get_feature_names_out(),\n    )\n    ```", "```py\n    {'light':\n        {‹sum_of_reoccurring_data_points': None,\n        ‹spkt_welch_density': [{'coeff': 8}],\n        'variance': None,\n        ‹agg_linear_trend': [\n            {‹attr': 'slope','chunk_len': 50,\n                'f_agg': 'var'},\n            {‹attr': 'intercept',\n                'chunk_len': 10,'f_agg':'var'}\n            ]\n        },\n    'co2':\n        {‹spkt_welch_density': [{'coeff': 2}],\n        'variance': None,\n        ‹sum_of_reoccurring_data_points': None\n        },\n        'temperature': {\n            'c3': [{'lag': 1}, {'lag': 2}, {'lag':3}],\n            'abs_energy': None}\n    }\n    ```", "```py\n     features = extract_features(\n        X,\n        column_id=\"id\",\n        column_sort=\"date\",\n        kind_to_fc_parameters=kind_to_fc_parameters,\n    )\n    ```", "```py\n     import pandas as pd\n    from sklearn.pipeline import Pipeline\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import classification_report\n    from tsfresh.transformers import (\n        RelevantFeatureAugmenter)\n    ```", "```py\n     X = pd.read_csv(\"occupancy.csv\", parse_dates=[\"date\"])\n    y = pd.read_csv(\n        \"occupancy_target.csv\",\n        index_col=\"id\")[\"occupancy\"]\n    ```", "```py\n     tmp = pd.DataFrame(index=y.index)\n    ```", "```py\n     X_train, X_test, y_train, y_test = train_test_split(\n        tmp, y, random_state=0)\n    ```", "```py\n     kind_to_fc_parameters = {\n        \"light\": {\n            \"c3\": [{\"lag\": 3}, {\"lag\": 2}, {\"lag\": 1}],\n            «abs_energy\": None,\n            «sum_values\": None,\n            «fft_coefficient\": [\n                {«attr\": \"real\", \"coeff\": 0},\n                {«attr\": \"abs\", \"coeff\": 0}],\n            «spkt_welch_density\": [\n                {«coeff\": 2}, {\"coeff\":5}, {\"coeff\": 8}\n            ],\n            «agg_linear_trend\": [\n                {«attr\": \"intercept\",\n                „chunk_len\": 50, „f_agg\": „var\"},\n                {\"attr\": \"slope\",\n                «chunk_len\": 50, \"f_agg\":\"var\"},\n            ],\n            «change_quantiles\": [\n                {«f_agg\": \"var\", \"isabs\": False,\n                «qh\": 1.0,\"ql\": 0.8},\n                {«f_agg\": \"var\", \"isabs\": True,\n                «qh\": 1.0,\"ql\": 0.8},\n            ],\n        },\n    \"co2\": {\n        «fft_coefficient\": [\n            {«attr\": \"real\", \"coeff\": 0},\n            {«attr\": \"abs\", \"coeff\": 0}],\n        \"c3\": [{\"lag\": 3}, {\"lag\": 2}, {\"lag\": 1}],\n        «sum_values\": None,\n        «abs_energy\": None,\n        «sum_of_reoccurring_data_points\": None,\n        «sum_of_reoccurring_values\": None,\n        },\n    \"temperature\": {\"c3\": [{\"lag\": 1},\n        {«lag»: 2},{«lag»: 3}], «abs_energy\": None},\n    }\n    ```", "```py\n     augmenter = RelevantFeatureAugmenter(\n        column_id=\"id\",\n        column_sort=\"date\",\n        kind_to_fc_parameters=kind_to_fc_parameters,\n    )\n    ```", "```py\n     pipe = Pipeline(\n        [\n            (\"augmenter\", augmenter),\n            («classifier», LogisticRegression(\n        random_state=10, C=0.01)),\n        ]\n    )\n    ```", "```py\n     pipe.set_params(augmenter__timeseries_container=X)\n    ```", "```py\n     pipe.fit(X_train, y_train)\n    ```", "```py\n     print(classification_report(\n        y_test, pipe.predict(X_test)))\n    ```", "```py\n     <st c=\"54007\">precision     recall  f1-score   support</st>\n     <st c=\"54041\">0         1.00        0.96        0.98</st> <st c=\"54058\">28</st>\n     <st c=\"54061\">1         0.86        1.00        0.92            6</st>\n     <st c=\"54080\">accuracy                                       0.97           34</st>\n    **<st c=\"54097\">macro avg         0.93        0.98        0.95           34</st>**\n    **<st c=\"54125\">weighted avg         0.97        0.97        0.97           34</st>**\n    ```"]