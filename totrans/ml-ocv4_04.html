<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">First Steps in Supervised Learning</h1>
                </header>
            
            <article>
                
<p>This is the moment you've been waiting for, isn't it?</p>
<p>We have covered all of the bases—we have a functioning Python environment, we have OpenCV installed, and we know how to handle data in Python. Now, it's time to build our first machine learning system! And what better way to start off than to focus on one of the most common and successful types of machine learning:<span> </span><strong>supervised learning</strong>?</p>
<p>From the previous chapter, we already know that supervised learning is all about learning regularities in training data by using the labels that come with it so that we can predict the labels of some new, never-seen-before test data. In this chapter, we want to dig a little deeper and learn how to turn our theoretical knowledge ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can refer to the code for this chapter at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03</a>.<a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03"/></p>
<p>The following is globally a summary of software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python 3.x<span> version </span>will be fine).</li>
<li>You will need Anaconda Python 3 to install Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, and Linux-based OSes<span>—</span>with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided with this book.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding supervised learning</h1>
                </header>
            
            <article>
                
<p>We have previously established that the goal of<span> </span>supervised<span> </span>learning is always to <span>predict</span> labels (or target values) for data. However, depending on the nature of these labels,<span> </span>supervised<span> </span>learning can come in two distinct forms:</p>
<ul>
<li><strong>Classification</strong>: Supervised learning is<span> </span>called<span> </span><strong>classification</strong><span> </span><span>whenever we use the data to predict categories. A good example of this is when we try to predict whether an image contains</span><span> </span>a cat or a dog. Here, the labe<span>ls of the data are categorical, either one or the other, but never a mixture of categories. For example, a picture contains either a cat or a dog, never 50% cat and 50% dog (before you ask, no, here we do not consider pictures of the cartoon character, CatDog), and our job ...</span></li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Having a look at supervised learning in OpenCV</h1>
                </header>
            
            <article>
                
<p>Just knowing how<span> </span>supervised<span> </span>learning works is not going to be of any use if we can't put it into practice. Thankfully,<span> </span>OpenCV<span> </span>provides a pretty straightforward interface for all its<span> </span>statistical learning models, which<span> </span>includes<span> </span>all supervised learning models.</p>
<p>In OpenCV, every machine<span> </span>learning<span> </span>model derives from the<span> </span><kbd>cv::ml::StatModel</kbd><span> </span>base class. This is fancy talk for saying that if we want to use a machine learning<span> </span>model<span> </span>in OpenCV, we have to<span> </span>provide<span> </span>all of the functionality<span> </span>that<span> </span><kbd>StatModel</kbd><span> </span>tells us to. This includes a method to train<span> </span>the<span> </span>model (called<span> </span><kbd>train</kbd>) and a<span> </span>method<span> </span>to measure the<span> </span>performance<span> </span>of the model (called<span> </span><kbd>calcError</kbd>).</p>
<div class="packt_infobox">In<span> </span><strong>Object-Oriented Programming</strong> (<strong>OOP</strong>), we deal primarily with objects or classes. An object consists of several functions, called<span> </span><strong>methods</strong>, as well as variables, called<span> </span><strong>members</strong><span> </span>or<span> </span><strong>attributes</strong>. You can learn more about OOP in Python at<span> </span><a href="https://docs.python.org/3/tutorial/classes.html">https://docs.python.org/3/tutorial/classes.html</a>.</div>
<p>Thanks to this organization of the software, setting up a<span> </span>machine<span> </span>learning model in<span> </span>OpenCV<span> </span>always follows the same logic, as we will see later:</p>
<ul>
<li><strong>Initialization</strong>: We call the model by name to create an empty instance of the model.</li>
<li><strong>Set parameters</strong>: If the model needs some parameters, we can set them via setter methods, which can be different for every model. For example, for a<span> </span>k-NN algorithm to work, we need to specify its open parameter,<span> </span><em>k</em><span> </span>(as we will find out later).</li>
<li><strong>Train the model</strong>: Every model must provide a method called<span> </span><kbd>train</kbd>, used to fit the model to some data.</li>
<li><strong>Predict new labels</strong>: Every model must provide a method called<span> </span><kbd>predict</kbd>, used to predict the labels of new data.</li>
<li><strong>Score the model</strong>: Every model must provide a method called<span> </span><kbd>calcError</kbd>, used to measure performance. This calculation might be different for every model.</li>
</ul>
<div class="packt_tip">Because OpenCV is a vast and community-driven project, not every algorithm follows these rules to the extent that we as users might expect. For example, the<span> </span>k-NN algorithm does most of its work in a<span> </span><kbd>findNearest</kbd><span> </span>method, although<span> </span><kbd>predict</kbd><span> </span>still works. We will make sure to point out these discrepancies as we work through different examples.</div>
<p>As we will make occasional use of scikit-learn to implement some machine learning algorithms that OpenCV does not provide, it is worth pointing out that learning algorithms in scikit-learn follow an almost identical logic. The most notable difference is that scikit-learn sets all of the required model parameters in the initialization step. Also, it calls the training function,<span> </span><kbd>fit</kbd>,<span> </span>instead of<span> </span><kbd>train</kbd> and the scoring function<span> </span><kbd>score</kbd><span> </span>instead of<span> </span><kbd>calcError</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Measuring model performance with scoring functions</h1>
                </header>
            
            <article>
                
<p>One of the most important parts of building a machine learning system is to<span> </span>find<span> </span>a way to measure the<span> </span>quality<span> </span>of model predictions. In real-life scenarios, a model will rarely get everything right. From earlier chapters, we know that we are supposed to use data from the test set to evaluate our model. But how exactly does that work?</p>
<p>The short, but not very helpful, answer is that it depends on the model. People have come up with all sorts of scoring functions that can be used to evaluate the trained model in all possible scenarios. The good news is that a lot of them are actually part of scikit-learn's<span> </span><kbd>metrics</kbd><span> </span>module.</p>
<p>Let's have a quick look at some of the most important scoring functions. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scoring classifiers using accuracy, precision, and recall</h1>
                </header>
            
            <article>
                
<p>In a binary classification task where there are only two different class labels, there are several different ways to measure<span> </span>classification<span> </span>performance. Some common metrics are as follows:</p>
<ul>
<li><kbd>accuracy_score</kbd>:<span> </span>Accuracy<span> </span>counts the number of<span> </span>data<span> </span>points in the test set that have been<span> </span>predicted<span> </span>correctly and returns that number as a fraction of the test set size. Sticking to the example of classifying pictures as cats or dogs, accuracy indicates the fraction of pictures that have been correctly classified as containing either a cat or a dog. This is the most basic scoring function for classifiers.</li>
<li><kbd>precision_score</kbd>:<span> </span>Precision describes the<span> </span>ability<span> </span>of a classifier not to label as a cat a<span> </span>picture<span> </span>that contains a dog. In other words, out of all of the pictures in the test set that the classifier thinks contain a cat, precision is the fraction of pictures that actually contain a cat.</li>
<li><kbd>recall_score</kbd>:<span> </span>Recall (or sensitivity) describes the ability of a<span> </span>classifier<span> </span>to retrieve all of the<span> </span>pictures<span> </span>that contains a cat. In other words, out of all of the pictures of cats in the test set, recall is the fraction of pictures that has been correctly identified as pictures of cats.</li>
</ul>
<p>Let's say, we have some<span> </span><kbd>ground truth</kbd><span> (correct according to the dataset we have) </span>class labels that are either zeros or ones. We can generate them at random using NumPy's random number generator. O<span>bviously, this means that, whenever we rerun our code, new data points will be generated at random. However, for the purpose of this book, this is not very helpful, as I want you to be able to run the code and always get the same result as me. A nice trick to achieve that is to fix the</span> seed of the rando<span>m number generator. This will make sure the generator is initialized the same way every time you run the script:</span></p>
<ol>
<li>We can fix the seed of the random number generator using the following code:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import numpy as np<br/>In [2]: np.random.seed(42)</pre>
<ol start="2">
<li>Then we can generate five random labels that are<span> </span>either<span> </span>zeros or ones by picking random<span> </span>integers<span> </span>in the range,<span> </span> <kbd><span>(</span>0,2)</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In [3]: y_true = np.random.randint(0, 2, size=5)<br/>...     y_true<br/>Out[3]: array([0, 1, 0, 0, 0])</pre>
<div class="packt_infobox"><span>In the literature, these two classes are sometimes also called <strong>positives</strong> (all data points with the class label, <kbd>1</kbd>) and <strong>negatives</strong> (all other data points).</span></div>
<p>Let's assume we have a classifier that tries to predict the class labels mentioned earlier. For the sake of argument, let's say the classifier is not very smart and always predicts the label, <kbd>1</kbd>. We can mock this behavior by hardcoding the prediction labels:</p>
<pre>In [4]: y_pred = np.ones(5, dtype=np.int32)<br/>...     y_pred<br/>Out[4]: array([1, 1, 1, 1, 1], dtype=int32)<strong><br/></strong></pre>
<p>What is the accuracy of our prediction?</p>
<p>As mentioned earlier, accuracy<span> </span><span>counts the number of data points in the test set</span><span> </span><span>that have been predicted correctly and returns that number as a fraction of the</span><span> </span><span>test set size. We correctly predicted only the second data point (where the true label is <kbd>1</kbd>). In all other cases, the true label was <kbd>0</kbd>, yet we predicted <kbd>1</kbd>. Hence, our accuracy should be 1/5 or 0.2.</span></p>
<p><span>A naive implementation of an accuracy metric might sum up all occurrences where the predicted class label matched the true class label:</span></p>
<pre>In [5]: test_set_size = len(y_true)<br/>In [6]: predict_correct = np.sum(y_true == y_pred)<br/>In [7]: predict_correct / test_set_size<br/>Out[7]: 0.2</pre>
<p>A smarter, and more convenient, implementation is provided by scikit-learn's<span> </span><kbd>metrics</kbd><span> </span>module:</p>
<pre>In [8]: from sklearn import metrics<br/>In [9]: metrics.accuracy_score(y_true, y_pred)<br/>Out[9]: 0.2</pre>
<p>That wasn't too hard, was it? However, to understand<span> </span>precision<span> </span>and recall, we need a general<span> </span>understanding<span> </span>of type I and type II errors. Let's recall that data points with the class label, <kbd>1</kbd>, are often called positives, and data points with the class label, <kbd>0</kbd> (or -1) are often called negatives. Then, classifying a specific<span> </span>data<span> </span>point can have one of four possible outcomes, as illustrated by the following confusion matrix:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly positive</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly negative</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted positive</strong></td>
<td class="CDPAlignCenter CDPAlign">True Positive</td>
<td class="CDPAlignCenter CDPAlign">False Positive</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted negative</strong></td>
<td class="CDPAlignCenter CDPAlign">False Negative</td>
<td class="CDPAlignCenter CDPAlign">True Negative</td>
</tr>
</tbody>
</table>
<p>Let's break this down. If a data point was<span> </span>truly<span> </span>positive, and we predicted a positive, we got it all right! In this case, the outcome is called a<span> </span><strong>true positive</strong>. If we thought the data point was a positive, but it was really a negative, we falsely predicted a positive (hence the term,<span> </span><strong>false positive</strong>). Analogously, if we<span> </span>thought<span> </span>the data point was negative, but it was really a positive, we falsely<span> </span>predicted<span> </span>a negative (false negative). Finally, if we predicted a<span> </span>negative<span> </span>and the data point was truly a negative, we found a true negative.</p>
<div class="packt_tip">In statistical hypothesis testing, false positives are also known as<span> </span><strong>type I errors</strong><span> </span>and false negatives are also known as<span> </span><strong>type II errors</strong>.</div>
<p>Let's quickly calculate these four metrics on our mock-up data. We have a true positive, where the true label is <kbd>1</kbd> and we predicted <kbd>1</kbd>:</p>
<pre>In [10]: truly_a_positive = (y_true == 1)<br/>In [11]: predicted_a_positive = (y_pred == 1)<br/>In [12]: true_positive = np.sum(predicted_a_positive * truly_a_positive )<br/>...      true_positive<br/>Out[12]: 1</pre>
<p>Similarly, a false positive is where we predicted <kbd>1</kbd> but <kbd>ground truth</kbd><span> </span>was really <kbd>0</kbd>:</p>
<pre>In [13]: false_positive = np.sum((y_pred == 1) * (y_true == 0))<br/>...      false_positive<br/>Out[13]: 4</pre>
<p><span>I'm sure by now you've got the hang of it. But d</span>o we even have to do the math to know about predicted negatives? Our not-so-smart classifier never predicted <kbd>0</kbd>, so<span> </span><kbd>(y_pred == 0)</kbd><span> </span>should never be true:</p>
<pre>In [14]: false_negative = np.sum((y_pred == 0) * (y_true == 1))<br/>...      false_negative<br/>Out[14]: 0<br/>In [15]: true_negative = np.sum((y_pred == 0) * (y_true == 0))<br/>...      true_negative<br/>Out[15]: 0</pre>
<p>Let's also draw the confusion matrix:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly positive</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly negative</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted positive</strong></td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">4</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted negative</strong></td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">0</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To make sure we did everything right, let's calculate the accuracy one more time. Accuracy should be the number of true positives plus the number of true negatives (that is, everything we got right) divided by the total number of data points:</p>
<pre>In [16]: accuracy = (true_positive + true_negative) / test_set_size<br/>...      accuracy<br/>Out[16]: 0.2</pre>
<p>Success! Precision is then given as the number of true positives divided by the number of all true predictions:</p>
<pre>In [17]: precision = true_positive / (true_positive + false_positive)<br/>...      precision<br/>Out[17]: 0.2</pre>
<p>It turns out that precision is no better than accuracy in our case. Let's check our math with scikit-learn:</p>
<pre>In [18]: metrics.precision_score(y_true, y_pred)<br/>Out[18]: 0.2</pre>
<p>Finally,<span> </span><kbd>recall</kbd><span> </span>is given as the<span> </span>fraction<span> </span>of all positives that we correctly classified as positives:</p>
<pre>In [19]: recall = true_positive / (true_positive + false_negative)<br/>...      recall<br/>Out[19]: 1.0<br/>In [20]: metrics.recall_score(y_true, y_pred)<br/>Out[20]: 1.0</pre>
<p>Perfect recall! But, going back to our mock-up data, it should be clear that this excellent recall score was mere luck. Since there was only a single <kbd>1</kbd> label in our mock-up dataset, and we happened to correctly classify it, we got a perfect recall score. Does that mean our classifier is perfect? Not really! But we have found three useful metrics that seem to measure complementary aspects of our classification performance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scoring regressors using mean squared error, explained variance, and R squared</h1>
                </header>
            
            <article>
                
<p>When it comes to regression models, our metrics, as shown earlier, don't work anymore. After all, we are now predicting continuous output values, not distinct classification labels. Fortunately, scikit-learn provides some other useful scoring functions:</p>
<ul>
<li><kbd>mean_squared_error</kbd>: The most<span> </span>commonly<span> </span>used error metric for regression problems is to measure the squared error between the predicted and the true target value for every data point in the<span> </span>training<span> </span>set, averaged across all of the data points.</li>
<li><kbd>explained_variance_score</kbd>: A more<span> </span>sophisticated<span> </span>metric is to measure to what degree a model can explain the variation or dispersion of the test data. Often, the amount of explained ...</li></ul></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using classification models to predict class labels</h1>
                </header>
            
            <article>
                
<p>With these tools in hand, we can now take on our first real classification example.</p>
<p>Consider the small town of Randomville, where people are<span> </span>crazy<span> </span>about their two sports teams, the<span> </span>Randomville Reds and the Randomville Blues. The Reds had been around for a long time, and people loved them. But then, some out-of-town millionaire came along and bought the Reds' top scorer and<span> </span>started<span> </span>a new team, the Blues. To the discontent of most Reds fans, the top scorer would go on to win the championship title with the Blues. Years later, he would return to the Reds, despite some backlash from fans who could never forgive him for his earlier career choices. But anyway, you can see why fans of the Reds don't necessarily get along with fans of the Blues. In fact, these two fan bases are so divided that they never even live next to each other. I've even heard stories where the Red fans deliberately moved away once Blues fans moved in next door. True story!</p>
<p>Anyway, we are new in town and are trying to sell some Blues merchandise to people by going from door to door. However, every now and then we come across a bleeding-heart Reds fan who yells at us for selling Blues stuff and chases us off their lawn. Not nice! It would be much less stressful, and a better use of our time, to avoid these houses altogether and just visit the Blues fans instead.</p>
<p>Confident that we can learn to predict where the Reds fans live, we start keeping track of our encounters. If we come by a Reds fan's house, we draw a red triangle on our handy town map; otherwise, we draw a blue square. After a while, we get a pretty good idea of where everyone lives:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-808 image-border" src="Images/ced3ad58-fea3-4305-8752-ecebfde7cc9f.png" style="width:18.50em;height:12.17em;" width="982" height="649"/></p>
<p>However, now, we approach the house that is marked as a green circle in the preceding map. Should we knock on their door? We try to find some clue as to what team they prefer (perhaps a team flag hanging from the back porch), but we can't see any. How can we know if it is safe to knock on their door?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>What this silly example illustrates is exactly the kind of problem a supervised learning algorithm can solve. We have a bunch of observations (houses, their locations, and their colors) that make up our training data. We can use this data to learn from experience so that, when we face the task of predicting the color of a new house, we can make a well-informed estimate.</p>
<p>As we mentioned earlier, fans of the Reds are really passionate about their team, so they would never move next to a Blues fan. Couldn't we use this information and look at all of the neighboring houses, to find out what kind of fan lives in the new house?</p>
<p>This is exactly what the<span> </span>k-NN algorithm would do.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding the k-NN algorithm</h1>
                </header>
            
            <article>
                
<p>The<span> </span>k-NN algorithm is arguably one of the<span> </span>simplest<span> </span>machine learning algorithms. The reason for this is that we basically only need to store the training dataset. Then, to predict a new data point, we only need to find the closest data point in the training dataset: its nearest neighbor.</p>
<p>In a nutshell, the<span> </span>k<span>-NN algorithm argues that a data point probably belongs to the same class as its neighbors. Think about it: if our neighbor is a Reds fan, we're probably Reds fans, too; otherwise, we would have moved away a long time ago. The same can be said for the Blues.</span></p>
<p><span>Of course, some neighborhoods might be a little more complicated. In this case, we would not just consider our closest neighbor (where</span><span> </span><em>k=1</em><span>), but instead ...</span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing k-NN in OpenCV</h1>
                </header>
            
            <article>
                
<p>Using OpenCV, we can easily create a<span> </span>k-NN model via the <kbd>cv2.ml.KNearest_create()</kbd> function. Building the model then involves the following steps:</p>
<ol>
<li>Generate some training data.</li>
<li>Create a<span> </span>k-NN object for a given number,<span> </span><em>k</em>.</li>
</ol>
<ol start="3">
<li>Find the<span> </span><em>k</em><span> </span>nearest neighbors of a new data point that we want to classify.</li>
<li>Assign the class label of the new data point by majority vote.</li>
<li>Plot the result.</li>
</ol>
<p>We first import all of the necessary modules: OpenCV for the<span> </span>k-NN algorithm, NumPy for data processing, and Matplotlib for plotting. If you are working in a Jupyter Notebook, don't forget to call the<span> </span><kbd>%matplotlib inline</kbd><span> </span>magic:</p>
<pre>In [1]: import numpy as np<br/>...     import cv2<br/>...     import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>In [2]: plt.style.use('ggplot')</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Generating the training data</h1>
                </header>
            
            <article>
                
<p>The first step is to generate some<span> </span>training<span> </span>data. For this, we will use NumPy's random number generator. As discussed in the previous section, we will fix the seed of the random number generator, so that re-running the script will always generate the same values:</p>
<pre>In [3]: np.random.seed(42)</pre>
<p>Alright, now let's get to it. What should our training data look like exactly?</p>
<p>In the previous example, each data point is a house on the town map. Every data point has two features (that is, the<span> </span><em>x</em><span> </span>and<span> </span><em>y</em><span> </span>coordinates of its location on the town map) and a class label (that is, a blue square if a Blues fan lives there and a red triangle if a Reds fan lives there).</p>
<p>The features of a single data point can, therefore, be represented ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the classifier</h1>
                </header>
            
            <article>
                
<p>As with all other machine learning functions, the<span> </span>k-NN classifier is part of the OpenCV 3.1<span> </span><kbd>ml</kbd><span> </span>module. We can create a new classifier using the following command:</p>
<pre>In [15]: knn = cv2.ml.KNearest_create()</pre>
<div class="packt_tip">In older versions of OpenCV, this function might be called <kbd>cv2.KNearest()</kbd> instead.</div>
<p>We then pass our training data to the<span> </span><kbd>train</kbd><span> </span>method:</p>
<pre>In [16]: knn.train(train_data, cv2.ml.ROW_SAMPLE, labels)<br/>Out[16]: True</pre>
<p>Here, we have to tell<span> </span><kbd>knn</kbd><span> </span>that our data is an<span> </span><em>N x 2</em><span> </span>array (that is, every row is a data point). Upon success, the function returns<span> </span><kbd>True</kbd>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Predicting the label of a new data point</h1>
                </header>
            
            <article>
                
<p>The other really helpful method that<span> </span><kbd>knn</kbd><span> </span>provides is called<span> </span><kbd>findNearest</kbd>. It can be used to predict the label of a new data point based on its nearest neighbors.</p>
<p>Thanks to our<span> </span><kbd>generate_data</kbd><span> </span>function, it is<span> </span>actually<span> </span>really easy to generate a new data point! We can think of a new data point as a dataset of size <kbd>1</kbd>:</p>
<pre>In [17]: newcomer, _ = generate_data(1)...      newcomerOut[17]: array([[91., 59.]], dtype=float32)</pre>
<p>Our function also returns a random label, but we are not interested in that. Instead, we want to predict it using our trained classifier! We can tell Python to ignore an output value with an underscore (<kbd>_</kbd>).</p>
<p>Let's have a look at our town map again. We will plot the training set as we did earlier, but ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using regression models to predict continuous outcomes</h1>
                </header>
            
            <article>
                
<p>Now, let's<span> </span>turn<span> </span>our attention to a regression problem. As I'm sure you can recite in your sleep by now, regression is all about predicting<span> </span>continuous<span> </span>outcomes rather than predicting discrete class labels.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding linear regression</h1>
                </header>
            
            <article>
                
<p>The easiest<span> </span>regression<span> </span>model is called<span> </span><strong>linear regression</strong>. The idea behind linear regression is to describe a target variable (such as Boston house pricing—recall the various datasets we studied in <a href="7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml">Chapter 1</a>, <em>A Taste of Machine Learning</em>) with a linear combination of features.</p>
<p>To keep things simple, let's just focus on two features. Let's say we want to predict tomorrow's stock prices using two features: today's stock price and yesterday's stock price. We will denote today's stock price as the first feature,<span> </span><em>f<sub>1</sub></em>, and yesterday's stock price as<span> </span><em>f<sub>2</sub></em>. Then, the goal of<span> </span>linear<span> </span>regression would be to learn two weight coefficients,<span> </span><em>w<sub>1</sub></em><span> </span>and<span> </span><em>w<sub>2</sub></em>,<span> </span>so that we can predict tomorrow's stock price as follows:</p>

<p>Here,<span> </span><span> </span>is the<span> </span></p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression in OpenCV</h1>
                </header>
            
            <article>
                
<p>Before trying out linear regression on a real-life dataset, let's understand how we can use the <kbd>cv2.fitLine</kbd> function to fit a line to a 2D or 3D point set:</p>
<ol>
<li>Let's start by generating some points. We will generate them by adding noise to the points lying on the line <img class="fm-editor-equation" src="Images/25934214-92e9-4194-937c-af8f014804a7.png" style="width:5.67em;height:1.25em;" width="860" height="200"/>:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import cv2<br/>...     import numpy as np<br/>...     import matplotlib.pyplot as plt<br/>...     from sklearn import linear_model<br/>...     from sklearn.model_selection import train_test_split<br/>...     plt.style.use('ggplot')<br/>...     %matplotlib inline<br/>In [2]: x = np.linspace(0,10,100)<br/>...     y_hat = x*5+5<br/>...     np.random.seed(42)<br/>...     y = x*5 + 20*(np.random.rand(x.size) - 0.5)+5<strong><br/></strong></pre>
<ol start="2">
<li>We can also visualize these points using the following code:</li>
</ol>
<pre style="padding-left: 60px">In [3]: plt.figure(figsize=(10, 6))<br/>...     plt.plot(x, y_hat, linewidth=4)<br/>...     plt.plot(x,y,'x')<br/>...     plt.xlabel('x')<br/>...     plt.ylabel('y')<strong><br/></strong></pre>
<p style="padding-left: 60px">This gives us the following diagram, where the red line is the true function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="Images/93344671-1736-4539-991e-8face2c9f3a6.png" style="width:33.58em;height:21.33em;" width="620" height="396"/></p>
<ol start="3">
<li>Next, we will split the points into training and testing sets. Here, we will split the data into a 70:30 ratio, meaning, 70% of the points will be used for training and 30% for testing:</li>
</ol>
<pre style="padding-left: 60px">In [4]: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)</pre>
<ol start="4">
<li>Now, let's use <kbd>cv2.fitLine</kbd> to fit a line to this 2D point set. This function takes in the following arguments:
<ul>
<li><kbd>points</kbd>: This is the set of points to which a line has to be fit.</li>
<li><kbd>distType</kbd><span>: This is the distance used by the M-estimator.</span></li>
<li><kbd>param</kbd><span>: This is the numerical parameter (C), which is used in some types of distances. We will keep it at 0 so that an optimal value can be chosen.</span></li>
<li><kbd>reps</kbd><span>: This is the accuracy of the distance between the origin and the line. <kbd>0.01</kbd> is a good default value for </span><kbd>reps</kbd><span>.</span></li>
<li><kbd>aeps</kbd><span>: This is the accuracy of the angle. <kbd>0.01</kbd> is a good default value for </span><kbd>aeps</kbd><span>.</span></li>
</ul>
</li>
</ol>
<div class="packt_infobox">For more information, have a look at the <a href="https://docs.opencv.org/4.0.0/d3/dc0/group__imgproc__shape.html#gaf849da1fdafa67ee84b1e9a23b93f91f">documentation</a>.</div>
<ol start="5">
<li>Let's see what kinds of result we get using different distance type options:</li>
</ol>
<pre style="padding-left: 60px">In [5]: distTypeOptions = [cv2.DIST_L2,\<br/>...                 cv2.DIST_L1,\<br/>...                 cv2.DIST_L12,\<br/>...                 cv2.DIST_FAIR,\<br/>...                 cv2.DIST_WELSCH,\<br/>...                 cv2.DIST_HUBER]<br/><br/>In [6]: distTypeLabels = ['DIST_L2',\<br/>...                 'DIST_L1',\<br/>...                 'DIST_L12',\<br/>...                 'DIST_FAIR',\<br/>...                 'DIST_WELSCH',\<br/>...                 'DIST_HUBER']<br/><br/>In [7]: colors = ['g','c','m','y','k','b']<br/>In [8]: <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="n">xi</span><span class="p">,</span><span class="n">yi</span><span class="p">)</span> <span class="k">for</span> <span class="n">xi</span><span class="p">,</span><span class="n">yi</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(x</span><span class="n">_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)])</span></pre>
<ol start="6">
<li>We will also use scikit-learn's <kbd>LinearRegression</kbd> to fit the training points and then use the <kbd>predict</kbd> function to predict the <em>y</em>-values for them:</li>
</ol>
<pre style="padding-left: 60px">In [9]: linreg = linear_model.LinearRegression()<br/>In [10]: linreg.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))<br/>Out[10]:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,normalize=False)<br/>In [11]: y_sklearn = linreg.predict(x.reshape(-1,1))<br/>In [12]: y_sklearn = list(y_sklearn.reshape(1,-1)[0])</pre>
<ol start="7">
<li>We use <kbd>reshape(-1,1)</kbd> and <kbd>reshape(1,-1)</kbd> to convert the NumPy arrays into a column vector and then back into a row vector:</li>
</ol>
<pre style="padding-left: 60px">In [13]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(x, y_hat,linewidth=2,label='Ideal')<br/>...      plt.plot(x,y,'x',label='Data')<br/><br/>...      for i in range(len(colors)):<br/>...          distType = distTypeOptions[i]<br/>...          distTypeLabel = distTypeLabels[i]<br/>...          c = colors[i]<br/>    <br/>...          [vxl, vyl, xl, yl] = cv2.fitLine(np.array(points, dtype=np.int32), distType, 0, 0.01, 0.01)<br/>...          y_cv = [vyl[0]/vxl[0] * (xi - xl[0]) + yl[0] for xi in x]<br/>...          plt.plot(x,y_cv,c=c,linewidth=2,label=distTypeLabel)<br/><br/>...      plt.plot(x,list(y_sklearn),c='0.5',\<br/>linewidth=2,label='Scikit-Learn API')<br/>...      plt.xlabel('x')<br/>...      plt.ylabel('y')<br/>...      plt.legend(loc='upper left')</pre>
<p style="padding-left: 60px">The only purpose of this preceding (and lengthy) code was to create a plot that could be used to compare the results obtained using different distance measures.</p>
<p style="padding-left: 60px">Let's have a look at the plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="Images/02196ce1-87a5-4273-b2b5-43bd4f62b56e.png" style="width:40.92em;height:25.83em;" width="628" height="395"/></p>
<p>As we can clearly see, scikit-learn's <kbd>LinearRegression</kbd> model performs much better than OpenCV's <kbd>fitLine</kbd> function. Now, let's use scikit-learn's API to predict Boston housing prices.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using linear regression to predict Boston housing prices</h1>
                </header>
            
            <article>
                
<p>To get a better understanding of<span> </span>linear<span> </span>regression, we want to build a simple model that can be applied to one of the most famous machine learning datasets: the<span> </span><strong>Boston housing prices dataset</strong>. Here, the goal is to predict the value of homes in several Boston neighborhoods in the 1970s, using<span> </span>information<span> </span>such as crime rate, property tax rate, distance to employment centers, and highway accessibility.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the dataset</h1>
                </header>
            
            <article>
                
<p>We can again thank scikit-learn for easy<span> </span>access<span> </span>to the dataset. We first import all of the necessary modules, as we did earlier:</p>
<pre>In [14]: from sklearn import datasets<br/>...      from sklearn import metrics<strong><br/></strong></pre>
<p>Then loading the dataset is a one-liner:</p>
<pre>In [15]: boston = datasets.load_boston()</pre>
<p>The structure of the<span> </span><kbd>boston</kbd><span> </span>object is identical to the<span> </span><kbd>iris</kbd><span> </span>object, as discussed in the preceding command. We can get more information about the dataset in<span> </span><kbd>'DESCR'</kbd> and find all data in<span> </span><kbd>'data'</kbd>, all feature names in<span> </span><kbd>'feature_names'</kbd>, the physical location of the Boston CSV dataset in <kbd>'filename'</kbd>, and all target values in<span> </span><kbd>'target'</kbd>:</p>
<pre>In [16]: dir(boston)<br/>Out[16]: ['DESCR', 'data', 'feature_names', 'filename', 'target']</pre>
<p>The dataset contains a total of <kbd>506</kbd> data points, each of which has <kbd>13</kbd> features:</p>
<pre>In [17]: boston.data.shape<br/>Out[17]: (506, 13)</pre>
<p>Of course, we have only a single target value, which is the housing price:</p>
<pre>In [18]: boston.target.shape<br/>Out[18]: (506,)</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Now let's create a <kbd>LinearRegression</kbd> model that we will then train on the training set:</p>
<pre>In [19]: linreg = linear_model.LinearRegression()</pre>
<p>In the preceding command, we want to split the data into training and test sets. We are free to make the split as we see fit, but usually it is a good idea to reserve between 10 percent and 30 percent for testing. Here, we choose 10 percent, using the<span> </span><kbd>test_size</kbd><span> </span>argument:</p>
<pre>In [20]: X_train, X_test, y_train, y_test = train_test_split(...            boston.data, boston.target, test_size=0.1,...            random_state=42...      )</pre>
<p>In scikit-learn, the<span> </span><kbd>train</kbd><span> </span>function is called<span> </span><kbd>fit</kbd> but otherwise behaves exactly the same as in OpenCV:</p>
<pre>In [21]: linreg.fit(X_train, y_train)Out[21]: LinearRegression(copy_X=True, fit_intercept=True, ...</pre></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing the model</h1>
                </header>
            
            <article>
                
<p>To test the<span> </span>generalization performance of the model, we calculate the<span> </span>mean<span> </span>squared error on the test data:</p>
<pre>In [24]: y_pred = linreg.predict(X_test)<br/>In [25]: metrics.mean_squared_error(y_test, y_pred)<br/>Out[25]: 14.995852876582541</pre>
<p>We note that the mean squared error is a little lower on the test set than the training set. This is good news, as we care mostly about the test error. However, from these numbers it is really hard to understand how good the model really is. Perhaps it's better to plot the data:</p>
<pre>In [26]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(y_test, linewidth=3, label='ground truth')<br/>...      plt.plot(y_pred, linewidth=3, label='predicted')<br/>...      plt.legend(loc='best')<br/>...      plt.xlabel('test data points')<br/>...      plt.ylabel('target value')<br/>Out[26]: &lt;matplotlib.text.Text at 0x7ff46783c7b8&gt;</pre>
<p>This produces the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-815 image-border" src="Images/2c458025-ac75-4430-91a7-23f68787efb4.png" style="width:31.92em;height:19.67em;" width="620" height="383"/></p>
<p>This makes more sense! Here, we<span> </span>see<span> </span>the<span> </span><kbd>ground truth</kbd><span> </span>housing prices for all test samples in red and our predicted housing prices in blue. It's pretty close, if you ask me. It is interesting to note, though, that the model tends to be off the most for really high or really low housing prices, such as the peak values of data points<span> </span><strong>12</strong>, <strong>18</strong>, and <strong>42</strong>. We can formalize the amount of variance in the data that we were able to explain by calculating R squared:</p>
<pre>In [27]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(y_test, y_pred, 'o')<br/>...      plt.plot([-10, 60], [-10, 60], 'k--')<br/>...      plt.axis([-10, 60, -10, 60])<br/>...      plt.xlabel('ground truth')<br/>...      plt.ylabel('predicted')</pre>
<p>This will plot the<span> </span><kbd>ground truth</kbd><span> </span>prices,<span> </span><kbd>y_test</kbd>, on the<span> </span><em>x</em><span> </span>axis and our predictions,<span> </span><kbd>y_pred</kbd>, on the<span> </span><em>y</em><span> </span>axis. We also plot a diagonal line for reference (using a black dashed line,<span> </span><kbd>'k--'</kbd>), as we will see soon. But we also want to display the R<sup>2</sup><span> </span>score and mean squared error in a textbox:</p>
<pre>...      scorestr = r'R$^2$ = %.3f' % linreg.score(X_test, y_test)<br/>...      errstr = 'MSE = %.3f' % metrics.mean_squared_error(y_test, y_pred)<br/>...      plt.text(-5, 50, scorestr, fontsize=12)<br/>...      plt.text(-5, 45, errstr, fontsize=12)<br/>Out[27]: &lt;matplotlib.text.Text at 0x7ff4642d0400&gt;</pre>
<p>This will produce the following diagram and is a professional way of plotting a model fit:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-816 image-border" src="Images/139e4726-1300-4c73-aa11-c8282dba5832.png" style="width:30.75em;height:18.42em;" width="653" height="391"/></p>
<p>If our model was perfect, then all data points would lie on the dashed diagonal, since <kbd>y_pred</kbd> would always be equal to <kbd>y_true</kbd>. Deviations from the diagonal indicate that the model made some errors, or that there is some variance in the data that the model was not able to explain. Indeed, <img class="fm-editor-equation" src="Images/a15dc10c-dde6-4547-ac44-71066ff64510.png" style="width:1.33em;height:1.17em;" width="230" height="200"/> indicates that we were able to explain 76% of the scatter in the data, with a mean squared error of 14.996. These are some performance measures we can use to compare the linear regression model to some more complicated ones.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Applying Lasso and ridge regression</h1>
                </header>
            
            <article>
                
<p>A common problem in machine learning is that an algorithm might work really well on the training set but, when applied to unseen data, it makes a lot of mistakes. You can see how this is problematic since, often, we are most interested in how a model generalizes to new data. Some<span> </span>algorithms<span> </span>(such as decision trees) are more susceptible to this phenomenon than others, but even linear regression can be affected.</p>
<div class="packt_infobox">This phenomenon is also known as<span> </span><strong>overfitting</strong>, and we will talk about it extensively in<span> </span><a href="5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml" target="_blank">Chapter 5</a>,<span> </span><em>Using Decision Trees to Make a Medical Diagnosis</em>, and<span> </span><a href="904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml" target="_blank">Chapter 11</a>,<span> </span><em>Selecting the Right Model with Hyperparameter Tuning</em>.</div>
<p>A common technique for reducing overfitting is called<span> </span><strong>regularization</strong>, which involves ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying iris species using logistic regression</h1>
                </header>
            
            <article>
                
<p>Another famous<span> </span>dataset<span> </span>in the world of machine learning is called the Iris dataset. The Iris dataset contains measurements of 150 iris flowers from<span> </span>three<span> </span>different species: Setosa, Versicolor, and Viriginica. These measurements include the length and width of the petals and the length and width of the sepals, all measured in centimeters.</p>
<p class="CDPAlignLeft CDPAlign">Our goal is to build a machine learning model that can learn the measurements of these iris flowers, the species of which are known, so that we can predict the species for a new iris flower.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Understanding logistic regression</h1>
                </header>
            
            <article>
                
<p>Before we start off with this section, let me issue warning—logistic regression, despite its name, is actually a model for classification, specifically when you have two classes. It derives its name from the  logistic function (or sigmoid) it uses to convert any real-valued input<span> </span><em>x</em><span> </span>into a predicted output value<span> </span><em>ŷ</em><span> </span>that<span> </span>takes<span> </span>values between <strong>0</strong> and <strong>1</strong>, as<span> </span>shown<span> </span>in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-817 image-border" src="Images/b9b54eb8-287c-4286-a68e-d5f9496af292.png" style="width:10.08em;height:9.42em;" width="712" height="657"/></p>
<p>Rounding<span> </span><em>ŷ</em><span> </span>to the nearest integer effectively classifies the input as belonging either to class <strong>0</strong> or <strong>1</strong>.</p>
<p>Of course, most often, our problems have more than one input or feature value,<span> </span><em>x</em>. For example, the Iris dataset provides a total ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the training data</h1>
                </header>
            
            <article>
                
<p>The Iris dataset is included with scikit-learn. We first load all of the<span> </span>necessary<span> </span>modules, as we did in our earlier examples:</p>
<pre>In [1]: import numpy as np<br/>...     import cv2<br/>...     from sklearn import datasets<br/>...     from sklearn import model_selection<br/>...     from sklearn import metrics<br/>...     import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>In [2]: plt.style.use('ggplot')</pre>
<p>Then loading the dataset is a one-liner:</p>
<pre>In [3]: iris = datasets.load_iris()</pre>
<p>This function returns a dictionary we call<span> </span><kbd>iris</kbd>, which contains a bunch of different fields:</p>
<pre>In [4]: dir(iris)<br/>Out[4]: ['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']</pre>
<p class="CDPAlignLeft CDPAlign">Here, all of the data points are contained in<span> </span><kbd>'data'</kbd>. There are <kbd>150</kbd> data points, each of which has <kbd>4</kbd> feature values:</p>
<pre>In [5]: iris.data.shape<br/>Out[5]: (150, 4)</pre>
<p class="CDPAlignLeft CDPAlign">These four features correspond to the sepal and petal dimensions mentioned earlier:</p>
<pre>In [6]: iris.feature_names<br/>Out[6]: ['sepal length (cm)',<br/>         'sepal width (cm)',<br/>         'petal length (cm)',<br/>         'petal width (cm)']</pre>
<p class="CDPAlignLeft CDPAlign">For every data point, we have a class label stored in<span> </span><kbd>target</kbd>:</p>
<pre>In [7]: iris.target.shape<br/>Out[7]: (150,)</pre>
<p class="CDPAlignLeft CDPAlign">We can also inspect the class labels and find that there is a total of three classes:</p>
<pre>In [8]: np.unique(iris.target)<br/>Out[8]: array([0, 1, 2])</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Making it a binary classification problem</h1>
                </header>
            
            <article>
                
<p>For the sake of simplicity, we want to focus on a binary classification problem for now, where we only have two classes. The easiest way to do this is to discard all data points belonging to a certain class, such as class label 2, by selecting all of the rows that do not belong to class <kbd>2</kbd>:</p>
<pre>In [9]: idx = iris.target != 2...     data = iris.data[idx].astype(np.float32)...     target = iris.target[idx].astype(np.float32)</pre>
<p>Next, let's inspect the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Inspecting the data</h1>
                </header>
            
            <article>
                
<p>Before you get started with setting up a model, it is always a good idea to<span> </span>have<span> </span>a look at the data. We did this earlier for the town map example, so let's repeat it here too. Using Matplotlib, we create a<span> </span><strong>scatter plot</strong><span> </span>where the color of each data point corresponds to the class label:</p>
<pre>In [10]: plt.scatter(data[:, 0], data[:, 1], c=target,  <br/>                     cmap=plt.cm.Paired, s=100)<br/>...      plt.xlabel(iris.feature_names[0])<br/>...      plt.ylabel(iris.feature_names[1])<br/>Out[10]: &lt;matplotlib.text.Text at 0x23bb5e03eb8&gt;</pre>
<p class="CDPAlignLeft CDPAlign">To make plotting easier, we limit ourselves to the first two features (<kbd>iris.feature_names[0]</kbd><span> </span>being the sepal length and<span> </span><kbd>iris.feature_names[1]</kbd><span> </span>being the sepal width). We can see a nice separation of classes in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-818 image-border" src="Images/02b2daa3-8a42-4601-b8c5-e84f87ed0cdb.png" style="width:38.33em;height:23.58em;" width="623" height="383"/></p>
<p>The preceding image shows the plot for the first two features of the Iris dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Splitting data into training and test sets</h1>
                </header>
            
            <article>
                
<p>We learned, in the<span> </span>previous<span> </span>chapter, that it is essential to keep<span> </span>training<span> </span>and test data separate. We can easily split the data using one of scikit-learn's many helper functions:</p>
<pre>In [11]: X_train, X_test, y_train, y_test = model_selection.train_test_split(...            data, target, test_size=0.1, random_state=42...      )</pre>
<p class="CDPAlignLeft CDPAlign">Here, we want to split the data into 90% training data and 10% test data, which we specify with<span> </span><kbd>test_size=0.1</kbd>. By inspecting the return arguments, we note that we ended up with exactly <kbd>90</kbd> training data points and <kbd>10</kbd> test data points:</p>
<pre>In [12]: X_train.shape, y_train.shapeOut[12]: ((90, 4), (90,))In [13]: X_test.shape, y_test.shapeOut[13]: ((10, 4), (10,))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training the classifier</h1>
                </header>
            
            <article>
                
<p>Creating a logistic<span> </span>regression<span> </span>classifier involves pretty much the same steps as setting up<span> </span>k-NN:</p>
<pre>In [14]: lr = cv2.ml.LogisticRegression_create()</pre>
<p>We then have to specify the desired training method. Here, we can choose<span> </span><kbd>cv2.ml.LogisticRegression_BATCH</kbd><span> </span>or<span> </span><kbd>cv2.ml.LogisticRegression_MINI_BATCH</kbd>. For now, all we need to know is that we want to update the model after every data point, which can be achieved with the following code:</p>
<pre>In [15]: lr.setTrainMethod(cv2.ml.LogisticRegression_MINI_BATCH)<br/>...      lr.setMiniBatchSize(1)</pre>
<p>We also want to specify the number of iterations the algorithm should run before it terminates:</p>
<pre>In [16]: lr.setIterations(100)</pre>
<p>We can then call the<span> </span><kbd>train</kbd><span> </span>method of the object (in the exact same way as we did earlier), which will return<span> </span><kbd>True</kbd><span> </span>upon success:</p>
<pre>In [17]: lr.train(X_train, cv2.ml.ROW_SAMPLE, y_train)<br/>Out[17]: True</pre>
<p>As we just saw, the goal of the training phase is to find a set of weights that best transform the feature values into an output label. A single data point is given by its four feature values (<em>f<sub>0</sub></em>,<span> </span><em>f<sub>1</sub></em>,<span> </span><em>f<sub>2</sub></em>, and<span> </span><em>f<sub>3</sub></em>). Since we have four features, we should also get four weights, so that<span> </span><em>x = w<sub>0</sub><span> </span>f<sub>0</sub><span> </span>+ w<sub>1</sub><span> </span>f<sub>1</sub><span> </span>+ w<sub>2</sub><span> </span>f<sub>2</sub><span> </span>+ w<sub>3</sub><span> </span>f<sub>3</sub></em>, and<span> </span><em>ŷ=<span>σ(x)</span></em>. However, as discussed previously, the algorithm adds an extra weight that acts as an offset or bias, so that<span> </span><em>x = w<sub>0</sub><span> </span>f<sub>0</sub><span> </span>+ w<sub>1</sub><span> </span>f<sub>1</sub><span> </span>+ w<sub>2</sub><span> </span>f<sub>2</sub><span> </span>+ w<sub>3</sub><span> </span>f<sub>3</sub><span> </span>+ w<sub>4</sub></em><span>. We can retrieve these weights as follows:</span></p>
<pre>In [18]: lr.get_learnt_thetas()<br/>Out[18]: array([[-0.04090132, -0.01910266, -0.16340332, 0.28743777, 0.11909772]], dtype=float32)</pre>
<p>This means that the input to the logistic function is<span> </span><em>x = -0.0409 f<sub>0</sub><span> </span>- 0.0191 f<sub>1</sub><span> </span>- 0.163 f<sub>2</sub><span> </span>+ 0.287 f<sub>3</sub><span> </span>+ 0.119</em>. Then, when we feed in a new data point (<em>f<sub>0</sub></em>,<span> </span><em>f<sub>1</sub></em>,<span> </span><em>f<sub>2</sub></em>,<span> </span><em>f<sub>3</sub></em>) that belongs to class 1, the output<span> </span><em>ŷ=<span>σ(x)</span></em><span> </span>should be close to 1. But how well does that actually work?</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing the classifier</h1>
                </header>
            
            <article>
                
<p>Let's see for ourselves by<span> </span>calculating<span> </span>the accuracy score on the training set:</p>
<pre>In [19]: ret, y_pred = lr.predict(X_train)In [20]: metrics.accuracy_score(y_train, y_pred)<span>Out[20]: 1.0</span></pre>
<p>Perfect score! However, this only means that the model was able to perfectly<span> </span><strong>memorize</strong><span> </span>the training dataset. This does not mean that the model would be able to classify a new, unseen data point. For this, we need to check the test dataset:</p>
<pre>In [21]: ret, y_pred = lr.predict(X_test)...      metrics.accuracy_score(y_test, y_pred)Out[21]: 1.0</pre>
<p>Luckily, we get another perfect score! Now we can be sure that the model we built is truly awesome.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered quite a lot of ground, didn't we?</p>
<p><span>In short, we learned a lot about different supervised learning algorithms, how to apply them to real datasets, and how to implement everything in OpenCV. We introduced classification algorithms such as</span><span> </span>k<span>-NN and logistic regression and discussed how they could be used to predict labels as two or more discrete categories. We introduced various variants of linear regression (such as Lasso regression and ridge regression) and discussed how they could be used to predict continuous variables. Last but not least, we got acquainted with the Iris and Boston datasets, two classics in the history of machine learning.</span></p>
<p>In the following chapters, we will go into much greater depth within these topics and explore some more interesting examples of where these concepts can be useful.</p>
<p>But first, we need to talk about another essential topic in machine learning, feature engineering. Often, data does not come in nicely formatted datasets, and it is our responsibility to represent the data in a meaningful way. Therefore, the next chapter will talk about representing features and engineering data.</p>


            </article>

            
        </section>
    </div>



  </body></html>