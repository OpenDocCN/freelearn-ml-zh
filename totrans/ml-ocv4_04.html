<html><head></head><body><div><h1 class="header-title">First Steps in Supervised Learning</h1>
                
            
            
                
<p>This is the moment you've been waiting for, isn't it?</p>
<p>We have covered all of the bases—we have a functioning Python environment, we have OpenCV installed, and we know how to handle data in Python. Now, it's time to build our first machine learning system! And what better way to start off than to focus on one of the most common and successful types of machine learning: <strong>supervised learning</strong>?</p>
<p>From the previous chapter, we already know that supervised learning is all about learning regularities in training data by using the labels that come with it so that we can predict the labels of some new, never-seen-before test data. In this chapter, we want to dig a little deeper and learn how to turn our theoretical knowledge ...</p></div>



  
<div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You can refer to the code for this chapter at the following link: <a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03">https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03</a>.<a href="https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter03"/></p>
<p>The following is globally a summary of software and hardware requirements:</p>
<ul>
<li>You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).</li>
<li>You will need Python version 3.6 (any Python 3.x version will be fine).</li>
<li>You will need Anaconda Python 3 to install Python and the required modules.</li>
<li>You can use any OS—macOS, Windows, and Linux-based OSes—with this book. We recommend you have at least 4 GB RAM in your system.</li>
<li>You don't need to have a GPU to run the code provided with this book.</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding supervised learning</h1>
                
            
            
                
<p>We have previously established that the goal of supervised learning is always to predict labels (or target values) for data. However, depending on the nature of these labels, supervised learning can come in two distinct forms:</p>
<ul>
<li><strong>Classification</strong>: Supervised learning is called <strong>classification</strong> whenever we use the data to predict categories. A good example of this is when we try to predict whether an image contains a cat or a dog. Here, the labels of the data are categorical, either one or the other, but never a mixture of categories. For example, a picture contains either a cat or a dog, never 50% cat and 50% dog (before you ask, no, here we do not consider pictures of the cartoon character, CatDog), and our job ...</li></ul></div>



  
<div><h1 class="header-title">Having a look at supervised learning in OpenCV</h1>
                
            
            
                
<p>Just knowing how supervised learning works is not going to be of any use if we can't put it into practice. Thankfully, OpenCV provides a pretty straightforward interface for all its statistical learning models, which includes all supervised learning models.</p>
<p>In OpenCV, every machine learning model derives from the <kbd>cv::ml::StatModel</kbd> base class. This is fancy talk for saying that if we want to use a machine learning model in OpenCV, we have to provide all of the functionality that <kbd>StatModel</kbd> tells us to. This includes a method to train the model (called <kbd>train</kbd>) and a method to measure the performance of the model (called <kbd>calcError</kbd>).</p>
<p>In <strong>Object-Oriented Programming</strong> (<strong>OOP</strong>), we deal primarily with objects or classes. An object consists of several functions, called <strong>methods</strong>, as well as variables, called <strong>members</strong> or <strong>attributes</strong>. You can learn more about OOP in Python at <a href="https://docs.python.org/3/tutorial/classes.html">https://docs.python.org/3/tutorial/classes.html</a>.</p>
<p>Thanks to this organization of the software, setting up a machine learning model in OpenCV always follows the same logic, as we will see later:</p>
<ul>
<li><strong>Initialization</strong>: We call the model by name to create an empty instance of the model.</li>
<li><strong>Set parameters</strong>: If the model needs some parameters, we can set them via setter methods, which can be different for every model. For example, for a k-NN algorithm to work, we need to specify its open parameter, <em>k</em> (as we will find out later).</li>
<li><strong>Train the model</strong>: Every model must provide a method called <kbd>train</kbd>, used to fit the model to some data.</li>
<li><strong>Predict new labels</strong>: Every model must provide a method called <kbd>predict</kbd>, used to predict the labels of new data.</li>
<li><strong>Score the model</strong>: Every model must provide a method called <kbd>calcError</kbd>, used to measure performance. This calculation might be different for every model.</li>
</ul>
<p>Because OpenCV is a vast and community-driven project, not every algorithm follows these rules to the extent that we as users might expect. For example, the k-NN algorithm does most of its work in a <kbd>findNearest</kbd> method, although <kbd>predict</kbd> still works. We will make sure to point out these discrepancies as we work through different examples.</p>
<p>As we will make occasional use of scikit-learn to implement some machine learning algorithms that OpenCV does not provide, it is worth pointing out that learning algorithms in scikit-learn follow an almost identical logic. The most notable difference is that scikit-learn sets all of the required model parameters in the initialization step. Also, it calls the training function, <kbd>fit</kbd>, instead of <kbd>train</kbd> and the scoring function <kbd>score</kbd> instead of <kbd>calcError</kbd>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Measuring model performance with scoring functions</h1>
                
            
            
                
<p>One of the most important parts of building a machine learning system is to find a way to measure the quality of model predictions. In real-life scenarios, a model will rarely get everything right. From earlier chapters, we know that we are supposed to use data from the test set to evaluate our model. But how exactly does that work?</p>
<p>The short, but not very helpful, answer is that it depends on the model. People have come up with all sorts of scoring functions that can be used to evaluate the trained model in all possible scenarios. The good news is that a lot of them are actually part of scikit-learn's <kbd>metrics</kbd> module.</p>
<p>Let's have a quick look at some of the most important scoring functions. ...</p></div>



  
<div><h1 class="header-title">Scoring classifiers using accuracy, precision, and recall</h1>
                
            
            
                
<p>In a binary classification task where there are only two different class labels, there are several different ways to measure classification performance. Some common metrics are as follows:</p>
<ul>
<li><kbd>accuracy_score</kbd>: Accuracy counts the number of data points in the test set that have been predicted correctly and returns that number as a fraction of the test set size. Sticking to the example of classifying pictures as cats or dogs, accuracy indicates the fraction of pictures that have been correctly classified as containing either a cat or a dog. This is the most basic scoring function for classifiers.</li>
<li><kbd>precision_score</kbd>: Precision describes the ability of a classifier not to label as a cat a picture that contains a dog. In other words, out of all of the pictures in the test set that the classifier thinks contain a cat, precision is the fraction of pictures that actually contain a cat.</li>
<li><kbd>recall_score</kbd>: Recall (or sensitivity) describes the ability of a classifier to retrieve all of the pictures that contains a cat. In other words, out of all of the pictures of cats in the test set, recall is the fraction of pictures that has been correctly identified as pictures of cats.</li>
</ul>
<p>Let's say, we have some <kbd>ground truth</kbd> (correct according to the dataset we have) class labels that are either zeros or ones. We can generate them at random using NumPy's random number generator. Obviously, this means that, whenever we rerun our code, new data points will be generated at random. However, for the purpose of this book, this is not very helpful, as I want you to be able to run the code and always get the same result as me. A nice trick to achieve that is to fix the seed of the random number generator. This will make sure the generator is initialized the same way every time you run the script:</p>
<ol>
<li>We can fix the seed of the random number generator using the following code:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import numpy as np<br/>In [2]: np.random.seed(42)</pre>
<ol start="2">
<li>Then we can generate five random labels that are either zeros or ones by picking random integers in the range,  <kbd>(0,2)</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In [3]: y_true = np.random.randint(0, 2, size=5)<br/>...     y_true<br/>Out[3]: array([0, 1, 0, 0, 0])</pre>
<p>In the literature, these two classes are sometimes also called <strong>positives</strong> (all data points with the class label, <kbd>1</kbd>) and <strong>negatives</strong> (all other data points).</p>
<p>Let's assume we have a classifier that tries to predict the class labels mentioned earlier. For the sake of argument, let's say the classifier is not very smart and always predicts the label, <kbd>1</kbd>. We can mock this behavior by hardcoding the prediction labels:</p>
<pre>In [4]: y_pred = np.ones(5, dtype=np.int32)<br/>...     y_pred<br/>Out[4]: array([1, 1, 1, 1, 1], dtype=int32)<strong><br/></strong></pre>
<p>What is the accuracy of our prediction?</p>
<p>As mentioned earlier, accuracy counts the number of data points in the test set that have been predicted correctly and returns that number as a fraction of the test set size. We correctly predicted only the second data point (where the true label is <kbd>1</kbd>). In all other cases, the true label was <kbd>0</kbd>, yet we predicted <kbd>1</kbd>. Hence, our accuracy should be 1/5 or 0.2.</p>
<p>A naive implementation of an accuracy metric might sum up all occurrences where the predicted class label matched the true class label:</p>
<pre>In [5]: test_set_size = len(y_true)<br/>In [6]: predict_correct = np.sum(y_true == y_pred)<br/>In [7]: predict_correct / test_set_size<br/>Out[7]: 0.2</pre>
<p>A smarter, and more convenient, implementation is provided by scikit-learn's <kbd>metrics</kbd> module:</p>
<pre>In [8]: from sklearn import metrics<br/>In [9]: metrics.accuracy_score(y_true, y_pred)<br/>Out[9]: 0.2</pre>
<p>That wasn't too hard, was it? However, to understand precision and recall, we need a general understanding of type I and type II errors. Let's recall that data points with the class label, <kbd>1</kbd>, are often called positives, and data points with the class label, <kbd>0</kbd> (or -1) are often called negatives. Then, classifying a specific data point can have one of four possible outcomes, as illustrated by the following confusion matrix:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly positive</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly negative</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted positive</strong></td>
<td class="CDPAlignCenter CDPAlign">True Positive</td>
<td class="CDPAlignCenter CDPAlign">False Positive</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted negative</strong></td>
<td class="CDPAlignCenter CDPAlign">False Negative</td>
<td class="CDPAlignCenter CDPAlign">True Negative</td>
</tr>
</tbody>
</table>
<p>Let's break this down. If a data point was truly positive, and we predicted a positive, we got it all right! In this case, the outcome is called a <strong>true positive</strong>. If we thought the data point was a positive, but it was really a negative, we falsely predicted a positive (hence the term, <strong>false positive</strong>). Analogously, if we thought the data point was negative, but it was really a positive, we falsely predicted a negative (false negative). Finally, if we predicted a negative and the data point was truly a negative, we found a true negative.</p>
<p>In statistical hypothesis testing, false positives are also known as <strong>type I errors</strong> and false negatives are also known as <strong>type II errors</strong>.</p>
<p>Let's quickly calculate these four metrics on our mock-up data. We have a true positive, where the true label is <kbd>1</kbd> and we predicted <kbd>1</kbd>:</p>
<pre>In [10]: truly_a_positive = (y_true == 1)<br/>In [11]: predicted_a_positive = (y_pred == 1)<br/>In [12]: true_positive = np.sum(predicted_a_positive * truly_a_positive )<br/>...      true_positive<br/>Out[12]: 1</pre>
<p>Similarly, a false positive is where we predicted <kbd>1</kbd> but <kbd>ground truth</kbd> was really <kbd>0</kbd>:</p>
<pre>In [13]: false_positive = np.sum((y_pred == 1) * (y_true == 0))<br/>...      false_positive<br/>Out[13]: 4</pre>
<p>I'm sure by now you've got the hang of it. But do we even have to do the math to know about predicted negatives? Our not-so-smart classifier never predicted <kbd>0</kbd>, so <kbd>(y_pred == 0)</kbd> should never be true:</p>
<pre>In [14]: false_negative = np.sum((y_pred == 0) * (y_true == 1))<br/>...      false_negative<br/>Out[14]: 0<br/>In [15]: true_negative = np.sum((y_pred == 0) * (y_true == 0))<br/>...      true_negative<br/>Out[15]: 0</pre>
<p>Let's also draw the confusion matrix:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"/>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly positive</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Is truly negative</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted positive</strong></td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">4</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Predicted negative</strong></td>
<td class="CDPAlignCenter CDPAlign">0</td>
<td class="CDPAlignCenter CDPAlign">0</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To make sure we did everything right, let's calculate the accuracy one more time. Accuracy should be the number of true positives plus the number of true negatives (that is, everything we got right) divided by the total number of data points:</p>
<pre>In [16]: accuracy = (true_positive + true_negative) / test_set_size<br/>...      accuracy<br/>Out[16]: 0.2</pre>
<p>Success! Precision is then given as the number of true positives divided by the number of all true predictions:</p>
<pre>In [17]: precision = true_positive / (true_positive + false_positive)<br/>...      precision<br/>Out[17]: 0.2</pre>
<p>It turns out that precision is no better than accuracy in our case. Let's check our math with scikit-learn:</p>
<pre>In [18]: metrics.precision_score(y_true, y_pred)<br/>Out[18]: 0.2</pre>
<p>Finally, <kbd>recall</kbd> is given as the fraction of all positives that we correctly classified as positives:</p>
<pre>In [19]: recall = true_positive / (true_positive + false_negative)<br/>...      recall<br/>Out[19]: 1.0<br/>In [20]: metrics.recall_score(y_true, y_pred)<br/>Out[20]: 1.0</pre>
<p>Perfect recall! But, going back to our mock-up data, it should be clear that this excellent recall score was mere luck. Since there was only a single <kbd>1</kbd> label in our mock-up dataset, and we happened to correctly classify it, we got a perfect recall score. Does that mean our classifier is perfect? Not really! But we have found three useful metrics that seem to measure complementary aspects of our classification performance.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Scoring regressors using mean squared error, explained variance, and R squared</h1>
                
            
            
                
<p>When it comes to regression models, our metrics, as shown earlier, don't work anymore. After all, we are now predicting continuous output values, not distinct classification labels. Fortunately, scikit-learn provides some other useful scoring functions:</p>
<ul>
<li><kbd>mean_squared_error</kbd>: The most commonly used error metric for regression problems is to measure the squared error between the predicted and the true target value for every data point in the training set, averaged across all of the data points.</li>
<li><kbd>explained_variance_score</kbd>: A more sophisticated metric is to measure to what degree a model can explain the variation or dispersion of the test data. Often, the amount of explained ...</li></ul></div>



  
<div><h1 class="header-title">Using classification models to predict class labels</h1>
                
            
            
                
<p>With these tools in hand, we can now take on our first real classification example.</p>
<p>Consider the small town of Randomville, where people are crazy about their two sports teams, the Randomville Reds and the Randomville Blues. The Reds had been around for a long time, and people loved them. But then, some out-of-town millionaire came along and bought the Reds' top scorer and started a new team, the Blues. To the discontent of most Reds fans, the top scorer would go on to win the championship title with the Blues. Years later, he would return to the Reds, despite some backlash from fans who could never forgive him for his earlier career choices. But anyway, you can see why fans of the Reds don't necessarily get along with fans of the Blues. In fact, these two fan bases are so divided that they never even live next to each other. I've even heard stories where the Red fans deliberately moved away once Blues fans moved in next door. True story!</p>
<p>Anyway, we are new in town and are trying to sell some Blues merchandise to people by going from door to door. However, every now and then we come across a bleeding-heart Reds fan who yells at us for selling Blues stuff and chases us off their lawn. Not nice! It would be much less stressful, and a better use of our time, to avoid these houses altogether and just visit the Blues fans instead.</p>
<p>Confident that we can learn to predict where the Reds fans live, we start keeping track of our encounters. If we come by a Reds fan's house, we draw a red triangle on our handy town map; otherwise, we draw a blue square. After a while, we get a pretty good idea of where everyone lives:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-808 image-border" src="img/ced3ad58-fea3-4305-8752-ecebfde7cc9f.png" style="width:18.50em;height:12.17em;" width="982" height="649"/></p>
<p>However, now, we approach the house that is marked as a green circle in the preceding map. Should we knock on their door? We try to find some clue as to what team they prefer (perhaps a team flag hanging from the back porch), but we can't see any. How can we know if it is safe to knock on their door?</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>What this silly example illustrates is exactly the kind of problem a supervised learning algorithm can solve. We have a bunch of observations (houses, their locations, and their colors) that make up our training data. We can use this data to learn from experience so that, when we face the task of predicting the color of a new house, we can make a well-informed estimate.</p>
<p>As we mentioned earlier, fans of the Reds are really passionate about their team, so they would never move next to a Blues fan. Couldn't we use this information and look at all of the neighboring houses, to find out what kind of fan lives in the new house?</p>
<p>This is exactly what the k-NN algorithm would do.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding the k-NN algorithm</h1>
                
            
            
                
<p>The k-NN algorithm is arguably one of the simplest machine learning algorithms. The reason for this is that we basically only need to store the training dataset. Then, to predict a new data point, we only need to find the closest data point in the training dataset: its nearest neighbor.</p>
<p>In a nutshell, the k-NN algorithm argues that a data point probably belongs to the same class as its neighbors. Think about it: if our neighbor is a Reds fan, we're probably Reds fans, too; otherwise, we would have moved away a long time ago. The same can be said for the Blues.</p>
<p>Of course, some neighborhoods might be a little more complicated. In this case, we would not just consider our closest neighbor (where <em>k=1</em>), but instead ...</p></div>



  
<div><h1 class="header-title">Implementing k-NN in OpenCV</h1>
                
            
            
                
<p>Using OpenCV, we can easily create a k-NN model via the <kbd>cv2.ml.KNearest_create()</kbd> function. Building the model then involves the following steps:</p>
<ol>
<li>Generate some training data.</li>
<li>Create a k-NN object for a given number, <em>k</em>.</li>
</ol>
<ol start="3">
<li>Find the <em>k</em> nearest neighbors of a new data point that we want to classify.</li>
<li>Assign the class label of the new data point by majority vote.</li>
<li>Plot the result.</li>
</ol>
<p>We first import all of the necessary modules: OpenCV for the k-NN algorithm, NumPy for data processing, and Matplotlib for plotting. If you are working in a Jupyter Notebook, don't forget to call the <kbd>%matplotlib inline</kbd> magic:</p>
<pre>In [1]: import numpy as np<br/>...     import cv2<br/>...     import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>In [2]: plt.style.use('ggplot')</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Generating the training data</h1>
                
            
            
                
<p>The first step is to generate some training data. For this, we will use NumPy's random number generator. As discussed in the previous section, we will fix the seed of the random number generator, so that re-running the script will always generate the same values:</p>
<pre>In [3]: np.random.seed(42)</pre>
<p>Alright, now let's get to it. What should our training data look like exactly?</p>
<p>In the previous example, each data point is a house on the town map. Every data point has two features (that is, the <em>x</em> and <em>y</em> coordinates of its location on the town map) and a class label (that is, a blue square if a Blues fan lives there and a red triangle if a Reds fan lives there).</p>
<p>The features of a single data point can, therefore, be represented ...</p></div>



  
<div><h1 class="header-title">Training the classifier</h1>
                
            
            
                
<p>As with all other machine learning functions, the k-NN classifier is part of the OpenCV 3.1 <kbd>ml</kbd> module. We can create a new classifier using the following command:</p>
<pre>In [15]: knn = cv2.ml.KNearest_create()</pre>
<p>In older versions of OpenCV, this function might be called <kbd>cv2.KNearest()</kbd> instead.</p>
<p>We then pass our training data to the <kbd>train</kbd> method:</p>
<pre>In [16]: knn.train(train_data, cv2.ml.ROW_SAMPLE, labels)<br/>Out[16]: True</pre>
<p>Here, we have to tell <kbd>knn</kbd> that our data is an <em>N x 2</em> array (that is, every row is a data point). Upon success, the function returns <kbd>True</kbd>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Predicting the label of a new data point</h1>
                
            
            
                
<p>The other really helpful method that <kbd>knn</kbd> provides is called <kbd>findNearest</kbd>. It can be used to predict the label of a new data point based on its nearest neighbors.</p>
<p>Thanks to our <kbd>generate_data</kbd> function, it is actually really easy to generate a new data point! We can think of a new data point as a dataset of size <kbd>1</kbd>:</p>
<pre>In [17]: newcomer, _ = generate_data(1)...      newcomerOut[17]: array([[91., 59.]], dtype=float32)</pre>
<p>Our function also returns a random label, but we are not interested in that. Instead, we want to predict it using our trained classifier! We can tell Python to ignore an output value with an underscore (<kbd>_</kbd>).</p>
<p>Let's have a look at our town map again. We will plot the training set as we did earlier, but ...</p></div>



  
<div><h1 class="header-title">Using regression models to predict continuous outcomes</h1>
                
            
            
                
<p>Now, let's turn our attention to a regression problem. As I'm sure you can recite in your sleep by now, regression is all about predicting continuous outcomes rather than predicting discrete class labels.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding linear regression</h1>
                
            
            
                
<p>The easiest regression model is called <strong>linear regression</strong>. The idea behind linear regression is to describe a target variable (such as Boston house pricing—recall the various datasets we studied in <a href="7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml">Chapter 1</a>, <em>A Taste of Machine Learning</em>) with a linear combination of features.</p>
<p>To keep things simple, let's just focus on two features. Let's say we want to predict tomorrow's stock prices using two features: today's stock price and yesterday's stock price. We will denote today's stock price as the first feature, <em>f<sub>1</sub></em>, and yesterday's stock price as <em>f<sub>2</sub></em>. Then, the goal of linear regression would be to learn two weight coefficients, <em>w<sub>1</sub></em> and <em>w<sub>2</sub></em>, so that we can predict tomorrow's stock price as follows:</p>

<p>Here,  is the </p></div>



  
<div><h1 class="header-title">Linear regression in OpenCV</h1>
                
            
            
                
<p>Before trying out linear regression on a real-life dataset, let's understand how we can use the <kbd>cv2.fitLine</kbd> function to fit a line to a 2D or 3D point set:</p>
<ol>
<li>Let's start by generating some points. We will generate them by adding noise to the points lying on the line <img class="fm-editor-equation" src="img/25934214-92e9-4194-937c-af8f014804a7.png" style="width:5.67em;height:1.25em;" width="860" height="200"/>:</li>
</ol>
<pre style="padding-left: 60px">In [1]: import cv2<br/>...     import numpy as np<br/>...     import matplotlib.pyplot as plt<br/>...     from sklearn import linear_model<br/>...     from sklearn.model_selection import train_test_split<br/>...     plt.style.use('ggplot')<br/>...     %matplotlib inline<br/>In [2]: x = np.linspace(0,10,100)<br/>...     y_hat = x*5+5<br/>...     np.random.seed(42)<br/>...     y = x*5 + 20*(np.random.rand(x.size) - 0.5)+5<strong><br/></strong></pre>
<ol start="2">
<li>We can also visualize these points using the following code:</li>
</ol>
<pre style="padding-left: 60px">In [3]: plt.figure(figsize=(10, 6))<br/>...     plt.plot(x, y_hat, linewidth=4)<br/>...     plt.plot(x,y,'x')<br/>...     plt.xlabel('x')<br/>...     plt.ylabel('y')<strong><br/></strong></pre>
<p style="padding-left: 60px">This gives us the following diagram, where the red line is the true function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-813 image-border" src="img/93344671-1736-4539-991e-8face2c9f3a6.png" style="width:33.58em;height:21.33em;" width="620" height="396"/></p>
<ol start="3">
<li>Next, we will split the points into training and testing sets. Here, we will split the data into a 70:30 ratio, meaning, 70% of the points will be used for training and 30% for testing:</li>
</ol>
<pre style="padding-left: 60px">In [4]: x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)</pre>
<ol start="4">
<li>Now, let's use <kbd>cv2.fitLine</kbd> to fit a line to this 2D point set. This function takes in the following arguments:
<ul>
<li><kbd>points</kbd>: This is the set of points to which a line has to be fit.</li>
<li><kbd>distType</kbd>: This is the distance used by the M-estimator.</li>
<li><kbd>param</kbd>: This is the numerical parameter (C), which is used in some types of distances. We will keep it at 0 so that an optimal value can be chosen.</li>
<li><kbd>reps</kbd>: This is the accuracy of the distance between the origin and the line. <kbd>0.01</kbd> is a good default value for <kbd>reps</kbd>.</li>
<li><kbd>aeps</kbd>: This is the accuracy of the angle. <kbd>0.01</kbd> is a good default value for <kbd>aeps</kbd>.</li>
</ul>
</li>
</ol>
<p>For more information, have a look at the <a href="https://docs.opencv.org/4.0.0/d3/dc0/group__imgproc__shape.html#gaf849da1fdafa67ee84b1e9a23b93f91f">documentation</a>.</p>
<ol start="5">
<li>Let's see what kinds of result we get using different distance type options:</li>
</ol>
<pre style="padding-left: 60px">In [5]: distTypeOptions = [cv2.DIST_L2,\<br/>...                 cv2.DIST_L1,\<br/>...                 cv2.DIST_L12,\<br/>...                 cv2.DIST_FAIR,\<br/>...                 cv2.DIST_WELSCH,\<br/>...                 cv2.DIST_HUBER]<br/><br/>In [6]: distTypeLabels = ['DIST_L2',\<br/>...                 'DIST_L1',\<br/>...                 'DIST_L12',\<br/>...                 'DIST_FAIR',\<br/>...                 'DIST_WELSCH',\<br/>...                 'DIST_HUBER']<br/><br/>In [7]: colors = ['g','c','m','y','k','b']<br/>In [8]: points = np.array([(xi,yi) for xi,yi in zip(x_train,y_train)])</pre>
<ol start="6">
<li>We will also use scikit-learn's <kbd>LinearRegression</kbd> to fit the training points and then use the <kbd>predict</kbd> function to predict the <em>y</em>-values for them:</li>
</ol>
<pre style="padding-left: 60px">In [9]: linreg = linear_model.LinearRegression()<br/>In [10]: linreg.fit(x_train.reshape(-1,1),y_train.reshape(-1,1))<br/>Out[10]:LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,normalize=False)<br/>In [11]: y_sklearn = linreg.predict(x.reshape(-1,1))<br/>In [12]: y_sklearn = list(y_sklearn.reshape(1,-1)[0])</pre>
<ol start="7">
<li>We use <kbd>reshape(-1,1)</kbd> and <kbd>reshape(1,-1)</kbd> to convert the NumPy arrays into a column vector and then back into a row vector:</li>
</ol>
<pre style="padding-left: 60px">In [13]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(x, y_hat,linewidth=2,label='Ideal')<br/>...      plt.plot(x,y,'x',label='Data')<br/><br/>...      for i in range(len(colors)):<br/>...          distType = distTypeOptions[i]<br/>...          distTypeLabel = distTypeLabels[i]<br/>...          c = colors[i]<br/>    <br/>...          [vxl, vyl, xl, yl] = cv2.fitLine(np.array(points, dtype=np.int32), distType, 0, 0.01, 0.01)<br/>...          y_cv = [vyl[0]/vxl[0] * (xi - xl[0]) + yl[0] for xi in x]<br/>...          plt.plot(x,y_cv,c=c,linewidth=2,label=distTypeLabel)<br/><br/>...      plt.plot(x,list(y_sklearn),c='0.5',\<br/>linewidth=2,label='Scikit-Learn API')<br/>...      plt.xlabel('x')<br/>...      plt.ylabel('y')<br/>...      plt.legend(loc='upper left')</pre>
<p style="padding-left: 60px">The only purpose of this preceding (and lengthy) code was to create a plot that could be used to compare the results obtained using different distance measures.</p>
<p style="padding-left: 60px">Let's have a look at the plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-814 image-border" src="img/02196ce1-87a5-4273-b2b5-43bd4f62b56e.png" style="width:40.92em;height:25.83em;" width="628" height="395"/></p>
<p>As we can clearly see, scikit-learn's <kbd>LinearRegression</kbd> model performs much better than OpenCV's <kbd>fitLine</kbd> function. Now, let's use scikit-learn's API to predict Boston housing prices.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using linear regression to predict Boston housing prices</h1>
                
            
            
                
<p>To get a better understanding of linear regression, we want to build a simple model that can be applied to one of the most famous machine learning datasets: the <strong>Boston housing prices dataset</strong>. Here, the goal is to predict the value of homes in several Boston neighborhoods in the 1970s, using information such as crime rate, property tax rate, distance to employment centers, and highway accessibility.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Loading the dataset</h1>
                
            
            
                
<p>We can again thank scikit-learn for easy access to the dataset. We first import all of the necessary modules, as we did earlier:</p>
<pre>In [14]: from sklearn import datasets<br/>...      from sklearn import metrics<strong><br/></strong></pre>
<p>Then loading the dataset is a one-liner:</p>
<pre>In [15]: boston = datasets.load_boston()</pre>
<p>The structure of the <kbd>boston</kbd> object is identical to the <kbd>iris</kbd> object, as discussed in the preceding command. We can get more information about the dataset in <kbd>'DESCR'</kbd> and find all data in <kbd>'data'</kbd>, all feature names in <kbd>'feature_names'</kbd>, the physical location of the Boston CSV dataset in <kbd>'filename'</kbd>, and all target values in <kbd>'target'</kbd>:</p>
<pre>In [16]: dir(boston)<br/>Out[16]: ['DESCR', 'data', 'feature_names', 'filename', 'target']</pre>
<p>The dataset contains a total of <kbd>506</kbd> data points, each of which has <kbd>13</kbd> features:</p>
<pre>In [17]: boston.data.shape<br/>Out[17]: (506, 13)</pre>
<p>Of course, we have only a single target value, which is the housing price:</p>
<pre>In [18]: boston.target.shape<br/>Out[18]: (506,)</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Training the model</h1>
                
            
            
                
<p>Now let's create a <kbd>LinearRegression</kbd> model that we will then train on the training set:</p>
<pre>In [19]: linreg = linear_model.LinearRegression()</pre>
<p>In the preceding command, we want to split the data into training and test sets. We are free to make the split as we see fit, but usually it is a good idea to reserve between 10 percent and 30 percent for testing. Here, we choose 10 percent, using the <kbd>test_size</kbd> argument:</p>
<pre>In [20]: X_train, X_test, y_train, y_test = train_test_split(...            boston.data, boston.target, test_size=0.1,...            random_state=42...      )</pre>
<p>In scikit-learn, the <kbd>train</kbd> function is called <kbd>fit</kbd> but otherwise behaves exactly the same as in OpenCV:</p>
<pre>In [21]: linreg.fit(X_train, y_train)Out[21]: LinearRegression(copy_X=True, fit_intercept=True, ...</pre></div>



  
<div><h1 class="header-title">Testing the model</h1>
                
            
            
                
<p>To test the generalization performance of the model, we calculate the mean squared error on the test data:</p>
<pre>In [24]: y_pred = linreg.predict(X_test)<br/>In [25]: metrics.mean_squared_error(y_test, y_pred)<br/>Out[25]: 14.995852876582541</pre>
<p>We note that the mean squared error is a little lower on the test set than the training set. This is good news, as we care mostly about the test error. However, from these numbers it is really hard to understand how good the model really is. Perhaps it's better to plot the data:</p>
<pre>In [26]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(y_test, linewidth=3, label='ground truth')<br/>...      plt.plot(y_pred, linewidth=3, label='predicted')<br/>...      plt.legend(loc='best')<br/>...      plt.xlabel('test data points')<br/>...      plt.ylabel('target value')<br/>Out[26]: &lt;matplotlib.text.Text at 0x7ff46783c7b8&gt;</pre>
<p>This produces the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-815 image-border" src="img/2c458025-ac75-4430-91a7-23f68787efb4.png" style="width:31.92em;height:19.67em;" width="620" height="383"/></p>
<p>This makes more sense! Here, we see the <kbd>ground truth</kbd> housing prices for all test samples in red and our predicted housing prices in blue. It's pretty close, if you ask me. It is interesting to note, though, that the model tends to be off the most for really high or really low housing prices, such as the peak values of data points <strong>12</strong>, <strong>18</strong>, and <strong>42</strong>. We can formalize the amount of variance in the data that we were able to explain by calculating R squared:</p>
<pre>In [27]: plt.figure(figsize=(10, 6))<br/>...      plt.plot(y_test, y_pred, 'o')<br/>...      plt.plot([-10, 60], [-10, 60], 'k--')<br/>...      plt.axis([-10, 60, -10, 60])<br/>...      plt.xlabel('ground truth')<br/>...      plt.ylabel('predicted')</pre>
<p>This will plot the <kbd>ground truth</kbd> prices, <kbd>y_test</kbd>, on the <em>x</em> axis and our predictions, <kbd>y_pred</kbd>, on the <em>y</em> axis. We also plot a diagonal line for reference (using a black dashed line, <kbd>'k--'</kbd>), as we will see soon. But we also want to display the R<sup>2</sup> score and mean squared error in a textbox:</p>
<pre>...      scorestr = r'R$^2$ = %.3f' % linreg.score(X_test, y_test)<br/>...      errstr = 'MSE = %.3f' % metrics.mean_squared_error(y_test, y_pred)<br/>...      plt.text(-5, 50, scorestr, fontsize=12)<br/>...      plt.text(-5, 45, errstr, fontsize=12)<br/>Out[27]: &lt;matplotlib.text.Text at 0x7ff4642d0400&gt;</pre>
<p>This will produce the following diagram and is a professional way of plotting a model fit:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-816 image-border" src="img/139e4726-1300-4c73-aa11-c8282dba5832.png" style="width:30.75em;height:18.42em;" width="653" height="391"/></p>
<p>If our model was perfect, then all data points would lie on the dashed diagonal, since <kbd>y_pred</kbd> would always be equal to <kbd>y_true</kbd>. Deviations from the diagonal indicate that the model made some errors, or that there is some variance in the data that the model was not able to explain. Indeed, <img class="fm-editor-equation" src="img/a15dc10c-dde6-4547-ac44-71066ff64510.png" style="width:1.33em;height:1.17em;" width="230" height="200"/> indicates that we were able to explain 76% of the scatter in the data, with a mean squared error of 14.996. These are some performance measures we can use to compare the linear regression model to some more complicated ones.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Applying Lasso and ridge regression</h1>
                
            
            
                
<p>A common problem in machine learning is that an algorithm might work really well on the training set but, when applied to unseen data, it makes a lot of mistakes. You can see how this is problematic since, often, we are most interested in how a model generalizes to new data. Some algorithms (such as decision trees) are more susceptible to this phenomenon than others, but even linear regression can be affected.</p>
<p>This phenomenon is also known as <strong>overfitting</strong>, and we will talk about it extensively in <a href="5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml" target="_blank">Chapter 5</a>, <em>Using Decision Trees to Make a Medical Diagnosis</em>, and <a href="904bc419-cb0e-44cd-ae3f-8ce97e15baa2.xhtml" target="_blank">Chapter 11</a>, <em>Selecting the Right Model with Hyperparameter Tuning</em>.</p>
<p>A common technique for reducing overfitting is called <strong>regularization</strong>, which involves ...</p></div>



  
<div><h1 class="header-title">Classifying iris species using logistic regression</h1>
                
            
            
                
<p>Another famous dataset in the world of machine learning is called the Iris dataset. The Iris dataset contains measurements of 150 iris flowers from three different species: Setosa, Versicolor, and Viriginica. These measurements include the length and width of the petals and the length and width of the sepals, all measured in centimeters.</p>
<p class="CDPAlignLeft CDPAlign">Our goal is to build a machine learning model that can learn the measurements of these iris flowers, the species of which are known, so that we can predict the species for a new iris flower.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Understanding logistic regression</h1>
                
            
            
                
<p>Before we start off with this section, let me issue warning—logistic regression, despite its name, is actually a model for classification, specifically when you have two classes. It derives its name from the  logistic function (or sigmoid) it uses to convert any real-valued input <em>x</em> into a predicted output value <em>ŷ</em> that takes values between <strong>0</strong> and <strong>1</strong>, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-817 image-border" src="img/b9b54eb8-287c-4286-a68e-d5f9496af292.png" style="width:10.08em;height:9.42em;" width="712" height="657"/></p>
<p>Rounding <em>ŷ</em> to the nearest integer effectively classifies the input as belonging either to class <strong>0</strong> or <strong>1</strong>.</p>
<p>Of course, most often, our problems have more than one input or feature value, <em>x</em>. For example, the Iris dataset provides a total ...</p></div>



  
<div><h1 class="header-title">Loading the training data</h1>
                
            
            
                
<p>The Iris dataset is included with scikit-learn. We first load all of the necessary modules, as we did in our earlier examples:</p>
<pre>In [1]: import numpy as np<br/>...     import cv2<br/>...     from sklearn import datasets<br/>...     from sklearn import model_selection<br/>...     from sklearn import metrics<br/>...     import matplotlib.pyplot as plt<br/>...     %matplotlib inline<br/>In [2]: plt.style.use('ggplot')</pre>
<p>Then loading the dataset is a one-liner:</p>
<pre>In [3]: iris = datasets.load_iris()</pre>
<p>This function returns a dictionary we call <kbd>iris</kbd>, which contains a bunch of different fields:</p>
<pre>In [4]: dir(iris)<br/>Out[4]: ['DESCR', 'data', 'feature_names', 'filename', 'target', 'target_names']</pre>
<p class="CDPAlignLeft CDPAlign">Here, all of the data points are contained in <kbd>'data'</kbd>. There are <kbd>150</kbd> data points, each of which has <kbd>4</kbd> feature values:</p>
<pre>In [5]: iris.data.shape<br/>Out[5]: (150, 4)</pre>
<p class="CDPAlignLeft CDPAlign">These four features correspond to the sepal and petal dimensions mentioned earlier:</p>
<pre>In [6]: iris.feature_names<br/>Out[6]: ['sepal length (cm)',<br/>         'sepal width (cm)',<br/>         'petal length (cm)',<br/>         'petal width (cm)']</pre>
<p class="CDPAlignLeft CDPAlign">For every data point, we have a class label stored in <kbd>target</kbd>:</p>
<pre>In [7]: iris.target.shape<br/>Out[7]: (150,)</pre>
<p class="CDPAlignLeft CDPAlign">We can also inspect the class labels and find that there is a total of three classes:</p>
<pre>In [8]: np.unique(iris.target)<br/>Out[8]: array([0, 1, 2])</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Making it a binary classification problem</h1>
                
            
            
                
<p>For the sake of simplicity, we want to focus on a binary classification problem for now, where we only have two classes. The easiest way to do this is to discard all data points belonging to a certain class, such as class label 2, by selecting all of the rows that do not belong to class <kbd>2</kbd>:</p>
<pre>In [9]: idx = iris.target != 2...     data = iris.data[idx].astype(np.float32)...     target = iris.target[idx].astype(np.float32)</pre>
<p>Next, let's inspect the data.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Inspecting the data</h1>
                
            
            
                
<p>Before you get started with setting up a model, it is always a good idea to have a look at the data. We did this earlier for the town map example, so let's repeat it here too. Using Matplotlib, we create a <strong>scatter plot</strong> where the color of each data point corresponds to the class label:</p>
<pre>In [10]: plt.scatter(data[:, 0], data[:, 1], c=target,  <br/>                     cmap=plt.cm.Paired, s=100)<br/>...      plt.xlabel(iris.feature_names[0])<br/>...      plt.ylabel(iris.feature_names[1])<br/>Out[10]: &lt;matplotlib.text.Text at 0x23bb5e03eb8&gt;</pre>
<p class="CDPAlignLeft CDPAlign">To make plotting easier, we limit ourselves to the first two features (<kbd>iris.feature_names[0]</kbd> being the sepal length and <kbd>iris.feature_names[1]</kbd> being the sepal width). We can see a nice separation of classes in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-818 image-border" src="img/02b2daa3-8a42-4601-b8c5-e84f87ed0cdb.png" style="width:38.33em;height:23.58em;" width="623" height="383"/></p>
<p>The preceding image shows the plot for the first two features of the Iris dataset.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Splitting data into training and test sets</h1>
                
            
            
                
<p>We learned, in the previous chapter, that it is essential to keep training and test data separate. We can easily split the data using one of scikit-learn's many helper functions:</p>
<pre>In [11]: X_train, X_test, y_train, y_test = model_selection.train_test_split(...            data, target, test_size=0.1, random_state=42...      )</pre>
<p class="CDPAlignLeft CDPAlign">Here, we want to split the data into 90% training data and 10% test data, which we specify with <kbd>test_size=0.1</kbd>. By inspecting the return arguments, we note that we ended up with exactly <kbd>90</kbd> training data points and <kbd>10</kbd> test data points:</p>
<pre>In [12]: X_train.shape, y_train.shapeOut[12]: ((90, 4), (90,))In [13]: X_test.shape, y_test.shapeOut[13]: ((10, 4), (10,))</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Training the classifier</h1>
                
            
            
                
<p>Creating a logistic regression classifier involves pretty much the same steps as setting up k-NN:</p>
<pre>In [14]: lr = cv2.ml.LogisticRegression_create()</pre>
<p>We then have to specify the desired training method. Here, we can choose <kbd>cv2.ml.LogisticRegression_BATCH</kbd> or <kbd>cv2.ml.LogisticRegression_MINI_BATCH</kbd>. For now, all we need to know is that we want to update the model after every data point, which can be achieved with the following code:</p>
<pre>In [15]: lr.setTrainMethod(cv2.ml.LogisticRegression_MINI_BATCH)<br/>...      lr.setMiniBatchSize(1)</pre>
<p>We also want to specify the number of iterations the algorithm should run before it terminates:</p>
<pre>In [16]: lr.setIterations(100)</pre>
<p>We can then call the <kbd>train</kbd> method of the object (in the exact same way as we did earlier), which will return <kbd>True</kbd> upon success:</p>
<pre>In [17]: lr.train(X_train, cv2.ml.ROW_SAMPLE, y_train)<br/>Out[17]: True</pre>
<p>As we just saw, the goal of the training phase is to find a set of weights that best transform the feature values into an output label. A single data point is given by its four feature values (<em>f<sub>0</sub></em>, <em>f<sub>1</sub></em>, <em>f<sub>2</sub></em>, and <em>f<sub>3</sub></em>). Since we have four features, we should also get four weights, so that <em>x = w<sub>0</sub> f<sub>0</sub> + w<sub>1</sub> f<sub>1</sub> + w<sub>2</sub> f<sub>2</sub> + w<sub>3</sub> f<sub>3</sub></em>, and <em>ŷ=σ(x)</em>. However, as discussed previously, the algorithm adds an extra weight that acts as an offset or bias, so that <em>x = w<sub>0</sub> f<sub>0</sub> + w<sub>1</sub> f<sub>1</sub> + w<sub>2</sub> f<sub>2</sub> + w<sub>3</sub> f<sub>3</sub> + w<sub>4</sub></em>. We can retrieve these weights as follows:</p>
<pre>In [18]: lr.get_learnt_thetas()<br/>Out[18]: array([[-0.04090132, -0.01910266, -0.16340332, 0.28743777, 0.11909772]], dtype=float32)</pre>
<p>This means that the input to the logistic function is <em>x = -0.0409 f<sub>0</sub> - 0.0191 f<sub>1</sub> - 0.163 f<sub>2</sub> + 0.287 f<sub>3</sub> + 0.119</em>. Then, when we feed in a new data point (<em>f<sub>0</sub></em>, <em>f<sub>1</sub></em>, <em>f<sub>2</sub></em>, <em>f<sub>3</sub></em>) that belongs to class 1, the output <em>ŷ=σ(x)</em> should be close to 1. But how well does that actually work?</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Testing the classifier</h1>
                
            
            
                
<p>Let's see for ourselves by calculating the accuracy score on the training set:</p>
<pre>In [19]: ret, y_pred = lr.predict(X_train)In [20]: metrics.accuracy_score(y_train, y_pred)Out[20]: 1.0</pre>
<p>Perfect score! However, this only means that the model was able to perfectly <strong>memorize</strong> the training dataset. This does not mean that the model would be able to classify a new, unseen data point. For this, we need to check the test dataset:</p>
<pre>In [21]: ret, y_pred = lr.predict(X_test)...      metrics.accuracy_score(y_test, y_pred)Out[21]: 1.0</pre>
<p>Luckily, we get another perfect score! Now we can be sure that the model we built is truly awesome.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we covered quite a lot of ground, didn't we?</p>
<p>In short, we learned a lot about different supervised learning algorithms, how to apply them to real datasets, and how to implement everything in OpenCV. We introduced classification algorithms such as k-NN and logistic regression and discussed how they could be used to predict labels as two or more discrete categories. We introduced various variants of linear regression (such as Lasso regression and ridge regression) and discussed how they could be used to predict continuous variables. Last but not least, we got acquainted with the Iris and Boston datasets, two classics in the history of machine learning.</p>
<p>In the following chapters, we will go into much greater depth within these topics and explore some more interesting examples of where these concepts can be useful.</p>
<p>But first, we need to talk about another essential topic in machine learning, feature engineering. Often, data does not come in nicely formatted datasets, and it is our responsibility to represent the data in a meaningful way. Therefore, the next chapter will talk about representing features and engineering data.</p>


            

            
        
    </div>



  </body></html>