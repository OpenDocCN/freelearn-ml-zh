- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML Deployments and Database Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The biggest sources of ongoing maintenance for *RPA* are due to handling changes
    to the business logic (Processes) and applications (Objects). With IA, another
    source of ongoing changes is added into the mix—updating BP to use new ML models
    as they’re changed. Imagine that models are only updated once per year and that
    we have five models. That’s still five times per year that we’ll need to modify
    how our IA solutions are configured due to ML model updates.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the updated ML model keeps the exact same API endpoint definition as
    before, we still need to understand what ML deployment strategy is being used,
    as this affects how we plan for **rollbacks** in case the model is not performing
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the **ML Deployer** User Role in [*Chapter 8*](B18416_08.xhtml#_idTextAnchor133).
    This User Role is responsible for regularly updating the ML models that are in
    production. In this chapter, we’ll discuss the most common ML deployment strategies,
    how they affect what needs to happen in BP to update the model, and the plan for
    rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Another way that IA changes our operations lies in database maintenance and
    ML data extraction. [*Chapter 8*](B18416_08.xhtml#_idTextAnchor133) also discussed
    the **ML Auditor** User Role, which can export Session Log data after logging
    into the BP software. If we don’t want to dedicate a Role within BP for this,
    we can instead regularly extract this data directly from the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics related to operations
    that emerge specifically due to IA:'
  prefs: []
  type: TYPE_NORMAL
- en: ML deployments and rollbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database maintenance and data exporting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML deployments and rollbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML is often developed and deployed by a team that’s outside of the IA organization.
    Once a new model is available, the IA team needs to update the connection points
    between BP and the ML model so that the automation can make use of it. The steps
    needed to safely update these connection points from BP differ based on which
    ML deployment strategy is being used. In this section, we’ll describe the most
    common ML deployment strategies. We’ll also discuss how the IA solution configuration
    should be changed to make use of a new ML model, given a particular strategy.
  prefs: []
  type: TYPE_NORMAL
- en: The ML deployment strategy also impacts how we can roll back from a new model
    to a previous one. Problems with ML models often don’t result in Session-halting
    failures that are easily detected. Models continue to produce predictions even
    if they’re flawed. If an issue is discovered with a model, we need to be ready
    to roll back to a previous one so that automation cases can continue to run. Of
    course, rolling back might not be an option if we’re using third-party-developed
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall the different ways that an ML algorithm can be triggered from BP, which
    were discussed in *Chapters 1*, *2*, and *3*: *Web* *APIs,* *CLI scripts*, and
    *Code Stages*. In the sections that follow, we’ll see how the deployment strategies
    for each of these differ and how our IA solution should be configured to maintain
    auditability. We’ll also discuss the steps needed to safely roll back an ML model
    to its previous version.'
  prefs: []
  type: TYPE_NORMAL
- en: Web API deployment strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Common ways of deploying ML Web APIs include *replacement*, *rolling*, *blue-green*,
    *canary*, and *shadow* deployments. I’ll explain the underlying concepts behind
    each of these methods and what the IA team should do to use the new model in production.
    For many of these strategies, we also need to consider whether the ML team *overwrites*
    the previous model by reusing the *same API endpoint* or offers a *new endpoint*
    in addition to previous endpoints. A new endpoint allows us to quickly switch
    back to an older model.
  prefs: []
  type: TYPE_NORMAL
- en: Replacement deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is the simplest Web API deployment strategy where *planned downtime* is
    required. The old ML model is taken completely offline and the new model is put
    into place. Coordination with the ML team is needed to ensure that no Sessions
    are running during the switch-over period.
  prefs: []
  type: TYPE_NORMAL
- en: API endpoint is overwritten
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If the API endpoint is reused after the new model is deployed, it means that
    the URL is kept exactly the same. We need to use the `Model Version` Environment
    Variable (present in all of the Process templates from [*Chapter 7*](B18416_07.xhtml#_idTextAnchor114))
    to keep track of when the model was actually updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The Model Version Environment Variable should be changed when
    the model is updated](img/image_00_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The Model Version Environment Variable should be changed when the
    model is updated
  prefs: []
  type: TYPE_NORMAL
- en: 'The sequence of steps needed to update the BP solution is shown in the following
    figure. Prior to deploying the new ML model, we have to ensure that Sessions that
    use the model have stopped running since downtime is required. After the new model
    is brought online, we change `Model Version` and start the Sessions again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Replacement deployments when the existing endpoint is overwritten](img/image_00_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Replacement deployments when the existing endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: 'Downtime will be required for the ML team to undo their deployment in case
    of a rollback. A sample rollback procedure is shown in the following image. Instead
    of waiting for in-flight Sessions to end, you might want to request for an `Model
    Version`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Rolling back a replacement deployment when the endpoint is overwritten](img/image_00_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Rolling back a replacement deployment when the endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: Since this replacement deployment strategy requires downtime, it’s better suited
    to IA Processes where the Work Queue volumes are low and the ML model changes
    are infrequent.
  prefs: []
  type: TYPE_NORMAL
- en: New API endpoint is created
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If there’s a new API endpoint, updating the IA solution to use the new endpoint
    is almost the same as the steps in *Figure 9**.2*. The only difference lies in
    updating the Web API Service configuration to use the new API URL, in addition
    to the `Model Version` Environment Variable. Recall that the details of changing
    a URL in a Web API Service don’t actually get saved into the Audit Logs, such
    that you can retrieve the URL’s value. If we’re using an Object and not a Web
    API Service, we need to change the Environment Variable that stores the endpoint
    URL instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Replacement deployments when a new endpoint is created](img/image_00_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Replacement deployments when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling back in this case is easy, as the previous API should still be available.
    All we need to do is to change the Web API Service URL (or the Object Environment
    Variable URL), and the `Model Version` back to what they were before. If you want
    to be even safer, you can retire the Schedules and wait for all Sessions to be
    completed before making these changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Rolling back a replacement deployment when a new endpoint is
    created](img/image_00_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Rolling back a replacement deployment when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: The *replacement* strategy is probably the simplest type of deployment that
    exists. The remaining strategies discussed in this chapter are more sophisticated
    and don’t require downtime. Since there’s no downtime, there won’t be a requirement
    to retire/unretire schedules or ensure that Sessions that use the ML model aren’t
    running. Of course, it’s still safer to perform solution deployments and changes
    during periods of downtime. The *rolling updates* strategy is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rolling deployments can be performed when there are multiple servers hosting
    the API behind a load balancer. An image of a simple rolling deployment can be
    seen in *Figure 9**.6*. We start with all API servers connected to the load balancer,
    running *version 1* of the API (A). *Server 1* is removed from the load balancer
    (*B*), and updated to serve *V2* of the API (*C*). After updating, *Server 1*
    is reconnected to the load balancer (*D*), meaning that both model versions (*V1*
    on *Server 1* and *V2* on *Server 2*) are being served simultaneously. Next, we
    remove *Server 2* from the load balancer (*E*), update it to *V2* (*F*), and connect
    it back to the load balancer (*G*). We repeat these steps until all servers are
    updated.
  prefs: []
  type: TYPE_NORMAL
- en: Note that there’s *no downtime* for the ML API service, but there are also periods
    of time (*D*) where predictions can be made against both the old and new model,
    which complicates auditing. There’s a chance that we won’t know which model was
    used to make a prediction during the timeframe when both the old and new models
    are potentially being served.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – An example rolling update deployment](img/image_00_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – An example rolling update deployment
  prefs: []
  type: TYPE_NORMAL
- en: Again, we have to consider two cases for this type of deployment. The first
    is when the existing API endpoint is overwritten and the second is when a new
    API endpoint is created.
  prefs: []
  type: TYPE_NORMAL
- en: API endpoint is overwritten
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the API endpoint is reused, we might not know which version of the API is
    being served while the servers behind the load balancer are being updated. The
    only way we would know is if the model version is returned as part of the API
    response, but that isn’t guaranteed. For auditing reasons, it’s safest if we suspend
    calling the ML endpoint just prior to when the rolling update begins and resume
    once we know that all servers have been updated. However, we probably won’t know
    when the ML team will start a rolling update, especially if they’re third-party
    developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, it’s simplest to just update the `Model Version` Environment
    Variable once we receive notice that the deployment is 100% complete. If you need
    to know exactly which ML model has been used during the deployment timeframe,
    you’ll need to ask the model developers to check their server logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Rolling deployments when the existing endpoint is overwritten](img/image_00_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Rolling deployments when the existing endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back is largely the same procedure as deploying a new model. A rollback
    is initiated and we wait to be notified that it’s complete. Once complete, we
    can update the `Model Version` Environment Variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Rolling back a rolling deployment when the endpoint is overwritten](img/image_00_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Rolling back a rolling deployment when the endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: New API endpoint is created
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If a new API endpoint is introduced, we need to update two things. The first
    is the `Model Version` Environment Variable.. The second depends on whether we’re
    using a *Web* API *Service* or an *Object* to call the API. Depending on which
    one we’re using, we either change the Web API Service configuration to use the
    new API URL or the Object Environment Variable/Data Item that stores the API URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Rolling deployments when a new endpoint is created](img/image_00_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Rolling deployments when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back the API to a previous version is simple since the previous API
    endpoint still exists. We reverse the changes to the `Model Version` Environment
    Variable, and the Web API Service configuration URL (or Object Environment Variable/Data
    Item).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Rolling back a rolling deployment when a new endpoint is created](img/image_00_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Rolling back a rolling deployment when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: A major disadvantage of rolling deployments is not knowing exactly which ML
    model was used for predictions made during the deployment phase. The next strategy
    that we’ll discuss, called blue-green deployments, doesn’t have this ambiguity
    and also has no downtime.
  prefs: []
  type: TYPE_NORMAL
- en: Blue-green deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a blue-green deployment, there’s duplicate infrastructure positioned behind
    a load balancer, but only one set of infrastructures is actively serving requests
    at any time. A sample blue-green deployment can be seen in *Figure 9**.11*. Suppose
    that we’re currently running *version 1* of the model on the set of *green* servers
    (*A*). When the model needs updating to *version 2*, it’s deployed onto a separate
    set of blue servers, which are normally not serving requests (*B*). Once model
    version 2 is ready to be used, the load balancer is changed to serve requests
    only to the blue servers (*C*). The green server with the previous model sits
    idle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – An example blue-green deployment](img/image_00_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – An example blue-green deployment
  prefs: []
  type: TYPE_NORMAL
- en: Similar to rolling updates, blue-green deployments shouldn’t have service downtime.
    The main difference with blue-green and rolling updates is that there’s always
    a copy of the previous model on standby that can be switched back to by reconfiguring
    the load balancer. There also isn’t an ambiguous period where multiple models
    can be serving predictions at once. Both of these points make blue-green deployments
    better suited for IA auditing when compared to rolling updates.
  prefs: []
  type: TYPE_NORMAL
- en: API endpoint is overwritten
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the URL endpoint is reused, we need to wait for a notification stating that
    the deployment is complete and change the `Model Version` Environment Variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Blue-green deployments when the existing endpoint is overwritten](img/image_00_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Blue-green deployments when the existing endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling back requires the ML/infrastructure team to change the configuration
    of their load balancer. Once that’s done, the IA team can change the `Model Version`
    Environment Variable back to its previous value. The speed of rolling back a blue-green
    deployment when the endpoint is overwritten should be much faster than rolling
    back a rolling update one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Rolling back a blue-green deployment when the endpoint is overwritten](img/image_00_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Rolling back a blue-green deployment when the endpoint is overwritten
  prefs: []
  type: TYPE_NORMAL
- en: New API endpoint is created
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When we have a new API endpoint, we need to update the `Model Version` Environment
    Variable, followed by the Object Environment Variable/Data Item API URL or Web
    API Service configuration URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Blue-green deployments when a new endpoint is created](img/image_00_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Blue-green deployments when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the current set of servers, whether that be blue or green, contain both
    the latest and previous versions of the ML model, rolling back doesn’t actually
    require reconfiguring the load balancer. We simply revert the Environment Variables/Data
    Items or Web API Service URL back to what they were before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Rolling back a blue-green deployment when a new endpoint is
    created](img/image_00_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Rolling back a blue-green deployment when a new endpoint is created
  prefs: []
  type: TYPE_NORMAL
- en: For the three deployment strategies that we’ve discussed so far (replacement,
    rolling, blue-green), the new ML model’s API URL can either be reused (stay the
    same) or changed. If the new model has a different URL, we need to actively change
    the IA solution configuration before the new ML model will be called. This is
    in contrast with the next two deployment methods that we’ll see, where the URL
    of the new model will always stay the same. If previous versions of the model
    are kept active, those will be given new URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Under canary deployments, both the new and old model are served in parallel
    during a period of ML hypercare (*Figure 9**.16*, *B*). Initially, only a small
    percentage of requests are routed to the new model; for example, 95% of requests
    will go to the old model, with 5% of the traffic going to the new one (*C*). The
    percentage of requests reaching the new ML model is increased, as confidence in
    the new model’s performance grows (*D*). This eventually reaches 100% (*E*) and
    the servers hosting the old model are shut down (*F*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – An example canary deployment](img/image_00_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – An example canary deployment
  prefs: []
  type: TYPE_NORMAL
- en: The IA team will likely have no visibility nor control over which requests get
    chosen to run against the new model. This means that we genuinely can’t tell which
    Sessions will be running on the old and new ML models during the switchover period
    unless that information is provided in the API response. If we need to know for
    certain which ML API version is being used for audit reasons, we’ll need to access
    the logs of the ML service directly and cross-reference them with the timestamps
    of the Session Logs, which might not be possible if the model is third-party developed.
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments imply that there’s a single URL endpoint that remains unchanged
    as the model gets updated. So, the only thing that needs to be done in BP is updating
    the `Model Version` Environment Variable once we receive a notification stating
    that all traffic is being directed to the new model. Similar to rolling updates,
    we won’t know for sure which model was being used to make predictions during the
    deployment period.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Canary deployments](img/image_00_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Canary deployments
  prefs: []
  type: TYPE_NORMAL
- en: Full rollbacks in canary deployments are rare, as their purpose is to catch
    issues during the deployment period. What normally happens is that during deployment,
    success criteria for the traffic directed at the new model aren’t met and everything
    is rolled back immediately. This kind of rollback doesn’t require any changes
    to BP because it occurs *before* we’ve made any configuration changes to the IA
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: However, if a rollback is needed after the full deployment has happened, we
    should only need to change back to the previous value of `Model Version`, as the
    API URL remains unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Rolling back a canary deployment](img/image_00_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Rolling back a canary deployment
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments (similar to rolling updates) result in *ambiguity* with respect
    to which model was actually used in production during the deployment period. Because
    of this, it’s best to avoid them (if possible) or to always return the model version
    in the API response. Instead, we can consider using shadow deployments (discussed
    next), which also run two models in parallel. Shadow deployments differ from canary
    deployments, as there’s a clear switchover point between the old and new models.
  prefs: []
  type: TYPE_NORMAL
- en: Shadow deployments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With shadow deployments, both the old and new predictions are called simultaneously
    against live data. However, the newer model is called behind the scenes and its
    API response isn’t sent back to the caller (*Figure 9**.19*, *A*). The shadow
    prediction results are retrieved directly from the server logs, which are analyzed
    by the ML team. Once the ML team is satisfied with the new model, it’s fully switched
    over (*B*) and the previous model is removed (*C*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – An example shadow deployment](img/image_00_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – An example shadow deployment
  prefs: []
  type: TYPE_NORMAL
- en: Shadow deployments are usually achieved by mirroring traffic from the load balancer
    to the server hosting the new ML model. Note that this is similar in concept to
    that in the *Process template for evaluating a new model* section presented in
    [*Chapter 6*](B18416_06.xhtml#_idTextAnchor093). The difference between shadow
    deployments and the template version is that the template version explicitly calls
    both predictions in sequence. The load balancer implementation only needs one
    API request and the call is made in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Shadow deployments are preferred over canary deployments for IA since there’s
    a clean cutover point to using the new model in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the live proving nature of shadow deployments, it’s implied that the
    endpoint URL stays the same. As such, all we need to do in terms of BP is to change
    `Model Version` once the deployment is complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Shadow deployments](img/image_00_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Shadow deployments
  prefs: []
  type: TYPE_NORMAL
- en: 'Rolling back a shadow deployment requires reversing the change to `Model Version`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Rolling back a shadow deployment](img/image_00_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – Rolling back a shadow deployment
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the discussion of Web API-based deployments. Let’s summarize
    the five strategies that we’ve seen.
  prefs: []
  type: TYPE_NORMAL
- en: Web API deployment summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we can influence how the ML models are deployed, the preferred methods are
    *blue-green* and *shadow*. This is because there’s a clean cutoff from the previous
    ML model to the new one, which makes it clear which model was used during a Session.
    Between these two, *blue-green* would likely have faster rollbacks, since the
    previous ML model version is waiting on standby. You could always pair the blue-green
    deployment with the *Process* template for evaluating a new model for a shadow
    deployment-like effect. Using the template will call the proposed ML model right
    after the actual model, generating logs for the ML team’s analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment period for *canary* and *rolling updates* will have ambiguity
    with regard to which model was actually called. To find out which ML model was
    actually served during the deployment period, we need to request logs from the
    servers that are hosting the models or have the API response indicate which version
    of an ML model was being used.
  prefs: []
  type: TYPE_NORMAL
- en: The four deployment methods mentioned don’t require downtime. If we’re allowed
    downtime, it’s reasonable to go with a *replacement* deployment strategy, as retiring
    Schedules and ensuring that no Sessions are active is the safest way to perform
    changes to any BP solution. We can of course decide to pair downtime with any
    of the other deployment strategies as well.
  prefs: []
  type: TYPE_NORMAL
- en: If the ML model is third-party developed or a hosted service, it’s highly unlikely
    that we’ll be able to influence the deployment strategy. It’s also unlikely that
    we’ll know when a deployment occurs. We basically need to keep an eye on the API
    documentation to find out when something has changed or hope that the API call
    returns the model version.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on from API deployments, let’s look at script-based deployments, which
    are comprised of batch files, PowerShell scripts, and Python scripts that are
    directly executed on the Digital Worker through the command line.
  prefs: []
  type: TYPE_NORMAL
- en: Script deployment strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two main strategies for script-based ML deployments. The first is
    to install the script and its dependencies *directly* onto each Digital Worker’s
    Windows system. The second is to *virtualize* the scripts and its dependencies.
    Regardless of which strategy is chosen, it’s important to version control the
    scripts, which helps to document the dependencies and simplify rollbacks.
  prefs: []
  type: TYPE_NORMAL
- en: Direct deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a direct deployment, we need to consider the *location* of the scripts and
    install the *dependencies* that are needed to run them. The scripts can be located
    in an executable network location or saved locally onto each Digital Worker itself.
  prefs: []
  type: TYPE_NORMAL
- en: The difficulty of deployment lies in properly configuring the Windows system
    to ensure that the dependencies required by the updated model are properly set
    up. This likely requires IT to manually install packages through the command line.
    This can be time-consuming if it needs to be repeated across many Digital Workers.
  prefs: []
  type: TYPE_NORMAL
- en: From a purely BP perspective, an update to the ML scripts shouldn’t result in
    many changes beyond modifying the path to the script executable, its arguments,
    and the `Model Version` Environment Variable. Unfortunately, these changes require
    *downtime*, as updates to Processes, Objects and Environment Variables are stored
    in the BP database. Modifications to these can only happen once after deployments
    to all VMs have finished.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Direct deployments](img/image_00_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – Direct deployments
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to prepare for rollbacks in a direct deployment is to take an
    image of the Digital Worker VM before deploying the new ML model. If we find an
    issue with the ML model after it’s deployed, we can roll back by reverting back
    to the previous image. Note that reverting the Digital Worker VM back to its previous
    state still requires undoing changes to the BP solution, such as changing the
    `Model Version` Environment Variable, the path to the script executable, its arguments,
    etc. Those changes are saved inside the BP database, and not the individual VMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Rolling back a direct deployment through image backups](img/image_00_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – Rolling back a direct deployment through image backups
  prefs: []
  type: TYPE_NORMAL
- en: If creating an image backup isn’t possible, you’ll need to roll it back manually.
    A manual rollback is error-prone because downgrading dependencies is rarely possible.
    It will likely require uninstalling and reinstalling packages from scratch. Doing
    this across many Digital Workers will be very tedious.
  prefs: []
  type: TYPE_NORMAL
- en: Virtualized deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another deployment option that makes rolling back simple is to use *virtualization*
    inside of the Digital Worker itself. You can deploy scripts inside of local containers
    (such as Docker), VM images, Windows Subsystem for Linux, etc. A virtualized deployment
    will come bundled with all of the necessary dependencies for a particular version
    of the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, changes to BP Processes, Objects, and Environment Variables are applied
    to all Sessions, so it isn’t possible to partially deploy an ML solution on just
    a few Digital Workers (outside of cloning the entire BP Release and renaming everything).
    Therefore, deploying a virtualized ML solution requires downtime, and the help
    of IT to ensure that the correct image is started automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Virtualized deployments](img/image_00_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – Virtualized deployments
  prefs: []
  type: TYPE_NORMAL
- en: 'To roll back a virtualized script deployment, revert to the previous version
    of the image. Again, we still need to undo changes that were made to the IA solution,
    as those are saved in the BP database:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Rolling back a virtualized deployment](img/image_00_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – Rolling back a virtualized deployment
  prefs: []
  type: TYPE_NORMAL
- en: Virtualized deployments will have heavier resource requirements (CPU and RAM)
    on the Digital Worker when compared with direct deployments. However, rolling
    back a locally running VM can likely be executed faster, as fewer teams need to
    get involved. Both methods have pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve completed the discussion on how to deploy and roll back script-based ML
    models. Next, let’s discuss Code Stage deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Code Stage deployment strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Code Stage ML deployments will be familiar to RPA teams, as they can be done
    as standard Object XML or `.bprelease` imports. Some might also require copying
    `.dll` files to the location specified in the Object, to a folder on the System
    Path, or to the BP installation folder. The only additional step compared to importing
    any other non-ML Code Stage is to change the `Model Version` Environment Variable.
    Downtime will be required if we need to copy new `.dll` files, because they require
    a Runtime Resource restart to be recognized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – Code Stage deployments](img/image_00_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – Code Stage deployments
  prefs: []
  type: TYPE_NORMAL
- en: Rolling back a Code Stage deployment is simple. Re-import a previous version
    of the Object or Release, copy back the correct `.dll` files, delete any newly
    added `.dll` files, and revert to a previous `Model Version` Environment Variable
    value. It’s important that you keep a copy of the `.dll` files needed for previous
    models for rollback purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Rolling back a Code Stage deployment](img/image_00_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Rolling back a Code Stage deployment
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the section on the deployment strategies for Web APIs, CLI scripts,
    and Code Stages. It’s important to understand the specific strategy being used
    to deploy the new ML model, as this affects how we implement changes in our IA
    solution, and how we can roll back to previous versions of the model. Next, let’s
    explore how IA changes the database management requirements of BP, and how we
    can use SQL to extract ML logs, instead of doing so through the BP software through
    the ML Auditor Role.
  prefs: []
  type: TYPE_NORMAL
- en: Database operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BP’s database is primarily used for operations and isn’t intended to be a permanent
    datastore for record keeping. As the database table sizes grow, the database starts
    to perform more slowly, which reduces the Digital Worker execution speed. Adding
    IA steps into a Process leads to additional logging, which increases the growth
    rate of certain tables. Ongoing database maintenance has always been a key factor
    in ensuring the smooth performance of BP RPA, and the need for database maintenance
    is only amplified once we start doing IA.
  prefs: []
  type: TYPE_NORMAL
- en: Table growth maintenance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tables that are most affected by IA are `BPAWorkQueueItem` and `BPASessionLog_*`.
    `BPAWorkQueueItem` stores the Item data of Work Queue Items.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: '`BPASessionLog_*`, means the various Session Logging tables, including `BPASessionLog_NonUnicode`,
    `BPASessionLog_NonUnicode_pre65`, `BPASessionLog_Unicode`, and `BPASessionLog_Unicode_pre65`.'
  prefs: []
  type: TYPE_NORMAL
- en: BPAWorkQueueItem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’re likely storing the entirety of the input data needed for the ML algorithm
    in the Work Queue Item data. If the ML algorithm has many columns of input data,
    `BPAWorkQueueItem` will grow very quickly in size. There are four primary ways
    to manage the growth rate of this table. The first is to delete Work Queue Items
    directly from the BP user interface. This isn’t recommended as the UI has limits
    on how many Work Queue Items can be selected and deleted at once.
  prefs: []
  type: TYPE_NORMAL
- en: The second way is to use a database script that’s provided by BP’s Customer
    Support. This SQL script will delete rows from the Work Queue related tables if
    they’re older than a certain date. For instance, you can configure the script
    to keep 30 days’ worth of Work Queue Items, and have the script run daily through
    the SQL Server Agent. This ensures that your Work Queue Item tables won’t have
    more than 30 days’ worth of data.
  prefs: []
  type: TYPE_NORMAL
- en: The third way is to use the **Blue Prism Archiver XBP** asset, which is available
    on the DX at [https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp](https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp).
    This asset allows you to move Work Queue Items out of the main BP database, into
    a different database. More details can be found on the DX page’s documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there’s another DX asset called the `BPAWorkQueueItem` and the various
    `BPASessionLog_*` tables, which are discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: BPASessionLog*
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: IA increases the growth rate of the various Session Log tables simply because
    there are more Process and Object Stages to execute, and log into the database.
    As mentioned in the previous paragraph, one way to manage the growth of this table
    is to use the **DB Servicer XBP** from the DX.
  prefs: []
  type: TYPE_NORMAL
- en: The `BPASessionLog_*` tables can also be managed through an SQL script that’s
    provided by Customer Support. The script lets us specify how many days of Session
    Logs we want to keep. Anything older than the number of days will be deleted from
    the database. Once set up, the SQL script needs to be run on a scheduled basis.
  prefs: []
  type: TYPE_NORMAL
- en: A third option to keep the Session Logging table sizes in check is to use the
    in-product archiving function. This can be found under `.gz` format. We must specify
    which Runtime Resource we want to perform the archiving (during its idle time),
    where on the Runtime Resource we want the archive files saved, and how many days’
    worth of logs we want to keep in the BP database.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – The in-product archiver, which can slow down the Session Log
    growth rate](img/image_00_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – The in-product archiver, which can slow down the Session Log growth
    rate
  prefs: []
  type: TYPE_NORMAL
- en: The final option is to delete Sessions by hand through the BP user interface.
    This is unrealistic for most companies as there are potentially thousands of Sessions
    (if not more) being run regularly.
  prefs: []
  type: TYPE_NORMAL
- en: IA requires us to refine our BP database maintenance practices. IA also leads
    to new requirements in terms of data exporting. For example, ML Auditors want
    to know what inputs have led to what predictions and how those predictions have
    been used. Data scientists want to know the manual verification results so that
    they can update the algorithm. If our ML model is third-party developed or hosted,
    the only way of getting these logs is through the BP database, as we won’t have
    access to the underlying ML server logs.
  prefs: []
  type: TYPE_NORMAL
- en: While exporting this data can be done through the BP user interface, it may
    be preferable to automate and export this data directly from the database.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Data exporting should be done during off hours, preferably on read-only versions
    of the database, to minimize the impact on BP production operations.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting ML prediction data from the database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the templates presented in [*Chapter 7*](B18416_07.xhtml#_idTextAnchor114),
    our ML prediction data can potentially be extracted from two database locations.
    The first is from the Work Queue tables, which works if we’re using the template
    that separates the ML predictions into its own Work Queue. If we’re not using
    the template with dedicated Work Queues, we can query the Session Log tables to
    find this data.
  prefs: []
  type: TYPE_NORMAL
- en: Querying Work Queue data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you’re using a template that has a dedicated Work Queue for ML, it’s straightforward
    to query for Work Queue data through SQL. The SQL following script allows you
    to provide data scientists with the input data to the algorithm, the predicted
    result, and the confidence score. The only thing that needs changing is the name
    of the Work Queue, and you could potentially add dates to limit the range of results
    that are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `data` column that’s selected is an XML string. This XML format can be directly
    opened by Excel or parsed by data scientists using their programming language
    of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Querying Session Log data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In all of the IA templates, we’ve deliberately enabled logging in some Stages
    to make extracting the prediction data easier in both the Session Log Viewer and
    through the database. The two Stages where logging is enabled are shown in the
    following figure and are named identically across the three IA templates. This
    common naming scheme is required for our SQL to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Common Stages to log ML prediction results](img/image_00_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.29 – Common Stages to log ML prediction results
  prefs: []
  type: TYPE_NORMAL
- en: We can query the Session Logs to specifically find these two Stages, by passing
    in the name of the Process. Depending on how BP is configured, you might also
    need to change the database table name from `BPASessionLog_NonUnicode` to `BPASessionLog_Unicode`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `modelversion` column is populated with the model version from `Log [Model
    Version]` Calculation Stage. The `attributexml` column contains the logged values
    from the `Set [Prediction] and [Confidence Score]` Multi Calc Stage.
  prefs: []
  type: TYPE_NORMAL
- en: The `LAG(result, 1, 0) OVER(ORDER BY logid)` SQL syntax allows us to select
    the value from `Log [Model Version]` and `Set [Prediction] and [Confidence Score]`
    on a single row, even though they belong to different Session Log entries. This
    relies on there being only `1` Stage of difference between the two. If there are
    more Session Log records between the two Stages, you’ll need to change the `1`
    value to match.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re using the **Example 4 – New Model Evaluation Process Template** from
    [*Chapter 6*](B18416_06.xhtml#_idTextAnchor093) to evaluate a second ML model
    in production, the following SQL can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Exporting reviewed prediction data from the database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reviewed prediction results might also be found in two locations depending
    on which template is used. If the two-Work Queue or three-Work Queue templates
    are used, we can find them from the `BPAWorkQueueItem` table. If not, we can look
    into the `BPASessionLog_*` table.
  prefs: []
  type: TYPE_NORMAL
- en: Querying Work Queue data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the HITL Review Work Queue, we only retrieve results where we know for
    certain that the review has occurred. We do this by checking for the presence
    of a Collection Field named `Confidence`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Querying Session Log data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In all three templates, there’s a commonly named Stage that has logging deliberately
    enabled, to capture the corrected prediction result and the justification reason.
    This Stage is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – A common Stage to log the review result and justification](img/image_00_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.30 – A common Stage to log the review result and justification
  prefs: []
  type: TYPE_NORMAL
- en: 'The SQL to find this Stage in the Session Log tables is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
