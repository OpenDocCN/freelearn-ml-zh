- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: ML Deployments and Database Operations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML部署和数据库操作
- en: The biggest sources of ongoing maintenance for *RPA* are due to handling changes
    to the business logic (Processes) and applications (Objects). With IA, another
    source of ongoing changes is added into the mix—updating BP to use new ML models
    as they’re changed. Imagine that models are only updated once per year and that
    we have five models. That’s still five times per year that we’ll need to modify
    how our IA solutions are configured due to ML model updates.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*RPA*持续维护的最大来源是处理业务逻辑（流程）和应用程序（对象）的变化。随着IA的应用，另一个持续变化的来源被加入其中——更新BP以使用新的ML模型。想象一下，模型每年只更新一次，而我们有五个模型。这意味着我们每年仍需要五次修改我们的IA解决方案的配置，以适应ML模型的更新。'
- en: Even if the updated ML model keeps the exact same API endpoint definition as
    before, we still need to understand what ML deployment strategy is being used,
    as this affects how we plan for **rollbacks** in case the model is not performing
    as expected.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 即使更新的ML模型保持了与之前完全相同的API端点定义，我们仍然需要了解正在使用哪种ML部署策略，因为这会影响我们计划**回滚**的方式，以防模型的表现不符合预期。
- en: We discussed the **ML Deployer** User Role in [*Chapter 8*](B18416_08.xhtml#_idTextAnchor133).
    This User Role is responsible for regularly updating the ML models that are in
    production. In this chapter, we’ll discuss the most common ML deployment strategies,
    how they affect what needs to happen in BP to update the model, and the plan for
    rollbacks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[*8章*](B18416_08.xhtml#_idTextAnchor133)中讨论了**ML部署者**用户角色。这个用户角色负责定期更新生产中的ML模型。在本章中，我们将讨论最常用的ML部署策略，它们如何影响BP中更新模型所需发生的事情，以及回滚计划。
- en: Another way that IA changes our operations lies in database maintenance and
    ML data extraction. [*Chapter 8*](B18416_08.xhtml#_idTextAnchor133) also discussed
    the **ML Auditor** User Role, which can export Session Log data after logging
    into the BP software. If we don’t want to dedicate a Role within BP for this,
    we can instead regularly extract this data directly from the database.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: IA改变我们操作方式的另一种方式在于数据库维护和ML数据提取。[*第8章*](B18416_08.xhtml#_idTextAnchor133)也讨论了**ML审计员**用户角色，该角色可以在登录BP软件后导出会话日志数据。如果我们不想在BP中为这个角色分配职责，我们可以定期直接从数据库中提取这些数据。
- en: 'In this chapter, we’re going to cover the following topics related to operations
    that emerge specifically due to IA:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论与IA相关的特定操作主题：
- en: ML deployments and rollbacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML部署和回滚
- en: Database maintenance and data exporting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库维护和数据导出
- en: ML deployments and rollbacks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ML部署和回滚
- en: ML is often developed and deployed by a team that’s outside of the IA organization.
    Once a new model is available, the IA team needs to update the connection points
    between BP and the ML model so that the automation can make use of it. The steps
    needed to safely update these connection points from BP differ based on which
    ML deployment strategy is being used. In this section, we’ll describe the most
    common ML deployment strategies. We’ll also discuss how the IA solution configuration
    should be changed to make use of a new ML model, given a particular strategy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ML通常由IA组织之外的团队开发和部署。一旦新的模型可用，IA团队需要更新BP和ML模型之间的连接点，以便自动化可以利用它。根据所使用的ML部署策略，从BP安全更新这些连接点的步骤会有所不同。在本节中，我们将描述最常用的ML部署策略。我们还将讨论在特定策略下，如何更改IA解决方案的配置以利用新的ML模型。
- en: The ML deployment strategy also impacts how we can roll back from a new model
    to a previous one. Problems with ML models often don’t result in Session-halting
    failures that are easily detected. Models continue to produce predictions even
    if they’re flawed. If an issue is discovered with a model, we need to be ready
    to roll back to a previous one so that automation cases can continue to run. Of
    course, rolling back might not be an option if we’re using third-party-developed
    models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ML部署策略还影响我们如何从新模型回滚到旧模型。ML模型的问题通常不会导致容易检测到的会话终止失败。即使有缺陷，模型仍然会继续产生预测。如果发现模型有问题，我们需要准备好回滚到旧模型，以便自动化案例可以继续运行。当然，如果我们使用第三方开发的模型，回滚可能不是一个选项。
- en: 'Recall the different ways that an ML algorithm can be triggered from BP, which
    were discussed in *Chapters 1*, *2*, and *3*: *Web* *APIs,* *CLI scripts*, and
    *Code Stages*. In the sections that follow, we’ll see how the deployment strategies
    for each of these differ and how our IA solution should be configured to maintain
    auditability. We’ll also discuss the steps needed to safely roll back an ML model
    to its previous version.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，ML 算法可以从 BP 触发的方式，这些方式在第 1、2 和 3 章中讨论过：*Web* *APIs*、*CLI 脚本*和*代码阶段*。在接下来的部分中，我们将看到每种部署策略的不同之处以及我们的
    IA 解决方案应如何配置以保持可审计性。我们还将讨论安全地将 ML 模型回滚到其先前版本所需的步骤。
- en: Web API deployment strategies
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Web API 部署策略
- en: Common ways of deploying ML Web APIs include *replacement*, *rolling*, *blue-green*,
    *canary*, and *shadow* deployments. I’ll explain the underlying concepts behind
    each of these methods and what the IA team should do to use the new model in production.
    For many of these strategies, we also need to consider whether the ML team *overwrites*
    the previous model by reusing the *same API endpoint* or offers a *new endpoint*
    in addition to previous endpoints. A new endpoint allows us to quickly switch
    back to an older model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 ML Web API 的常见方法包括*替换*、*滚动*、*蓝绿*、*金丝雀*和*影子*部署。我将解释这些方法背后的基本概念以及 IA 团队应如何操作以在生产中使用新模型。对于这些策略中的许多，我们还需要考虑
    ML 团队是否通过重用*相同的 API 端点*来*覆盖*先前的模型，或者除了先前的端点外还提供*新端点*。新端点允许我们快速切换回旧模型。
- en: Replacement deployments
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 替换部署
- en: This is the simplest Web API deployment strategy where *planned downtime* is
    required. The old ML model is taken completely offline and the new model is put
    into place. Coordination with the ML team is needed to ensure that no Sessions
    are running during the switch-over period.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是需要计划停机的最简单 Web API 部署策略。旧 ML 模型完全离线，新模型被放置到位。需要与 ML 团队协调，以确保在切换期间没有会话正在运行。
- en: API endpoint is overwritten
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API 端点被覆盖
- en: 'If the API endpoint is reused after the new model is deployed, it means that
    the URL is kept exactly the same. We need to use the `Model Version` Environment
    Variable (present in all of the Process templates from [*Chapter 7*](B18416_07.xhtml#_idTextAnchor114))
    to keep track of when the model was actually updated:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在部署新模型后重新使用 API 端点，这意味着 URL 完全保持不变。我们需要使用`模型版本`环境变量（存在于所有流程模板中，见[*第 7 章*](B18416_07.xhtml#_idTextAnchor114)）来跟踪模型实际更新的时间：
- en: '![Figure 9.1 – The Model Version Environment Variable should be changed when
    the model is updated](img/image_00_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 当模型更新时，应更改模型版本环境变量](img/image_00_001.jpg)'
- en: Figure 9.1 – The Model Version Environment Variable should be changed when the
    model is updated
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 当模型更新时，应更改模型版本环境变量
- en: 'The sequence of steps needed to update the BP solution is shown in the following
    figure. Prior to deploying the new ML model, we have to ensure that Sessions that
    use the model have stopped running since downtime is required. After the new model
    is brought online, we change `Model Version` and start the Sessions again:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 BP 解决方案所需的步骤顺序如图所示。在部署新 ML 模型之前，我们必须确保使用该模型的会话已停止运行，因为需要停机时间。新模型上线后，我们更改`模型版本`并重新启动会话：
- en: '![Figure 9.2 – Replacement deployments when the existing endpoint is overwritten](img/image_00_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 当现有端点被覆盖时的替换部署](img/image_00_002.jpg)'
- en: Figure 9.2 – Replacement deployments when the existing endpoint is overwritten
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 当现有端点被覆盖时的替换部署
- en: 'Downtime will be required for the ML team to undo their deployment in case
    of a rollback. A sample rollback procedure is shown in the following image. Instead
    of waiting for in-flight Sessions to end, you might want to request for an `Model
    Version`:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要回滚，ML 团队将需要停机以撤销其部署。以下图像显示了示例回滚程序。您可能不想等待正在进行的会话结束，而是请求一个`模型版本`：
- en: '![Figure 9.3 – Rolling back a replacement deployment when the endpoint is overwritten](img/image_00_003.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 当端点被覆盖时回滚替换部署](img/image_00_003.jpg)'
- en: Figure 9.3 – Rolling back a replacement deployment when the endpoint is overwritten
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 当端点被覆盖时回滚替换部署
- en: Since this replacement deployment strategy requires downtime, it’s better suited
    to IA Processes where the Work Queue volumes are low and the ML model changes
    are infrequent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种替换部署策略需要停机时间，因此更适合 IA 流程，其中工作队列的容量较低且 ML 模型更改不频繁。
- en: New API endpoint is created
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 新的 API 端点已创建
- en: 'If there’s a new API endpoint, updating the IA solution to use the new endpoint
    is almost the same as the steps in *Figure 9**.2*. The only difference lies in
    updating the Web API Service configuration to use the new API URL, in addition
    to the `Model Version` Environment Variable. Recall that the details of changing
    a URL in a Web API Service don’t actually get saved into the Audit Logs, such
    that you can retrieve the URL’s value. If we’re using an Object and not a Web
    API Service, we need to change the Environment Variable that stores the endpoint
    URL instead:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有新的 API 端点，更新 IA 解决方案以使用新端点几乎与 *图 9*.2* 中的步骤相同。唯一的区别在于更新 Web API 服务配置以使用新的
    API URL，以及 `模型版本` 环境变量。回想一下，在 Web API 服务中更改 URL 的详细信息实际上并没有被保存到审计日志中，这样你就可以检索
    URL 的值。如果我们使用的是对象而不是 Web API 服务，我们需要更改存储端点 URL 的环境变量：
- en: '![Figure 9.4 – Replacement deployments when a new endpoint is created](img/image_00_004.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 创建新端点时的替换部署](img/image_00_004.jpg)'
- en: Figure 9.4 – Replacement deployments when a new endpoint is created
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 创建新端点时的替换部署
- en: 'Rolling back in this case is easy, as the previous API should still be available.
    All we need to do is to change the Web API Service URL (or the Object Environment
    Variable URL), and the `Model Version` back to what they were before. If you want
    to be even safer, you can retire the Schedules and wait for all Sessions to be
    completed before making these changes:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下回滚很容易，因为之前的 API 仍然可用。我们只需要更改 Web API 服务 URL（或对象环境变量 URL），以及 `模型版本` 回到之前的状态。如果你想更加安全，你可以退役计划，等待所有会话完成后再进行这些更改：
- en: '![Figure 9.5 – Rolling back a replacement deployment when a new endpoint is
    created](img/image_00_005.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – 创建新端点时回滚替换部署](img/image_00_005.jpg)'
- en: Figure 9.5 – Rolling back a replacement deployment when a new endpoint is created
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 创建新端点时回滚替换部署
- en: The *replacement* strategy is probably the simplest type of deployment that
    exists. The remaining strategies discussed in this chapter are more sophisticated
    and don’t require downtime. Since there’s no downtime, there won’t be a requirement
    to retire/unretire schedules or ensure that Sessions that use the ML model aren’t
    running. Of course, it’s still safer to perform solution deployments and changes
    during periods of downtime. The *rolling updates* strategy is discussed next.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*替换* 策略可能是最简单的部署类型。本章中讨论的其余策略更为复杂，且不需要停机。由于没有停机时间，因此不需要退役/恢复计划或确保使用 ML 模型的会话没有运行。当然，在停机期间进行解决方案部署和更改仍然更安全。接下来将讨论
    *滚动更新* 策略。'
- en: Rolling updates
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 滚动更新
- en: Rolling deployments can be performed when there are multiple servers hosting
    the API behind a load balancer. An image of a simple rolling deployment can be
    seen in *Figure 9**.6*. We start with all API servers connected to the load balancer,
    running *version 1* of the API (A). *Server 1* is removed from the load balancer
    (*B*), and updated to serve *V2* of the API (*C*). After updating, *Server 1*
    is reconnected to the load balancer (*D*), meaning that both model versions (*V1*
    on *Server 1* and *V2* on *Server 2*) are being served simultaneously. Next, we
    remove *Server 2* from the load balancer (*E*), update it to *V2* (*F*), and connect
    it back to the load balancer (*G*). We repeat these steps until all servers are
    updated.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当有多个服务器在负载均衡器后面托管 API 时，可以执行滚动部署。可以在 *图 9*.6* 中看到简单滚动部署的图像。我们开始时所有 API 服务器都连接到负载均衡器，运行
    API 的 *版本 1*（A）。*服务器 1* 被从负载均衡器中移除（*B*），并更新为服务 API 的 *V2*（*C*）。更新后，*服务器 1* 重新连接到负载均衡器（*D*），这意味着两个模型版本（*V1*
    在 *服务器 1* 上和 *V2* 在 *服务器 2* 上）正在同时提供服务。接下来，我们移除 *服务器 2* 从负载均衡器（*E*），更新它到 *V2*（*F*），并将其重新连接到负载均衡器（*G*）。我们重复这些步骤，直到所有服务器都更新。
- en: Note that there’s *no downtime* for the ML API service, but there are also periods
    of time (*D*) where predictions can be made against both the old and new model,
    which complicates auditing. There’s a chance that we won’t know which model was
    used to make a prediction during the timeframe when both the old and new models
    are potentially being served.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ML API 服务没有停机时间，但也有时间（*D*）可以针对旧模型和新模型进行预测，这会复杂化审计。有可能在旧模型和新模型都可能被服务的时间段内，我们不知道使用了哪个模型进行预测。
- en: '![Figure 9.6 – An example rolling update deployment](img/image_00_006.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 滚动更新部署示例](img/image_00_006.jpg)'
- en: Figure 9.6 – An example rolling update deployment
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 滚动更新部署示例
- en: Again, we have to consider two cases for this type of deployment. The first
    is when the existing API endpoint is overwritten and the second is when a new
    API endpoint is created.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们必须考虑这种类型部署的两种情况。第一种是现有API端点被覆盖，第二种是新API端点被创建。
- en: API endpoint is overwritten
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API端点被覆盖
- en: If the API endpoint is reused, we might not know which version of the API is
    being served while the servers behind the load balancer are being updated. The
    only way we would know is if the model version is returned as part of the API
    response, but that isn’t guaranteed. For auditing reasons, it’s safest if we suspend
    calling the ML endpoint just prior to when the rolling update begins and resume
    once we know that all servers have been updated. However, we probably won’t know
    when the ML team will start a rolling update, especially if they’re third-party
    developers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果API端点被重用，当服务器后面的负载均衡器正在更新时，我们可能不知道正在提供哪个版本的API。我们唯一知道的方式是如果模型版本作为API响应的一部分返回，但这并不保证。出于审计原因，在滚动更新开始之前暂停调用ML端点，并在我们知道所有服务器都已更新后恢复，是最安全的。然而，我们可能不知道ML团队何时开始滚动更新，特别是如果他们是第三方开发者。
- en: 'In this case, it’s simplest to just update the `Model Version` Environment
    Variable once we receive notice that the deployment is 100% complete. If you need
    to know exactly which ML model has been used during the deployment timeframe,
    you’ll need to ask the model developers to check their server logs:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，一旦我们收到部署100%完成的通知，最简单的方法就是更新`模型版本`环境变量。如果您需要确切知道在部署时间段内使用了哪个ML模型，您需要要求模型开发者检查他们的服务器日志：
- en: '![Figure 9.7 – Rolling deployments when the existing endpoint is overwritten](img/image_00_007.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 当现有端点被覆盖时的滚动部署](img/image_00_007.jpg)'
- en: Figure 9.7 – Rolling deployments when the existing endpoint is overwritten
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 当现有端点被覆盖时的滚动部署
- en: Rolling back is largely the same procedure as deploying a new model. A rollback
    is initiated and we wait to be notified that it’s complete. Once complete, we
    can update the `Model Version` Environment Variable.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚基本上与部署新模型的程序相同。回滚被启动，我们等待收到完成的通知。一旦完成，我们可以更新`模型版本`环境变量。
- en: '![Figure 9.8 – Rolling back a rolling deployment when the endpoint is overwritten](img/image_00_008.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 当端点被覆盖时回滚滚动部署](img/image_00_008.jpg)'
- en: Figure 9.8 – Rolling back a rolling deployment when the endpoint is overwritten
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 当端点被覆盖时回滚滚动部署
- en: New API endpoint is created
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 新API端点被创建
- en: 'If a new API endpoint is introduced, we need to update two things. The first
    is the `Model Version` Environment Variable.. The second depends on whether we’re
    using a *Web* API *Service* or an *Object* to call the API. Depending on which
    one we’re using, we either change the Web API Service configuration to use the
    new API URL or the Object Environment Variable/Data Item that stores the API URL:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果引入了新的API端点，我们需要更新两件事。第一是`模型版本`环境变量。第二取决于我们是否使用*Web* API *服务*或*对象*来调用API。根据我们使用的是哪一个，我们要么更改Web
    API服务配置以使用新的API URL，要么更改存储API URL的对象环境变量/数据项：
- en: '![Figure 9.9 – Rolling deployments when a new endpoint is created](img/image_00_009.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 创建新端点时进行滚动部署](img/image_00_009.jpg)'
- en: Figure 9.9 – Rolling deployments when a new endpoint is created
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 创建新端点时进行滚动部署
- en: Rolling back the API to a previous version is simple since the previous API
    endpoint still exists. We reverse the changes to the `Model Version` Environment
    Variable, and the Web API Service configuration URL (or Object Environment Variable/Data
    Item).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将API回滚到先前版本很简单，因为先前的API端点仍然存在。我们撤销对`模型版本`环境变量和Web API服务配置URL（或对象环境变量/数据项）的更改。
- en: '![Figure 9.10 – Rolling back a rolling deployment when a new endpoint is created](img/image_00_010.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 创建新端点时回滚滚动部署](img/image_00_010.jpg)'
- en: Figure 9.10 – Rolling back a rolling deployment when a new endpoint is created
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 创建新端点时回滚滚动部署
- en: A major disadvantage of rolling deployments is not knowing exactly which ML
    model was used for predictions made during the deployment phase. The next strategy
    that we’ll discuss, called blue-green deployments, doesn’t have this ambiguity
    and also has no downtime.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动部署的一个主要缺点是不知道在部署阶段做出的预测使用了哪个ML模型。我们将讨论的下一种策略，称为蓝绿部署，没有这种歧义，也没有停机时间。
- en: Blue-green deployments
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: 'In a blue-green deployment, there’s duplicate infrastructure positioned behind
    a load balancer, but only one set of infrastructures is actively serving requests
    at any time. A sample blue-green deployment can be seen in *Figure 9**.11*. Suppose
    that we’re currently running *version 1* of the model on the set of *green* servers
    (*A*). When the model needs updating to *version 2*, it’s deployed onto a separate
    set of blue servers, which are normally not serving requests (*B*). Once model
    version 2 is ready to be used, the load balancer is changed to serve requests
    only to the blue servers (*C*). The green server with the previous model sits
    idle:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在蓝绿色部署中，负载均衡器后面有重复的基础设施，但在任何给定时间只有一组基础设施正在积极处理请求。一个示例蓝绿色部署可以在 *图 9**.11* 中看到。假设我们目前在
    *绿色* 服务器集上运行 *版本 1* 的模型 (*A*)。当模型需要更新到 *版本 2* 时，它被部署到一组单独的蓝色服务器上，这些服务器通常不处理请求
    (*B*)。一旦模型版本 2 准备好使用，负载均衡器被更改以仅向蓝色服务器发送请求 (*C*)。带有先前模型的绿色服务器处于闲置状态：
- en: '![Figure 9.11 – An example blue-green deployment](img/image_00_011.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – 蓝绿色部署示例](img/image_00_011.jpg)'
- en: Figure 9.11 – An example blue-green deployment
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – 蓝绿色部署示例
- en: Similar to rolling updates, blue-green deployments shouldn’t have service downtime.
    The main difference with blue-green and rolling updates is that there’s always
    a copy of the previous model on standby that can be switched back to by reconfiguring
    the load balancer. There also isn’t an ambiguous period where multiple models
    can be serving predictions at once. Both of these points make blue-green deployments
    better suited for IA auditing when compared to rolling updates.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与滚动更新类似，蓝绿色部署不应有服务中断。与蓝绿色和滚动更新的主要区别是，始终有一个先前模型的副本处于待命状态，可以通过重新配置负载均衡器切换回。也不会存在多个模型同时提供预测的模糊期。这两个点使得蓝绿色部署比滚动更新更适合
    IA 审计。
- en: API endpoint is overwritten
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API 端点被覆盖
- en: If the URL endpoint is reused, we need to wait for a notification stating that
    the deployment is complete and change the `Model Version` Environment Variable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 URL 端点被重复使用，我们需要等待通知表明部署已完成，并更改 `模型版本` 环境变量。
- en: '![Figure 9.12 – Blue-green deployments when the existing endpoint is overwritten](img/image_00_012.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12 – 覆盖现有端点时的蓝绿色部署](img/image_00_012.jpg)'
- en: Figure 9.12 – Blue-green deployments when the existing endpoint is overwritten
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 – 覆盖现有端点时的蓝绿色部署
- en: 'Rolling back requires the ML/infrastructure team to change the configuration
    of their load balancer. Once that’s done, the IA team can change the `Model Version`
    Environment Variable back to its previous value. The speed of rolling back a blue-green
    deployment when the endpoint is overwritten should be much faster than rolling
    back a rolling update one:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚需要机器学习/基础设施团队更改其负载均衡器的配置。一旦完成，IA 团队可以将 `模型版本` 环境变量改回其先前值。当端点被覆盖时回滚蓝绿色部署的速度应该比回滚滚动更新快得多：
- en: '![Figure 9.13 – Rolling back a blue-green deployment when the endpoint is overwritten](img/image_00_013.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13 – 当端点被覆盖时回滚蓝绿色部署](img/image_00_013.jpg)'
- en: Figure 9.13 – Rolling back a blue-green deployment when the endpoint is overwritten
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 – 当端点被覆盖时回滚蓝绿色部署
- en: New API endpoint is created
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 新 API 端点已创建
- en: 'When we have a new API endpoint, we need to update the `Model Version` Environment
    Variable, followed by the Object Environment Variable/Data Item API URL or Web
    API Service configuration URL:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有新的 API 端点时，我们需要更新 `模型版本` 环境变量，然后是对象环境变量/数据项 API URL 或 Web API 服务配置 URL：
- en: '![Figure 9.14 – Blue-green deployments when a new endpoint is created](img/image_00_014.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – 创建新端点时的蓝绿色部署](img/image_00_014.jpg)'
- en: Figure 9.14 – Blue-green deployments when a new endpoint is created
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – 创建新端点时的蓝绿色部署
- en: 'Since the current set of servers, whether that be blue or green, contain both
    the latest and previous versions of the ML model, rolling back doesn’t actually
    require reconfiguring the load balancer. We simply revert the Environment Variables/Data
    Items or Web API Service URL back to what they were before:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于当前的服务器集合，无论是蓝色还是绿色，都包含机器学习模型的最新和之前版本，因此回滚实际上不需要重新配置负载均衡器。我们只需将环境变量/数据项或 Web
    API 服务 URL 回滚到之前的状态：
- en: '![Figure 9.15 – Rolling back a blue-green deployment when a new endpoint is
    created](img/image_00_005.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – 创建新端点时回滚蓝绿色部署](img/image_00_005.jpg)'
- en: Figure 9.15 – Rolling back a blue-green deployment when a new endpoint is created
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 – 在创建新端点时回滚蓝绿部署
- en: For the three deployment strategies that we’ve discussed so far (replacement,
    rolling, blue-green), the new ML model’s API URL can either be reused (stay the
    same) or changed. If the new model has a different URL, we need to actively change
    the IA solution configuration before the new ML model will be called. This is
    in contrast with the next two deployment methods that we’ll see, where the URL
    of the new model will always stay the same. If previous versions of the model
    are kept active, those will be given new URLs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们迄今为止讨论的三个部署策略（替换、滚动、蓝绿），新ML模型的API URL可以重用（保持不变）或更改。如果新模型有不同的URL，我们需要在新ML模型被调用之前主动更改IA解决方案的配置。这与我们将要看到的下两种部署方法形成对比，其中新模型的URL将始终保持不变。如果保留模型的前版本，那些将获得新的URL。
- en: Canary deployments
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: 'Under canary deployments, both the new and old model are served in parallel
    during a period of ML hypercare (*Figure 9**.16*, *B*). Initially, only a small
    percentage of requests are routed to the new model; for example, 95% of requests
    will go to the old model, with 5% of the traffic going to the new one (*C*). The
    percentage of requests reaching the new ML model is increased, as confidence in
    the new model’s performance grows (*D*). This eventually reaches 100% (*E*) and
    the servers hosting the old model are shut down (*F*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀部署下，新模型和旧模型在ML超关注期间并行提供服务（*图9**.16*，*B*）。最初，只有一小部分请求被路由到新模型；例如，95%的请求将流向旧模型，5%的流量流向新模型（*C*）。随着对新模型性能的信心增加，到达新ML模型的请求百分比逐渐增加（*D*）。这最终达到100%（*E*），托管旧模型的服务器被关闭（*F*）：
- en: '![Figure 9.16 – An example canary deployment](img/image_00_016.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图9.16 – 一个示例金丝雀部署](img/image_00_016.jpg)'
- en: Figure 9.16 – An example canary deployment
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 – 一个示例金丝雀部署
- en: The IA team will likely have no visibility nor control over which requests get
    chosen to run against the new model. This means that we genuinely can’t tell which
    Sessions will be running on the old and new ML models during the switchover period
    unless that information is provided in the API response. If we need to know for
    certain which ML API version is being used for audit reasons, we’ll need to access
    the logs of the ML service directly and cross-reference them with the timestamps
    of the Session Logs, which might not be possible if the model is third-party developed.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: IA团队可能无法看到也无法控制哪些请求会被选择运行在新模型上。这意味着除非API响应中提供了该信息，否则我们真的无法确定在切换期间哪些会话将在旧模型和新模型上运行。如果我们需要确定用于审计目的的ML
    API版本，我们需要直接访问ML服务的日志，并将它们与会话日志的时间戳进行交叉引用，如果模型是第三方开发的，这可能是不可能的。
- en: Canary deployments imply that there’s a single URL endpoint that remains unchanged
    as the model gets updated. So, the only thing that needs to be done in BP is updating
    the `Model Version` Environment Variable once we receive a notification stating
    that all traffic is being directed to the new model. Similar to rolling updates,
    we won’t know for sure which model was being used to make predictions during the
    deployment period.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署意味着在模型更新时，有一个单一的URL端点保持不变。因此，一旦我们收到通知，表示所有流量都被导向新模型，我们只需要更新`模型版本`环境变量一次。类似于滚动更新，我们不会确切知道在部署期间使用了哪个模型进行预测。
- en: '![Figure 9.17 – Canary deployments](img/image_00_017.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17 – 金丝雀部署](img/image_00_017.jpg)'
- en: Figure 9.17 – Canary deployments
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 – 金丝雀部署
- en: Full rollbacks in canary deployments are rare, as their purpose is to catch
    issues during the deployment period. What normally happens is that during deployment,
    success criteria for the traffic directed at the new model aren’t met and everything
    is rolled back immediately. This kind of rollback doesn’t require any changes
    to BP because it occurs *before* we’ve made any configuration changes to the IA
    solution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀部署中，完全回滚是罕见的，因为其目的是在部署期间捕捉问题。通常发生的情况是，在部署期间，针对新模型的流量成功标准未达到，一切都会立即回滚。这种回滚不需要对BP进行任何更改，因为它发生在我们尚未对IA解决方案进行任何配置更改之前。
- en: However, if a rollback is needed after the full deployment has happened, we
    should only need to change back to the previous value of `Model Version`, as the
    API URL remains unchanged.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在完整部署之后需要回滚，我们只需要将`模型版本`的值改回之前的状态，因为API URL保持不变。
- en: '![Figure 9.18 – Rolling back a canary deployment](img/image_00_018.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图9.18 – 回滚金丝雀部署](img/image_00_018.jpg)'
- en: Figure 9.18 – Rolling back a canary deployment
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 – 回滚金丝雀部署
- en: Canary deployments (similar to rolling updates) result in *ambiguity* with respect
    to which model was actually used in production during the deployment period. Because
    of this, it’s best to avoid them (if possible) or to always return the model version
    in the API response. Instead, we can consider using shadow deployments (discussed
    next), which also run two models in parallel. Shadow deployments differ from canary
    deployments, as there’s a clear switchover point between the old and new models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署（类似于滚动更新）在部署期间实际用于生产的模型方面会产生*歧义*。因此，最好避免它们（如果可能的话）或在API响应中始终返回模型版本。相反，我们可以考虑使用影子部署（下文将讨论），它也并行运行两个模型。影子部署与金丝雀部署不同，因为旧模型和新模型之间存在明确的切换点。
- en: Shadow deployments
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 影子部署
- en: 'With shadow deployments, both the old and new predictions are called simultaneously
    against live data. However, the newer model is called behind the scenes and its
    API response isn’t sent back to the caller (*Figure 9**.19*, *A*). The shadow
    prediction results are retrieved directly from the server logs, which are analyzed
    by the ML team. Once the ML team is satisfied with the new model, it’s fully switched
    over (*B*) and the previous model is removed (*C*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用影子部署时，对旧模型和新模型同时针对实时数据进行调用。然而，在幕后调用的是新模型，并且其API响应不会发送回调用者（*图9.19*，*A*）。影子预测结果直接从服务器日志中检索，由机器学习团队进行分析。一旦机器学习团队对新模型满意，它将被完全切换（*B*），并且旧模型将被移除（*C*）：
- en: '![Figure 9.19 – An example shadow deployment](img/image_00_019.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19 – 一个示例影子部署](img/image_00_019.jpg)'
- en: Figure 9.19 – An example shadow deployment
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 – 一个示例影子部署
- en: Shadow deployments are usually achieved by mirroring traffic from the load balancer
    to the server hosting the new ML model. Note that this is similar in concept to
    that in the *Process template for evaluating a new model* section presented in
    [*Chapter 6*](B18416_06.xhtml#_idTextAnchor093). The difference between shadow
    deployments and the template version is that the template version explicitly calls
    both predictions in sequence. The load balancer implementation only needs one
    API request and the call is made in parallel.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 影子部署通常是通过将负载均衡器中的流量镜像到托管新机器学习模型的服务器来实现的。请注意，这与在[*第6章*](B18416_06.xhtml#_idTextAnchor093)中介绍的“*评估新模型的流程模板*”部分在概念上相似。影子部署与模板版本之间的区别在于模板版本明确地按顺序调用两次预测。负载均衡器实现只需要一个API请求，并且并行调用。
- en: Shadow deployments are preferred over canary deployments for IA since there’s
    a clean cutover point to using the new model in production.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在将新模型用于生产时有一个干净的切换点，影子部署比金丝雀部署更适合IA。
- en: 'Due to the live proving nature of shadow deployments, it’s implied that the
    endpoint URL stays the same. As such, all we need to do in terms of BP is to change
    `Model Version` once the deployment is complete:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于影子部署的实时验证性质，端点URL保持不变是隐含的。因此，在部署完成后，我们只需要在BP中将`模型版本`更改一次：
- en: '![Figure 9.20 – Shadow deployments](img/image_00_020.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20 – 影子部署](img/image_00_020.jpg)'
- en: Figure 9.20 – Shadow deployments
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 – 影子部署
- en: 'Rolling back a shadow deployment requires reversing the change to `Model Version`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚影子部署需要将`模型版本`的更改撤销：
- en: '![Figure 9.21 – Rolling back a shadow deployment](img/image_00_021.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图9.21 – 回滚影子部署](img/image_00_021.jpg)'
- en: Figure 9.21 – Rolling back a shadow deployment
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21 – 回滚影子部署
- en: This concludes the discussion of Web API-based deployments. Let’s summarize
    the five strategies that we’ve seen.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了基于Web API的部署讨论。让我们总结一下我们看到的五种策略。
- en: Web API deployment summary
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Web API部署摘要
- en: If we can influence how the ML models are deployed, the preferred methods are
    *blue-green* and *shadow*. This is because there’s a clean cutoff from the previous
    ML model to the new one, which makes it clear which model was used during a Session.
    Between these two, *blue-green* would likely have faster rollbacks, since the
    previous ML model version is waiting on standby. You could always pair the blue-green
    deployment with the *Process* template for evaluating a new model for a shadow
    deployment-like effect. Using the template will call the proposed ML model right
    after the actual model, generating logs for the ML team’s analysis.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以影响ML模型的部署方式，首选的方法是*蓝绿*和*影子*。这是因为从上一个ML模型到新模型有一个清晰的截止点，这使得在会话期间使用的模型非常明确。在这两种方法之间，*蓝绿*可能具有更快的回滚速度，因为上一个ML模型版本正在待命。您可以将蓝绿部署与*流程*模板结合使用，以评估新模型，产生类似于影子部署的效果。使用模板将在实际模型之后立即调用建议的ML模型，为ML团队的分析生成日志。
- en: The deployment period for *canary* and *rolling updates* will have ambiguity
    with regard to which model was actually called. To find out which ML model was
    actually served during the deployment period, we need to request logs from the
    servers that are hosting the models or have the API response indicate which version
    of an ML model was being used.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*金丝雀*和*滚动更新*的部署期间，关于实际调用的是哪个模型会有模糊性。要找出部署期间实际提供的是哪个ML模型，我们需要从托管模型的服务器请求日志，或者让API响应指示正在使用哪个ML模型版本。
- en: The four deployment methods mentioned don’t require downtime. If we’re allowed
    downtime, it’s reasonable to go with a *replacement* deployment strategy, as retiring
    Schedules and ensuring that no Sessions are active is the safest way to perform
    changes to any BP solution. We can of course decide to pair downtime with any
    of the other deployment strategies as well.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 提到的四种部署方法不需要停机时间。如果我们允许停机时间，采用*替换*部署策略是合理的，因为退役计划并确保没有会话处于活动状态是执行任何BP解决方案更改的最安全方式。我们当然也可以决定将停机时间与任何其他部署策略相结合。
- en: If the ML model is third-party developed or a hosted service, it’s highly unlikely
    that we’ll be able to influence the deployment strategy. It’s also unlikely that
    we’ll know when a deployment occurs. We basically need to keep an eye on the API
    documentation to find out when something has changed or hope that the API call
    returns the model version.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果ML模型是第三方开发或托管服务，我们几乎不可能影响部署策略。也不太可能知道部署何时发生。我们基本上需要关注API文档，以了解何时发生变化，或者希望API调用返回模型版本。
- en: Moving on from API deployments, let’s look at script-based deployments, which
    are comprised of batch files, PowerShell scripts, and Python scripts that are
    directly executed on the Digital Worker through the command line.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从API部署过渡到脚本部署，脚本部署包括批处理文件、PowerShell脚本和Python脚本，这些脚本通过命令行直接在数字工作者上执行。
- en: Script deployment strategies
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本部署策略
- en: There are two main strategies for script-based ML deployments. The first is
    to install the script and its dependencies *directly* onto each Digital Worker’s
    Windows system. The second is to *virtualize* the scripts and its dependencies.
    Regardless of which strategy is chosen, it’s important to version control the
    scripts, which helps to document the dependencies and simplify rollbacks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于脚本的ML部署，主要有两种策略。第一种是将脚本及其依赖项*直接*安装在每个数字工作者的Windows系统上。第二种是将脚本及其依赖项*虚拟化*。无论选择哪种策略，都要对脚本进行版本控制，这有助于记录依赖项并简化回滚。
- en: Direct deployment
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接部署
- en: For a direct deployment, we need to consider the *location* of the scripts and
    install the *dependencies* that are needed to run them. The scripts can be located
    in an executable network location or saved locally onto each Digital Worker itself.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直接部署，我们需要考虑脚本的*位置*并安装运行它们所需的*依赖项*。脚本可以位于可执行的网络位置，或者保存在每个数字工作者本地。
- en: The difficulty of deployment lies in properly configuring the Windows system
    to ensure that the dependencies required by the updated model are properly set
    up. This likely requires IT to manually install packages through the command line.
    This can be time-consuming if it needs to be repeated across many Digital Workers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 部署的难度在于正确配置Windows系统，以确保更新的模型所需的依赖项被正确设置。这很可能需要IT通过命令行手动安装包。如果需要在许多数字工作者之间重复，这可能会很耗时。
- en: From a purely BP perspective, an update to the ML scripts shouldn’t result in
    many changes beyond modifying the path to the script executable, its arguments,
    and the `Model Version` Environment Variable. Unfortunately, these changes require
    *downtime*, as updates to Processes, Objects and Environment Variables are stored
    in the BP database. Modifications to these can only happen once after deployments
    to all VMs have finished.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从纯 BP 视角来看，ML 脚本的更新不应导致许多变化，除了修改脚本可执行文件的路径、其参数和 `Model Version` 环境变量。不幸的是，这些更改需要
    *停机时间*，因为流程、对象和环境变量的更新存储在 BP 数据库中。对这些更改的修改只能在所有 VM 的部署完成后进行一次。
- en: '![Figure 9.22 – Direct deployments](img/image_00_022.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.22 – 直接部署](img/image_00_022.jpg)'
- en: Figure 9.22 – Direct deployments
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 – 直接部署
- en: The easiest way to prepare for rollbacks in a direct deployment is to take an
    image of the Digital Worker VM before deploying the new ML model. If we find an
    issue with the ML model after it’s deployed, we can roll back by reverting back
    to the previous image. Note that reverting the Digital Worker VM back to its previous
    state still requires undoing changes to the BP solution, such as changing the
    `Model Version` Environment Variable, the path to the script executable, its arguments,
    etc. Those changes are saved inside the BP database, and not the individual VMs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接部署中为回滚做准备的最简单方法是，在部署新的 ML 模型之前对数字工作者 VM 进行快照。如果我们发现部署后的 ML 模型存在问题，我们可以通过回滚到先前的图像来解决问题。请注意，将数字工作者
    VM 回滚到其先前状态仍然需要撤销对 BP 解决方案的更改，例如更改 `Model Version` 环境变量、脚本可执行文件的路径、其参数等。这些更改保存在
    BP 数据库中，而不是单个 VM 中。
- en: '![Figure 9.23 – Rolling back a direct deployment through image backups](img/image_00_023.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.23 – 通过图像备份回滚直接部署](img/image_00_023.jpg)'
- en: Figure 9.23 – Rolling back a direct deployment through image backups
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 – 通过图像备份回滚直接部署
- en: If creating an image backup isn’t possible, you’ll need to roll it back manually.
    A manual rollback is error-prone because downgrading dependencies is rarely possible.
    It will likely require uninstalling and reinstalling packages from scratch. Doing
    this across many Digital Workers will be very tedious.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法创建图像备份，您将需要手动回滚。手动回滚容易出错，因为降级依赖项很少可能。这很可能需要从头开始卸载和重新安装软件包。在许多数字工作者上这样做将会非常繁琐。
- en: Virtualized deployment
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟化部署
- en: Another deployment option that makes rolling back simple is to use *virtualization*
    inside of the Digital Worker itself. You can deploy scripts inside of local containers
    (such as Docker), VM images, Windows Subsystem for Linux, etc. A virtualized deployment
    will come bundled with all of the necessary dependencies for a particular version
    of the ML model.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使回滚变得简单的部署选项是在数字工作者内部使用 *虚拟化*。您可以在本地容器（如 Docker）中部署脚本，VM 图像，Windows Subsystem
    for Linux 等。虚拟化部署将附带特定版本 ML 模型的所有必要依赖项。
- en: 'Again, changes to BP Processes, Objects, and Environment Variables are applied
    to all Sessions, so it isn’t possible to partially deploy an ML solution on just
    a few Digital Workers (outside of cloning the entire BP Release and renaming everything).
    Therefore, deploying a virtualized ML solution requires downtime, and the help
    of IT to ensure that the correct image is started automatically:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，对 BP 流程、对象和环境变量的更改将应用于所有会话，因此不可能仅对少数数字工作者（除了克隆整个 BP 发布版本并重命名所有内容之外）部分部署
    ML 解决方案。因此，部署虚拟化 ML 解决方案需要停机时间，并需要 IT 的帮助以确保正确启动图像：
- en: '![Figure 9.24 – Virtualized deployments](img/image_00_024.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.24 – 虚拟化部署](img/image_00_024.jpg)'
- en: Figure 9.24 – Virtualized deployments
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.24 – 虚拟化部署
- en: 'To roll back a virtualized script deployment, revert to the previous version
    of the image. Again, we still need to undo changes that were made to the IA solution,
    as those are saved in the BP database:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要回滚虚拟化脚本部署，请恢复到图像的先前版本。再次强调，我们仍然需要撤销对 IA 解决方案所做的更改，因为这些更改已保存在 BP 数据库中：
- en: '![Figure 9.25 – Rolling back a virtualized deployment](img/image_00_025.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.25 – 回滚虚拟化部署](img/image_00_025.jpg)'
- en: Figure 9.25 – Rolling back a virtualized deployment
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25 – 回滚虚拟化部署
- en: Virtualized deployments will have heavier resource requirements (CPU and RAM)
    on the Digital Worker when compared with direct deployments. However, rolling
    back a locally running VM can likely be executed faster, as fewer teams need to
    get involved. Both methods have pros and cons.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接部署相比，虚拟化部署在数字工作者上对资源的需求（CPU和RAM）更大。然而，回滚本地运行的虚拟机可能执行得更快，因为涉及的团队更少。两种方法都有利弊。
- en: We’ve completed the discussion on how to deploy and roll back script-based ML
    models. Next, let’s discuss Code Stage deployments.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了关于如何部署和回滚基于脚本的机器学习模型的讨论。接下来，让我们讨论代码阶段部署。
- en: Code Stage deployment strategies
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码阶段部署策略
- en: 'Code Stage ML deployments will be familiar to RPA teams, as they can be done
    as standard Object XML or `.bprelease` imports. Some might also require copying
    `.dll` files to the location specified in the Object, to a folder on the System
    Path, or to the BP installation folder. The only additional step compared to importing
    any other non-ML Code Stage is to change the `Model Version` Environment Variable.
    Downtime will be required if we need to copy new `.dll` files, because they require
    a Runtime Resource restart to be recognized:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 代码阶段机器学习部署对RPA团队来说很熟悉，因为它们可以作为标准的对象XML或`.bprelease`导入完成。有些人可能还需要将`.dll`文件复制到对象中指定的位置，到系统路径上的一个文件夹，或者到BP安装文件夹。与导入任何其他非机器学习代码阶段相比，唯一的额外步骤是更改`模型版本`环境变量。如果我们需要复制新的`.dll`文件，则需要重启运行时资源以被识别，这将需要停机时间：
- en: '![Figure 9.26 – Code Stage deployments](img/image_00_026.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图9.26 – 代码阶段部署](img/image_00_026.jpg)'
- en: Figure 9.26 – Code Stage deployments
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 – 代码阶段部署
- en: Rolling back a Code Stage deployment is simple. Re-import a previous version
    of the Object or Release, copy back the correct `.dll` files, delete any newly
    added `.dll` files, and revert to a previous `Model Version` Environment Variable
    value. It’s important that you keep a copy of the `.dll` files needed for previous
    models for rollback purposes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚代码阶段部署很简单。重新导入对象或发布的先前版本，复制回正确的`.dll`文件，删除任何新添加的`.dll`文件，并将`模型版本`环境变量值恢复到先前版本。保留先前模型所需的`.dll`文件的副本对于回滚目的很重要。
- en: '![Figure 9.27 – Rolling back a Code Stage deployment](img/image_00_027.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图9.27 – 回滚代码阶段部署](img/image_00_027.jpg)'
- en: Figure 9.27 – Rolling back a Code Stage deployment
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.27 – 回滚代码阶段部署
- en: This concludes the section on the deployment strategies for Web APIs, CLI scripts,
    and Code Stages. It’s important to understand the specific strategy being used
    to deploy the new ML model, as this affects how we implement changes in our IA
    solution, and how we can roll back to previous versions of the model. Next, let’s
    explore how IA changes the database management requirements of BP, and how we
    can use SQL to extract ML logs, instead of doing so through the BP software through
    the ML Auditor Role.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了关于Web API、CLI脚本和代码阶段部署策略的章节。了解正在使用的特定部署策略对于部署新的机器学习模型非常重要，因为这会影响我们在IA解决方案中实施更改的方式，以及我们如何回滚到模型的先前版本。接下来，让我们探讨IA如何改变BP的数据库管理需求，以及我们如何使用SQL提取机器学习日志，而不是通过BP软件通过ML审计员角色进行。
- en: Database operations
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据库操作
- en: BP’s database is primarily used for operations and isn’t intended to be a permanent
    datastore for record keeping. As the database table sizes grow, the database starts
    to perform more slowly, which reduces the Digital Worker execution speed. Adding
    IA steps into a Process leads to additional logging, which increases the growth
    rate of certain tables. Ongoing database maintenance has always been a key factor
    in ensuring the smooth performance of BP RPA, and the need for database maintenance
    is only amplified once we start doing IA.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: BP的数据库主要用于操作，并不打算作为记录的永久数据存储。随着数据库表大小的增长，数据库开始运行得更慢，这降低了数字工作者的执行速度。将IA步骤添加到流程中会导致额外的日志记录，这增加了某些表的增长速度。持续的数据库维护一直是确保BP
    RPA平稳运行的关键因素，而进行IA后，数据库维护的需求只会加剧。
- en: Table growth maintenance
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表增长维护
- en: The tables that are most affected by IA are `BPAWorkQueueItem` and `BPASessionLog_*`.
    `BPAWorkQueueItem` stores the Item data of Work Queue Items.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 受IA影响最大的表是`BPAWorkQueueItem`和`BPASessionLog_*`。`BPAWorkQueueItem`存储工作队列项的数据。
- en: Important note
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '`BPASessionLog_*`, means the various Session Logging tables, including `BPASessionLog_NonUnicode`,
    `BPASessionLog_NonUnicode_pre65`, `BPASessionLog_Unicode`, and `BPASessionLog_Unicode_pre65`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`BPASessionLog_*` 表示各种会话日志表，包括 `BPASessionLog_NonUnicode`、`BPASessionLog_NonUnicode_pre65`、`BPASessionLog_Unicode`
    和 `BPASessionLog_Unicode_pre65`。'
- en: BPAWorkQueueItem
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BPAWorkQueueItem
- en: We’re likely storing the entirety of the input data needed for the ML algorithm
    in the Work Queue Item data. If the ML algorithm has many columns of input data,
    `BPAWorkQueueItem` will grow very quickly in size. There are four primary ways
    to manage the growth rate of this table. The first is to delete Work Queue Items
    directly from the BP user interface. This isn’t recommended as the UI has limits
    on how many Work Queue Items can be selected and deleted at once.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很可能会在“工作队列项”数据中存储用于机器学习算法的所有输入数据。如果机器学习算法有多个输入数据列，`BPAWorkQueueItem`的大小会迅速增长。管理此表增长率的四种主要方法如下。第一种是直接从BP用户界面删除工作队列项。这不推荐，因为UI在选择和删除工作队列项时有限制。
- en: The second way is to use a database script that’s provided by BP’s Customer
    Support. This SQL script will delete rows from the Work Queue related tables if
    they’re older than a certain date. For instance, you can configure the script
    to keep 30 days’ worth of Work Queue Items, and have the script run daily through
    the SQL Server Agent. This ensures that your Work Queue Item tables won’t have
    more than 30 days’ worth of data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是使用BP客户支持提供的数据库脚本。此SQL脚本会在超过特定日期的行从工作队列相关表中删除。例如，您可以配置脚本以保留30天的“工作队列项”，并通过SQL
    Server Agent每天运行脚本。这确保了您的“工作队列项”表不会超过30天的数据。
- en: The third way is to use the **Blue Prism Archiver XBP** asset, which is available
    on the DX at [https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp](https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp).
    This asset allows you to move Work Queue Items out of the main BP database, into
    a different database. More details can be found on the DX page’s documentation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是使用**Blue Prism Archiver XBP**资产，该资产可在DX上找到，网址为[https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp](https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp)。此资产允许您将工作队列项从主BP数据库移动到不同的数据库。更多详细信息可以在DX页面文档中找到。
- en: Finally, there’s another DX asset called the `BPAWorkQueueItem` and the various
    `BPASessionLog_*` tables, which are discussed next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一个DX资产，名为`BPAWorkQueueItem`以及各种`BPASessionLog_*`表，将在下文中讨论。
- en: BPASessionLog*
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BPASessionLog*
- en: IA increases the growth rate of the various Session Log tables simply because
    there are more Process and Object Stages to execute, and log into the database.
    As mentioned in the previous paragraph, one way to manage the growth of this table
    is to use the **DB Servicer XBP** from the DX.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: IA通过增加执行并记录到数据库的流程和对象阶段数量，简单地增加了各种会话日志表的增长率。如前一段所述，管理此表增长的一种方法是从DX使用**DB Servicer
    XBP**。
- en: The `BPASessionLog_*` tables can also be managed through an SQL script that’s
    provided by Customer Support. The script lets us specify how many days of Session
    Logs we want to keep. Anything older than the number of days will be deleted from
    the database. Once set up, the SQL script needs to be run on a scheduled basis.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`BPASessionLog_*`表也可以通过客户支持提供的SQL脚本来管理。该脚本允许我们指定我们想要保留多少天的会话日志。任何超过指定天数的日志都将从数据库中删除。一旦设置好，SQL脚本需要定期运行。'
- en: A third option to keep the Session Logging table sizes in check is to use the
    in-product archiving function. This can be found under `.gz` format. We must specify
    which Runtime Resource we want to perform the archiving (during its idle time),
    where on the Runtime Resource we want the archive files saved, and how many days’
    worth of logs we want to keep in the BP database.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 保持会话日志表大小在可控范围内的第三种选项是使用产品内存档功能。这可以在`.gz`格式下找到。我们必须指定我们想要进行存档的运行时资源（在其空闲时间），在运行时资源上我们想要保存存档文件的位置，以及我们想要在BP数据库中保留多少天的日志。
- en: '![Figure 9.28 – The in-product archiver, which can slow down the Session Log
    growth rate](img/image_00_028.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图9.28 – 产品内存档器，可以减缓会话日志的增长速度](img/image_00_028.jpg)'
- en: Figure 9.28 – The in-product archiver, which can slow down the Session Log growth
    rate
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.28 – 产品内存档器，可以减缓会话日志的增长速度
- en: The final option is to delete Sessions by hand through the BP user interface.
    This is unrealistic for most companies as there are potentially thousands of Sessions
    (if not more) being run regularly.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种选项是通过BP用户界面手动删除会话。对于大多数公司来说，这是不现实的，因为可能定期运行数千个（如果不是更多）会话。
- en: IA requires us to refine our BP database maintenance practices. IA also leads
    to new requirements in terms of data exporting. For example, ML Auditors want
    to know what inputs have led to what predictions and how those predictions have
    been used. Data scientists want to know the manual verification results so that
    they can update the algorithm. If our ML model is third-party developed or hosted,
    the only way of getting these logs is through the BP database, as we won’t have
    access to the underlying ML server logs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: IA要求我们改进我们的BP数据库维护实践。IA还导致数据导出方面的新要求。例如，ML审计员想知道哪些输入导致了哪些预测以及这些预测是如何被使用的。数据科学家想知道手动验证结果，以便他们可以更新算法。如果我们的ML模型是第三方开发或托管，获取这些日志的唯一方法是通过BP数据库，因为我们无法访问底层的ML服务器日志。
- en: While exporting this data can be done through the BP user interface, it may
    be preferable to automate and export this data directly from the database.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以通过BP用户界面导出这些数据，但可能更倾向于自动化并直接从数据库导出这些数据。
- en: Important note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Data exporting should be done during off hours, preferably on read-only versions
    of the database, to minimize the impact on BP production operations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 数据导出应在非工作时间进行，最好是在数据库的只读版本上，以最小化对BP生产操作的影响。
- en: Extracting ML prediction data from the database
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据库中提取机器学习预测数据
- en: Based on the templates presented in [*Chapter 7*](B18416_07.xhtml#_idTextAnchor114),
    our ML prediction data can potentially be extracted from two database locations.
    The first is from the Work Queue tables, which works if we’re using the template
    that separates the ML predictions into its own Work Queue. If we’re not using
    the template with dedicated Work Queues, we can query the Session Log tables to
    find this data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在[*第7章*](B18416_07.xhtml#_idTextAnchor114)中展示的模板，我们的机器学习预测数据可能从两个数据库位置提取。第一个是从工作队列表，如果我们使用将机器学习预测分离到其自己的工作队列的模板，则此方法有效。如果我们不使用具有专用工作队列的模板，我们可以查询会话日志表以找到这些数据。
- en: Querying Work Queue data
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询工作队列数据
- en: 'If you’re using a template that has a dedicated Work Queue for ML, it’s straightforward
    to query for Work Queue data through SQL. The SQL following script allows you
    to provide data scientists with the input data to the algorithm, the predicted
    result, and the confidence score. The only thing that needs changing is the name
    of the Work Queue, and you could potentially add dates to limit the range of results
    that are returned:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是具有专用工作队列的ML模板，通过SQL查询工作队列数据是直接的。以下SQL脚本允许你为数据科学家提供算法的输入数据、预测结果和置信度分数。唯一需要更改的是工作队列的名称，你还可以添加日期以限制返回结果的范围：
- en: '[PRE0]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `data` column that’s selected is an XML string. This XML format can be directly
    opened by Excel or parsed by data scientists using their programming language
    of choice.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所选的`data`列是一个XML字符串。这种XML格式可以直接由Excel打开或由数据科学家使用他们选择的编程语言进行解析。
- en: Querying Session Log data
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询会话日志数据
- en: 'In all of the IA templates, we’ve deliberately enabled logging in some Stages
    to make extracting the prediction data easier in both the Session Log Viewer and
    through the database. The two Stages where logging is enabled are shown in the
    following figure and are named identically across the three IA templates. This
    common naming scheme is required for our SQL to work:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有的IA模板中，我们故意在一些阶段启用了日志记录，以便在会话日志查看器和数据库中更容易提取预测数据。以下图中显示的启用了日志记录的两个阶段在三个IA模板中名称相同。这种常见的命名方案是我们SQL运行所必需的：
- en: '![Figure 9.29 – Common Stages to log ML prediction results](img/image_00_029.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图9.29 – 记录机器学习预测结果的常见阶段](img/image_00_029.jpg)'
- en: Figure 9.29 – Common Stages to log ML prediction results
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.29 – 记录机器学习预测结果的常见阶段
- en: We can query the Session Logs to specifically find these two Stages, by passing
    in the name of the Process. Depending on how BP is configured, you might also
    need to change the database table name from `BPASessionLog_NonUnicode` to `BPASessionLog_Unicode`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过传递进程的名称来查询会话日志，以特别找到这两个阶段。根据BP的配置，你可能还需要将数据库表名从`BPASessionLog_NonUnicode`更改为`BPASessionLog_Unicode`。
- en: '[PRE1]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `modelversion` column is populated with the model version from `Log [Model
    Version]` Calculation Stage. The `attributexml` column contains the logged values
    from the `Set [Prediction] and [Confidence Score]` Multi Calc Stage.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`modelversion`列填充了来自`Log [模型版本]`计算阶段的模型版本。`attributexml`列包含来自`Set [预测]和[置信度分数]`多计算阶段记录的值。'
- en: The `LAG(result, 1, 0) OVER(ORDER BY logid)` SQL syntax allows us to select
    the value from `Log [Model Version]` and `Set [Prediction] and [Confidence Score]`
    on a single row, even though they belong to different Session Log entries. This
    relies on there being only `1` Stage of difference between the two. If there are
    more Session Log records between the two Stages, you’ll need to change the `1`
    value to match.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`LAG(result, 1, 0) OVER(ORDER BY logid)` SQL 语法允许我们从 `Log [模型版本]` 和 `Set [预测]
    和 [置信度分数]` 的单行中选择值，即使它们属于不同的会话日志条目。这依赖于两个阶段之间只有 `1` 个阶段的差异。如果两个阶段之间存在更多的会话日志记录，您需要将
    `1` 的值更改为匹配。'
- en: 'If you’re using the **Example 4 – New Model Evaluation Process Template** from
    [*Chapter 6*](B18416_06.xhtml#_idTextAnchor093) to evaluate a second ML model
    in production, the following SQL can be used:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 **示例 4 – 新模型评估流程模板**（来自 [*第 6 章*](B18416_06.xhtml#_idTextAnchor093)）在生产中评估第二个机器学习模型，可以使用以下
    SQL 语句：
- en: '[PRE2]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Exporting reviewed prediction data from the database
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据库中导出审查后的预测数据
- en: The reviewed prediction results might also be found in two locations depending
    on which template is used. If the two-Work Queue or three-Work Queue templates
    are used, we can find them from the `BPAWorkQueueItem` table. If not, we can look
    into the `BPASessionLog_*` table.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的模板，审查后的预测结果可能出现在两个位置。如果使用双工作队列或三工作队列模板，我们可以从 `BPAWorkQueueItem` 表中找到它们。如果不使用这些模板，我们可以查看
    `BPASessionLog_*` 表。
- en: Querying Work Queue data
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询工作队列数据
- en: 'For the HITL Review Work Queue, we only retrieve results where we know for
    certain that the review has occurred. We do this by checking for the presence
    of a Collection Field named `Confidence`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 HITL 审查工作队列，我们只检索我们知道已经发生审查的结果。我们通过检查名为 `Confidence` 的收集字段的是否存在来完成此操作：
- en: '[PRE3]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Querying Session Log data
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询会话日志数据
- en: 'In all three templates, there’s a commonly named Stage that has logging deliberately
    enabled, to capture the corrected prediction result and the justification reason.
    This Stage is shown in the following figure:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三个模板中，都有一个名为阶段的常见名称，该阶段故意启用了日志记录，以捕获修正后的预测结果和理由。此阶段在以下图中显示：
- en: '![Figure 9.30 – A common Stage to log the review result and justification](img/image_00_030.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.30 – 记录审查结果和理由的常见阶段](img/image_00_030.jpg)'
- en: Figure 9.30 – A common Stage to log the review result and justification
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30 – 记录审查结果和理由的常见阶段
- en: 'The SQL to find this Stage in the Session Log tables is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在会话日志表中查找此阶段的 SQL 语句如下：
- en: '[PRE4]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
