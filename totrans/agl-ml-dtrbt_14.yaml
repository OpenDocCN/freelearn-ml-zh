- en: '*Chapter 11*: Working with Geospatial Data, NLP, and Image Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book thus far, we have focused mainly on numeric and categorical features.
    This is not always the case in big data, as with big data comes an increasing
    data variety. Image, text, and geospatial data is becoming increasingly valuable
    in gaining insight and providing solutions to the most complex problems. Recently,
    for instance, **location-based** data has been used to improve the effectiveness
    of advertising campaigns. For example, different ads can be shown to users according
    to their location; if they are coffee lovers and close to coffee shops, push notifications
    could be sent to their mobile devices. In other cases, chatbots, based on advanced
    text analytics or natural language processing, provide businesses with an efficient
    and effective avenue to solve customer problems. What is most interesting and
    an emerging approach to solving commercial problems is the use of **multimodal**
    datasets, which combine different variable types in the same project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Understandably, the topic of analyzing different variable types is enough to
    be covered in a book in its own right. Yet providing an overview of the analysis
    of different variable types is key in grounding the use of DataRobot in building
    multimodal models that involve text, image, and location data. With that foremost
    in mind, in this chapter, we will delve into the definitions and approaches to
    analytics with text, image, and geospatial data. Thereafter, we will use DataRobot
    to build and make predictions with a model that capitalizes on the uniqueness
    of a multimodal dataset in predicting house prices. As such, the topics that will
    be covered are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A conceptual introduction to geospatial, text, and image data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and setting up multimodal data in DataRobot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building models using a multimodal dataset in DataRobot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making predictions using multimodal datasets in DataRobot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the analysis and modeling carried out in this chapter requires access
    to the DataRobot software. Some manipulations were carried out using other tools,
    including MS Excel. The dataset utilized in this chapter is the House Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: House Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The House Dataset can be accessed at Eman Hamed Ahmed's GitHub account ([https://github.com/emanhamed](https://github.com/emanhamed)).
    Each row in this dataset represents a specific house. The initial feature set
    describes its characteristics, price, zip code, images of the bedroom, bathroom,
    kitchen, and frontal view. There was no missing data. We went on to develop text
    descriptions for each house, based on the number of bedrooms, bathrooms, city,
    country, state, and actual size of the property. Elsewhere, the ZIP codes were
    converted into latitude and longitude, which were added to the dataset as columns.
    More information on the base features is provided at the GitHub link and the data
    is provided in `.csv` format.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Citation
  prefs: []
  type: TYPE_NORMAL
- en: '*House Price Estimation from Visual and Textual Features. In Proceedings of
    the 8th International Joint Conference on Computational Intelligence*, *H. Ahmed
    E. and Moustafa M.* *(2016). (IJCCI 2016) ISBN 978-989-758-201-1, pages 62â€“68\.
    DOI: 10.5220/0006040700620068*'
  prefs: []
  type: TYPE_NORMAL
- en: A conceptual introduction to geospatial, text, and image data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like we use different senses to holistically understand objects around
    us, a **machine learning** (**ML**) model also benefits from data coming from
    different types of sensors and sources. Having only one type of data (for instance,
    numeric or categorical) limits the level of understanding, predictability, and
    robustness of a model. In this section, we will present a more in-depth discussion
    of the business importance of different data types in building models, the associated
    challenges, and the preprocessing steps necessary to mitigate these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Geospatial AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Geospatial understanding has had long-standing implications for decision-making
    in certain industries, including mineral exploitation, insurance, retail, and
    real estate. While the commercial importance of data science is well established,
    location-based AI is just beginning to gain recognition. The use of ML in improving
    business performance has brought to the fore the importance of augmenting datasets
    with location-based information and features in building predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: Typical ML models built mainly from categorical and numeric data have contributed
    immensely to realizing business goals, but decisions are governed by more than
    numeric and categorical information. Indeed, the events take place at certain
    locations. ML models need location-based information in order for the location
    context to effectively present commercial insight and predictions. What works
    in one geography may not work in another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The potential commercial impact of using ML and location-based information
    comes with several challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: A lack of datasets, tools, and people skills.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting ML pipelines to native location-based analysis techniques is not
    straightforward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only a few R and Python packages have geospatial capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these capabilities requires further education and training for
    analysts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataRobot's location AI capability helps alleviate some of these challenges.
    The location AI capability complements the existing AutoML experience by adding
    in a repertoire of geospatial analytic and modeling tools. With DataRobot, location
    features could be selected from the dataset, but the location AI capability enables
    the platform to automatically recognize geospatial data and create geospatial
    features. A variety of geospatial data file formats can be uploaded. These include
    GeoJSON, Esri shapefiles, and geodatabases, PostGIS tables, as well as traditional
    latitude and longitude data.
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As humans, we communicate effectively via a vast range of words with or without
    limitations on the volume of words to use. More than words, body language, tonality,
    and words' context are crucial to effective communication. For example, using
    the same set of words, *the cat is bigger than the dog* has a different meaning
    to *the dog is bigger than the cat*. Naturally, humans understand, draw conclusions,
    and make predictions of the future based on free text. The use of free text comes
    with valuable information and rich insights can be harvested from it. Yet, since
    free text fails to follow a consistent structure, they pose challenges to being
    processed by machines.
  prefs: []
  type: TYPE_NORMAL
- en: Conversations and other forms of free text are messy and unstructured as they
    do not fit neatly into traditional tables with rows and columns. **Natural Language
    Processing** (**NLP**) sits at the intersection of data science and linguistics
    and involves the systematic use of advanced processes for analysis, understanding,
    and the extraction of data from free text. Through NLP, scientists can leverage
    free text to generate valuable insight, which is then integrated as features in
    building better-performing models. Text mining allows the identification of unique
    words or groups of words that are associated with certain outcomes. For example,
    in the house price prediction case, the description of the house improves the
    predictability of the models in estimating the house price. Thinking about it,
    the description also contributes to an individuals' decision of buying a house.
    Individuals' propensity to buy houses influences property pricing. NLP algorithms
    can identify the effect of word sequencing and influence words or phrases, and
    a word's context within sentences.
  prefs: []
  type: TYPE_NORMAL
- en: NLP is key to machines being able to extract important information from text.
    Consequently, NLP allows machines to decide feelings described in free text by
    giving a number score to a text, indicating its sentiment to a topic or event.
    Similarly, it aids in the identification of classes that certain words most likely
    belong to. This capability has given birth to several applications, including
    text classification, named entity recognition, sentiment analysis, and summarization
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get free text to provide useful insight or be integrated into models is
    not an easy task. As earlier alluded to, raw text has no structure, so structure
    needs to be introduced. Also, numerous words have the same meaning and you could
    have the same word mean different things in different contexts. In a typical analytics
    process, there are numerous steps taken to normalize free text. At least four
    steps are required:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in most text processing is splitting text corpus into separate
    words. This step, also called **tokenization**, enables the identification of
    keywords and phrases. The separated words are referred to as tokens. N-grams are
    the basic units for text analytics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, there are certain words that contribute little or nothing to the meaning
    of a text. These are generally common words; for instance, in the English language,
    we have words such as *the*, *that*, *is*, and *these*. Within the context of
    text mining, these words are referred to as noise or are sometimes called stop
    words. So, this step is called **noise removal**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, words are converted to their root meanings. There are a few approaches
    to this. As an illustration, **stemming** typically converts to the root word
    stem by eliminating certain letters. So, words like *happy*, *happiness*, *happily*,
    and *happiest* will all be returned to the root word *happ*. Because the same
    words could have multiple meanings, disambiguation of words becomes crucial in
    text processing. Whereas stemming returns words to their roots by cutting off
    their prefix or suffix, **lemmatization** examines the context of words to ensure
    stemmed words are converted to logical bases called **lemma**. For example, the
    word *anticipate* when stemmed might be returned to *ant*. Within the context,
    however, *ant* would not make sense; as such, lemming will ensure that the word
    *anticipate* is retained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A final, yet important, step is the process of `0` and `255`, referred to as
    `0` is shown as completely black, while completely white gives `255`. On the other
    hand, color images have 3D arrays with blue, green, and red layers. Like black
    on the grayscale images, each of those layers has its own values from `0` to `255`,
    where the final color is a combination of corresponding values on each of the
    three layers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image processing typically follows predefined steps in extracting useful and
    consistent features from images that align with the purpose of extraction. `.csv`
    file sitting next to the image folder. Four new image columns were created for
    the bedroom, bathroom, frontal view, and kitchen. As such, each data row on the
    `.csv` file had paths to their images, as shown in *Table 11.2*. The paths point
    to locations on the corresponding image file in the ZIP file. Within the ZIP file,
    the `.csv` file with the tabular features sits needs to the folder, containing
    all the images. Each image has a unique name that is consistent with the image
    path columns on the `.csv` file. The setup for the ZIP file and `HousePrice` folder
    is shown in *Figure 11.1*. That said, the dataset could still be ingested using
    the AI Catalog. This also gives DataRobot the ability to connect to other data
    sources for the images. Furthermore, the Paxata tool can be deployed in the data
    preprocessing if you have access to that tool:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.3 â€“ Data setup for the ZIP file (left) and image folder (right)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17159_11_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.3 â€“ Data setup for the ZIP file (left) and image folder (right)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The image on the left of *Figure 11.1* shows how the ZIP file is set up. The
    folder containing the image files, `Houses Dataset`, is next to the `HousePrice.csv`
    file. The image on the right presents image files within the `Houses Dataset`
    folder. Here, the images are labeled, with the locations, consistent with cells
    on data, `HousePrice.csv` (as shown in *Table 11.2*). With the data completely
    set up, the next step is to commence model development.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Building models using multimodal datasets in DataRobot
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Having fully set up our ZIP file with the multimodal dataset, we proceed into
    initiating the project within DataRobot. The data ingestion using the drag and
    drop method is like the earlier project, except in this case we upload the ZIP
    file. Following the upload of the ZIP file, the price is selected as the target
    variable. DataRobot automatically detects the text, image, and geospatial fields
    (see *Figure 11.2*). The geometry feature is a location-based feature made up
    of the latitude and longitude variables in the original dataset. Apart from latitude
    and longitude coordinates, location features can be formed from other native geospatial
    formats, such as Esri shapefiles, GeoJSON, and PostGIS databases. These can be
    uploaded using drag and drop, AI Catalog, or URL methods:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.4 â€“ Feature Name list'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17159_11_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.4 â€“ Feature Name list
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The location-based visual representation of the listing price can be viewed
    by selecting the `Price` option in the `Feature Name` list. This `Geospatial Map`
    tab and clicking on the `Compute feature over map` button. As seen in *Figure
    11.3*, the `Geospatial Maps` window offers a location-based analytics visualization
    of the dataset. It shows the distribution of properties over space â€“ the number
    of houses in each area and their average prices:![Figure 11.5 â€“ Geospatial Map
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_11_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.5 â€“ Geospatial Map
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The map legend offers vital information about the map. It highlights that the
    color of the hexagon shows the average house prices within the location. Elsewhere,
    it presents the frequency of cases by the height of the hexagons. This ESDA feature
    shows not only the visual distribution of house prices across the map but also
    an illustration of house counts in differing areas. Similar geospatial analysis
    can be conducted for other features, such as house area variables and bedrooms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This preliminary examination of image features can be conducted by selecting
    any of the image variables. This shows a sample of images within the **Feature
    Name** list. Here, differing image features can be seen and organized by house
    price ranges.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To further explore the image features at a property level, we click on `.csv`
    file with image paths, the images are integrated into the dataset (see *Figure
    11.4*). For each of the rows, the images are clearly displayed. A further scroll
    will show the free text description of the listed properties. This multimodal
    dataset of text, location, and image features can now be used to build a more
    robust model and make predictions of house prices:![Figure 11.6 â€“ The DataRobot
    view of the multimodal data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B17159_11_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.6 â€“ The DataRobot view of the multimodal data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As with earlier projects, we click on **Start** to commence the model-building
    process. On completion of the modeling process, the models are evaluated using
    the **RMSE metric**. The leaderboard shows DataRobot has built 36 models in total.
    The top-performing is the **Nystroem Kernel SVM Regressor** model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As can be seen in *Figure 11.5*, opening the model presents its blueprint,
    outlining all the steps necessary to make the data ready for this model. Because
    of the multimodal nature of the data, the preprocessing steps are quite complex.
    DataRobot conducted geospatial processing, which was integrated with some numeric
    variables and high-level image and text processing (the latter not visible in
    *Figure 11.5*). For more information on each step, a click on the step box provides
    some insight on the modeling step and a link to comprehensive documentation on
    the step:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.7 â€“ The model blueprint for multimodal data modeling'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B17159_11_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 11.7 â€“ The model blueprint for multimodal data modeling
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Within the `Area` is the most impactful feature of the house; next is the `FullDescription`
    text feature. Thereafter, the `Bedrooms` and `Image_kitchen` features follow suit.
    What is rather interesting is the fact that `Image_bathroom` seems to have a negative
    impact on the model accuracy. This suggests that insights from these images lead
    the model away from actual house prices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 â€“ Feature Impact for multimodal models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.8 â€“ Feature Impact for multimodal models
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, leading the model away from improved performance, we use the
    `bathroom` images to make predictions. By doing so, we will use the `bathroom`
    feature to demonstrate the image feature exploration capabilities available in
    DataRobot. DataRobot conducts unsupervised learning to cluster images according
    to their similarity. Still within the `bathroom` views. We can see DataRobot clusters
    similar images together. It seems that images that are dominantly white goods
    are presented in the right-hand and upper parts of the visualization. We can filter
    the visualization in accordance with house prices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 â€“ Image embedding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.9 â€“ Image embedding
  prefs: []
  type: TYPE_NORMAL
- en: '`image_bathroom` variable. It appears the model makes its predictions mainly
    from white fixtures in the bathroom. This might offer insight into why `image_bathroom`
    is seen as having a negative impact on the model performance. It is possible that
    this extraction from white fixtures misleads the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 â€“ Activation map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.10 â€“ Activation map
  prefs: []
  type: TYPE_NORMAL
- en: 'Location-based information comes with significant information complementing
    other data types. However, some models struggle in certain geographical areas.
    Inspecting the performance of models and considering locations empowers the analyst
    in taking actions on model performance improvement. DataRobot''s **Accuracy Over
    Space** capability presents a spatial representation of a models'' residual at
    differing locations (see *Figure 11.9* for an example). This chart could lead
    the analyst into considering the rationale behind higher residuals in certain
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 â€“ Accuracy Over Space'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.11 â€“ Accuracy Over Space
  prefs: []
  type: TYPE_NORMAL
- en: For instance, as evident in *Figure 11.9*, the Phoenix area has a higher average
    residual price of over $380k than most places. This area might for instance be
    considered an area of low income. This visualization could point the data scientist
    toward including features around localized economic indices. This might provide
    an explanation for the high residual. Including such features could therefore
    improve the overall performance of the model. The data partition for measuring
    accuracy could be set by altering between the validation, cross-validation, or
    holdout partitions. Also, the accuracy metric type and aggregation can be adjusted
    in accordance with the user's requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complementing its location-based features engineering, DataRobot''s location-based
    analytics capabilities can exploit its location awareness to create `eXtreme Gradient
    Boosted Trees Regressor (Gamma Loss)` model''s fourth most important feature,
    `GEO_KNL_K10_LAG1_Price`, is one such feature (see *Figure 11.10*). This feature
    describes the spatial dependence structure for price using a kernel size augmented
    by distance. The **k-nearest neighbor** approach can also be deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 â€“ Spatial lag features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.12 â€“ Spatial lag features
  prefs: []
  type: TYPE_NORMAL
- en: Text analytics information such as the Word Cloud is not available within the
    `FullDescription` text feature, which is indeed one of the most impactful features
    of this model. Though not visible on the model blueprint (*Figure 11.10*), the
    text variable was scored using another model, `Auto-Tuned Word N-Gram Text Modeler
    using token occurrences â€“ FullDescription`, which essentially develops scores
    using `FullDescription` feature was converted into tokens for a differing number
    of words (as *N* in *N-grams*) and scored. Thereafter, this feature was transformed
    on the link scale and standardized. For text-related insights, we turn to the
    **Insights** view, offering two important text insight capabilities, **Word Cloud**
    and **text mining**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `FullDescription` feature in influencing the house price. The size of the
    words, as shown in *Figure 11.11*, highlights the frequency of the tokens, while
    the color suggests its effect coefficient. This coefficient is standardized typically
    between -1.5 and 1.5\. The closer the color of the words is to red, the greater
    the coefficient and, consequently, the greater the house price is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.13 â€“ Word Cloud for a multimodal dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.13 â€“ Word Cloud for a multimodal dataset
  prefs: []
  type: TYPE_NORMAL
- en: We can assume that when the `FullDescription` variable contains words in orange,
    **alameda county**, and big, the prices are likely to be high. Similarly, with
    small-sized words, and **city riverside**, a lower price is expected. The text
    mining capability displays similar information to the Word Cloud using a bar graph.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have been able to build models using multimodal datasets, conduct
    analysis on their features, and evaluate the performance of those models, we will
    next focus on making predictions with models.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using a multimodal dataset on DataRobot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After building a model, there are many ways to make predictions on a DataRobot.
    For this use case, we will illustrate the prediction capability using the `Make
    Prediction` method, which is available within the **Predict** tab. We initially
    create a prediction ZIP file dataset using the step outline in the *Defining and
    setting up multimodal data in DataRobot* section of this chapter. The developed
    prediction dataset is either dragged and dropped into the highlighted area or
    locally imported. As seen in *Figure 11.12*, we select the features we are interested
    in, including the prediction dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.14 â€“ Making a prediction from multimodal datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.14 â€“ Making a prediction from multimodal datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'In this illustration, we selected `House_id`, `FullDescription`, `Bedrooms`,
    `City`, and `State`. We can also see that the prediction dataset has 400 houses.
    Finally, `.csv` file, which has all the requested columns (see *Table 11.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 11.15 â€“ A prediction table from a multimodal dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17159_11_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 11.15 â€“ A prediction table from a multimodal dataset
  prefs: []
  type: TYPE_NORMAL
- en: The **Prediction** column presents the predicted price for each row. This finalizes
    the process of making predictions with multimodal datasets. As expected, after
    models made from multimodal datasets have been deployed, predictions can be made
    against them.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored how insights can be generated from an image,
    location, and free text. In so doing, we highlighted the benefits that these data
    types present, as well as the challenges that come with each of them. We also
    pointed out how these are typically addressed in the mainstream. We proceeded
    to build models with a multimodal dataset using DataRobot and make predictions
    from the model. We also looked at a variety of ways to derive insights from the
    location, free text, and image aspects of the models. By demonstrating the process
    of model building using a multimodal dataset, we showed how DataRobot simplifies
    the handling of the challenges different data types pose.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, it is important to draw attention to the fact that DataRobot
    appears to have some limitations in terms of free text processing. Whilst the
    platform significantly simplifies the process of text processes, at the time of
    this publication, we are unsure of the extent to which domain-specific stop words
    can be included in the DataRobot process. It appears generic stop words are dropped,
    but sometimes there are domain-specific stop words that need to be accounted for.
    Elsewhere, within the context of multimodal modeling, we are unsure whether the
    text aspects of models could be tuned to include and alter the methods of stemming
    and lemming. It is therefore recommended that you perform your own text processing
    and feature engineering before feeding text into DataRobot to achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, as well as the previous ones, we have interfaced with DataRobot
    using the platform. Although the platform comes with numerous capabilities, these
    capabilities come with some limitations. These limitations, together with how
    they can be alleviated using programmatic access to the platform, are extensively
    covered in [*Chapter 12*](B17159_12_Final_NM_ePub.xhtml#_idTextAnchor176)*, DataRobot
    Python API*.
  prefs: []
  type: TYPE_NORMAL
