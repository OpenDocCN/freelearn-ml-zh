<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Deep Learning with Scala</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="fontstyle0">Throughout <a href="f649db9f-aea9-4509-b9b8-e0b7d5fb726a.xhtml" target="_blank">Chapter 2</a>, <em>Scala for Regression Analysis</em>, to <a href="4935de5b-9527-4ff2-82ed-927dea04c77a.xhtml" target="_blank">Chapter 6</a>, <em>Scala for Recommender System</em>, we have learned about linear and classic <strong>machine learning</strong> (<strong>ML</strong>) algorithms through real-life examples. In this chapter, we will explain some basic concepts of</span> <span class="fontstyle2"><strong>deep learning</strong> (<strong>DL</strong>)</span><span class="fontstyle0">. We will start with DL, which is one of the emerging branches of ML. We will briefly discuss some of the most well-known and widely used neural network architectures and DL frameworks and libraries.<br/></span></p>
<p class="mce-root"><span class="fontstyle0">Finally, we will use the <strong><span class="fontstyle2">Long Short-Term Memory</span></strong> (<strong><span class="fontstyle2">LSTM</span></strong>) architecture for cancer type classification from a very high-dimensional dataset curated from <strong>The Cancer Genome Atlas</strong> (<strong>TCGA</strong>). The following topics will be covered in this chapter:</span></p>
<ul>
<li class="mce-root"><span class="fontstyle0">DL versus ML<br/></span></li>
<li class="mce-root"><span class="fontstyle0">DL and neural networks<br/></span></li>
<li class="mce-root"><span class="fontstyle0">Deep neural network architectures</span></li>
<li>DL frameworks</li>
<li class="mce-root"><span class="fontstyle0">Getting started with learning</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Make sure Scala 2.11.x and Java 1.8.x are installed and configured on your machine.</p>
<p>The code files of this chapters can be found on GitHub:</p>
<p><a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07" target="_blank">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide/tree/master/Chapter07</a></p>
<p class="mce-root"/>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2vwrxzb" target="_blank">http://bit.ly/2vwrxzb</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL versus ML</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">Simple ML methods that were used in small-scale data analysis are not effective anymore because the effectiveness of ML methods diminishes with large and high-dimensional datasets. Here comes DL—a branch of ML based on a set of algorithms that attempt to model high-level abstractions in data.</span> <span class="fontstyle2">Ian Goodfellow <em>et al.</em> (Deep Learning, MIT Press, 2016) defined DL as follows:</span></p>
<div class="packt_quote"><span class="fontstyle2"><br/></span><span class="fontstyle3">"Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones."</span></div>
<p>Similar to the ML model, a DL model also takes in an input, <em>X</em>, and learns high-level abstractions or patterns from it to predict an output of <em>Y</em>. For example, based on the stock prices of the past week, a DL model can predict the stock price for the next day. When performing training on such historical stock data, a DL model tries to minimize the difference between the prediction and the actual values. This way, a DL model tries to generalize to inputs that it hasn't seen before and makes predictions on test data. <span class="fontstyle2"><br/></span></p>
<p><span class="fontstyle2">Now, you might be wondering, if an ML model can do the same tasks, why do we need DL for this? Well, DL models tend to perform well with large amounts of data, whereas old ML models stop improving after a certain point. The core concept of DL is inspired by the structure and function of the brain, which are called <strong>artificial neural networks</strong> (<strong>ANNs</strong>). Being at the core of DL, ANNs help you learn the associations between sets of inputs and outputs in order to make more robust and accurate predictions. However, DL is not only limited to ANNs; there have been many theoretical advances, software stacks, and hardware improvements that bring DL to the masses. Let's look at an example; suppose we want to develop a predictive analytics model, such as an animal recognizer, where our system has to resolve two problems:</span></p>
<ul>
<li><span class="fontstyle2">To classify whether an image represents a cat or a dog</span></li>
<li><span class="fontstyle2">To cluster images of dogs and cats</span></li>
</ul>
<p><span class="fontstyle2">If we solve the first problem using a typical ML method, we must define the facial features (ears, eyes, whiskers, and so on) and write a method to identify which features (typically nonlinear) are more important when classifying a particular animal.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p><span class="fontstyle2">However, at the same time, we cannot address the second problem because classical ML algorithms for clustering images (such as</span> <span class="fontstyle4">k-means</span><span class="fontstyle2">) cannot handle nonlinear features. Take a look at the following diagram, which shows a workflow that we would follow whether we wanted to classify if the given image is of a cat:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-714 image-border" src="assets/a07fad8a-a998-4ae3-9b5f-662fa5a9dbe0.png" style="width:46.92em;height:12.42em;"/></p>
<p><span class="fontstyle2">DL algorithms will take these two problems one step further, and the most important features will be extracted automatically after determining which features are the most important for classification or clustering.</span> <span class="fontstyle0">In contrast, when using a classical ML algorithm, we would have to provide the features manually.</span></p>
<p><span class="fontstyle0">A DL algorithm would take more sophisticated steps instead. For example, first, it would</span> <span class="fontstyle0">identify the edges that are the most relevant when clustering cats or dogs. It would then try to find various combinations of shapes and edges hierarchically. This step is called <strong>extract, transform, and load</strong> (<strong>ETL</strong>). Then a</span><span class="fontstyle0">fter several iterations, hierarchical identification of complex concepts and features would be carried out. Then, based on the identified features, the DL algorithm would decide which of these features are most significant for</span> <span class="fontstyle0">classifying the animal. This step is known as feature extraction.</span> <span class="fontstyle0">Finally, it would take out the label column and perform unsupervised training using</span> <strong><span class="fontstyle2">autoencoders</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle2">AEs</span></strong><span class="fontstyle0">) to extract the latent features to be redistributed to k-means for clustering.</span> <span class="fontstyle0">Then, the <strong>clustering assignment hardening loss</strong> (<strong>CAH loss</strong>) and reconstruction loss are jointly optimized toward optimal clustering assignment.</span></p>
<p><span class="fontstyle0">However, in practice, a DL algorithm is fed with a raw image representations, which doesn't see an image as we see it because it only knows the position of each pixel and its color. The image is divided into various layers of analysis.</span> <span class="fontstyle0">At a lower level, the software analyzes, for example, a grid of a few pixels with the task of detecting a type of color or various nuances. If it finds something, it informs the next level, which at this point checks whether or not that given color belongs to a larger form, such as a line.</span></p>
<p><span class="fontstyle0">The process continues to the upper levels until the algorithm understand what is shown in the following diagram:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/1ed01b07-4a89-4d9c-ada7-91ca8d11c792.png" style="width:30.08em;height:11.50em;"/></p>
<p><span class="fontstyle0">Although <em>dog versus cat</em> is an example of a very simple classifier, software that's capable of doing these types of things is now widespread and is found in systems for recognizing faces, or in those for searching an image on Google, for example. This kind of software is based on DL algorithms. On the contrary, by using a linear ML algorithm, we cannot build such applications since these algorithms are incapable of handling nonlinear image features.</span></p>
<p><span class="fontstyle0">Also, using ML approaches, we typically only handle a few hyperparameters. However, when neural networks are brought to the mix, things become too complex. In each layer, there are millions or even billions of hyperparameters to tune—<span>so many</span> that the cost function becomes non-convex.</span> <span class="fontstyle0">Another reason for this is that the activation functions that are used in hidden layers are nonlinear, so the cost is non-convex.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL and ANNs</h1>
                </header>
            
            <article>
                
<p>ANNs, which are <span class="fontstyle2">in</span><span class="fontstyle0">spired by how a human brain works,</span> form the core of deep learning and its true realization. Today's revolution around deep learning would have not been possible without ANNs. Thus, to understand DL, we need to understand how neural networks work<span class="fontstyle0">.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ANNs and the human brain</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">ANNs represent one aspect of the human nervous system and how the nervous system consists of a number of neurons that communicate with each other using axons</span><span class="fontstyle0">. The receptors receive the stimuli either internally or from the external world. Then, they pass this information into the biological</span> <span class="fontstyle2">neurons</span> <span class="fontstyle0">for further processing.</span></p>
<p><span class="fontstyle0">There are a number of dendrites, in addition to another long extension called the</span> <span class="fontstyle3">axon</span><span class="fontstyle0">. Toward its extremities, there are minuscule structures called</span> <span class="fontstyle3">synaptic terminals, which are</span> <span class="fontstyle0">used to connect one neuron to the dendrites of other neurons. Biological neurons receive short electrical impulses called</span> <span class="fontstyle3">signals</span> <span class="fontstyle0">from other neurons, and in response, they trigger their own signals.</span></p>
<p><span class="fontstyle0">We can thus summarize that the neuron comprises a cell body (also known as the soma), one or more</span> <span class="fontstyle2">dendrites</span> <span class="fontstyle0">for receiving signals from other neurons, and an</span> <span class="fontstyle2">axon</span> <span class="fontstyle0">for carrying out the signals that are generated by the neurons. A neuron is in an active state when it is sending signals to other neurons. However, when it is receiving signals from other neurons, it is in an inactive state. In an idle state, a neuron accumulates all the signals that are received before reaching a certain activation threshold. This whole thing motivated researchers to test out ANNs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief history of neural networks</h1>
                </header>
            
            <article>
                
<p>The most significant progress in ANNs and DL can be described with the following timeline. We have already seen how the artificial neurons and perceptrons provided the base in 1943 and 1958, respectively. Then, the XOR was formulated as a linearly non-separable problem in 1969 by Minsky <em>et al.</em>, but later, in 1974, Werbos <em>et al</em>. demonstrated the backpropagation algorithm for training the perceptron.</p>
<p>However, the most significant advancement happened in the 1980s when John Hopfield <em>et al.</em> proposed the Hopfield network in 1982. Then, one of the godfathers of the neural network and DL, Hinton and his team proposed the Boltzmann machine in 1985. However, probably one of the most significant advances happened in 1986 when Hinton <em>et al.</em> successfully trained the MLP and Jordan <em>et al.</em> proposed RNNs. In the same year, Smolensky <em>et al.</em> also proposed the improved version of Boltzmann machine called the <strong>Restricted Boltzmann Machine</strong> (<strong>RBM</strong>).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>However, in the 90s era, the most significant year was 1997, when Lecun <em>et al.</em> proposed LeNet in 1990, and Jordan <em>et al.</em> proposed the Recurrent Neural Network in 1997. In the same year, Schuster <em>et al.</em> proposed the improved version of LSTM and the improved version of the original RNN, called the bidirectional RNN. <span class="fontstyle2">The following timeline provides a brief glimpse into the history of different neural network architectures:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-715 image-border" src="assets/7c03a0a3-f5ec-46e1-8a0a-7930fcfcb77c.png" style="width:162.50em;height:46.17em;"/></p>
<p>Despite significant advances in computing, from 1997 to 2005, we didn't experience much advancement until Hinton struck again in 2006 when he and his team proposed the <strong>Deep Belief Network</strong> (<strong>DBN</strong>) by stacking multiple RBMs. Then, in 2012, Hinton invented dropout, which significantly improves regularization and overfitting in deep neural networks.</p>
<p>After that, Ian Goodfellow <em>et al.</em> introduced GANs, which was a significant milestone in image recognition. In 2017, Hinton proposed CapsNet to overcome the limitation of regular CNNs, which was one of the most significant milestones so far.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How does an ANN learn?</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">Based on the concept of biological neurons, the term and idea of ANNs arose. Similar to biological neurons, the artificial neuron consists of the following:<br/></span></p>
<ul>
<li><span class="fontstyle2">One or more incoming connections that aggregate signals from neurons</span></li>
<li><span class="fontstyle2">One or more output connections for carrying the signal to the other neurons</span></li>
<li><span class="fontstyle2">An</span> <span class="fontstyle3">activation function</span><span class="fontstyle2">, which determines the numerical value of the output signal</span></li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span class="fontstyle2">Besides the state of a neuron, synaptic weight is considered, which influences the connection within the network. Each weight has a numerical value indicated by <em>W<sub>ij</sub></em></span><span class="fontstyle2">, which is the synaptic weight connecting neuron</span> <em><span class="fontstyle3">i</span></em> <span class="fontstyle2">to neuron <em>j</em></span><span class="fontstyle2">.</span> <span class="fontstyle0">No</span><span class="fontstyle0">w, for each neuron</span> <em><span class="fontstyle2">i</span></em><span class="fontstyle0">, an input vector can be defined</span> <span class="fontstyle0">by <em>x<sub>i</sub> = (x<sub>1</sub>, x<sub>2</sub>,…x<sub>n</sub>)</em></span> <span class="fontstyle0">and a weight vector can be defined by <em>w<sub>i</sub> = (w<sub>i1</sub>, w<sub>i2</sub>,…w<sub>in</sub>)</em></span><span class="fontstyle0">. Now, depending on the position of a neuron, the weights and the output function determine the behavior of an individual neuron. Then, during forward propagation, each unit in the hidden layer gets the following signal:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="fontstyle0"><img class="fm-editor-equation" src="assets/92d1e4fa-5382-4d0e-badf-3dd0b12929e5.png" style="width:15.42em;height:3.00em;"/><br/></span></p>
<p class="CDPAlignLeft CDPAlign"><span class="fontstyle0">Nevertheless, among the weights, there is also a special type of weight called a</span> <span class="fontstyle2">bias</span> <span class="fontstyle0">unit,</span> <em>b</em><span class="fontstyle2">.</span> <span class="fontstyle0">Technically, bias units aren't connected to any previous layer, so they don't have true activity. But still, the bias</span> <em><span class="fontstyle2">b</span></em> <span class="fontstyle0">value allows the neural network to shift the activation function to the left or right. By taking the bias unit into consideration, the modified network output is formulated as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1b946acf-c821-45ca-bada-d40d862b86ba.png" style="width:15.33em;height:2.58em;"/></p>
<p><span class="fontstyle0">The preceding equation signifies that each hidden unit gets the sum of inputs, multiplied by the corresponding weight—<span>this is known as the </span></span><span class="fontstyle0"><strong>Summing junction</strong>. Then, the resultant output in the <strong>Summing junction</strong> is passed through the activation function, which squashes the output, as depicted in the following diagram:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-716 image-border" src="assets/10255eed-9719-413b-abc4-ce71fb87c327.png" style="width:34.17em;height:15.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="fontstyle2"><span class="fontstyle2">Working principal of an artificial neuron model</span></span></div>
<p>A practical neural network architecture, however, is composed of input, hidden, and output layers that are composed of <em>nodes</em> that make up a network structure, but still follow the working principal of an artificial neuron model, as shown in the preceding diagram. The input layer only accepts numeric data, such as features in real numbers, images with pixel values, and so on:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-717 image-border" src="assets/ab4cea48-30f0-49c0-a88d-e9f40e6f7fd8.png" style="width:40.67em;height:23.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">A neural network with one input layer, three hidden layers, and an output layer</div>
<p>Here, the hidden layers perform most of the computation to learn the patterns and the network evaluates how accurate its prediction is compared to the actual output using a special mathematical function called the loss function. It could be a complex one or a very simple mean squared error, which can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e4eeafcd-746d-48a5-8a91-7e3c3016ba17.png" style="width:13.50em;height:3.42em;"/></p>
<p>In the preceding equation, <img class="fm-editor-equation" src="assets/2fe01c92-f29a-441d-b7b0-094ae8aaedc5.png" style="width:1.08em;height:1.42em;"/> signifies the prediction made by the network, while <em>Y</em> represents the actual or expected output. Finally, when the error is no longer being reduced, the neural network converges and makes prediction through the output layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">The learning process for a neural network is configured as an</span> <span class="fontstyle4">iterative process</span> <span class="fontstyle2">of the</span> <span class="fontstyle4">optimization</span> <span class="fontstyle2">of the</span> <span class="fontstyle4">weights</span><span class="fontstyle2">. The weights are updated in each epoch. Once the training starts, the aim is to generate predictions by minimizing the loss function. The performance of the network is then evaluated on the test set.</span> <span class="fontstyle0">We already know about the simple concept of an artificial neuron. However, generating only some artificial signals is not enough to learn a complex task. As such, a commonly used supervised learning algorithm is the backpropagation algorithm, which is very commonly used to train a complex ANN.</span></p>
<p>Ultimately, training such a neural network is an optimization problem too, in which we try to minimize the error by adjusting network weights and biases iteratively, by using backpropagation through <strong>gradient descent</strong> (<strong>GD</strong>). This approach forces the network to backtrack through all its layers to update the weights and biases across nodes in the opposite direction of the loss function.</p>
<p><span class="fontstyle0">However, this process using GD does not guarantee that the global minimum is reached. The presence of hidden units and the nonlinearity of the output function means that the behavior of the error is very complex and has many local minimas.</span> <span class="fontstyle0">This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that minimize the cost function. The training process ends when the error on the validation set begins to increase, because this could mark the beginning of a phase overfitting.</span></p>
<p><span class="fontstyle0">The downside of using GD is that it takes too long to converge, which makes it impossible to meet the demand of handling large-scale training data. Therefore, a faster GD, called</span> <strong>Stochastic Gradient Descent</strong> <span class="fontstyle0">(</span><strong><span class="fontstyle2">SDG</span></strong><span class="fontstyle0">) was proposed, which is also a widely used optimizer in DNN training. In SGD, we use only one training sample per iteration from the training set to update the network parameters, which is a stochastic approximation of the true cost gradient.</span></p>
<div class="packt_infobox"><span class="fontstyle0">There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad, Momentum, and so on. Each of them is either an direct or indirect optimized version of SGD.</span> <span class="fontstyle0"><br/></span></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight and bias initialization</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">Now, here's a tricky question: how do we initialize the weights? Well, if we initialize all the weights to the same value (for example, 0 or 1), each hidden neuron will get exactly the same signal. Let's try to break it down:</span></p>
<ul>
<li><span class="fontstyle0">If all weights are initialized to 1, then each unit gets a signal equal to the sum of the inputs</span></li>
<li><span class="fontstyle0">If all weights are 0, which is even worse, then every neuron in a hidden layer will get zero signal<br/></span></li>
</ul>
<p><span class="fontstyle0">For network weight initialization, Xavier initialization is used widely. It is similar to random initialization but often turns out to work much better, since it can identify the rate of initialization depending on the total number of input and output neurons by default.</span></p>
<p><span class="fontstyle0">You may be wondering whether you can get rid of random initialization while training a regular DNN. Well, recently, some researchers have been talking about random orthogonal matrix initialization's that perform better than just any random initialization for training DNNs. When it comes to initializing the biases, we can initialize them to be zero.</span></p>
<p><span class="fontstyle0">But setting the biases to a small constant value, such as 0.01 for all biases, ensures that all</span> <strong><span class="fontstyle2">rectified linear units</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle2">ReLUs</span></strong><span class="fontstyle0">) can propagate some gradient. However, it neither performs well nor shows consistent improvement. Therefore, sticking with zero is recommended.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">To allow a neural network to learn complex decision boundaries, we apply a non-linear activation function to some of its layers. Commonly used functions include Tanh, ReLU, softmax, and variants of these. More technically, each neuron receives a signal of the weighted sum of the synaptic weights and the activation values of the neurons that are connected as input.</span> <span class="fontstyle2">One of the most widely used functions for this purpose is the so-called</span> <span class="fontstyle3">sigmoid logistic function, which is</span> <span class="fontstyle2">defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="fontstyle2"><img class="fm-editor-equation" src="assets/485cc8f4-cfc3-42a1-8d14-b6f5d143320d.png" style="width:8.92em;height:2.83em;"/></span></p>
<p class="CDPAlignLeft CDPAlign"><span class="fontstyle2">The domain of this function includes all real numbers, and the co-domain is (</span><span class="fontstyle4">0, 1</span><span class="fontstyle2">). This means that any value obtained as an output from a neuron (as per the calculation of its activation state) will always be between zero and one. The <strong>Sigmoid</strong> function, as represented in the following diagram, provides an interpretation of the saturation rate of a neuron, from not being active (</span><span class="fontstyle4">equal to <strong>0</strong></span><span class="fontstyle2">) to complete saturation, which occurs at a predetermined maximum value (</span><span class="fontstyle4">equal to <strong>1</strong></span><span class="fontstyle2">):</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="fontstyle0"><img class="alignnone size-full wp-image-602 image-border" src="assets/71b9595a-1e70-44ca-bb40-9c67cbb70d24.png" style="width:22.67em;height:15.67em;"/></span></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="fontstyle2">Sigmoid versus Tanh activation function</span></div>
<p><span class="fontstyle2">On the other hand, a hyperbolic tangent, or</span> <strong><span class="fontstyle3">Tanh</span></strong><span class="fontstyle2">, is another form of activation function. <strong>Tanh</strong> <span><span class="SDZsVb">flatten</span></span>s a real-valued number between <strong>-1</strong> and <strong>1</strong></span><span class="fontstyle2">. <span class="fontstyle0">The preceding graph shows the difference between <strong>Tanh</strong> and <strong>Sigmoid</strong> activation functions.</span> In particular, mathematically, <em>tanh</em> activation function can be expressed as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a68918c2-518e-4731-8602-c27cdd1d5beb.png" style="width:9.75em;height:1.25em;"/></p>
<p><span class="fontstyle0">In general, in the last level of an</span> <strong><span class="fontstyle3">feedforward neural network</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle3">FFNN</span></strong><span class="fontstyle0">), the softmax function is applied as the decision boundary. This is a common case, especially when solving a classification problem. The softmax function used for the probability distribution over</span> <span class="fontstyle4">the</span> <span class="fontstyle0">possible classes in a multiclass classification problem. </span></p>
<div class="packt_tip"><span class="fontstyle0">For a regression problem, we do not need to use any activation function since the network generates continuous values—that is, </span><span class="fontstyle0">probabilities. However, I've seen people using the IDENTITY activation function for regression problems nowadays.<br/></span></div>
<p><span class="fontstyle0">To conclude, choosing proper activation functions and network weight initializations are two problems that make a network perform at its best and help to obtain good training. Now that we know the brief history of neural networks, let's deep-dive into different architectures in the next section, which will give us an idea on their usage.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network architectures</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">We can categorize DL architectures into four groups:</span></p>
<ul>
<li><strong><span class="fontstyle3">Deep neural networks</span></strong> <span class="fontstyle2">(</span><strong><span class="fontstyle3">DNNs</span></strong><span class="fontstyle2">)</span></li>
<li><strong><span class="fontstyle3">Convolutional neural networks</span></strong> <span class="fontstyle2">(</span><strong><span class="fontstyle3">CNNs</span></strong><span class="fontstyle2">)</span></li>
<li><strong><span class="fontstyle3">Recurrent neural networks</span></strong> <span class="fontstyle2">(</span><strong><span class="fontstyle3">RNNs</span></strong><span class="fontstyle2">)</span></li>
<li><strong><span class="fontstyle3">Emergent architectures</span></strong> <span class="fontstyle2">(</span><strong><span class="fontstyle3">EAs</span></strong><span class="fontstyle2">)</span></li>
</ul>
<p><span class="fontstyle2">However, DNNs, CNNs, and RNNs have many improved variants.</span> <span class="fontstyle2">Although most of the variants are proposed or developed for solving domain-specific research problems, the basic working principles still follow the original DNN, CNN, and RNN architectures.</span> <span class="fontstyle2">T</span><span class="fontstyle2">he following subsections will give you a brief introduction to these architectures.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNNs</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">DNNs are neural networks that have a complex and deeper architecture with a large number of neurons in each layer, and many connections between them. Although DNN refers to a very deep network, for simplicity, we consider MLP,</span> <span class="fontstyle3"><strong>stacked autoencoder</strong></span> <span class="fontstyle2">(</span><strong><span class="fontstyle3">SAE</span></strong><span class="fontstyle2">), and</span> <span class="fontstyle3"><strong>deep belief networks</strong></span> <span class="fontstyle2">(</span><strong><span class="fontstyle3">DBNs</span></strong><span class="fontstyle2">) as DNN architectures. These architectures mostly work as an FFNN, meaning information propagates from input to output layers.<br/></span></p>
<p class="mce-root"><span class="fontstyle2">Multiple perceptrons are stacked together as MLPs, where layers are connected as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since it has three layers: an input layer, a hidden layer, and an output layer. This way, the signal propagates one way, from the input layer to the hidden layers to the output layer, as shown in the following diagram<span class="fontstyle3">:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="fontstyle2"><img class="alignnone size-full wp-image-720 image-border" src="assets/a34e4760-33b6-4a15-9cb4-cb5cd82e407d.png" style="width:28.42em;height:16.58em;"/><br/></span></p>
<p><span class="fontstyle2">Autoencoders and <span class="fontstyle3">RBMs</span> are the basic building blocks for SAEs and DBNs, respectively</span><span class="fontstyle2">. Unlike MLP, which is an FFNN that's trained in a supervised way, both SAEs and DBNs are trained in two phases: unsupervised pre-training and supervised fine-tuning.</span> <span class="fontstyle0">In unsupervised pre-training, layers are stacked in order and trained in a layer-wise manner with used, unlabeled data. In supervised fine-tuning, an output classifier layer is stacked and the complete neural network is optimized by retraining with labeled data.</span></p>
<p><span class="fontstyle2">One problem with MLP is that it often overfits the data, so it doesn't generalize well. To overcome this issue, DBN was proposed by Hinton <em>et al.</em> It uses a greedy, layer-by-layer, pre-training algorithm.</span> <span class="fontstyle0">DBNs are composed of a visible layer and multiple hidden unit layers</span><span class="fontstyle0">.</span> <span class="fontstyle0">The building blocks of a DBN are RBMs, as shown in the following diagram, where several RBMs are</span> <span class="fontstyle4">stacked</span> <span class="fontstyle0">one after another: </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-604 image-border" src="assets/d3e89b54-3a46-4dd1-b198-34325301e352.png" style="width:31.00em;height:19.42em;"/></p>
<p><span class="fontstyle0">The top two layers have undirected, symmetric connections in between, but the lower layers have directed connections from the preceding layer.</span> <span class="fontstyle0">Despite numerous successes, DBNs are now being replaced with AEs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle1">AEs are also special types of neural networks <span class="markup--quote markup--p-quote is-other">that learn automatically from the input data</span>. AEs consists of two components: the encoder and the decoder. The encoder compresses the input into a</span> latent-space representation. Th<span class="fontstyle1">en, the decoder part tries to reconstruct the original input data from this representation:</span></p>
<ul class="postList">
<li class="graf graf--li graf-after--p"><strong>Encoder</strong>: Encodes or compresses the input into a latent-space representation using a function known as <em>h=f(x)</em></li>
<li class="graf graf--li graf-after--li"><strong>Decoder</strong>: Decodes or reconstructs the input from the latent space representation using a function known as <em>r=g(h)</em></li>
</ul>
<p><span class="fontstyle1">So, an AE can be described by a function of <em>g(f(x)) = o</em>, where we want <em>0</em> as close as the original input of</span> <em>x</em><span class="fontstyle1">. The following diagram shows how an AE typically works:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-722 image-border" src="assets/6a6851b9-4411-461d-b66e-7e92955c723c.png" style="width:30.83em;height:9.25em;"/></p>
<p><span class="fontstyle1">AEs are very useful at data denoising and dimensionality reduction for data visualization. AEs can learn data projections, called representations, more effectively than PCA.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">CNNs have achieved much and have a wide adoption in computer vision (for example, image recognition). In CNN networks, the connections schemes are significantly different compared to an MLP or DBN. </span>A few of the convolutional layers are connected in a cascade style. Each layer is backed up by a ReLU layer, a pooling layer, and additional convolutional layers (+ReLU), and another pooling layer<span class="fontstyle2">, which is followed by a fully connected layer and a softmax layer. <span class="fontstyle0">The following diagram is a schematic of the architecture of a CNN that's used for facial recognition, which takes facial images as input and predicts emotions such as anger, disgust, fear, happy, sad and so on.<br/></span></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-723 image-border" src="assets/27a311c6-d8b2-467b-aacd-a482d6de6746.png" style="width:46.58em;height:14.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="fontstyle2">A schematic architecture of a CNN used for facial recognition</span></div>
<p><span class="fontstyle0"><span class="fontstyle2">Importantly, DNNs have no prior knowledge of how the pixels are organized because they do not know that nearby pixels are close. CNNs embed this prior knowledge using lower layers by using feature maps in small areas of the image, while the higher layers combine lower-level features into larger features.</span></span></p>
<p><span class="fontstyle0"><span class="fontstyle2">This works well with most of the natural images, giving CNNs a decisive head start over DNNs. </span>The output from each <span>convolutional</span> layer is a set of objects, called feature maps, that are generated by a single kernel filter. Then, the feature maps can be used to define a new input to the next layer. Each neuron in a CNN network produces an output, followed by an activation threshold, which is proportional to the input and not bound.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RNNs</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">In</span> <span class="fontstyle3">RNNs,</span> <span class="fontstyle2">connections between units form a directed cycle. The RNN architecture was originally conceived by Hochreiter and Schmidhuber in 1997. RNN architectures have standard MLPs, plus added loops so that they can exploit the powerful nonlinear mapping capabilities of the MLP. They also have some form of memory. <span class="fontstyle0">The following diagram shows a very basic RNN that has an input layer, two recurrent layers, and an output layer: </span><br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-725 image-border" src="assets/a79f8197-278c-46ab-9f04-735a6f4720a0.png" style="width:25.08em;height:11.00em;"/></p>
<p><span class="fontstyle0">However, this basic RNN suffers from gradient vanishing and the exploding problem, and cannot model long-term dependencies. These architectures include</span> <span class="fontstyle2">LSTM</span><span class="fontstyle0">,</span> <strong><span class="fontstyle2">gated recurrent units</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle2">GRUs</span></strong><span class="fontstyle0">),</span> <span class="fontstyle2">bidirectional-LSTM,</span> <span class="fontstyle0">and other variants. Consequently, LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding problem and long-short term dependency.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks (GANs)</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">Ian Goodfellow <em>et al.</em> introduced GANs in a paper named <em>Generative Adversarial Nets</em> (see more at</span> <a href="https://arxiv.org/pdf/1406.2661v1.pdf"><span class="fontstyle3">https:/</span><span class="fontstyle4">​</span><span class="fontstyle3">/</span><span class="fontstyle4">​</span><span class="fontstyle3">arxiv.</span><span class="fontstyle4">​</span><span class="fontstyle3">org/</span><span class="fontstyle3">abs/</span><span class="fontstyle4">​</span><span class="fontstyle3">1406.</span><span class="fontstyle4">​</span><span class="fontstyle3">2661v1</span></a><span class="fontstyle2">)</span><span class="fontstyle5">.</span> <span class="fontstyle6">The following diagram briefly shows the working principles of a GAN:<br/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-726 image-border" src="assets/704ace27-a78b-4d83-9a16-a160d9c4230c.png" style="width:43.17em;height:19.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="fontstyle6">Working principles of GAN</span></div>
<p><span class="fontstyle2">GANs are deep neural network architectures that consist of two networks, a generator, and a discriminator, that are pitted against each other (hence the name, <em>adversarial</em>):</span></p>
<ul>
<li><span class="fontstyle2">The <strong>Generator</strong> tries to generate data samples out of a specific probability distribution and is very similar to the actual object</span></li>
<li><span class="fontstyle2">The <strong>Discriminator</strong> will judge whether its input is coming from the original training set or from the generator part</span></li>
</ul>
<p><span class="fontstyle2">Many DL practitioners think that GANs were one of the most important advancements because GANs can be used to mimic any distribution of data, and based on the data distribution, GANs can be taught to create robot artist images, s<span>uper-resolution images, text-to-image synthesis,</span> music, speech, and more.</span></p>
<p class="mce-root"/>
<p><span class="fontstyle2">For example, because of the concept of adversarial training, Facebook's AI research director, Yann LeCun, called GANs the most interesting idea in the last 10 years of ML.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capsule networks</h1>
                </header>
            
            <article>
                
<p>In CNNs, each layer understands an image at a much more granular level through a slow receptive field or max pooling operations. If the images have rotation, tilt, or very different shapes or orientation, CNNs fail to extract such spatial information and show very poor performance at image processing tasks. Even the pooling operations in CNNs cannot much help against such positional invariance. <span class="fontstyle2">This issue in CNNs has led us to the recent advancement of CapsNet through the paper titled</span> <em><span class="fontstyle7">Dynamic Routing Between Capsules</span></em> <span class="fontstyle2">(see more at</span> <a href="https://arxiv.org/pdf/1710.09829.pdf"><span class="fontstyle3">https:/</span><span class="fontstyle4">​</span><span class="fontstyle3">/</span><span class="fontstyle4">​</span><span class="fontstyle3">arxiv.</span><span class="fontstyle4">​</span><span class="fontstyle3">org/</span><span class="fontstyle4">​</span><span class="fontstyle3">abs/</span><span class="fontstyle4">​</span><span class="fontstyle3">1710.</span><span class="fontstyle4">​</span><span class="fontstyle3">09829</span></a><span class="fontstyle2">) by Geoffrey Hinton <em>et al</em>:</span></p>
<div class="packt_quote">"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity, such as an object or an object part."</div>
<p><span class="fontstyle0">Unlike a regular DNN, where we keep on adding layers, in CapsNets, the idea is to add more layers inside a single layer. This way, a CapsNet is a nested set of neural layers.</span> In CapsNet, the vector inputs and outputs of a capsule are computed using the routing algorithm, which iteratively transfers information and process <strong>self-consistent field</strong> (<strong>SCF</strong>) procedure, used in physics:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-728 image-border" src="assets/ba38228e-2977-4355-8110-f87b8e1970b2.png" style="width:46.67em;height:13.17em;"/></p>
<p>The preceding diagram shows a schematic diagram of a simple three-layer CapsNet. The length of the activity vector of each capsule in the <strong>DigiCaps</strong> layer indicates the presence of an instance of each class, which is used to calculate the loss.</p>
<p>Now that we have learned about the working principles of neural networks and the different neural network architectures, implementing something hands-on would be great. However, before that, let's take a look at some popular DL libraries and frameworks, which come with the implementation of these network architectures.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL frameworks</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">There are several popular DL frameworks. Each of them comes with some pros and cons. Some of them are desktop-based and some of them are cloud-based platforms where you can deploy/run your DL applications. However, most of the libraries that are released under an open license help when people are using graphics processors, which can ultimately help in speeding up the learning process. </span></p>
<p>Such frameworks and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j, H2O, and t<span class="fontstyle2">he</span> <strong><span class="fontstyle0">Microsoft Cognitive Toolkit</span></strong> <span class="fontstyle2">(</span><strong><span class="fontstyle0">CNTK</span></strong><span class="fontstyle2">)</span>. Even a few years back, other implementations including Theano, Caffee, and Neon were used widely. However, these are now obsolete. <span class="fontstyle2">Since we will focus on learning in Scala, JVM-based DL libraries such as Deeplearning4j can be a reasonable choice. <strong>Deeplearning4j</strong> (<strong>DL4J</strong>) is one of the first commercial-grade, open source, distributed DL libraries that was built for Java and Scala. This also provides integrated support for Hadoop and Spark. DL4J is built for use in business environments on distributed GPUs and CPUs. DL4J aims to be cutting-edge and Plug and Play, with more convention than c</span><span class="fontstyle2">onfiguration, which allows for fast prototyping for non-researchers.</span> The following diagram shows last year's Google Trends, illustrating how popular TensorFlow is:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-729 image-border" src="assets/f27e1f6d-cc5a-4c23-bde9-1f0576077c29.png" style="width:162.50em;height:37.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Trends of different DL frameworks—TensorFlow and Keras are dominating the most; however, Theano is losing its popularity; on the other hand, Deeplearning4j is emerging for JVM</div>
<p><span class="fontstyle2">Its numerous libraries can be integrated with DL4J and will make your JVM experience easier, regardless of whether you are developing your ML application in Java or Scala. Similar to NumPy for JVM, ND4J comes up with basic operations of linear algebra (matrix creation, addition, and multiplication). However, ND4S is a scientific computing library for linear algebra and matrix manipulation. It also provides n-dimensional arrays for JVM-based languages.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Apart from the preceding libraries, there are some recent initiatives for DL on the cloud. The idea is to bring DL capability to big data with millions of billions of data points and high dimensional data. For example, <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Microsoft Azure, Google Cloud Platform, and <strong>NVIDIA GPU Cloud</strong> (<strong>NGC</strong>) all offer machine and DL services that are native to their public clouds.</p>
<p>In October 2017, AWS released <strong>Deep Learning AMIs</strong> (<strong>DLAMIs</strong>) for <strong>Amazon Elastic Compute Cloud</strong> (<strong>Amazon EC2</strong>) P3 instances. These AMIs come pre-installed with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with Source Code.</p>
<p>The CNTK is Azure's open source, DL service. Similar to AWS' offering, it focuses on tools that can help developers build and deploy DL applications. The toolkit is installed in Python 2.7, in the root environment. Azure also provides a model gallery that includes resources, such as code samples, to help enterprises get started with the service.</p>
<p>On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated containers (see <a href="https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/">https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/</a>). The NGC features containerized deep learning frameworks such as TensorFlow, PyTorch, MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the latest NVIDIA GPUs on participating cloud-service providers. Nevertheless, there are also third-party services available through their respective marketplaces.</p>
<p>Now that you know the working principles of neural network architectures and have seen a brief overview on available DL frameworks for implementing DL solutions, let's move on to the next section for some hands-on learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with learning</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle0">Large-scale cancer genomics data often comes in multi-platform and heterogeneous forms. These datasets impose great challenges in terms of the bioinformatics approach and computational algorithms. Numerous researchers have proposed to utilize this data to overcome several challenges, using classical ML algorithms as either the primary subject or a supporting element for cancer diagnosis and prognosis. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Description of the dataset</h1>
                </header>
            
            <article>
                
<p><span class="fontstyle2">Genomics data covers all data related to DNA on living things. Although in this thesis we will also use other types of data, as such as transcriptomic data (RNA and miRNA), for convenience purposes, all data will be termed as genomics data. Research on human genetics made a huge breakthrough in recent years due to the success of the <strong>Human Genome Project</strong> (<strong>HGP</strong>) (1984-2000) on sequencing the full sequence of human DNA. <span class="fontstyle0">Now, let's see what a real-life dataset looks like that can be used for our purposes. We will be using the <em>gene expression cancer RNA-Seq</em> dataset, which can be downloaded from the UCI ML repository (see</span> <a href="https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq" target="_blank">https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq</a> <span class="fontstyle0">for more information).</span> <br/></span></p>
<p><span class="fontstyle0">This dataset is a random subset of another dataset that was reported in the following paper: Weinstein, John N., <em>et al</em>.</span> <em><span class="fontstyle2">The cancer genome atlas pan-cancer analysis project</span><span class="fontstyle0">.</span></em> <span class="fontstyle2">Nature Genetics 45.10 (2013): 1113-1120</span><span class="fontstyle0">.</span> <span class="fontstyle0">The name of the project is The Pan-Cancer analysis project. It assembled data from thousands of patients with primary tumors occurring in different sites of the body. It covered 12 tumor types, including the following:</span></p>
<ul>
<li><strong><span class="fontstyle3">Glioblastoma multiforme</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle3">GBM</span></strong><span class="fontstyle0">)</span></li>
<li><strong><span class="fontstyle3">Lymphoblastic acute myeloid leukemia</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle3">AML</span></strong><span class="fontstyle0">)</span></li>
<li><strong><span class="fontstyle3">Head and neck squamous carcinoma</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle3">HNSC</span></strong><span class="fontstyle0">)</span></li>
<li><strong><span class="fontstyle3">Lung adenocarcinoma</span></strong> <span class="fontstyle0">(</span><strong><span class="fontstyle3">LUAD</span></strong><span class="fontstyle0">)</span></li>
<li><strong>L</strong><span class="fontstyle0"><strong>ung squamous carcinoma</strong> (<strong>LUSC</strong>)</span></li>
<li><span class="fontstyle0"><strong>Breast carcinoma</strong> (<strong>BRCA</strong>)</span></li>
<li><span class="fontstyle0"><strong>Kidney renal clear cell carcinoma</strong> (<strong>KIRC</strong>)</span></li>
<li><span class="fontstyle0"><strong>Ovarian carcinoma</strong> (<strong>OV</strong>)</span></li>
<li><span class="fontstyle0"><strong>Bladder carcinoma</strong> (<strong>BLCA</strong>)</span></li>
<li><span class="fontstyle0"><strong>Colon adenocarcinoma</strong> (<strong>COAD</strong>)</span></li>
<li><span class="fontstyle0"><strong>Uterine cervical and endometrial carcinoma</strong> (<strong>UCEC</strong>)</span></li>
<li><span class="fontstyle0"><strong>Rectal adenocarcinoma</strong> (<strong>READ</strong>)<br/></span></li>
</ul>
<p><span class="fontstyle0">This collection of data is a part of the RNA-Seq (HiSeq) PANCAN dataset. It is a random extraction of gene expressions of patients that have different types of tumors: BRCA, KIRC, COAD, LUAD, and PRAD.</span></p>
<p class="mce-root"/>
<p><span class="fontstyle0">This dataset is a random collection of cancer patients from 801 patients, each having 20,531 attributes. Samples (<kbd>instances</kbd>) are stored row-wise. Variables (<kbd>attributes</kbd>) of each sample are RNA-Seq gene expression levels measured by the Illumina HiSeq platform. A dummy name (<kbd>gene_XX</kbd>) is provided for each attribute. The attributes are ordered consistently with the original submission. For example, <kbd>gene_1</kbd> on <kbd>sample_0</kbd> is significantly and differentially expressed with a value of <kbd>2.01720929003</kbd>.<br/></span></p>
<p><span class="fontstyle0">When you download the dataset, you will see that there are two CSV files:</span></p>
<ul>
<li><span class="fontstyle0"><kbd>data.csv</kbd>: Contains the gene expression data of each sample</span></li>
<li><span class="fontstyle0"><kbd>labels.csv</kbd>: The labels associated with each sample<br/></span></li>
</ul>
<p><span class="fontstyle0">Let's take a look at the processed dataset. Note that we will only look at a few select features considering the high dimensionality in the following screenshot, where the first column represents sample IDs (that is, anonymous patient IDs). The rest of the columns represent how a certain gene expression occurs in the tumor samples of the patients:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-611 image-border" src="assets/5a15ea25-0ace-4c47-9600-0e95ac5624dd.png" style="width:37.25em;height:21.75em;"/></p>
<p>Now, look at the labels in the following table. Here, the <kbd>id</kbd> column contains the sample IDs and the <kbd>Class</kbd> column represents the cancer labels:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-730 image-border" src="assets/0001e482-8aeb-42d1-9307-0c8eba945c9d.png" style="width:10.17em;height:11.50em;"/></p>
<p>Now, you can imagine why I have chosen this dataset. Although we will not have many samples, the dataset is still highly dimensional. In addition, this type of high dimensional dataset is very suitable for applying a DL algorithm. Therefore, if the features and labels are given, can we classify these samples based on features and the ground truth? Why not? We will try to solve this problem with the DL4J library. First, we have to configure our programming environment so that we can write our code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the programming environment</h1>
                </header>
            
            <article>
                
<p>In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J before getting started with the coding. The following are the prerequisites that you must take into account when working with DL4J:</p>
<ul>
<li>Java 1.8+ (64-bit only)</li>
<li>Apache Maven for an automated build and dependency manager</li>
<li>IntelliJ IDEA or Eclipse IDE</li>
<li>Git for version control and CI/CD</li>
</ul>
<p>The following libraries can be integrated with DJ4J to enhance your JVM experience while developing your ML applications:</p>
<ul>
<li><strong>DL4J</strong>: The core neural network framework, which comes with many DL architectures and underlying functionalities.</li>
<li><strong>ND4J</strong>: Can be considered as the NumPy of JVM. It comes with some basic operations of linear algebra. Examples are matrix creation, addition, and multiplication.</li>
<li><strong>DataVec</strong>: This library enables ETL operations while performing feature engineering.</li>
<li><strong>JavaCPP</strong>: This library acts as the bridge between Java and Native C++.</li>
<li><strong>Arbiter</strong>: This library provides basic evaluation functionalities for the DL algorithms.</li>
<li><strong>RL4J</strong>: Deep reinforcement learning for the JVM.</li>
<li><strong>ND4S</strong>: This is a scientific computing library, and it also supports n-dimensional arrays for JVM-based languages.</li>
</ul>
<p>If you are using Maven on your preferred IDE, let's define the project properties to mention these versions in the <kbd>pom.xml</kbd> file:</p>
<pre><strong>&lt;properties&gt;</strong><br/>     &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br/>     &lt;jdk.version&gt;1.8&lt;/jdk.version&gt;<br/>     &lt;spark.version&gt;2.2.0&lt;/spark.version&gt;<br/>     &lt;nd4j.version&gt;1.0.0-alpha&lt;/nd4j.version&gt;<br/>     &lt;dl4j.version&gt;1.0.0-alpha&lt;/dl4j.version&gt;<br/>     &lt;datavec.version&gt;1.0.0-alpha&lt;/datavec.version&gt;<br/>     &lt;arbiter.version&gt;1.0.0-alpha&lt;/arbiter.version&gt;<br/>     &lt;logback.version&gt;1.2.3&lt;/logback.version&gt;<br/><strong>&lt;/properties&gt;</strong></pre>
<p>Then, use all the dependencies required for DL4J, ND4S, and ND4J, as shown in the <kbd>pom.xml</kbd> file. By the way, DL4J comes with Spark 2.1.0. Additionally, if a native system BLAS is not configured on your machine, ND4J's performance will be reduced. You will experience the following warning once you execute any simple code written in Scala:</p>
<pre><strong>****************************************************************</strong><br/><strong> WARNING: COULD NOT LOAD NATIVE SYSTEM BLAS</strong><br/><strong> ND4J performance WILL be reduced</strong><br/><strong> ****************************************************************</strong></pre>
<p>However, installing and configuring a BLAS, such as OpenBLAS or IntelMKL, is not that difficult; you can invest some time and do it. Refer to the following URL for further details:</p>
<p><a href="http://nd4j.org/getstarted.html#open" target="_blank">http://nd4j.org/getstarted.html#open</a></p>
<p>Well done! Our programming environment is ready for simple DL application development. Now, it's time to get our hands dirty with some sample code.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing</h1>
                </header>
            
            <article>
                
<p>Since we do not have any unlabeled data, I would like to select some samples randomly for testing. One more thing to note is that features and labels come in two separate files. Therefore, we can perform the necessary preprocessing and then join them together so that our preprocessed data will have features and labels together.</p>
<p>Then, the rest of the data will be used for training. Finally, we'll save the training and testing sets in a separate CSV file to be used later on. Follow these steps to get started:</p>
<ol>
<li>First, let's load the samples and see the statistics. Here, we use the <kbd>read()</kbd> method of Spark, but specify the necessary options and format too:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> data = spark.read.option("maxColumns", 25000).format("com.databricks.spark.csv")<br/>      .option("header", "true") // Use first line of all files as header<br/>      .option("inferSchema", "true") // Automatically infer data types<br/>      .load("TCGA-PANCAN/TCGA-PANCAN-HiSeq-801x20531/data.csv");// set this path accordingly</pre>
<ol start="2">
<li>Then, we will see some related statistics, such as the number of features and the number of samples:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> numFeatures = data.columns.length<br/><strong>val</strong> numSamples = data.count()<br/>println("Number of features: " + numFeatures)<br/>println("Number of samples: " + numSamples)</pre>
<p style="padding-left: 60px">Therefore, there are <kbd>801</kbd> samples from <kbd>801</kbd> distinct patients and the dataset is too high in dimensions, since it has <kbd>20532</kbd> features:</p>
<pre style="padding-left: 60px"><strong>Number of features: 20532</strong><br/><strong>Number of samples: 801</strong></pre>
<ol start="3">
<li>In addition, since the <kbd>id</kbd> column represents only the patient's anonymous ID, so we can simply drop it:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> numericDF = data.drop("id") // now 20531 features left</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Then, we load the labels using the <kbd>read()</kbd> method of Spark and also specify the necessary options and format:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> labels = spark.read.format("com.databricks.spark.csv")<br/>      .option("header", "true") <br/>      .option("inferSchema", "true") <br/>      .load("TCGA-PANCAN/TCGA-PANCAN-HiSeq-801x20531/labels.csv") <br/>labels.show(10)</pre>
<p>We have already seen what the label DataFrame looks like. We will skip the <kbd>id</kbd>. However, the <kbd>Class</kbd> column is categorical. As we mentioned previously, DL4J does not support categorical labels that need to be predicted. Therefore, we have to convert it into a numeric format (an integer, to be more specific); for that, I would use <kbd>StringIndexer()</kbd> from Spark:</p>
<ol>
<li>First, we create a <kbd>StringIndexer()</kbd>, we apply the index operation to the <kbd>Class</kbd> column, and rename it <kbd>label</kbd>. Additionally, we <kbd>skip</kbd> null entries:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> indexer = new StringIndexer().setInputCol("Class")<br/>              .setOutputCol("label")<br/>              .setHandleInvalid("skip"); // skip null/invalid values    </pre>
<ol start="2">
<li>Then, we perform the indexing operation by calling the <kbd>fit()</kbd> and <kbd>transform()</kbd> operations, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> indexedDF = indexer.fit(labels).transform(labels)<br/>                       .select(col("label")<br/>                       .cast(DataTypes.IntegerType)); // casting data types to integer</pre>
<ol start="3">
<li>Now, let's take a look at the indexed DataFrame:</li>
</ol>
<pre style="padding-left: 60px">indexedDF.show()</pre>
<p style="padding-left: 60px" class="CDPAlignLeft CDPAlign">The preceding line of code should convert the <kbd>label</kbd> column in numeric format:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-613 image-border" src="assets/583202d7-f812-471d-a3af-03284a68f1b0.png" style="width:13.75em;height:18.75em;"/></p>
<ol start="4">
<li>Fantastic! Now, all the columns (including features and labels) are numeric. Thus, we can join both features and labels into a single DataFrame. For that, we can use the <kbd>join()</kbd> method from Spark, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> combinedDF = numericDF.join(indexedDF)</pre>
<ol start="5">
<li>Now, we can generate both the training and test sets by randomly splitting <kbd>combinedDF</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px"><strong>val</strong> splits = combinedDF.randomSplit(Array(0.7, 0.3), 12345L) //70% for training, 30% for testing<br/><strong>val</strong> trainingDF = splits(0)<br/><strong>val</strong> testDF = splits(1)</pre>
<ol start="6">
<li>Now, let's see the <kbd>count</kbd> of samples in each set:</li>
</ol>
<pre style="padding-left: 60px">println(trainingDF.count())// number of samples in training set<br/>println(testDF.count())// number of samples in test set</pre>
<ol start="7">
<li>There should be 561 samples in the training set and 240 samples in the test set. Finally, we save them in separate CSV files to be used later on:</li>
</ol>
<pre style="padding-left: 60px">trainingDF.coalesce(1).write<br/>      .format("com.databricks.spark.csv")<br/>      .option("header", "false")<br/>      .option("delimiter", ",")<br/>      .save("output/TCGA_train.csv")<br/><br/>testDF.coalesce(1).write<br/>      .format("com.databricks.spark.csv")<br/>      .option("header", "false")<br/>      .option("delimiter", ",")<br/>      .save("output/TCGA_test.csv")</pre>
<ol start="8">
<li>Now that we have the training and test sets, we can train the network with the training set and evaluate the model with the test set.</li>
</ol>
<div class="packt_infobox">Spark will generate CSV files under the <kbd>output</kbd> folder, under the project root. However, you might see a very different name. I suggest that you to rename them <kbd>TCGA_train.csv</kbd> and <kbd>TCGA_test.csv</kbd> for the training and test sets, respectively.</div>
<p>Considering the high dimensionality, I would rather try a better network such as LSTM, which is an improved variant of RNN. At this point, some contextual information about LSTM would be helpful to grasp this idea, and will be provided after the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset preparation</h1>
                </header>
            
            <article>
                
<p>In the previous section, we prepared the training and test sets. However, we need to put some extra effort into making them consumable by DL4J. To be more specific, DL4J expects the training data in numeric format and the last column to be the <kbd>label</kbd> column. The remaining data should be features.</p>
<p>We will now try to prepare our training and test sets like that. First, we will find the files where we saved the training and test sets:</p>
<pre>// Show data paths<br/><strong>val</strong> trainPath = "TCGA-PANCAN/TCGA_train.csv"<br/><strong>val</strong> testPath = "TCGA-PANCAN/TCGA_test.csv"</pre>
<p>Then, we will define the required parameters, such as the number of features, number of classes, and batch size. Here, I am using <kbd>128</kbd> as the <kbd>batchSize</kbd>, but you can adjust it accordingly:</p>
<pre>// Preparing training and test set.<br/><strong>val</strong> labelIndex = 20531<br/><strong>val</strong> numClasses = 5<br/><strong>val</strong> batchSize = 128</pre>
<p class="mce-root"/>
<p>This dataset is used for training:</p>
<pre><strong>val</strong> trainingDataIt: DataSetIterator = readCSVDataset(trainPath, batchSize, labelIndex, numClasses)</pre>
<p>This is the data we want to classify:</p>
<pre><strong>val</strong> testDataIt: DataSetIterator = readCSVDataset(testPath, batchSize, labelIndex, numClasses)</pre>
<p>As you can see from the preceding two lines of code, <kbd>readCSVDataset()</kbd> is basically a wrapper that reads the data in CSV format, and then the <kbd>RecordReaderDataSetIterator()</kbd> method converts the record reader into a dataset iterator.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM network construction</h1>
                </header>
            
            <article>
                
<p>Creating a neural network with DL4J starts with <kbd>MultiLayerConfiguration</kbd>, which organizes network layers and their hyperparameters. Then, the created layers are added using the <kbd>NeuralNetConfiguration.Builder()</kbd> interface. As shown in the following diagram, the LSTM network consists of five layers: an input layer, which is followed by three LSTM layers. The last layer is an RNN layer, which is also the output layer in this case:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-732 image-border" src="assets/6eb2b39a-02bf-43e6-bf32-2b55178d0b74.png" style="width:53.17em;height:14.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">An LSTM network for cancer type prediction, which takes 20,531 features and fixed bias (that is, 1) and generates multi-class outputs</div>
<p>To create LSTM layers, DL4J provides the implementation of an LSTM class. However, before we start creating layers for the network, let's define some hyperparameters, such as the number of input/hidden/output nodes (neurons):</p>
<pre>// Network hyperparameters<br/>val numInputs = labelIndex<br/>val numOutputs = numClasses<br/>val numHiddenNodes = 5000</pre>
<p>We then create the network by specifying layers. The first, second, and third layers are LSTM layers. The last layer is an RNN layer. For all the hidden LSTM layers, we specify the number of input and output units, and we use ReLU as the activation function. However, since it's a multiclass classification problem, we use <kbd>SOFTMAX</kbd> as the <kbd>activation</kbd> function for the output layer, with <kbd>MCXNET</kbd> as the loss function:</p>
<pre>//First LSTM layer<br/><strong>val</strong> layer_0 = new LSTM.Builder()<br/>      .nIn(numInputs)<br/>      .nOut(numHiddenNodes)<br/>      .activation(Activation.RELU)<br/>      .build()<br/><br/>//Second LSTM layer<br/><strong>val</strong> layer_1 = new LSTM.Builder()<br/>      .nIn(numHiddenNodes)<br/>      .nOut(numHiddenNodes)<br/>      .activation(Activation.RELU)<br/>      .build()<br/><br/>//Third LSTM layer<br/><strong>val</strong> layer_2 = new LSTM.Builder()<br/>      .nIn(numHiddenNodes)<br/>      .nOut(numHiddenNodes)<br/>      .activation(Activation.RELU)<br/>      .build()<br/><br/>//RNN output layer<br/><strong>val</strong> layer_3 = new RnnOutputLayer.Builder()<br/>      .activation(Activation.SOFTMAX)<br/>      .lossFunction(LossFunction.MCXENT)<br/>      .nIn(numHiddenNodes)<br/>      .nOut(numOutputs)<br/>      .build()</pre>
<p>In the preceding code block, the softmax <kbd>activation</kbd> function gives a probability distribution over classes, and <kbd>MCXENT</kbd> is the cross-entropy loss function in a multiclass classification setting. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">Then, with DL4J, we add the layers we created earlier using the <kbd>NeuralNetConfiguration.Builder()</kbd> interface. First, we add all the LSTM layers, which are followed by the final RNN output layer:</p>
<pre>//Create network configuration and conduct network training<br/><strong>val</strong> LSTMconf: MultiLayerConfiguration = new NeuralNetConfiguration.Builder()<br/>      .seed(seed) //Random number generator seed for improved repeatability. Optional.<br/>      .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)<br/>      .weightInit(WeightInit.XAVIER)<br/>      .updater(new Adam(5e-3))<br/>      .l2(1e-5)<br/>      .list()<br/>          .layer(0, layer_0)<br/>          .layer(1, layer_1)<br/>          .layer(2, layer_2)<br/>          .layer(3, layer_3)<br/>      .pretrain(false).backprop(true).build()</pre>
<p>In the preceding code block, we used SGD as the optimizer, which tries to optimize the <kbd>MCXNET</kbd> loss function. Then, we initialize the network weight using <kbd>XAVIER</kbd>, and <kbd>Adam</kbd> acts as the network updater with SGD. Finally, we initialize a multilayer network using the preceding multilayer configuration:</p>
<pre><strong>val</strong> model: MultiLayerNetwork = new MultiLayerNetwork(LSTMconf)<br/>model.init()</pre>
<p class="mce-root">Additionally, we can inspect the number of hyperparameters across layers and in the whole network. Typically, this type of network has a lot of hyperparameters. Let's print the number of parameters in the network (and for each layer):</p>
<pre>//print the score with every 1 iteration<br/>model.setListeners(new ScoreIterationListener(1))<br/><br/>//Print the number of parameters in the network (and for each layer)<br/><strong>val</strong> layers = model.getLayers()<br/><strong>var</strong> totalNumParams = 0<br/><strong>var</strong> i = 0<br/><br/>for (i &lt;- 0 to layers.length-1) {<br/>      val nParams = layers(i).numParams()<br/>      println("Number of parameters in layer " + i + ": " + nParams)<br/>      totalNumParams = totalNumParams + nParams<br/>}<br/>println("Total number of network parameters: " + totalNumParams)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output of the preceding code is as follows:</p>
<pre><strong>Number of parameters in layer 0: 510640000</strong><br/><strong>Number of parameters in layer 1: 200020000</strong><br/><strong>Number of parameters in layer 2: 200020000</strong><br/><strong>Number of parameters in layer 3: 25005</strong><br/><strong>Total number of network parameters: 910705005</strong></pre>
<p>As I stated previously, our network has 910 million parameters, which is huge. This also poses a great challenge while tuning hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network training</h1>
                </header>
            
            <article>
                
<p>First, we will create a <kbd>MultiLayerNetwork</kbd> using the preceding <kbd>MultiLayerConfiguration</kbd>. Then, we will initialize the network and start the training on the training set:</p>
<pre><strong>var</strong> j = 0<br/>println("Train model....")<br/>for (j &lt;- 0 to numEpochs-1) {<br/>   model.fit(trainingDataIt)</pre>
<p>Finally, we also specify that we do not need to do any pre-training (which is typically needed in DBN or stacked autoencoders).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Once the training has been completed, the next task is to evaluate the model, which we'll do on the test set here. For the evaluation, we will be using the <kbd>Evaluation()</kbd> method. This method creates an evaluation object with five possible classes.</p>
<p>First, let's iterate the evaluation over every test sample and get the network's prediction from the trained model. Finally, the <kbd>eval()</kbd> method checks the prediction against the true class:</p>
<pre>println("Evaluate model....")<br/><strong>val</strong> eval: Evaluation = new Evaluation(5) //create an evaluation object with 5 possible classes    <br/>while (testDataIt.hasNext()) {<br/>      <strong>val</strong> next:DataSet = testDataIt.next()<br/>      <strong>val</strong> output:INDArray  = model.output(next.getFeatureMatrix()) //get the networks prediction<br/>      eval.eval(next.getLabels(), output) //check the prediction against the true class<br/>    }<br/>println(eval.stats())<br/>println("****************Example finished********************")<br/>  }</pre>
<p>The following is the output:</p>
<pre><strong>==========================Scores========================================</strong><br/><strong>  # of classes:    5</strong><br/><strong>  Accuracy:        0.9900</strong><br/><strong>  Precision:       0.9952</strong><br/><strong>  Recall:          0.9824</strong><br/><strong>  F1 Score:        0.9886</strong><br/><strong> Precision, recall &amp; F1: macro-averaged (equally weighted avg. of 5 classes)</strong><br/><strong> ========================================================================</strong><br/><strong> ****************Example finished******************</strong></pre>
<p>Wow! Unbelievable! Our LSTM network has accurately classified the samples. Finally, let's see how the classifier predicts across each class:</p>
<pre><strong>Actual label 0 predicted by the model as 0: 82 times</strong><br/><strong>Actual label 1 predicted by the model as 0: 1 times</strong><br/><strong>Actual label 1 predicted by the model as 1: 17 times</strong><br/><strong>Actual label 2 predicted by the model as 2: 35 times</strong><br/><strong>Actual label 3 predicted by the model as 0: 1 times</strong><br/><strong>Actual label 3 predicted by the model as 3: 30 times</strong><span class="packt_screen"> </span></pre>
<p>The predictive accuracy for cancer type prediction using LSTM is suspiciously higher, isn't it? Did our model underfit? Did our model overfit?</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Observing the training using Deeplearning4j UI</h1>
                </header>
            
            <article>
                
<p>As our accuracy is suspiciously higher, we can observe how the training went. Yes, there are ways to find out if it went through overfitting, since we can observe the training, validation, and test losses on the DL4J UI. However, I won't discuss the details here. Take a look at <a href="https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization">https://deeplearning4j.org/docs/latest/deeplearning4j-nn-visualization</a> for more information on how to do this.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how to classify cancer patients on the basis of tumor types from a very high-dimensional gene expression dataset curated from TCGA. Our LSTM architecture managed to achieve 99% accuracy, which is outstanding. Nevertheless, we discussed many aspects of DL4J, which will be helpful in upcoming chapters. Finally, we saw answers to some frequent questions related to this project, LSTM networks, and DL4J hyperparameters/network tuning.</p>
<p><span class="fontstyle0">This is, more or less, the end of our little journey in developing ML projects using Scala and different open source frameworks. Throughout these chapters, I have tried to provide you with several examples of how to use these wonderful technologies efficiently for developing ML projects. While writing this book, I had to keep many constraints in my mind; for example, the page count, API availability, and of course, my expertise.</span></p>
<p><span class="fontstyle0">However, overall, I tried to make the book simple by avoiding unnecessary details on the theory, as you can read about that in many books, blogs, and websites. I will also keep the code of this book updated on the GitHub repository at</span> <a href="https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide">https://github.com/PacktPublishing/Machine-Learning-with-Scala-Quick-Start-Guide</a><span class="fontstyle0">. Feel free to open a new issue or any pull request to improve the code and stay tuned.</span></p>
<p><span class="fontstyle0">Nevertheless, I'll upload the solution to each chapter as Zeppelin notebooks so that you can run the code interactively. By the way, Zeppelin is a web-based notebook that enables data-driven, interactive data analytics, and collaborative documents with SQL and Scala. Once you have configured Zeppelin on your preferred platform, you can download the notebook from the GitHub repository, import them into Zeppelin, and get going. For more detail, you can take a look at <a href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a>.</span></p>


            </article>

            
        </section>
    </body></html>