<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer247">
			<h1 id="_idParaDest-190"><em class="italic"><a id="_idTextAnchor189"/>Chapter 12</em>: Distributed Machine Learning on Azure</h1>
			<p>In the previous chapter, we learned about hyperparameter tuning through search and optimization, using HyperDrive as well as Automated Machine Learning as a special case of hyperparameter optimization, involving feature engineering, model selection, and model stacking. Automated Machine Learning is <strong class="bold">machine learning as a service</strong> (<strong class="bold">MLaaS</strong>), whereby the only input is your data, an ML task, and an error metric. It's hard to imagine running all experiments and parameter combinations for Automated Machine Learning on a single machine or a single CPU/GPUwe are looking into ways to speed up the training process through parallelization and distributed computing.</p>
			<p>In this chapter, we will look into distributed and parallel computing algorithms and frameworks for efficiently training ML models in parallel. The goal of this chapter is to build an environment in Azure where you can speed up the training process of classical ML and deep learning models by adding more machines to your training environment, thereby scaling out the cluster.</p>
			<p>First, we will take a look at the different methods and fundamental building blocks for <strong class="bold">distributed ML</strong>. You will grasp the difference between training independent models in parallel, as done in HyperDrive and Automated Machine Learning, and training a single model ensemble on a large dataset in parallel by partitioning the training data. We will then look into distributed ML for single models and discover data-distributed and model-distributed training methods. Both methods are often used in real-world scenarios for speeding up or enabling the training of large deep neural networks.</p>
			<p>After that, we will discover the most popular frameworks for distributed ML and how they can be used in Azure and in combination with Azure Machine Learning compute. The transition between execution engines, communication libraries, and functionality for distributed ML libraries is smooth but often hard to understand. However, after reading this chapter, you will understand the difference between running Apache Spark in Databricks with MLlib and using Horovod, Gloo, PyTorch, and TensorFlow parameter servers.</p>
			<p>In the final section, we will take a look at two practical examples of how to implement the functionality we'll be covering in Azure and integrate it with Azure Machine Learning compute.</p>
			<p>This chapter covers the following topics:</p>
			<ul>
				<li>Exploring methods for distributed ML</li>
				<li>Using distributed ML in Azure</li>
			</ul>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create decision-tree-based ensemble classifiers:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0 </strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0 </strong></li>
				<li><strong class="source-inline">horovod 0.23.0 </strong></li>
				<li><strong class="source-inline">tensorflow 2.6.0 </strong></li>
				<li><strong class="source-inline">pyspark 3.2.0 </strong></li>
				<li><strong class="source-inline">numpy 1.19.5 </strong></li>
				<li><strong class="source-inline">pandas 1.3.2 </strong></li>
				<li><strong class="source-inline">scikit-learn 0.24.2</strong></li>
			</ul>
			<p>Similar to previous chapters, you can execute this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book, found at <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter12</a>.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor191"/>Exploring methods for distributed ML</h1>
			<p>The journey<a id="_idIndexMarker1398"/> of implementing ML pipelines is very similar for<a id="_idIndexMarker1399"/> a lot of users and is often similar to the steps described in the previous chapters. When users start switching from experimentation to real-world data or from small examples to larger models, they often experience a similar issue: training large parametric models on large amounts of data—especially DL models—takes a very long time. Sometimes, epochs last hours, and training takes days to converge.</p>
			<p>Waiting hours or even days for a model to converge means precious time wasted for many engineers, as it makes it a lot harder to interactively tune the training process. Therefore, many ML engineers need to speed up their training process by leveraging various distributed <a id="_idIndexMarker1400"/>computing techniques. The idea of distributed ML is as simple as speeding up a training process by adding more compute resources. In the best case, the training performance improves linearly by adding more machines to the training cluster (scaling out). In this section, we will take a look at the most <a id="_idIndexMarker1401"/>common patterns of distributed ML and try to understand and reason about them. In the next section of this chapter, we will also apply them to some real-world examples.</p>
			<p>Most modern ML pipelines use some of the techniques discussed in this chapter to speed up the training process once their data or models become larger. This is similar to the need for big data platforms—such as Spark, Hive, and so on—for data preprocessing, once the data gets large. Hence, while this chapter seems overly complex, we would recommend revisiting it whenever you are waiting for your model to converge or want to produce better results faster.</p>
			<p>There are generally three patterns for leveraging distributed computing for ML, as presented here:</p>
			<ul>
				<li>Training independent models on small data in parallel</li>
				<li>Training copies of a model in parallel on different subsets of the data</li>
				<li>Training different parts of the same model in parallel</li>
			</ul>
			<p>Let's take a look at each of these methods.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor192"/>Training independent models on small data in parallel</h2>
			<p>We will first <a id="_idIndexMarker1402"/>look at the easiest example: training (small) independent models on a (small) dataset. A typical use case for this parallel training is performing a hyperparameter search or the optimization of a classic ML model or a small neural network. This is very similar to what we covered in <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>. Even Automated Machine Learning—where multiple individual independent models are trained and compared—uses this approach under the hood. In parallel training, we aim to speed up the training of multiple independent models with different parameters by training these models in parallel.</p>
			<p>The following diagram<a id="_idIndexMarker1403"/> shows this case, where instead of training the individual models in sequence on a single machine, we train them in parallel:</p>
			<div>
				<div id="_idContainer241" class="IMG---Figure">
					<img src="image/B17928_12_01.jpg" alt="Figure 12.1 – Parallel processing " width="884" height="301"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.1 – Parallel processing</p>
			<p>You can see that no communication or synchronization is required during the training process of the individual models. This means that we can train either on multiple CPUs/GPUs on the same machine or on multiple machines. </p>
			<p>When using Azure Machine Learning for hyperparameter tuning, this parallelization is easy to achieve by configuring an Azure Machine Learning compute target with multiple nodes and selecting the number of concurrent runs through the <strong class="source-inline">max_concurrent_runs</strong> parameter of the HyperDrive configuration. In Azure Machine Learning HyperDrive, all it takes is to specify an estimator and <strong class="source-inline">param_sampling</strong>, and submit the HyperDrive configuration as an experiment in order to run the individual task in parallel, as shown here:</p>
			<p class="source-code">from azureml.train.hyperdrive import HyperDriveConfig</p>
			<p class="source-code">hyperdrive_run_config = HyperDriveConfig(</p>
			<p class="source-code">    estimator=estimator,</p>
			<p class="source-code">    hyperparameter_sampling=param_sampling, </p>
			<p class="source-code">    primary_metric_name="accuracy", </p>
			<p class="source-code">    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,</p>
			<p class="source-code">    max_total_runs=100,</p>
			<p class="source-code">    max_concurrent_runs=4)</p>
			<p class="source-code">from azureml.core.experiment import Experiment</p>
			<p class="source-code">experiment = Experiment(workspace, experiment_name)</p>
			<p class="source-code">hyperdrive_run = experiment.submit(hyperdrive_run_config)</p>
			<p>Here are some<a id="_idIndexMarker1404"/> formulas to compute the value for <strong class="source-inline">max_concurrent_runs</strong> for HyperDrive or any other distributed computing setup:</p>
			<ul>
				<li>For CPU-based training, the maximal number of concurrent training runs is limited by the number of available CPUs and compute nodes. The available physical memory is also a limitation, but swapping to virtual memory allow us to consume more memory than is physically available.</li>
				<li>For GPU-based training, the maximal number of concurrent training runs is limited by the number of available GPUs and compute nodes, as well as the amount of available GPU memory. Typically, one training run is pinned to one physical GPU, but through GPU virtualization we can also train multiple models on a single physical GPU if enough GPU memory is available.</li>
			</ul>
			<p>Here is a guide to how to estimate how much memory a single model will consume:</p>
			<p><strong class="bold">Size of a single parameter</strong>:</p>
			<ul>
				<li>Half-precision float: 16 bits (2 bytes).</li>
				<li>Single-precision float: 32 bits (4 bytes)—this is often the default.</li>
				<li>Double-precision float: 64 bits (8 bytes).</li>
			</ul>
			<p><strong class="bold">Number of parameters required for a model</strong>:</p>
			<ul>
				<li>Parametric model: Sum of all parameters</li>
				<li>Non-parametric model: Number of representations (for example, decision trees) * number of a representation's parameters</li>
			</ul>
			<p>Then, you multiply additional factors, as follows:</p>
			<ul>
				<li>Models using backpropagation: overall memory * 2</li>
				<li>Models using batching: overall memory * batch size</li>
				<li>Models using (recurrent) states: memory per state * number of recurrent steps</li>
			</ul>
			<p>While this use<a id="_idIndexMarker1405"/> case seems very similar, let's move on to the next use case where we are given a large dataset that cannot be copied onto every machine.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/>Training a model ensemble on large datasets in parallel</h2>
			<p>The next thing we<a id="_idIndexMarker1406"/> will discuss is a very common optimization within ML, particularly when training models on large datasets. In order to train models, we usually require a large amount of data that rarely all fits into the memory of a single machine. Therefore, it is often required to split the data into chunks and train multiple individual models on the different chunks.</p>
			<p>The following screenshot shows two ways of splitting data into smaller chunks—by splitting the rows horizontally (left) or by splitting the columns vertically (right):</p>
			<div>
				<div id="_idContainer242" class="IMG---Figure">
					<img src="image/B17928_12_02.jpg" alt="Figure 12.2 – Data split: horizontal (row-wise) versus vertical (column-wise) " width="1203" height="294"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.2 – Data split: horizontal (row-wise) versus vertical (column-wise)</p>
			<p>You could also mix both techniques to extract a subset from your training data. Whenever you are using tools from the big data domain—such as MapReduce, Hive, or Spark—partitioning your data will help you to speed up your training process or enable training over huge amounts of data in the first place.</p>
			<p>A good example of performing data-distributed training is to train a massive tree ensemble of completely separate decision-tree models, also<a id="_idIndexMarker1407"/> called a random forest. By splitting the data into <a id="_idIndexMarker1408"/>many thousands of randomized chunks, you can train one decision tree per chunk of data and combine all trained trees into a single ensemble model. Apache Hivemall is a library based on Hive and Spark that does exactly this on either of the two execution engines. Here is an example of training multiple XGBoost multi-class ensemble <a id="_idIndexMarker1409"/>models on Hive using <strong class="bold">Hive Query Language</strong> (<strong class="bold">HiveQL</strong>) and Apache Hivemall:</p>
			<p class="source-code">-- explicitly use 3 reducers</p>
			<p class="source-code">-- set mapred.reduce.tasks=3;</p>
			<p class="source-code">create table xgb_softmax_model as</p>
			<p class="source-code">select </p>
			<p class="source-code">  train_xgboost(features, label, </p>
			<p class="source-code">    '-objective multi:softmax -num_class 10 -num_round 10') </p>
			<p class="source-code">    as (model_id, model)</p>
			<p class="source-code">from (</p>
			<p class="source-code">  select features, (label - 1) as label</p>
			<p class="source-code">  from data_train</p>
			<p class="source-code">  cluster by rand(43) -- shuffle data to reducers</p>
			<p class="source-code">) data;</p>
			<p>In the preceding function, we use the <strong class="source-inline">cluster</strong> keyword to randomly move rows of data to the reducers. This will partition the data horizontally and train an XGBoost model per partition on each reducer. By defining the number of reducers, we also define the number of models trained in parallel. The resulting models are stored in a table where each row defines the parameters of one model. In a prediction, we would simply combine all individual models and perform an average-voting criterion to retrieve the final result.</p>
			<p>Another example of this approach would be a standard Spark pipeline that trains multiple independent models on vertical and horizontal data partitions. When we've finished training the<a id="_idIndexMarker1410"/> individual models, we can use an average-voting criterion during inference to find the optimal result for a prediction task. Here is a small example script for training multiple models on horizontally partitioned data in parallel using Python, PySpark, and scikit-learn:</p>
			<p class="source-code">from pyspark.sql import SparkSession</p>
			<p class="source-code">spark = SparkSession.builder \</p>
			<p class="source-code">    .appName("Distributed Training") \</p>
			<p class="source-code">    .master("local") \</p>
			<p class="source-code">    .getOrCreate()</p>
			<p class="source-code"># read the input data</p>
			<p class="source-code">df = spark.read.parquet("data/")</p>
			<p class="source-code"># define your training function</p>
			<p class="source-code">from sklearn.ensemble import RandomForestClassifier</p>
			<p class="source-code">def train_model(data):</p>
			<p class="source-code">    clf = RandomForestClassifier(n_estimators=10)</p>
			<p class="source-code">    return clf.fit(data['train_x'], data['train_y'])</p>
			<p class="source-code"># split your data into partitions and train models</p>
			<p class="source-code">num_models = 100</p>
			<p class="source-code">models = df.rdd.repartition(num_models) \</p>
			<p class="source-code">  .mapPartitions(train_model) \</p>
			<p class="source-code">  .collect()</p>
			<p>In the preceding function, we can now load almost any amount of data and repartition it such that each partition fits into the local memory of a single node. If we have 1 <strong class="bold">terabyte</strong> (<strong class="bold">TB</strong>) of training<a id="_idIndexMarker1411"/> data, we could split it into 100 partitions of 10-<strong class="bold">gigabyte</strong> (<strong class="bold">GB</strong>) chunks of data, which we distribute over 10 12-core worker nodes with 128 GB <strong class="bold">random-access memory</strong> (<strong class="bold">RAM</strong>) each. The training time will, at most, take a couple of <a id="_idIndexMarker1412"/>seconds for the training of the 100 models in parallel. Once all the models are trained, we use the <strong class="source-inline">collect()</strong> method to return all trained models to the head node.</p>
			<p>We could have also decided to just store the models from each individual worker on disk or in a distributed filesystem, but it might be nicer to just combine the results on a single node. In this example, you see we have the freedom to choose either of the two methods because all models are independent of each other. This is not true for cases where the models are suddenly dependent on each other—for example, when minimizing a global gradient<a id="_idIndexMarker1413"/> or splitting a single model over multiple machines, which are both common use cases when training DNNs in the same way. In this case, we need some new operators to steer the control flow of the data and gradients. Let's look into these operators in the following section.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor194"/>Fundamental building blocks for distributed ML</h2>
			<p>As we saw in the previous <a id="_idIndexMarker1414"/>example, we need some fundamental building blocks or operators to manage the data flow in a<a id="_idIndexMarker1415"/> distributed system. We call these operators <strong class="bold">collective algorithms</strong>. These algorithms implement common<a id="_idIndexMarker1416"/> synchronization and communication patterns for distributed computing and are required when training ML models. Before we jump into distributed training methods for DNNs, we will have a quick look at these patterns to understand the foundations.</p>
			<p>The most <a id="_idIndexMarker1417"/>common communication patterns in distributed systems are listed here:</p>
			<ul>
				<li>One-to-one</li>
				<li>One-to-many (also called <em class="italic">broadcast</em> or <em class="italic">scatter</em> patterns)</li>
				<li>Many-to-one (also called <em class="italic">gather</em> or <em class="italic">reduce</em> patterns)</li>
				<li>Many-to-many (also called <em class="italic">all-gather</em> or <em class="italic">all-reduce</em> patterns)</li>
			</ul>
			<p>The following screenshot <a id="_idIndexMarker1418"/>gives a great overview of these patterns and shows how the data flows between the individual actors of a system:</p>
			<div>
				<div id="_idContainer243" class="IMG---Figure">
					<img src="image/B17928_12_03.jpg" alt="Figure 12.3 – Communication patterns in distributed systems " width="817" height="162"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.3 – Communication patterns in distributed systems</p>
			<p>We can immediately think back to the hyperparameter optimization technique of Bayesian optimization. First, we need to <strong class="bold">broadcast</strong> the training data from the master to all worker nodes. Then, we can choose parameter combinations from the parameter space on the master and broadcast those to the worker nodes as well. Finally, we perform training on the worker nodes, before then <strong class="bold">gathering</strong> all the model validation scores from the <a id="_idIndexMarker1419"/>worker nodes on the master. By comparing the scores and applying Bayes' theorem, we can predict the next possible <a id="_idIndexMarker1420"/>parameter combinations and repeat broadcasting them to the worker nodes.</p>
			<p>Did you notice something in the preceding algorithm? How can we know that all worker nodes finished the training process, and gather all scores from all worker nodes? To do this, we will use another building<a id="_idIndexMarker1421"/> block called synchronization, or <strong class="bold">barrier synchronization</strong>. With barrier synchronization, we can schedule the execution of a task such that it needs to wait for all other distributed tasks to be finished. The following screenshot shows a good overview of the synchronization pattern in multiprocessors:</p>
			<div>
				<div id="_idContainer244" class="IMG---Figure">
					<img src="image/B17928_12_04.jpg" alt="Figure 12.4 – Synchronization mechanism " width="962" height="360"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.4 – Synchronization mechanism</p>
			<p>As you can see, we implicitly used these algorithms already in the previous chapter, where they were hidden from us behind the term <em class="italic">optimization</em>. Now, we will use them explicitly by changing the optimizers in order to train a single model over multiple machines.</p>
			<p>As you might have <a id="_idIndexMarker1422"/>already realized, these patterns are not new and are used by your operating system many times per second. However, in this case, we can take advantage of these patterns and apply them to the execution graph of a distributed training process, and through specialized hardware (for example, by connecting<a id="_idIndexMarker1423"/> two GPUs together using <strong class="bold">InfiniBand</strong> (<strong class="bold">IB</strong>)).</p>
			<p>In order to use this collective<a id="_idIndexMarker1424"/> algorithm with a different level of hardware support (GPU support and vectorization), you need to select a communication backend. These backends are libraries that often run as a separate process and implement communication and synchronization patterns. Popular libraries for collective algorithms include <strong class="bold">Gloo</strong>, <strong class="bold">Message Passing Interface</strong> (<strong class="bold">MPI</strong>), and <strong class="bold">NVIDIA Collective Communications Library</strong> (<strong class="bold">NCCL</strong>).</p>
			<p>Most DL frameworks, such as PyTorch or TensorFlow, provide their own higher-level abstractions on one of these communication backends—for example, PyTorch <strong class="bold">Remote Procedure Call</strong> (<strong class="bold">RPC</strong>) and a TensorFlow<a id="_idIndexMarker1425"/> <strong class="bold">parameter server (PS)</strong>. Instead of using a different execution and communication framework, you could also choose a general-purpose framework for distributed computing, such as Spark.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The PyTorch documentation has an up-to-date guide on when to use which collective communication library: https://pytorch.org/docs/stable/distributed.html#which-backend-to-use.</p>
			<p>As you can see, the<a id="_idIndexMarker1426"/> list of possible choices is endless, and multiple combinations are possible. We haven't even talked about Horovod, a framework used to add distributed training to other DL frameworks through<a id="_idIndexMarker1427"/> distributed optimizers. The good part is that most of these frameworks and libraries are provided in all Azure Machine Learning runtimes as well as being supported through the Azure ML SDK. This means you will <a id="_idIndexMarker1428"/>often only specify the desired backend, supply your model to any specific framework, and let Azure Machine Learning handle the setup, initialization, and management of these tools. We will see this in action in the second half of this chapter.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor195"/>Speeding up deep learning with data-parallel training</h2>
			<p>Another variation<a id="_idIndexMarker1429"/> of distributed data-parallel training is <a id="_idIndexMarker1430"/>very common in DL. In order to speed up the training of larger models, we can run multiple training iterations with different chunks of data on distributed copies<a id="_idIndexMarker1431"/> of the same model. This is especially crucial when each training iteration takes a significant amount of time (for example, multiple seconds), which is a typical scenario for training large DNNs where we want to take advantage of multi-GPU environments.</p>
			<p>Data-distributed training for DL is <a id="_idIndexMarker1432"/>based on the idea of using a <strong class="bold">distributed gradient descent</strong> (<strong class="bold">DGD</strong>) algorithm, as follows:</p>
			<ol>
				<li>Distribute a copy of the model to each node.</li>
				<li>Distribute a chunk of data to each node.</li>
				<li>Run a full pass through the network on each node and compute the gradient.</li>
				<li>Collect all gradients on a single node and compute the average gradient.</li>
				<li>Send the average gradient to all nodes.</li>
				<li>Update all models using the average gradient.</li>
			</ol>
			<p>The following diagram<a id="_idIndexMarker1433"/> shows this in action for multiple models, running the forward/backward pass individually and sending the gradient back to the parameter server:</p>
			<div>
				<div id="_idContainer245" class="IMG---Figure">
					<img src="image/B17928_12_05.jpg" alt="Figure 12.5 – Data-parallel training " width="1191" height="511"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.5 – Data-parallel training</p>
			<p>As seen here, the server <a id="_idIndexMarker1434"/>computes the average gradient, which is sent back to all other nodes. We can immediately see that, suddenly, communication is required between the worker nodes and a primary node (let's call it the <em class="italic">parameter server</em>), and that <a id="_idIndexMarker1435"/>synchronization is required too while waiting for all models to finish computing the gradient.</p>
			<p>A great example <a id="_idIndexMarker1436"/>of this use case is speeding up the training process of DL models by parallelizing the backpropagation step and combining the gradients from each node to an overall gradient. TensorFlow currently supports this distribution mode using a so-called parameter server. The <em class="italic">Horovod</em> framework developed at Uber provides a handy abstraction for distributed optimizers and plugs into many available ML frameworks or distributed execution <a id="_idIndexMarker1437"/>engines, such as TensorFlow, PyTorch, and Apache Spark. We will take a look at practical examples of using Horovod and Azure Machine Learning in the <em class="italic">Horovod – a distributed DL training framework</em> section.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/>Training large models with model-parallel training</h2>
			<p>Lastly, another <a id="_idIndexMarker1438"/>common use case in DL is to train models that are larger than the provided GPU memory of a single GPU. This approach is a bit trickier as it requires the model execution graph to be split among different GPUs or even different machines. While this is not a<a id="_idIndexMarker1439"/> big problem in CPU-based execution and is often done in Spark, Hive, or TensorFlow, we also need to transfer the intermediate results between multiple GPU memories. In order to do this effectively, extra hardware and drivers such as <strong class="bold">Infiniband</strong> (GPU-to-GPU communication) and <strong class="bold">GPUDirect</strong> (efficient GPU memory<a id="_idIndexMarker1440"/> access) are required.</p>
			<p>The following diagram displays the difference between computing multiple gradients in parallel (on the left) and computing a single forward pass of a distributed model (on the right):</p>
			<div>
				<div id="_idContainer246" class="IMG---Figure">
					<img src="image/B17928_12_06.jpg" alt="Figure 12.6 – Model-parallel training " width="1182" height="447"/>
				</div>
			</div>
			<p class="figure-caption">Figure 12.6 – Model-parallel training</p>
			<p>The latter is a lot more complicated as data has to be exchanged during forward and backward passes between multiple GPUs and/or multiple nodes.</p>
			<p>In general, we choose between two scenarios: multi-GPU training on a single machine and multi-GPU training on multiple machines. As you might expect, the latter is a lot more difficult, as it requires communication between and the synchronization of multiple machines over a network.</p>
			<p>In the following script, we create a<a id="_idIndexMarker1441"/> simple model running distributed on two GPUs using PyTorch. Using <strong class="source-inline">.to('cuda:*')</strong> methods throughout the model, we define the GPU on which an operation should be performed. In addition, we also need to add the same<a id="_idIndexMarker1442"/> annotation to the input data for these computations:</p>
			<p class="source-code">import torch</p>
			<p class="source-code">import torch.nn as nn</p>
			<p class="source-code">import torch.optim as optim</p>
			<p class="source-code">class ParallelModel(nn.Module):</p>
			<p class="source-code">    def __init__(self):</p>
			<p class="source-code">        super(ParallelModel, self).__init__()</p>
			<p class="source-code">        self.net1 = torch.nn.Linear(10, 10)<strong class="bold">.to('cuda:0')</strong></p>
			<p class="source-code">        self.relu = torch.nn.ReLU()</p>
			<p class="source-code">        self.net2 = torch.nn.Linear(10, 5)<strong class="bold">.to('cuda:1')</strong></p>
			<p class="source-code">    def forward(self, x):</p>
			<p class="source-code">        x = self.relu(self.net1(x<strong class="bold">.to('cuda:0')</strong>))</p>
			<p class="source-code">        return self.net2(x<strong class="bold">.to('cuda:1')</strong>)</p>
			<p>As we can see in the preceding code, we configure the network to compute the first fully connected layer on GPU <strong class="source-inline">0</strong> whereas the second fully connected layer is computed on GPU <strong class="source-inline">1</strong>. When configuring forward steps, we also need to configure the inputs to both layers accordingly.</p>
			<p>Training the model using a built-in optimizer and loss function is not very different from non-distributed models. The only difference is that we also have to define the target GPU for the training labels<a id="_idIndexMarker1443"/> so that the loss can be computed, as follows: </p>
			<p class="source-code">model = ParallelModel()</p>
			<p class="source-code">loss_fn = nn.MSELoss()</p>
			<p class="source-code">optimizer = optim.SGD(model.parameters(), lr=0.001)</p>
			<p class="source-code">optimizer.zero_grad()</p>
			<p class="source-code">outputs = model(torch.randn(20, 10))</p>
			<p class="source-code">labels = torch.randn(20, 5)<strong class="bold">.to('cuda:1')</strong></p>
			<p class="source-code">loss_fn(outputs, labels).backward()</p>
			<p class="source-code">optimizer.step()</p>
			<p>As you can see, we have split individual layers to run on multiple GPUs, while the data between these layers needs to be<a id="_idIndexMarker1444"/> transferred during forward and backward passes. We have to apply code changes to the model itself in order to specify which parts of the model should run on which GPU.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Please note that we could also make this split dynamic, such that we split the model into <em class="italic">x</em> consecutive subgraphs that are executed on <em class="italic">x</em> GPUs.</p>
			<p>It's interesting to note that many of the techniques discussed in this chapter can be combined. We could, for example, train one multi-GPU model per machine, while partitioning the data into chunks and computing multiple parts of the gradient on multiple machines—hence adopting a data-distributed model-parallel approach.</p>
			<p>In the next section, we will learn how to put these concepts into practice.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor197"/>Using distributed ML in Azure</h1>
			<p>The <em class="italic">Exploring methods for distributed ML</em> section contained an overwhelming amount of different parallelization scenarios, various communication backends for collective algorithms, and <a id="_idIndexMarker1445"/>code examples using different ML frameworks and even execution engines. The amount of choice when it comes to ML frameworks is <a id="_idIndexMarker1446"/>quite large, and making an educated decision is not easy. This choice gets even more complicated as some frameworks are <a id="_idIndexMarker1447"/>supported out of the box in Azure Machine Learning while others have to be installed, configured, and managed by the user.</p>
			<p>In this section, we will go through the most common scenarios, learn how to choose the correct combination of frameworks, and implement a distributed ML pipeline in Azure.</p>
			<p>In general, you have three choices for<a id="_idIndexMarker1448"/> running distributed ML in Azure, as follows:</p>
			<ul>
				<li>The first obvious choice is using Azure Machine Learning, the notebook environment, the Azure Machine Learning SDK, and Azure Machine Learning compute clusters. This will be the easiest solution for many complex use cases. Huge datasets can be stored on Azure Blob Storage, and models can be trained as data-parallel and/or model-parallel models with different communication backends. Everything is managed for you by wrapping your training script with an estimator abstraction.</li>
				<li>The second choice is to use a different authoring and execution engine for your code instead of Azure Machine Learning notebooks and Azure Machine Learning compute clusters. A popular option is Azure Databricks with integrated interactive notebooks and Apache Spark as a distributed execution engine. Using Databricks, you can use the pre-built ML images and auto-scaling clusters, which provides a great environment for running distributed ML training.</li>
				<li>The third choice is to build and roll out your own custom solution. To do so, you need to build a separate<a id="_idIndexMarker1449"/> cluster with virtual machines or Kubernetes and orchestrate the setup, installation, and management of the infrastructure and code. While this is the most flexible solution, it is also—by far—the most complex and time-consuming to set up. </li>
			</ul>
			<p>For this book, we will first look into Horovod optimizers, Azure Databricks, and Apache Spark before diving deeper into Azure Machine Learning.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor198"/>Horovod – a distributed deep learning training framework</h2>
			<p><strong class="bold">Horovod</strong> is a framework for <a id="_idIndexMarker1450"/>enabling <strong class="bold">distributed DL</strong> and was initially developed and <a id="_idIndexMarker1451"/>made open source by Uber. It provides a unified way to support the distributed training of<a id="_idIndexMarker1452"/> existing DL training code for the following supported frameworks—TensorFlow, Keras, PyTorch, and Apache MXNet. The design goal was to make the transition from single-node training to data-parallel training extremely simple for any existing project, and hence enable these models to train faster on multiple GPUs in a distributed environment.</p>
			<p>Horovod is an excellent choice as a drop-in replacement for optimizers in any of the supported frameworks for data-parallel training. It integrates nicely with the supported frameworks through initialization and update steps or update hooks, by simply abstracting the GPUs from the DL code. From a user's perspective, only minimal code changes have to be done to support data-parallel training for your model. Let's take a look at an example using Keras and implement the following steps:</p>
			<ol>
				<li value="1">Initialize Horovod.</li>
				<li>Configure Keras to read GPU information from Horovod.</li>
				<li>Load a model and split training data.</li>
				<li>Wrap the Keras optimizer as a Horovod distributed optimizer.</li>
				<li>Implement model training.</li>
				<li>Execute the script using <strong class="source-inline">horovodrun</strong>.</li>
			</ol>
			<p>The detailed steps are listed here:</p>
			<ol>
				<li value="1">The first step is the same for any script using Horovod—we first need to load <strong class="source-inline">horovod</strong> from the correct package and initialize it, as follows:<p class="source-code">import horovod.keras as hvd</p><p class="source-code">hvd.init()</p></li>
				<li>Next, we need to perform a custom setup step, which varies depending on the framework used. This <a id="_idIndexMarker1453"/>step will set up the GPU configuration for the<a id="_idIndexMarker1454"/> framework, and ensure that it can call the abstracted versions through Horovod. The code is illustrated in the following snippet:<p class="source-code">from tensorflow.keras import backend as K</p><p class="source-code">import tensorflow as tf</p><p class="source-code"># pin GPU to be used to process local rank.</p><p class="source-code"># one GPU per process</p><p class="source-code">config = tf.ConfigProto()</p><p class="source-code">config.gpu_options.allow_growth = True</p><p class="source-code">config.gpu_options.visible_device_list = str(hvd.local_rank())</p><p class="source-code">K.set_session(tf.Session(config=config))</p></li>
				<li>Now, we can simply take our single-node, single-GPU Keras model and define all parameters and the training and validation data. There is nothing special required during this step, as we can see here:<p class="source-code"># standard model and data</p><p class="source-code">batch_size = 10</p><p class="source-code">epochs = 100</p><p class="source-code">model = load_model(...)</p><p class="source-code">x_train, y_train = load_train_data(...)</p><p class="source-code">x_test, y_test = load_test_data(...)</p></li>
				<li>Finally, we arrive at the magical part, where we wrap the framework optimizer—in this case, Adadelta from Keras—as a Horovod distributed optimizer. For all subsequent code, we will simply use the distributed optimizer instead of the default one. We also<a id="_idIndexMarker1455"/> need to adjust the learning rate to the number of used GPUs, as the resulting gradient will be<a id="_idIndexMarker1456"/> averaged from the individual changes. This can be done using the following code:<p class="source-code">from tensorflow.keras.optimizers import Adadelta</p><p class="source-code"># adjust learning rate based on number of GPUs</p><p class="source-code">opt = Adadelta(1.0 * hvd.size())</p><p class="source-code"># add Horovod Distributed Optimizer</p><p class="source-code">opt = hvd.DistributedOptimizer(opt)</p></li>
				<li>The remaining part looks fairly simple. It involves compiling the model, fitting the model, and evaluating the model, just as with the single-node counterpart. It's worth mentioning that we need to add a callback to initialize all gradients during the training process. The code is illustrated in the following snippet:<p class="source-code">model.compile(loss=keras.losses.categorical_crossentropy,</p><p class="source-code">              optimizer=opt, </p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">callbacks = [</p><p class="source-code">  hvd.callbacks.BroadcastGlobalVariablesCallback(0)</p><p class="source-code">]</p><p class="source-code">model.fit(x_train,</p><p class="source-code">          y_train,</p><p class="source-code">          batch_size=batch_size,</p><p class="source-code">          callbacks=callbacks,</p><p class="source-code">          epochs=epochs,</p><p class="source-code">          verbose=1 if hvd.rank() == 0 else 0,</p><p class="source-code">          validation_data=(x_test, y_test))</p><p class="source-code">score = model.evaluate(x_test, y_test)</p><p class="source-code">print('Test loss:', score[0])</p><p class="source-code">print('Test accuracy:', score[1])</p></li>
			</ol>
			<p>When looking at the preceding code, it's fair to say that Horovod is not over-promising on making it easy to extend your code for distributed execution using a data-parallel approach<a id="_idIndexMarker1457"/> and distributed gradient computation. If you have looked into the native TensorFlow or PyTorch versions, you will have seen that this requires far fewer code changes and is a lot more readable and portable than a parameter server or RPC framework.</p>
			<ol>
				<li value="6">The Horovod framework <a id="_idIndexMarker1458"/>uses a communication based on MPI to handle collective algorithms under the hood, and usually requires one running process per GPU per node. However, it can also run on top of the Gloo backend or a custom MPI backend through a configuration option. Here is a sample snippet of how to use the <strong class="source-inline">horovodrun</strong> command to start a training process on two machines, <strong class="source-inline">server1</strong> and <strong class="source-inline">server2</strong>, each using four separate GPUs:<p class="source-code"><strong class="bold">horovodrun -np 8 -H server1:4,server2:4 python train.py</strong></p></li>
			</ol>
			<p>Running and debugging Horovod on your own cluster can still be painful when you only want to speed up your training progress by scaling out your cluster. Therefore, Azure Machine Learning compute provides a wrapper that does all the heavy lifting for you, requiring only a training script with Horovod annotations. We will see this in the <em class="italic">Training models with Horovod on Azure Machine Learning</em> section.</p>
			<p>Model-parallel training can be combined with Horovod by using the model-parallel features of the underlying framework and using only one Horovod process per machine instead of per GPU. However, this is a custom configuration and is currently not supported in Azure Machine Learning.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/>Implementing the HorovodRunner API for a Spark job</h2>
			<p>In many companies, ML is an <a id="_idIndexMarker1459"/>additional data processing step on top of existing data pipelines. Therefore, if you have huge amounts of data and you are already managing Spark clusters or using <a id="_idIndexMarker1460"/>Azure Databricks to process that data, it is easy to also add distributed training capabilities.</p>
			<p>As we have seen in the <em class="italic">Exploring methods for distributed ML</em> section of this chapter, we can simply train multiple models using parallelization or by partitioning the training data. However, we could also train DL models and benefit from distributed ML techniques to speed up the training process.</p>
			<p>When using the Databricks ML runtime, you can leverage Horovod for Spark to distribute your training process. This <a id="_idIndexMarker1461"/>functionality is available through the <strong class="source-inline">HorovodRunner</strong> API and is powered by Spark's barrier-mode execution engine to provide a stable communication backend for long-running jobs. Using <strong class="source-inline">HorovodRunner</strong> on the head node, it will send the training function to the workers and start the function using the MPI backend. This all happens under the hood within the Spark process.</p>
			<p>Again, this is one of the reasons why Horovod is quite easy to use, as it is literally just a drop-in replacement for your current optimizer. Imagine that you usually run your Keras model on Azure Databricks using the PySpark engine; however, you would like to add Horovod to speed up the training process by leveraging other machines in the cluster and splitting the gradient descent over multiple machines. In order to do so, you would have to add literally only two lines of code to the example from the previous section, as seen here:</p>
			<p class="source-code">hr = HorovodRunner(np=2)</p>
			<p class="source-code">def train():</p>
			<p class="source-code">    # Perform your training here..</p>
			<p class="source-code">    import horovod.keras as hvd</p>
			<p class="source-code">    hvd.init()</p>
			<p class="source-code">    ...</p>
			<p class="source-code">hr.run(train)</p>
			<p>In the preceding code snippet, we observe that we only need to initialize <strong class="source-inline">HorovodRunner()</strong> with the number of worker nodes. Calling the <strong class="source-inline">run()</strong> method with the training function will automatically start the new workers and the MPI communication backend and will <a id="_idIndexMarker1462"/>send the training code to the workers, executing the training in parallel. Therefore, you can now add data-parallel training to your long-running<a id="_idIndexMarker1463"/> Spark ML jobs.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/>Training models with Horovod on Azure Machine Learning</h2>
			<p>One of the benefits of moving to<a id="_idIndexMarker1464"/> a cloud service is that you can consume functionality as a service rather than managing<a id="_idIndexMarker1465"/> infrastructure on your own. Good examples are managed databases, lambda functions, managed Kubernetes, or container instances, where choosing a managed service means that you can focus on your application code while the infrastructure is managed for you in the cloud.</p>
			<p>The Azure Machine Learning service sits in a similar spot where you can consume many of the different functionalities through an SDK (such as model management, optimization, training, and deployments) so that you don't have to maintain an ML cluster infrastructure. This brings a huge benefit when it comes to speeding up DNNs through distributed ML. If you have stuck with Azure Machine Learning compute until now, then moving to data-parallel training is as difficult as adding a single parameter to your training configuration—for any of the various choices discussed in this chapter.</p>
			<p>Let's think about running the Keras training script in data-parallel mode using a Horovod optimizer in a distributed <a id="_idIndexMarker1466"/>environment. You <a id="_idIndexMarker1467"/>need to make sure all the correct versions of your tools are set up (from <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) to <strong class="bold">CUDA Deep Neural Network</strong> (<strong class="bold">cuDNN</strong>), GPUDirect, MPI, Horovod, TensorFlow, and Keras) and play together nicely with your current operating system and hardware. Then, you need to distribute the training code to all machines, start the MPI process, and then call the script using Horovod and the relevant command-line argument on every machine in the cluster. And we haven't even talked about authentication, data access, or auto-scaling.</p>
			<p>With Azure Machine Learning, you get an ML environment that just works and will be kept up to date for you. Let's take a look at the previous Horovod and Keras training script, which we stored in a <strong class="source-inline">train.py</strong> file. Now, similar to the previous chapters, we create an estimator to wrap the training call for the Azure Machine Learning SDK. To enable multi-GPU data-parallel<a id="_idIndexMarker1468"/> training using Horovod and the MPI backend, we simply add the relevant parameters. The<a id="_idIndexMarker1469"/> resulting script looks like this:</p>
			<p class="source-code">from azureml.core import ScriptRunConfig</p>
			<p class="source-code">from azureml.core.runconfig import MpiConfiguration</p>
			<p class="source-code">run_config = get_run_config(aml_cluster, [</p>
			<p class="source-code">    'numpy', 'pandas', 'scikit-learn', 'joblib',</p>
			<p class="source-code">    'tensorflow', 'horovod'])</p>
			<p class="source-code">distr_config = MpiConfiguration(process_count_per_node=1,</p>
			<p class="source-code">                                node_count=2)</p>
			<p class="source-code">src = ScriptRunConfig(source_directory=script_folder,</p>
			<p class="source-code">                      script='train.py',</p>
			<p class="source-code">                      run_config=run_config,</p>
			<p class="source-code">                      arguments=script_params</p>
			<p class="source-code">                      distributed_job_config=distr_config)</p>
			<p>Using the <strong class="source-inline">use_gpu</strong> flag, we can enable GPU-specific machines and their corresponding images with precompiled binaries for our Azure Machine Learning compute cluster. Using <strong class="source-inline">node_count</strong> and <strong class="source-inline">process_count_per_node</strong>, we specify the level of concurrency for the data-parallel training, where <strong class="source-inline">process_count_per_node</strong> should correspond with the number of GPUs available per node. Finally, we set the <strong class="source-inline">distributed_backend</strong> parameter to <strong class="source-inline">mpi</strong> to enable the MPI communication backend for this estimator. Another possible option would be using <strong class="source-inline">ps</strong> to enable the TensorFlow <strong class="source-inline">ParameterServer</strong> backend.</p>
			<p>Finally, to start up the job, we simply submit the experiment, which will automatically set up the MPI session on each node and call the training script with the relevant arguments for us. I don't know <a id="_idIndexMarker1470"/>how you feel about this, but for me, this is a really big step forward from the previous manual examples. The following line of code shows how you can submit the experiment:</p>
			<p class="source-code">run = experiment.submit(src)</p>
			<p>Wrapping your training as part of an Azure Machine Learning estimator gives you the benefit of fine-tuning <a id="_idIndexMarker1471"/>your training script configuration for multiple environments, be it multi-GPU data-parallel models for distributed gradient descent training or single-node instances for fast inference. By combining distributed DL with Azure Machine Learning compute auto-scaling clusters, you can get the most from the cloud by using pre-built managed services instead of manually fiddling with infrastructure and configurations.</p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor201"/>Summary</h1>
			<p>Distributed ML is a great approach to scaling out your training infrastructure in order to gain speed in your training process. It is applied in many real-world scenarios and is very easy to use with Horovod and Azure Machine Learning.</p>
			<p>Parallel execution is similar to hyperparameter searching, while distributed execution is similar to Bayesian optimization, which we discussed in detail in the previous chapter. Distributed executions need methods to perform communication (such as one-to-one, one-to-many, many-to-one, and many-to-many) and synchronization (such as barrier synchronization) efficiently. These so-called collective algorithms are provided by communication backends (MPI, Gloo, and NCCL) and allow efficient GPU-to-GPU communication.</p>
			<p>DL frameworks build higher-level abstractions on top of communication backends to perform model-parallel and data-parallel training. In data-parallel training, we partition the input data to compute multiple independent parts of the model on different machines and add up the results in a later step. A common technique in DL is distributed gradient descent, where each node performs gradient descent on a partition of the input batch, and a master collects all the separate gradients to compute the overall average gradient of the combined model. In model-parallel training, you distribute a single model over multiple machines. This is often the case when a model doesn't fit into the GPU memory of a single GPU.</p>
			<p>Horovod is an abstraction on top of existing optimizers of other ML frameworks, such as TensorFlow, Keras, PyTorch, and Apache MXNet. It provides an easy-to-use interface to add data-distributed training to an existing model without many code changes. While you could run Horovod on a standalone cluster, the Azure Machine Learning service provides good integration by wrapping its functionality as an estimator object. You learned how to run Horovod on an Azure Machine Learning compute cluster to speed up your training process through distributed ML with a few lines of Horovod initialization and a wrapper over the current optimizer.</p>
			<p>In the next chapter, we will use all the knowledge from the previous chapters to train recommendation engines on Azure. Recommendation engines often build on top of other NLP feature extraction or classification models and hence combine many of the techniques we have learned about so far.</p>
		</div>
	</div>
</div>
</body></html>