- en: 'Chapter 7: Supervised Machine Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：监督机器学习
- en: As you begin to progress your career and skill set in the field of data science,
    you will encounter many different types of models that fall into one of the two
    categories of either supervised or unsupervised learning. Recall that in applications
    of unsupervised learning, models are generally trained to either cluster or transform
    data in order to group or reshape data to extract insights when labels are not
    available for the given dataset. Within this chapter, we will now discuss the
    applications of **supervised learning** as they apply to the areas of classification
    and regression to develop powerful predictive models to make educated guesses
    about a dataset's labels.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始在数据科学领域提升你的职业和技能时，你将遇到许多不同类型的模型，这些模型可以分为监督学习或无监督学习两大类。回想一下，在无监督学习的应用中，模型通常被训练来聚类或转换数据，以便将数据分组或重塑以提取洞察，当给定数据集没有标签时。在本章中，我们将现在讨论监督学习的应用，这些应用适用于分类和回归领域，以开发强大的预测模型，对数据集的标签做出明智的猜测。
- en: 'Over the course of this chapter, we will discuss the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们将讨论以下主题：
- en: Understanding supervised learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监督学习
- en: Measuring success in supervised machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量监督机器学习中的成功
- en: Understanding classification in supervised machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监督机器学习中的分类
- en: Understanding regression in supervised machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监督机器学习中的回归
- en: With our objectives in mind, let's now go ahead and get started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 带着我们的目标，现在让我们开始吧。
- en: Understanding supervised learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督学习
- en: As you begin to explore data science either on your own or within an organization,
    you will often be asked the question, *What exactly does supervised machine learning
    mean?* Let's go ahead and come up with a definition. We can define supervised
    learning as a general subset of machine learning in which data, like its associated
    labels, is used to train models that can learn or generalize from the data to
    make predictions, preferably with a high degree of certainty. Thinking back to
    [*Chapter 5*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082), *Introduction to
    Machine Learning*, we can recall the example we completed concerning the breast
    cancer dataset in which we classified tumors as being either malignant or benign.
    This example, alongside the definition we created, is an excellent way to learn
    and understand the meaning behind supervised learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始探索数据科学，无论是自己还是在一个组织中，你经常会遇到这样的问题：“监督机器学习究竟是什么意思？”让我们继续并给出一个定义。我们可以将监督学习定义为机器学习的一个一般子集，其中数据（以及其关联的标签）被用来训练模型，这些模型可以从数据中学习或泛化以做出预测，最好是具有高度的确定性。回顾一下[*第5章*](B17761_05_Final_JM_ePub.xhtml#_idTextAnchor082)，*机器学习简介*，我们可以回忆起我们完成的关于乳腺癌数据集的例子，其中我们将肿瘤分类为恶性或良性。这个例子，加上我们创建的定义，是学习和理解监督学习含义的极好方式。
- en: With the definition of supervised machine learning now in our minds, let's go
    ahead and talk about its different subtypes, namely, classification and regression.
    If you recall, **classification** within the scope of machine learning is the
    act of predicting a **category** for a given set of data, such as classifying
    a tumor as malignant or benign, an email as spam or not spam, or even a protein
    as alpha or beta. In each of these cases, the model will output a **discrete**
    value. On the other hand, **regression** is the prediction of an **exact value**
    using a given set of data, such as the lipophilicity of a small molecule, the
    isoelectric point of a **Monoclonal Antibody** (**mAb**), or the LCAP of an LCMS
    peak. In each of these cases, the model will output a **continuous** value.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们心中有了监督机器学习的定义后，让我们继续讨论它的不同子类型，即分类和回归。如果你还记得，在机器学习的范围内，**分类**是指为给定的一组数据预测一个**类别**，例如将肿瘤分类为恶性或良性，将电子邮件分类为垃圾邮件或非垃圾邮件，甚至将蛋白质分类为α或β。在这些案例中，模型将输出一个**离散**值。另一方面，**回归**是使用给定的一组数据预测一个**确切值**，例如小分子的亲脂性、单克隆抗体（**mAb**）的等电点或LCMS峰的LCAP。在这些案例中，模型将输出一个**连续**值。
- en: 'Many different models exist within the two categories of supervised learning.
    Within the scope of this book, we will focus on four main models for each of these
    two categories. When it comes to classification, we will discuss **K-Nearest Neighbor**
    (**KNN**), **Support Vector Machines** (**SVMs**), **d****ecision trees**, and
    **random forests**, as well as XGBoost classification. When it comes to regression,
    we will discuss **linear regression**, **logistic regression**, **random forest
    regression**, and even **gradient boosting regression**. We can see these depicted
    in *Figure 7.1*:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的两个类别中存在许多不同的模型。在本书的范围内，我们将重点关注这两个类别中的每个类别的主要四种模型。在分类方面，我们将讨论**K-最近邻**（**KNN**）、**支持向量机**（**SVMs**）、**决策树**和**随机森林**，以及XGBoost分类。在回归方面，我们将讨论**线性回归**、**逻辑回归**、**随机森林回归**，甚至**梯度提升回归**。我们可以在*图
    7.1*中看到这些：
- en: '![Figure 7.1 – The two areas of supervised machine learning ](img/B17761_07_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 监督机器学习的两个领域](img/B17761_07_001.jpg)'
- en: Figure 7.1 – The two areas of supervised machine learning
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 监督机器学习的两个领域
- en: Our main objective in each of these models is to train a new instance of that
    model for a particular dataset. We will **fit** our model with the data, and **tune**
    or adjust the parameters to give us the best outcomes. To determine what the best
    outcomes should be, we will need to know how to measure success within our models.
    We will learn about that in the following section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些模型中，我们的主要目标是针对特定数据集训练该模型的新实例。我们将用数据**拟合**我们的模型，并**调整**或调整参数以获得最佳结果。为了确定最佳结果应该是什么，我们需要知道如何在模型中衡量成功。我们将在下一节中了解这一点。
- en: Measuring success in supervised machine learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督机器学习中的成功衡量
- en: As we begin to train our supervised classifiers and regressors, we will need
    to implement a few ways to determine which models are performing better, thus
    allowing us to effectively tune the model's parameters and maximize its performance.
    The best way to achieve this is to understand what success looks like ahead of
    time before diving into the model development process. There are many different
    methods for measuring success depending on the situation. For example, accuracy
    can be a good metric for classifiers, but not regressors. Similarly, a business
    case for a classifier may not necessarily require accuracy to be the primary metric
    of interest. It simply depends on the situation at hand. Let's take a look at
    some of the most common metrics used for each of the fields of **classification**
    and **regression**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始训练监督分类器和回归器时，我们需要实施几种方法来确定哪些模型表现更好，从而有效地调整模型的参数并最大化其性能。实现这一目标的最有效方法是提前了解成功的外观，然后再深入到模型开发过程。根据情况，有许多不同的方法可以衡量成功。例如，准确率可以是一个好的分类器指标，但不适用于回归器。同样，一个分类器的业务案例可能不一定需要准确率成为主要关注的指标。这完全取决于具体情况。让我们看看**分类**和**回归**领域中常用的某些最常见指标。
- en: '![Figure 7.2 – Common success metrics for regression and classification ](img/B17761_07_002.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 回归和分类的常见成功指标](img/B17761_07_002.jpg)'
- en: Figure 7.2 – Common success metrics for regression and classification
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 回归和分类的常见成功指标
- en: Although there are many other metrics you can use for a given scenario, the
    eight listed in *Figure 7.2* are some of the most common ones you will likely
    encounter. Selecting a given metric can be difficult as it should always align
    with the given use case. Let's go ahead and explore this when it comes to classification.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对于特定场景你可以使用许多其他指标，但*图 7.2*中列出的八种可能是你可能会遇到的最常见的一些。选择一个特定的指标可能很困难，因为它应该始终与给定的用例相一致。让我们在讨论分类时进一步探讨这一点。
- en: Measuring success with classifiers
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分类器衡量成功
- en: 'Take, for example, the tumor dataset we have worked with thus far. We defined
    our success metric as accuracy, and therefore maximizing accuracy was our model''s
    main training objective. This, however, is not always the case, and the success
    metric you choose to use will almost always be dependent on both the model and
    the business problem at hand. Let''s go ahead and take a closer look at some metrics
    commonly used in the data science space and define them:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们迄今为止所使用的肿瘤数据集为例。我们定义我们的成功指标为准确率，因此最大化准确率是我们的模型的主要训练目标。然而，这并不总是如此，你选择的成功指标几乎总是取决于模型和手头的业务问题。让我们进一步仔细看看数据科学领域常用的某些指标，并定义它们：
- en: '**Accuracy**: A measurement that agrees closely with the accepted value'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确度**：一种与公认值非常一致的度量'
- en: '**Precision**: A measurement that agrees with other measurements in the sense
    that they are similar to one another'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：一种与其他测量结果相似性的度量，即它们彼此相似'
- en: 'An easier way to think about accuracy and precision is by picturing the results
    displayed using a bullseye depiction. The difference between precision and accuracy
    is in the sense of both how close the results are to one another, and how close
    the results are to their true or actual values, respectively. We can see a visual
    depiction of this in *Figure 7.3*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑准确度和精确度的更简单的方法是通过想象使用靶心表示法显示的结果。精确度和准确度的区别在于结果彼此之间有多接近，以及结果与它们的真实或实际值有多接近。我们可以在*图
    7.3*中看到这种视觉表示：
- en: '![Figure 7.3 – Graphical illustration of the difference between accuracy and
    precision ](img/B17761_07_003.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 精确度和精确度差异的图形说明](img/B17761_07_003.jpg)'
- en: Figure 7.3 – Graphical illustration of the difference between accuracy and precision
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 精确度和精确度差异的图形说明
- en: 'In addition to the visual depiction, we can also think of precision as a calculation
    representing the results as subsets relative to a total population. In this case,
    we will also need to define a new metric known as **recall**. We can think of
    recall and precision mathematically in the context of positive and negative results
    through what is known as a **confusion matrix**. When comparing the results of
    a prediction relative to the actual results, we can get a good sense of the model''s
    performance by comparing a few of these values. We can see a visual depiction
    of this in *Figure 7.4*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 除了视觉表示外，我们还可以将精确度视为一个计算，它将结果表示为相对于总体人群的子集。在这种情况下，我们还需要定义一个新的指标，称为**召回率**。我们可以在正负结果的上下文中通过所谓的**混淆矩阵**来数学地思考召回率和精确度。当比较预测结果与实际结果时，通过比较这些值中的几个，我们可以很好地了解模型的表现。我们可以在*图
    7.4*中看到这种视觉表示：
- en: '![Figure 7.4 – Graphical illustration of a confusion matrix ](img/B17761_07_004.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 混淆矩阵的图形说明](img/B17761_07_004.jpg)'
- en: Figure 7.4 – Graphical illustration of a confusion matrix
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 混淆矩阵的图形说明
- en: 'With this table in mind, we can define **recall** as the fraction of fraudulent
    cases that a given model identifies, or, from a mathematical perspective, we can
    define it as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个表格，我们可以将**召回率**定义为给定模型识别的欺诈案例的比例，或者从数学角度来看，我们可以将其定义为如下：
- en: '![](img/B17761_Formula_07_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17761_Formula_07_001.jpg)'
- en: 'Whereas we can define **precision** in the same context as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 而我们可以在相同语境下定义**精确度**如下：
- en: '![](img/B17761_Formula_07_002.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17761_Formula_07_002.jpg)'
- en: 'We can visualize accuracy, precision, and recall in a similar manner in the
    following diagram, in which each metric represents a specific calculation of the
    overall results:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用类似的方式在以下图表中可视化准确度、精确度和召回率，其中每个指标代表整体结果的具体计算：
- en: '![Figure 7.5 – Graphical illustration explaining accuracy, precision, and recall
    ](img/B17761_07_005.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 解释精确度、精确度和召回率的图形说明](img/B17761_07_005.jpg)'
- en: Figure 7.5 – Graphical illustration explaining accuracy, precision, and recall
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 解释精确度、精确度和召回率的图形说明
- en: 'Finally, there is one last commonly used metric that is usually considered
    to be a loose *combination* of precision and recall known as the **F1 score**.
    We can define the *F1 score* as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一个常用的指标，通常被认为是精确度和召回率的松散*组合*，称为**F1 分数**。我们可以将*F1 分数*定义为如下：
- en: '![](img/B17761_Formula_07_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17761_Formula_07_003.jpg)'
- en: So how do you determine which metric to use? There is no *best* metric that
    you should always use as it is highly dependent on each situation. When determining
    the best metric, you should always ask yourself, *What is the main objective for
    the model, as well as the business?* In the eyes of the model, accuracy may be
    the best metric. On the other hand, in the eyes of the business, recall may be
    the best metric.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何确定使用哪个指标呢？没有一种*最佳*的指标是你应该始终使用的，因为它高度依赖于每种情况。在确定最佳指标时，你应该始终问自己，*模型以及业务的主要目标是什么？*在模型的眼中，精确度可能是最好的指标。另一方面，在业务的眼中，召回率可能是最好的指标。
- en: Ultimately, recall could be considered more useful when overlooked cases (defined
    above as false negatives) are more important. Consider, for example, a model that
    is predicting a patient's diagnosis – we would likely care more about false negatives
    than false positives. On the other hand, precision can be more important when
    false positives are more costly to us. It all depends on the given business case
    and requirements. So far, we have investigated success as it relates to classification,
    so let's now investigate these ideas as they relate to regression.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，当被忽视的案例（如上所述为假阴性）更重要时，召回率可能更有用。例如，考虑一个预测患者诊断的模型 – 我们可能更关心假阴性而不是假阳性。另一方面，当假阳性对我们来说成本更高时，精确度可能更重要。这完全取决于给定的业务案例和需求。到目前为止，我们已经研究了与分类相关的成功，现在让我们研究这些想法与回归相关的内容。
- en: Measuring success with regressors
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用回归器衡量成功
- en: Although we have not yet taken a deep dive into the field of regression, we
    have defined the main idea as the development of a model whose output is a continuous
    numerical value. Take, for example, the molecular toxicity dataset containing
    many columns of data whose values are all continuous floats. Hypothetically, you
    could use this dataset to make predictions on the **Total Polar Surface Area**
    (**TPSA**). In this case, the metrics of accuracy, precision, and recall would
    not be the most useful to us to best understand the performance of our models.
    Alternatively, we will need some metrics better catered to continuous values.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们还没有深入研究回归领域，但我们已经定义了主要思想，即开发一个输出为连续数值的模型。以包含许多连续浮点数值的分子毒性数据集为例。假设你可以使用这个数据集来预测
    **总极性表面积**（**TPSA**）。在这种情况下，准确度、精确度和召回率的指标对我们最好理解模型性能并不是最有用的。相反，我们需要一些更适合连续值的指标。
- en: 'One of the most common metrics for defining success in many models (not necessarily
    machine learning) is the **Pearson correlation coefficient**, also known as **R2**.
    This calculation is a common method used to measure the linearity of data, as
    it represents the proportion of variance in the dependent variable. We can define
    **R2** as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多模型（不一定是机器学习）中定义成功最常用的指标之一是 **皮尔逊相关系数**，也称为 **R2**。这种计算是衡量数据线性度的常用方法，因为它代表了因变量变差的比例。我们可以如下定义
    **R2**：
- en: '![](img/B17761_Formula_07_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![公式 7.4](img/B17761_Formula_07_004.jpg)'
- en: In this equation, ![](img/B17761_Formula_07_005.png) is the predicted value,
    and ![](img/B17761_Formula_07_006.png) is the mean value.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![公式 7.5](img/B17761_Formula_07_005.png) 是预测值，而![公式 7.6](img/B17761_Formula_07_006.png)
    是平均值。
- en: 'Take, for example, a dataset in which experimental (actual) values were known,
    and predicted values were calculated. We could plot the graphs of these values
    against one another and measure the correlation. In theory, a perfect model would
    have an ideal correlation (as close to a value of 1.00 as possible). We can see
    a depiction of high and low correlation in *Figure 7.6*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个已知实验（实际）值和预测值的示例数据集为例，我们可以绘制这些值相互对应的图表并测量相关性。理论上，一个完美的模型将会有一个理想的关联度（尽可能接近
    1.00 的值）。我们可以在 *图 7.6* 中看到高关联度和低关联度的描述：
- en: '![Figure 7.6 – Difference between high and low correlation in scatter plots
    ](img/B17761_07_006.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 散点图中高相关与低相关的差异](img/B17761_07_006.jpg)'
- en: Figure 7.6 – Difference between high and low correlation in scatter plots
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 散点图中高相关与低相关的差异
- en: 'Although this metric can give you a good estimate of a model''s performance,
    there are a few others that can give you a better sense of the model''s error:
    **Mean Absolute Error** (**MAE**), **Mean Squared Error** (**MSE**), and **Root
    Mean Squared Error** (**RMSE**). Let''s go ahead and define these:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个指标可以给你一个关于模型性能的良好估计，但还有一些其他指标可以给你更好的模型误差感：**平均绝对误差**（**MAE**）、**平均平方误差**（**MSE**）和**均方根误差**（**RMSE**）。让我们继续定义这些：
- en: '**MAE**: The average of the absolute differences between the actual and predicted
    values in a given dataset. This measure tends to be more robust when handling
    datasets with outliers:![](img/B17761_Formula_07_007.jpg)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAE**：给定数据集中实际值和预测值之间绝对差异的平均值。当处理含有异常值的数据集时，这个度量通常更稳健：![公式 7.7](img/B17761_Formula_07_007.jpg)'
- en: In which ![](img/B17761_Formula_07_008.png) is the predicted value, and ![](img/B17761_Formula_07_009.png)
    is the mean value.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ![公式 7.8](img/B17761_Formula_07_008.png) 是预测值，而 ![公式 7.9](img/B17761_Formula_07_009.png)
    是平均值。
- en: '**MSE**: The average of the squared differences between the actual and predicted
    values in a given dataset:![](img/B17761_Formula_07_010.jpg)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MSE**：给定数据集中实际值和预测值之间平方差的平均值：![公式 7.1](img/B17761_Formula_07_010.jpg)'
- en: '**RMSE**: Square root of the MSE to measure the standard deviation of the values.
    This metric is commonly used to compare regression models against each other:![](img/B17761_Formula_07_011.jpg)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE**：MSE的平方根，用于衡量值的标准差。这个指标通常用于比较回归模型：![公式 7.2](img/B17761_Formula_07_011.jpg)'
- en: When it comes to regression, there are many different metrics you can use depending
    on the given situation. In most regression models, RMSE is generally used to compare
    the performance of multiple models as it is quite simple to calculate and differentiable.
    On the other hand, datasets with outliers are generally compared to one another
    using MSE and MAE. Now that we have gained a better sense of measuring success
    through various metrics, let's now go ahead and explore the area of classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到回归时，你可以根据给定情况使用许多不同的指标。在大多数回归模型中，RMSE通常用于比较多个模型的性能，因为它计算简单且可微分。另一方面，具有异常值的数据集通常使用MSE和MAE进行比较。现在我们已经更好地理解了通过各种指标来衡量成功，那么我们现在可以继续探索分类领域。
- en: Understanding classification in supervised machine learning
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督机器学习中的分类
- en: Classification models in the context of machine learning are supervised models
    whose objectives are to classify or categorize items based on previously learned
    examples. You will encounter classification models in many forms as they tend
    to be some of the most common models used in the field of data science. There
    are three main types of classifiers that we can develop based on the outputs of
    the model.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，分类模型是监督模型，其目标是根据先前学习到的例子对项目进行分类或归类。你将遇到许多形式的分类模型，因为它们往往是数据科学领域中使用的最常见模型之一。我们可以根据模型的输出开发三种主要的分类器。
- en: '![Figure 7.7 – The three types of supervised classification ](img/B17761_07_007.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 三种监督分类类型](img/B17761_07_007.jpg)'
- en: Figure 7.7 – The three types of supervised classification
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 三种监督分类类型
- en: The first type is known as a **binary classifier**. As the name suggests, this
    is a classifier that predicts in a binary fashion in the sense that an output
    is one of two options, such as emails being spam or not spam, or molecules being
    toxic or not toxic. There is no third option in either of these cases, rendering
    the model a binary classifier.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种被称为**二元分类器**。正如其名所示，这是一种以二元方式预测的分类器，即输出是两个选项之一，例如电子邮件是否为垃圾邮件，或分子是否有毒。在这两种情况下都没有第三种选择，这使得模型成为二元分类器。
- en: The second type of classifier is known as a **multiclass classifier**. This
    type of classifier is trained on more than two different outputs. For example,
    many types of proteins can be classified based on structure and function. Some
    of these examples include structural proteins, enzymes, hormones, storage proteins,
    and toxins. Developing a model that would predict the type of protein based on
    some of the protein's characteristics would be regarded as a multiclass classifier
    in the sense that each row of data could have only one possible class or output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种分类器被称为**多类分类器**。这种分类器是在超过两个不同的输出上进行训练的。例如，许多类型的蛋白质可以根据结构和功能进行分类。这些例子包括结构蛋白、酶、激素、储存蛋白和毒素。开发一个基于蛋白质的一些特性的模型来预测蛋白质类型，将被视为多类分类器，因为在数据集中的每一行数据只能有一个可能的类别或输出。
- en: Finally, we also have **multilabel classifiers**. These classifiers, unlike
    their multiclass counterparts, are able to predict multiple outputs for a given
    row of data. For example, when screening patients for clinical trials, you may
    want to build patient profiles using many different types of labels, such as gender,
    age, diabetic status, and smoker status. When trying to predict what a certain
    group of patients might look like, we need to be able to predict all of these
    labels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还有**多标签分类器**。这些分类器与它们的多元对应物不同，能够为给定行数据预测多个输出。例如，在筛选临床试验患者时，你可能希望使用许多不同类型的标签来构建患者档案，如性别、年龄、糖尿病状态和吸烟状态。在尝试预测某个患者群体可能的样子时，我们需要能够预测所有这些标签。
- en: Now that we have broken down classification into a few different types, you
    are likely thinking about the many different areas in projects you are working
    on where a classifier may be of great value. The good news here is that many of
    the standard or popular classification models we are about to explore can be easily
    recycled and fitted with new data. As we begin to explore the many different models
    in the following section, think about the projects that you are working on and
    the datasets you have available, and which models they may fit the best.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将分类分解为几种不同类型，你可能正在考虑你在项目中工作的许多不同领域，在这些领域中分类器可能非常有价值。好消息是，我们即将探索的许多标准或流行分类模型可以很容易地回收并使用新数据拟合。在我们开始探索下一节中的许多不同模型时，请考虑你正在工作的项目和你拥有的数据集，以及哪些模型最适合它们。
- en: Exploring different classification models
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索不同的分类模型
- en: 'As we explore a number of machine learning models, we will test out their performances
    on a new dataset concerning *single-cell RNA sequences*, published by *Nestorowa
    et al.* in 2016\. We will focus on using this structured dataset in order to develop
    a number of different classifiers. Let''s go ahead and import the data and prepare
    it for the classification models. First, we will go ahead and import our dataset
    of interest using the `read_csv()` function in `pandas`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索多种机器学习模型的过程中，我们将测试这些模型在2016年由Nestorowa等人发布的关于*单细胞RNA序列*的新数据集上的性能。我们将专注于使用这个结构化数据集来开发多种不同的分类器。让我们继续导入数据并为分类模型做准备。首先，我们将使用`pandas`中的`read_csv()`函数导入我们感兴趣的数据集：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will use the index to isolate our labels (classes) for each of the
    rows, using the first four characters of each row:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用索引来隔离每行的标签（类别），使用每行的前四个字符：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can use the `head()` function to take a look at the data. What we will notice
    is that there is more than 3,992 columns'' worth of data. As any good data scientist
    knows, developing models with too many columns will lead to many inefficiencies,
    and therefore it would be best to reduce these down using an unsupervised learning
    technique, such as `StandardScaler` class in `sklearn`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`head()`函数查看数据。我们会注意到有超过3,992列的数据。正如任何优秀的数据科学家都知道的，使用太多列开发模型会导致许多低效，因此最好使用无监督学习技术（如`sklearn`中的`StandardScaler`类）来减少这些列：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we can apply PCA to reduce our dataset from 3,992 columns down to 15:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以应用PCA将数据集从3,992列减少到15列：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With the data now in a much more reduced state, we can check the **explained
    variance ratio** to see how this compares with the original dataset. We will see
    that the sum of all columns totals 0.17, which is relatively low. We will want
    to aim for a value around 0.8, so let''s go ahead and increase the total number
    of columns in order to increase the percentage of variance:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据状态大大减少，我们可以检查**解释方差比**以查看这与原始数据集相比如何。我们将看到所有列的总和为0.17，相对较低。我们希望达到大约0.8的值，因此让我们继续增加列的总数以提高方差百分比：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: With the PCA model applied, we managed to reduce the total number of columns
    by roughly 77%.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 应用PCA模型后，我们成功将列总数减少了大约77%。
- en: 'With this completed, we are now prepared to split our dataset using the `train_test_split()`
    class:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们现在准备使用`train_test_split()`类来分割我们的数据集：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: With the dataset now split into training and test sets, we are now ready to
    begin the classification model development process!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已分为训练集和测试集，我们已准备好开始分类模型开发过程！
- en: K-Nearest Neighbors
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K-最近邻
- en: One of the classic, easy-to-develop, and most commonly discussed classification
    models is known as the **KNN** model, first developed by Evelyn Fix and Joseph
    Hodges in 1951\. The main idea behind this model is determining class membership
    based on proximity to the closest neighbors. Take, for example, a 2D **binary**
    dataset in which items are classified as either A or B. As a new dataset is added,
    the model will determine its membership or class based on its proximity (usually
    **Euclidean**) to other items in the same dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 经典、易于开发且最常讨论的分类模型之一被称为**KNN**模型，该模型最早由Evelyn Fix和Joseph Hodges于1951年开发。该模型背后的主要思想是基于最近邻的邻近度来确定类别成员资格。以一个二维**二值**数据集为例，其中项目被分类为A或B。当添加新的数据集时，该模型将根据其与其他项目（通常是**欧几里得**距离）的邻近度来确定其成员资格或类别。
- en: '![Figure 7.8 – Graphical representation of the KNN model ](img/B17761_07_008.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – KNN模型的图形表示](img/B17761_07_008.jpg)'
- en: Figure 7.8 – Graphical representation of the KNN model
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – KNN模型的图形表示
- en: 'KNN is regarded as one of the easiest machine learning models to develop and
    implement given its simple nature and clever design. The model, although simple
    in application, does require some tuning in order to be fully effective. Let''s
    go ahead and explore the use of this model for the single-cell RNA classification
    dataset:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其简单性和巧妙的设计，KNN被认为是最容易开发和实现的机器学习模型之一。虽然模型在应用上很简单，但为了完全有效，确实需要一些调整。让我们继续探索这个模型在单细胞RNA分类数据集上的应用：
- en: 'We can begin by importing the `KNeighborsClassifier` model from `sklearn`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以从导入`sklearn`中的`KNeighborsClassifier`模型开始：
- en: '[PRE6]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we can instantiate a new instance of this model in Python, the number
    of neighbors to a value of `5`, and fit the model to our training data:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以在Python中实例化这个模型的新实例，将邻居的数量设置为`5`，并将模型拟合到我们的训练数据：
- en: '[PRE7]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With the model fit, we can now go ahead and predict the outcomes of the model
    and set those to a variable we will call `y_pred`, and finally use the `classification_report`
    function to see the results:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型拟合后，我们现在可以预测模型的输出，并将这些输出设置为我们称之为`y_pred`的变量，并最终使用`classification_report`函数查看结果：
- en: '[PRE8]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the classification report function, we can get a sense of the precision,
    recall, and `F1` scores for each of the three classes. We can see that the precision
    was relatively high for the `LT.H` class, but slightly lower for the other two.
    Alternatively, `recall` was very low for the `LT.H` class, but quite high for
    the `Prog` class. In total, an average precision of `0.63` was calculated for
    this model:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用分类报告功能，我们可以了解每个类别的精确度、召回率和`F1`分数。我们可以看到，对于`LT.H`类，精确度相对较高，但其他两个类别的精确度略低。另一方面，对于`LT.H`类，召回率非常低，但对于`Prog`类却相当高。总的来说，这个模型的平均精确度为`0.63`：
- en: '![Figure 7.9 – Results of the KNN model ](img/B17761_07_009.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图7.9 – KNN模型的结果](img/B17761_07_009.jpg)'
- en: Figure 7.9 – Results of the KNN model
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.9 – KNN模型的结果
- en: With these results in mind, let's go ahead and tune one of the parameters, namely,
    the `n_neighbours` parameter in the range of `1`-`10`.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到这些结果，让我们继续调整一个参数，即`n_neighbours`参数，其范围在`1`到`10`之间。
- en: 'We can use a simple `for` loop to accomplish this:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用一个简单的`for`循环来完成这个任务：
- en: '[PRE9]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If we take a look at the results, we can see the number of `neighbors` as well
    as the overall model accuracy. Immediately, we notice that the option value based
    on this metric alone is `n=2`, giving an accuracy of approximately 60%.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们查看结果，我们可以看到`neighbors`的数量以及整体模型准确率。立即注意到，仅基于这个指标的最佳选项值是`n=2`，准确率大约为60%。
- en: '![Figure 7.10 – Results of the KNN model at different neighbors ](img/B17761_07_010.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 不同邻居数量的KNN模型结果](img/B17761_07_010.jpg)'
- en: Figure 7.10 – Results of the KNN model at different neighbors
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 不同邻居数量的KNN模型结果
- en: KNN is one of the simplest and fastest models for the development of classifiers;
    however, it is not always the best model for a complex dataset such as this one.
    You will notice that the results varied heavily from class to class, indicating
    the model was not able to effectively distinguish between them based on their
    proximity to other members alone. Let's go ahead and explore another model known
    as an SVM, which tries to classify items in a slightly different way.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是开发分类器中最简单和最快的模型之一；然而，对于像这样一个复杂的数据集，它并不总是最好的模型。你会注意到结果在各个类别之间差异很大，这表明模型无法仅根据它们与其他成员的邻近度来有效地区分它们。让我们继续探索另一种称为SVM的模型，它试图以稍微不同的方式对项目进行分类。
- en: Support Vector Machines
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**SVMs** are a class of supervised machine learning models commonly used for
    both classification and regression, first developed in 1992 by AT&T Bell Laboratories.
    The main idea behind SVMs is the ability to separate classes using a **hyperplane**.
    There are three main types of SVMs that you will likely hear about in discussions
    or encounter in the data science field: linear SVMs, polynomial SVMs, and RBF
    SVMs.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机（SVMs**）是一类常用的监督机器学习模型，用于分类和回归，最早由AT&T Bell实验室在1992年开发。SVMs背后的主要思想是使用**超平面**来分离类别。在讨论或数据科学领域中，你可能会听到或遇到三种主要的SVM类型：线性SVMs、多项式SVMs和RBF
    SVMs。'
- en: '![Figure 7.11 – Visual explanation of the different SVMs ](img/B17761_07_011.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 不同SVMs的视觉解释](img/B17761_07_011.jpg)'
- en: Figure 7.11 – Visual explanation of the different SVMs
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 不同SVMs的视觉解释
- en: The main idea behind the three models lies in how the classes are separated.
    For example, in **linear** models, the hyperplane is a linear line separating
    the two classes from each other. Alternatively, the hyperplane may consist of
    a **polynomial**, allowing the model to account for non-linear features. Finally,
    and most popularly, the model can use a **Radial Basis Function** (**RBF**) to
    determine a datapoint's membership, which is based on two parameters, **gamma**
    and **C**, which account for the decision region, how it is spread out, and the
    penalty for a misclassification. With this in mind, let's now take a closer look
    at the idea of a hyperplane.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 三种模型背后的主要思想在于如何分离类别。例如，在**线性**模型中，超平面是一条将两个类别分开的线性线。或者，超平面可能由**多项式**组成，使模型能够考虑非线性特征。最后，也是最流行的方法，模型可以使用**径向基函数**（**RBF**）来确定数据点的成员资格，这基于两个参数，**gamma**和**C**，它们负责决策区域、其分布以及误分类的惩罚。考虑到这一点，我们现在更详细地看看超平面的概念。
- en: 'The **hyperplane** is a function that attempts to clearly define and allow
    for the differentiation between classes in either a linear or non-linear fashion.
    The hyperplane can be described mathematically as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**超平面**是一个函数，试图以线性或非线性方式清楚地定义并允许区分类别。超平面可以用以下方式数学描述：'
- en: '![](img/B17761_Formula_07_012.jpg)![](img/B17761_Formula_07_013.jpg)![](img/B17761_Formula_07_014.jpg)![](img/B17761_Formula_07_015.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_012.jpg)![](img/B17761_Formula_07_013.jpg)![](img/B17761_Formula_07_014.jpg)![](img/B17761_Formula_07_015.jpg)'
- en: In which ![](img/B17761_Formula_07_016.png) are the vectors, ![](img/B17761_Formula_07_017.png)
    is the bias term, and ![](img/B17761_Formula_07_018.png) are the variables.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中![](img/B17761_Formula_07_016.png)是向量，![](img/B17761_Formula_07_017.png)是偏置项，![](img/B17761_Formula_07_018.png)是变量。
- en: Taking a quick break from the RNA dataset, let's go ahead and demonstrate the
    use of a linear support vector using the enrollment dataset in Python – a dataset
    concerning patient enrolment in which respondent data was summarized via `Likely`,
    `Very Likely`, or `Unlikely` to enroll. The main objective of a **linear SVM**
    is to *draw a line* clearly separating the data based on class.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从RNA数据集短暂休息一下，让我们继续使用Python中的enrollment数据集来演示线性支持向量机（SVM）的使用——这是一个关于患者入组的数据集，其中受访者的数据通过`Likely`（可能）、`Very
    Likely`（非常可能）或`Unlikely`（不可能）入组的方式进行总结。**线性SVM**的主要目标是**绘制一条线**，根据类别清晰地分离数据。
- en: 'Before we begin using SVM, let''s go ahead and import our dataset:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用SVM之前，让我们先导入我们的数据集：
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For simplicity, let''s eliminate the `Likely` class and keep the `Very Likely`
    and `Unlikely` classes:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，让我们消除`Likely`类别，保留`Very Likely`和`Unlikely`类别：
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s go ahead and draw a line separating the data shown in the scatter plot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一条分隔散点图中数据的线：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Upon executing this code, this yields the following diagram:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，得到以下图表：
- en: '![Figure 7.12 – Two clusters separated by an initial SVM hyperplane ](img/B17761_07_012.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12 – 由初始SVM超平面分隔的两个簇](img/B17761_07_012.jpg)'
- en: Figure 7.12 – Two clusters separated by an initial SVM hyperplane
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 由初始SVM超平面分隔的两个簇
- en: 'Notice within the plot that this linear line could be drawn in multiple ways
    with different slopes, yet still successfully separate the two classes within
    the dataset. However, as new datapoints begin to encroach toward the middle ground
    between the two clusters, the slope and location of the hyperplane will begin
    to grow in importance. One way to address this issue is by defining the slope
    and location of the plane based on the closest datapoints. If the line contained
    a margin of width *x* in relation to the closest datapoints, then a more improved
    `fill_between` function, as portrayed in the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在图中，这条线性线可以用不同的斜率以多种方式绘制，但仍能成功地在数据集内分隔两个类别。然而，当新的数据点开始向两个簇之间的中间地带靠近时，超平面的斜率和位置将开始变得重要。解决这个问题的方法之一是根据最近的数据点定义平面的斜率和位置。如果线包含与最近数据点相关的宽度为*x*的边界，那么一个更改进的`fill_between`函数，如以下代码所示：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Upon executing this code, this yields the following figure:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，得到以下图表：
- en: '![Figure 7.13 – Two clusters separated by an initial SVM hyperplane with specified
    margins ](img/B17761_07_013.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – 由指定边界的初始SVM超平面分隔的两个簇](img/B17761_07_013.jpg)'
- en: Figure 7.13 – Two clusters separated by an initial SVM hyperplane with specified
    margins
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 由指定边界的初始SVM超平面分隔的两个簇
- en: The datapoints from both classes that are within the margin width of the hyperplane
    are known as **support vectors**. The main intuition is the idea that the further
    away the support vectors are from the hyperplane, the higher the probability that
    a correct class is identified for a new datapoint.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 位于超平面边缘宽度内的来自两个类别的数据点被称为**支持向量**。主要直觉是，支持向量离超平面越远，正确识别新数据点的正确类别的概率就越高。
- en: 'We can train a new SVM classifier using the SVC class from the `scikit-learn`
    library. We begin by importing the class, splitting the data, and then training
    a model using the dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `scikit-learn` 库中的 SVC 类来训练一个新的 SVM 分类器。我们首先导入类，分割数据，然后使用数据集训练模型：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With that, we have now fitted our model to the dataset. As a final step, we
    can show the scatter plot, identify the hyperplane, and also specify which datapoints
    were the support vectors for this particular example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们现在已经将模型拟合到数据集上。作为最后一步，我们可以展示散点图，识别超平面，并指定哪些数据点是这个特定示例的支持向量：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Upon executing this code, this yields the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，得到以下图：
- en: '![Figure 7.14 – Two clusters separated by an initial SVM hyperplane with select
    support vectors ](img/B17761_07_014.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 由初始 SVM 超平面分隔的两个聚类，带有选定的支持向量](img/B17761_07_014.jpg)'
- en: Figure 7.14 – Two clusters separated by an initial SVM hyperplane with select
    support vectors
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 由初始 SVM 超平面分隔的两个聚类，带有选定的支持向量
- en: Now that we have gained a better understanding of how SVMs operate in relation
    to their hyperplanes using this basic example, let's go ahead and test out this
    model using the single-cell RNA classification dataset we have been working with.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过这个基本示例更好地理解了 SVM 如何相对于其超平面操作，让我们继续使用我们一直在工作的单细胞 RNA 分类数据集来测试这个模型。
- en: 'Following roughly the same steps as the KNN model, we will now implement the
    SVM model by first importing the library, instantiating the model with a linear
    kernel, fitting our training data, and subsequently making predictions on the
    test data:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 按照与 KNN 模型大致相同的步骤，我们现在将实现 SVM 模型，首先导入库，用线性核实例化模型，拟合我们的训练数据，然后在测试数据上做出预测：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Upon printing the report, this yields the following results:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 打印报告后，得到以下结果：
- en: '![Figure 7.15 – Results of the SVM model ](img/B17761_07_015.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.15 – SVM 模型的结果](img/B17761_07_015.jpg)'
- en: Figure 7.15 – Results of the SVM model
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 – SVM 模型的结果
- en: We can see that the model was, in fact, quite robust, with our dataset yielding
    some high metrics and giving us a total average precision of 88%. SVMs are fantastic
    models to use with complex datasets as their main objective is to separate data
    via a hyperplane. Let's now explore a model that takes a very different approach
    by using decision trees to arrive at final results.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，实际上模型非常稳健，我们的数据集产生了一些高指标，并给出了总平均精度为 88%。SVMs 是非常适合用于复杂数据集的模型，因为它们的主要目标是通过对数据进行超平面分离来实现这一点。现在让我们探索一个采用非常不同方法来得出最终结果的模型，即使用决策树。
- en: Decision trees and random forests
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树和随机森林
- en: '**Decision trees** are one of the most popular and commonly used machine learning
    models when it comes to structured datasets for both classification and regression.
    Decision trees consist of three elements: **nodes**, **edges**, and **leaf nodes**.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**是结构化数据集在分类和回归中最受欢迎和常用的机器学习模型之一。决策树由三个元素组成：**节点**、**边**和**叶节点**。'
- en: Nodes generally consist of a question allowing for the process to split into
    an arbitrary number of child nodes, shown in orange in the following diagram.
    The root node is the first node that the entire tree is referenced through. Edges
    are the connections between nodes shown in blue. When nodes have no children,
    then this final destination is called a leaf, shown in green. In some cases, a
    decision tree will have nodes containing the same parent – these are called sibling
    nodes. The more nodes there are in a tree, the *deeper* the tree is said to be.
    The depth of the decision tree is a measure of *complexity*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 节点通常由一个问题组成，允许过程分割成任意数量的子节点，以下图中用橙色表示。根节点是整个树通过的第一个节点。边是节点之间的连接，用蓝色表示。当节点没有子节点时，这个最终目的地被称为叶节点，用绿色表示。在某些情况下，决策树将会有包含相同父节点的节点
    – 这些被称为兄弟节点。树中的节点越多，树就越**深**。决策树的深度是**复杂度**的一个度量。
- en: 'A tree that is not complex enough will not arrive at an accurate result, and
    a tree that is too complex will be overtrained. Identifying a good balance is
    one of the primary objectives in the training process. Using these elements, you
    can construct a decision tree, allowing for processes to flow from the top to
    the bottom, thereby arriving at a particular destination or *decision*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果树不够复杂，将无法得到准确的结果，而过于复杂的树则会被过度训练。在训练过程中，找到一个良好的平衡是主要目标之一。使用这些元素，你可以构建一个决策树，使过程从上到下流动，从而到达特定的目的地或*决策*：
- en: '![Figure 7.16 – Illustration of decision trees when it comes to nodes, leaves,
    and edges ](img/B17761_07_016.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.16 – 节点、叶子和边在决策树中的表示](img/B17761_07_016.jpg)'
- en: Figure 7.16 – Illustration of decision trees when it comes to nodes, leaves,
    and edges
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 – 节点、叶子和边在决策树中的表示
- en: '**Decision trees** operate in quite a genius way. We begin with our initial
    dataset in which all datapoints are labeled and ready to go. The first objective
    is to split the dataset using a decision boundary that is the most informative
    – which, in this case, is at *y=m.* This has successfully isolated a class from
    the two others; however, the two others are still not yet isolated from one another.
    The algorithm then splits the dataset again at *x = n*, thus completely separating
    the three clusters. If more clusters were present, this process would iteratively
    and recursively continue until all classes are optimally separated. We can see
    a visual representation of this in *Figure 7.17*:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树**以相当天才的方式运行。我们从一个初始数据集开始，其中所有数据点都已标记并准备就绪。第一个目标是使用最有信息量的决策边界来分割数据集 –
    在这种情况下，是在 *y=m.* 这成功地隔离了一个类别与另外两个类别；然而，另外两个类别之间仍然没有完全隔离。然后算法再次在 *x = n* 处分割数据集，从而完全分离了三个簇。如果有更多的簇存在，这个过程将迭代和递归地继续，直到所有类别都得到最佳分离。我们可以在
    *图 7.17* 中看到这个过程的视觉表示：'
- en: '![Figure 7.17 – A graphical representation of the process that decision trees
    take ](img/B17761_07_017.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.17 – 决策树所采取过程的图形表示](img/B17761_07_017.jpg)'
- en: Figure 7.17 – A graphical representation of the process that decision trees
    take
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 – 决策树所采取过程的图形表示
- en: 'Decision trees determine where and how to split the data using various splitting
    criteria, known as attribute selection measures. These prominent attribute selection
    measures include:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过使用各种称为属性选择度量的分割标准来确定数据在哪里以及如何分割。这些突出的属性选择度量包括：
- en: Information gain
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息增益
- en: Gain ratio
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增益率
- en: Gini index
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉尼指数
- en: Let's now take a closer look at these three items.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这三项内容。
- en: '**Information gain** is an attribute concerning the amount of information required
    to further describe the tree. This attribute minimizes the information needed
    for data classification while utilizing the least randomness in the partitions.
    Think of information gain of a random variable determined from the observation
    of a random variable, *A*, as a function such that:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**信息增益**是一个关于进一步描述树所需信息量的属性。这个属性在利用最少随机性的分区的同时，最小化了数据分类所需的信息。将随机变量*A*的观察结果中确定的随机变量的信息增益视为一个函数，可以这样想：'
- en: '![](img/B17761_Formula_07_019.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_019.jpg)'
- en: 'Broadly speaking, the information gain is the change in entropy (information
    entropy) ![](img/B17761_Formula_07_020.png)such that:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '广义而言，信息增益是熵（信息熵）的变化 ![](img/B17761_Formula_07_020.png)such that:'
- en: '![](img/B17761_Formula_07_021.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_021.jpg)'
- en: In which ![](img/B17761_Formula_07_022.png) represents the conditional entropy
    of ![](img/B17761_Formula_07_023.png) given the attribute ![](img/B17761_Formula_07_024.png).
    In summary, information gain answers the question, *How much information do we
    obtain from this variable given another variable?*
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17761_Formula_07_022.png) 表示在属性 ![](img/B17761_Formula_07_023.png)
    给定的情况下 ![](img/B17761_Formula_07_024.png) 的条件熵。总之，信息增益回答了这样的问题，*从这个变量中获得多少信息，给定另一个变量？*
- en: 'On the other hand, the **gain ratio** is the information gain relative to the
    intrinsic information. In other words, this measure is biased toward tests that
    result in many outcomes, thus forcing a preference in favor of features of this
    nature. The gain ratio can be represented as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**增益率**是相对于固有信息的相对信息增益。换句话说，这个度量是有偏的，偏向于导致许多结果测试，从而强制对这种性质的特征给予优先考虑。增益率可以表示为：
- en: '![](img/B17761_Formula_07_025.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_025.jpg)'
- en: 'In which ![](img/B17761_Formula_07_026.png) is represented as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B17761_Formula_07_026.png) 表示为：
- en: '![](img/B17761_Formula_07_027.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_027.jpg)'
- en: In summary, the gain ratio penalizes variables with more distinct values, which
    will help decide the next split at the next level.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，增益率惩罚具有更多不同值的变量，这有助于决定下一级中的下一个分割。
- en: 'Finally, we arrive at the **Gini index**, which is an attribute selection measure
    representing how often a randomly selected element is incorrectly labeled. The
    Gini index can be calculated by subtracting the sum of square probabilities of
    each class:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达了**基尼指数**，这是一个属性选择度量，表示随机选择的一个元素被错误标记的频率。基尼指数可以通过减去每个类别的平方概率之和来计算：
- en: '![](img/B17761_Formula_07_028.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_028.jpg)'
- en: This methodology in determining a split naturally favors larger partitions as
    opposed to information gain, which favors smaller ones. The objective of any data
    scientist is to explore different methods with your dataset and determine the
    best path forward.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这种确定分割的方法自然倾向于较大的分区，而不是信息增益，后者倾向于较小的分区。任何数据科学家的目标都是探索不同的方法，并确定最佳前进路径。
- en: 'Now that we have a much more detailed explanation of decision trees and how
    the model operates, let''s now go ahead and implement this model using the previous
    single-cell RNA classification dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对决策树及其模型的工作原理有了更详细的解释，接下来让我们使用之前的单细胞 RNA 分类数据集来实现这个模型：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Upon printing the report, this yields the following results:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 打印报告后，得到以下结果：
- en: '![Figure 7.18 – Results of the decision tree classifier ](img/B17761_07_018.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.18 – 决策树分类器的结果](img/B17761_07_018.jpg)'
- en: Figure 7.18 – Results of the decision tree classifier
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 – 决策树分类器的结果
- en: We can see that the model, without any tuning, was able to deliver a total precision
    score of 77% using a `max_depth` value of `4`. Using the same method as the *KNN*
    model, we can iterate over a range of `max_depth` values to determine the optimal
    value. Doing so would result in an ideal `max_depth` value of 3, yielding a total
    precision of 82%.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型在没有任何调整的情况下，使用`max_depth`值为`4`的情况下，能够提供 77% 的总精确度分数。使用与*KNN*模型相同的方法，我们可以遍历一系列`max_depth`值，以确定最佳值。这样做将导致理想的`max_depth`值为
    3，总精确度达到 82%。
- en: As we begin to train many of our models, one of the most common issues that
    we will face is **overfitting** our data in one way or another. Take, for example,
    a decision tree model that was very finely tuned for a specific selection of data
    since decision trees are built on an entire dataset using the features and variables
    of interest. In this case, the model may be prone to overfitting. On the other
    hand, another model known as a **random forest** is built using many different
    decision trees to help mitigate any potential overfitting.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始训练许多模型时，我们将面临的最常见问题之一是以某种方式**过拟合**我们的数据。以一个决策树模型为例，该模型针对特定数据集进行了非常精细的调整，因为决策树是使用整个数据集的特征和感兴趣变量构建的。在这种情况下，模型可能容易过拟合。另一方面，另一个称为**随机森林**的模型使用多个不同的决策树构建，以帮助减轻任何潜在的过拟合。
- en: 'Random forests are robust ensemble models based on decision trees. Where decision
    trees are generally designed to develop a model using the dataset as a whole,
    random forests randomly select features and rows and subsequently build multiple
    decision trees that are then averaged in their weights. Random forests are powerful
    models given their ability to limit overfitting while avoiding a substantial increase
    in error due to **bias**. We can see a visual representation of this in *Figure
    7.19*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是基于决策树的鲁棒集成模型。决策树通常设计为使用整个数据集来构建模型，而随机森林随机选择特征和行，然后构建多个决策树，并在它们的权重中平均。由于随机森林能够限制过拟合，同时避免由于**偏差**导致的错误显著增加，因此它们是强大的模型。我们可以在*图
    7.19*中看到这种效果的视觉表示：
- en: '![Figure 7.19 – Graphical explanation of random forest models ](img/B17761_07_019.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.19 – 随机森林模型的图形解释](img/B17761_07_019.jpg)'
- en: Figure 7.19 – Graphical explanation of random forest models
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 – 随机森林模型的图形解释
- en: There are two main ways in which random forests can help reduce **variance**.
    The first method is by training on different samples of data. Consider the preceding
    example using the patient enrollment data. If the model was trained on samples
    not containing those *in between* the clusters, then determining the score on
    the test set will result in significantly lower accuracy.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林可以通过两种主要方式帮助减少**方差**。第一种方法是通过训练不同的数据样本。考虑先前的患者入组数据示例。如果模型是在不包含那些*介于*聚类之间的样本上训练的，那么在测试集上确定分数将导致显著降低的准确率。
- en: 'The second method involves using a random subset of features to train on, allowing
    for the determination of the concept of feature importance within the model. Let''s
    go ahead and take a look at this model using the single-cell RNA classification
    dataset:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法涉及使用随机特征子集进行训练，从而允许在模型中确定特征重要性的概念。让我们看一下使用单细胞RNA分类数据集的此模型：
- en: '[PRE18]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Upon printing the report, this yields the following results:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 打印报告后，得到以下结果：
- en: '![Figure 7.20 – Results of the random forest model ](img/B17761_07_020.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图7.20 – 随机森林模型的结果](img/B17761_07_020.jpg)'
- en: Figure 7.20 – Results of the random forest model
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 – 随机森林模型的结果
- en: We can immediately observe that the model has a precision of roughly 74%, slightly
    lower than the decision tree above, indicating that the tree may have overfitted
    the data slightly.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即观察到模型的精确度大约为74%，略低于上面的决策树，这表明树可能略微过度拟合了数据。
- en: '**Random forest** models are very commonly used in the biotechnology and life
    sciences industries given their remarkable methods of avoiding overfitting, as
    well as their ability to develop predictive models with smaller datasets. Many
    applications of machine learning within the biotech space generally suffer from
    a concept known as the **low-N** problem, in the sense that use cases exist, but
    little to no data has been collected or organized with which to develop a model.
    Random forests are commonly used for applications in this space given their ensemble
    nature. Let''s now take a look at a very different model that splits data not
    based on decisions, but based on statistical probability instead.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**模型在生物技术和生命科学行业中非常常用，因为它们有出色的避免过度拟合的方法，以及它们能够使用较小的数据集开发预测模型的能力。生物技术空间内机器学习的许多应用通常都存在一个称为**低N**问题的概念，即存在用例，但收集或组织的数据很少或没有，以开发模型。由于随机森林具有集成特性，因此它们通常用于这个空间的应用。现在让我们看看一个非常不同的模型，它不是基于决策而是基于统计概率来分割数据的。'
- en: Extreme Gradient Boosting (XGBoost)
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 极端梯度提升（XGBoost）
- en: Over the last few years, a number of robust machine learning models have begun
    to enter the data science space, thus changing the machine learning landscape
    quite effectively – one of these models being the **Extreme Gradient Boosting**
    (**XGBoost**) model. The main idea behind this model is that it is an implementation
    of **Gradient Boosted Models** (**GBMs**), specifically, *decision trees*, in
    which speed and performance were highly optimized. Because of the highly efficient
    and highly effective nature of this model, it began to dominate many areas of
    data science and eventually became the go-to algorithm for many data science competitions
    on [Kaggle.com](http://Kaggle.com).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，许多稳健的机器学习模型开始进入数据科学领域，从而有效地改变了机器学习格局——其中之一就是**极端梯度提升**（**XGBoost**）模型。此模型背后的主要思想是它是**梯度提升模型**（**GBMs**）的实现，特别是*决策树*，其中速度和性能得到了高度优化。由于该模型的高效性和高效性，它开始主导数据科学的许多领域，并最终成为[Kaggle.com](http://Kaggle.com)上许多数据科学竞赛的首选算法。
- en: 'There are many reasons why GBMs are so effective with structured/tabular datasets.
    Let''s go ahead and explore three of these reasons:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多原因使得GBMs在结构化/表格数据集上非常有效。让我们探索这三个原因：
- en: '**Parallelization**: The *XGBoost* model implements a method known as parallelization.
    The main idea here is that it can parallelize processes in the construction of
    each of the trees. In essence, each of the branches of a single tree is trained
    separately.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行化**：*XGBoost*模型实现了一种称为并行化的方法。这里的主要思想是它可以并行化构建每棵树的过程。本质上，单棵树的每个分支都是单独训练的。'
- en: '`max_depth` parameter, and then begins the pruning process backward and eventually
    removes splits after which there is no longer a positive gain.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`参数，然后开始向后剪枝过程，最终在不再有正增益的分割后移除。'
- en: '**Regularization**: In the context of tree-based methods overall, regularization
    is an algorithmic method to define a minimum gain in order to prompt another split
    in the tree. In essence, regularization shrinks scores, thereby prompting the
    final prediction to be more conservative, which, in return, helps prevent overfitting
    within the model.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：在基于树的算法的总体背景下，正则化是一种算法方法，用于定义一个最小增益，以促使树中的另一个分割。本质上，正则化会缩小分数，从而促使最终预测更加保守，这反过来有助于防止模型过拟合。'
- en: 'Now that we have gained a much better understanding of XGBoost and some of
    the reasons behind its robust performance, let''s go ahead and implement this
    model on our RNA dataset. We will begin by installing the model''s library using
    `pip`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经对XGBoost及其稳健性能背后的原因有了更好的理解，我们现在就可以在我们的RNA数据集上实现这个模型。我们将首先使用`pip`安装模型的库：
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the model installed, let''s now go ahead and import the model and then
    create a new instance of the model in which we specify the `n_estimators` parameter
    to have a value of `10000`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型安装完成后，我们现在可以继续导入模型，并创建一个新的模型实例，在这个实例中我们指定`n_estimators`参数的值为`10000`：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Similar to the previous models, we can now go ahead and fit our model with
    the training datasets and print the results of our predictions:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的模型类似，我们现在可以使用训练数据集来拟合我们的模型，并打印出预测结果：
- en: '[PRE21]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Upon printing the report, this yields the following results:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 打印报告后，得到以下结果：
- en: '![Figure 7.21 – Results of the XGBoost model ](img/B17761_07_021.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21 – XGBoost模型的预测结果](img/B17761_07_021.jpg)'
- en: Figure 7.21 – Results of the XGBoost model
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 – XGBoost模型的预测结果
- en: With that, we can see that we managed to achieve a precision of `0.86`, much
    higher than some of the other models we tested. The highly optimized nature of
    this model allows it to be very fast and robust relative to most others.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以看到我们成功实现了`0.86`的精确度，这比我们测试的其他一些模型要高得多。这个模型高度优化的特性使其相对于大多数其他模型来说非常快且稳健。
- en: Over the course of this section, we managed to cover quite a wide scope of **classification**
    models. We began with the simple **KNN** model, which attempts to predict the
    class of a new value relative to its closest neighbors. Next, we covered **SVM**
    models, which attempt to assign labels based on specified boundaries drawn by
    support vectors. We then covered both **decision trees** and **random** **forests**,
    which operate based on nodes, leaves, and splits, and then finally saw a working
    example of **XGBoost**, a highly optimized model that implements many of the features
    we saw in other models.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们成功覆盖了相当广泛的**分类**模型。我们从简单的**KNN**模型开始，该模型试图根据新值与其最近邻的关系来预测其类别。接下来，我们介绍了**SVM**模型，这些模型试图根据支持向量绘制的指定边界来分配标签。然后，我们介绍了**决策树**和**随机森林**，它们基于节点、叶子和分割操作，最后我们看到了**XGBoost**的一个工作示例，这是一个高度优化的模型，实现了我们在其他模型中看到的大多数功能。
- en: As you begin to dive into the many different models for new datasets, you will
    likely investigate the idea of automating the model selection process. If you
    think about it, each of the steps we have taken above could be automated in one
    way or another to identify which model operates the best under a specific set
    of metric requirements. Luckily for us, a library already exists that can assist
    us in this space. Over the course of the following tutorial, we will investigate
    the use of these models alongside some automated machine learning capabilities
    on **Google Cloud Platform** (**GCP**).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始深入研究针对新数据集的许多不同模型时，你可能会考虑自动化模型选择过程。如果你这样考虑，我们上面采取的每个步骤都可以以某种方式自动化，以确定在特定的一组度量要求下哪个模型表现最佳。幸运的是，我们已经有一个库可以在这个领域帮助我们。在接下来的教程中，我们将研究这些模型与在**Google
    Cloud Platform**（**GCP**）上的一些自动化机器学习功能一起的使用。
- en: 'Tutorial: Classification of proteins using GCP'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程：使用GCP对蛋白质进行分类
- en: During this tutorial, we will investigate a number of classification models,
    followed by an implementation of some automated machine learning capabilities.
    Our main objective will be to automatically develop a model for the classification
    of proteins using a dataset from the **Research Collaboratory for Structural Bioinformatics**
    (**RCSB**) **Protein Data Bank** (**PDB**). If you recall, we used data from RCSB
    PDB in a previous chapter to plot a 3D protein structure. The dataset we will
    be working with in this chapter consists of two parts—a **structured dataset**
    with rows columns, one column of which is the designated classification of each
    of the proteins, and a series of RNA sequences for each of the proteins. We will
    save this second set of sequence-based data for analysis in a later chapter and
    focus on the structured dataset for now.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将研究多种分类模型，并随后实现一些自动化机器学习功能。我们的主要目标将是使用来自**结构生物信息学研究合作实验室**（**RCSB**）的**蛋白质数据银行**（**PDB**）的数据集自动开发一个用于蛋白质分类的模型。如果你还记得，我们在前一章中使用RCSB
    PDB的数据来绘制一个3D蛋白质结构。在本章中我们将使用的数据集由两部分组成——一个具有行和列的**结构化数据集**，其中一列是每个蛋白质指定的分类，以及每个蛋白质的一系列RNA序列。我们将把这一组基于序列的数据保存到后续章节中进行分析，现在我们专注于结构化数据集。
- en: In this chapter, we will use this structured dataset containing many different
    types of proteins in an attempt to develop a classifier. Given the large nature
    of this dataset, we will take this opportunity to move our development environment
    from our local installation of **Jupyter Notebook** to an online notebook in **GCP**.
    Before we can do so, we will need to create a new GCP account. Let's go ahead
    and get started.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用包含许多不同类型蛋白质的结构化数据集来尝试开发一个分类器。鉴于这个数据集的规模很大，我们将利用这个机会将我们的开发环境从本地安装的**Jupyter
    Notebook**迁移到GCP的在线笔记本。在我们这样做之前，我们需要创建一个新的GCP账户。让我们开始吧。
- en: Getting started in GCP
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在GCP入门
- en: 'Getting started in **GCP** is quite simple, and can be accomplished in a few
    simple steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在**GCP**（Google Cloud Platform）入门非常简单，只需几个简单的步骤即可完成：
- en: We can begin by navigating to [https://cloud.google.com/](https://cloud.google.com/)
    and registering for a new account. You will need to provide a few details, such
    as your name, email, and a few other items.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以从导航到[https://cloud.google.com/](https://cloud.google.com/)并注册一个新账户开始。你需要提供一些详细信息，例如你的名字、电子邮件和一些其他项目。
- en: Once registered, you will be able to navigate to the console by clicking the
    **Console** button on the upper right-hand side of the page:![Figure 7.22 – A
    screenshot of the Console button ](img/B17761_07_022.jpg)
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册后，你可以通过点击页面右上角的**控制台**按钮来导航到控制台：![图7.22 – 控制台按钮的截图](img/B17761_07_022.jpg)
- en: Figure 7.22 – A screenshot of the Console button
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.22 – 控制台按钮的截图
- en: Within the console page, you will be able to see all items relating to your
    current project, such as general information, resources used, API usage, and even
    billing:![Figure 7.23 – An example of the console page ](img/B17761_07_023.jpg)
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台页面内，你可以看到与你的当前项目相关的所有项目，例如一般信息、使用的资源、API使用情况，甚至账单：![图7.23 – 控制台页面的示例](img/B17761_07_023.jpg)
- en: Figure 7.23 – An example of the console page
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.23 – 控制台页面的示例
- en: 'You will likely not have any projects set up yet. In order to create a new
    project, navigate to the drop-down menu on the upper left-hand side and select
    the **New Project** option. Give your project a name and then click **CREATE**.
    You can navigate between different projects using that same drop-down menu:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能还没有设置任何项目。为了创建一个新的项目，导航到页面左上角的下拉菜单，并选择**新建项目**选项。给你的项目起一个名字，然后点击**创建**。你可以使用相同的下拉菜单在不同的项目之间导航：
- en: '![Figure 7.24 – A screenshot of the project name and location pane in GCP ](img/B17761_07_024.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图7.24 – GCP中项目名称和位置窗口的截图](img/B17761_07_024.jpg)'
- en: Figure 7.24 – A screenshot of the project name and location pane in GCP
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 – GCP中项目名称和位置窗口的截图
- en: With that last step completed, you are now all set up to take full advantage
    of the GCP platform. We will cover a few of the GCP capabilities in this tutorial
    to get us started in the data science space; however, I highly encourage new users
    to explore and learn the many tools and resources available here.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 完成最后一步后，你现在已经准备好充分利用GCP平台了。在本教程中，我们将介绍一些GCP功能，以帮助我们开始数据科学领域的工作；然而，我强烈建议新用户探索并学习这里提供的许多工具和资源。
- en: Uploading data to GCP BigQuery
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据上传到GCP BigQuery
- en: 'There are many different ways in which you can upload data within GCP; however,
    we will focus on one particular capability unique to the GCP, known as **BigQuery**.
    The main idea behind BigQuery is that it is a serverless data warehouse with built-in
    machine learning capabilities that supports the use of queries with the **SQL**
    language. If you recall, we previously developed and deployed an **AWS RDS** to
    manage our data using an **EC2** instance as a server, whereas BigQuery, on the
    other hand, operates using a serverless architecture. We can set up BigQuery and
    start uploading our data in a few simple steps:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中上传数据有多种不同的方式；然而，我们将专注于GCP特有的一个特定功能，称为**BigQuery**。BigQuery背后的主要思想是它是一个具有内置机器学习能力的无服务器数据仓库，支持使用**SQL**语言进行查询。如果你还记得，我们之前开发并部署了一个**AWS
    RDS**，使用**EC2**实例作为服务器来管理我们的数据，而另一方面，BigQuery使用的是无服务器架构。我们可以通过几个简单的步骤来设置BigQuery并开始上传我们的数据：
- en: Using the navigation menu on the left-hand side of the page, scroll down to
    the **Products** section, hover over the **BigQuery** option, and select **SQL
    workspace**. Given that this is the first time you are using this tool, you may
    need to activate the API. This will be true for all tools that you have never
    used before:![Figure 7.25 – A screenshot of the BigQuery menu in GCP ](img/B17761_07_025.jpg)
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用页面左侧的导航菜单，滚动到**产品**部分，将鼠标悬停在**BigQuery**选项上，并选择**SQL工作区**。由于这是你第一次使用这个工具，你可能需要激活API。这将对所有你从未使用过的工具都适用：![图7.25
    – GCP中BigQuery菜单的截图](img/B17761_07_025.jpg)
- en: Figure 7.25 – A screenshot of the BigQuery menu in GCP
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.25 – GCP中BigQuery菜单的截图
- en: Within this list, you will find the project that you created in the previous
    section. Click on the options button to the right and select `protein_structure_sequence`,
    leaving all the other options as their default values. You can then click **Create
    dataset**.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个列表中，你会找到你在上一节中创建的项目。点击右侧的选项按钮，选择`protein_structure_sequence`，保留所有其他选项为默认值。然后你可以点击**创建数据集**。
- en: On the left-hand menu, you will see the newly created dataset listed under the
    project name. If you click **Options** followed by **Open**, you will be directed
    to the dataset's main page. Within this page, you will find information relating
    to that particular dataset. Let's now go ahead and create a new table here by
    clicking the **Create Table** option at the top. Change the source to reflect
    the upload option and navigate to the CSV file pertaining to the protein classifications
    from RCSB PDB. Give the table a new name, and while leaving all other options
    as their default values, click **Create Table**:![Figure 7.26 – A screenshot of
    the Create table pane in GCP ](img/B17761_07_026.jpg)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧菜单中，你将看到在项目名称下列出的新创建的数据集。如果你点击**选项**然后点击**打开**，你将被引导到数据集的主页。在这个页面上，你可以找到与该特定数据集相关的信息。现在让我们点击顶部的**创建表格**选项，在这里创建一个新的表格。将源更改为反映上传选项，并导航到与RCSB
    PDB中的蛋白质分类相关的CSV文件。给表格起一个新的名字，在保留所有其他选项为默认值的情况下，点击**创建表格**：![图7.26 – GCP中创建表格界面的截图](img/B17761_07_026.jpg)
- en: Figure 7.26 – A screenshot of the Create table pane in GCP
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.26 – GCP中创建表格界面的截图
- en: 'If you navigate back to Explorer, you will see the newly created table listed
    under the dataset, which is listed under your project:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你导航回资源管理器，你将看到新创建的表格列在你的项目下的数据集下：
- en: '![Figure 7.27 – An example of the table created within a dataset ](img/B17761_07_027.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图7.27 – 数据集中创建的表格示例](img/B17761_07_027.jpg)'
- en: Figure 7.27 – An example of the table created within a dataset
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27 – 数据集中创建的表格示例
- en: If you managed to follow all of these steps correctly, you should now have data
    available for you to use in BigQuery. In the following section, we will prepare
    a new notebook and start parsing some of our data in this dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正确地完成了所有这些步骤，你现在应该有可用于在BigQuery中使用的数据。在下一节中，我们将准备一个新的笔记本，并开始解析这个数据集中的部分数据。
- en: Creating a notebook in GCP
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在GCP中创建笔记本
- en: 'In this section, we will create a notebook equivalent to that of the Jupyter
    notebooks we have been using to carry out our data science work on. We can get
    a new notebook set up in a few simple steps:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个与我们在进行数据科学工作时使用的Jupyter笔记本等效的笔记本。我们可以通过几个简单的步骤来设置一个新的笔记本：
- en: In the navigation pane on the left-hand side of the screen, scroll down to the
    **ARTIFICIAL INTELLIGENCE** section, hover over **AI Platform**, and select the
    **Notebooks** option. Remember that you may need to activate this API once again
    if you have not done so already:![Figure 7.28 – A visual of the AI Platform menu
    ](img/B17761_07_028.jpg)
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕左侧的导航面板中，向下滚动到 **人工智能** 部分，悬停在 **AI 平台** 上，并选择 **笔记本** 选项。记住，如果你还没有这样做，你可能需要再次激活此
    API：![图 7.28 – AI 平台菜单的视觉图](img/B17761_07_028.jpg)
- en: Figure 7.28 – A visual of the AI Platform menu
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.28 – AI 平台菜单的视觉图
- en: Next, navigate to the top of the screen and select the **New Instance** option.
    There are many different options available for you depending on your needs. For
    the purposes of this tutorial, we can select the first option for **Python 3**:![Figure
    7.29 – A screenshot of the instance options ](img/B17761_07_029.jpg)
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导航到屏幕顶部并选择 **新建实例** 选项。根据你的需求，有许多不同的选项可供选择。在本教程的目的上，我们可以选择第一个 **Python 3**
    选项：![图 7.29 – 实例选项的截图](img/B17761_07_029.jpg)
- en: Figure 7.29 – A screenshot of the instance options
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.29 – 实例选项的截图
- en: If you are familiar with notebook instances and are comfortable customizing
    them, I recommend creating a customized instance to suit your exact needs.
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你熟悉笔记本实例并且能够自定义它们，我建议创建一个定制的实例以满足你的具体需求。
- en: Once the notebook is created and the instance is online, you will be able to
    see it in the main **Notebook Instances** section. Go ahead and click on the **OPEN
    JUPYTERLAB** button. A new window will open up containing Jupyter Lab:![Figure
    7.30 – A screenshot of the instance menu ](img/B17761_07_030.jpg)
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦创建了笔记本并实例在线，你将在主 **笔记本实例** 部分中看到它。点击 **打开 JupyterLab** 按钮。将打开一个新窗口，其中包含 Jupyter
    Lab：![图 7.30 – 实例菜单的截图](img/B17761_07_030.jpg)
- en: Figure 7.30 – A screenshot of the instance menu
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.30 – 实例菜单的截图
- en: 'In the `home` directory, create a new directory called `biotech-machine-learning`
    for us to save our notebooks in. Open the directory and create a new notebook
    by clicking the **Python 3** notebook option on the right:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `home` 目录下，创建一个名为 `biotech-machine-learning` 的新目录，以便我们保存笔记本。打开目录，通过点击右侧的 **Python
    3** 笔记本选项来创建一个新的笔记本：
- en: '![Figure 7.31 – A screenshot of Jupyter Lab on GCP ](img/B17761_07_031.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.31 – GCP 上 Jupyter Lab 的截图](img/B17761_07_031.jpg)'
- en: Figure 7.31 – A screenshot of Jupyter Lab on GCP
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31 – GCP 上 Jupyter Lab 的截图
- en: With the instance now provisioned and the notebook created, you are now all
    set to run all of your data science models on GCP. Let's now take a closer look
    at the data and begin training a few machine learning models.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在实例已配置并创建了笔记本，你现在可以准备在 GCP 上运行所有你的数据科学模型了。现在让我们更仔细地查看数据，并开始训练几个机器学习模型。
- en: Using auto-sklearn in GCP Notebooks
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 GCP 笔记本中使用 auto-sklearn
- en: If you open your newly created notebook, you see the very familiar environment
    of Jupyter Lab that we have been working with all along. The two main benefits
    here are that we now have the ability to manage our datasets within this same
    environment and can provision larger resources to process our data relative to
    the few **CPUs** and **GPUs** we have on our local machines.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开你刚刚创建的笔记本，你会看到我们一直使用的非常熟悉的 Jupyter Lab 环境。这里的主要好处是，我们现在能够在这个相同的环境中管理我们的数据集，并且可以分配更多的资源来处理我们的数据，相对于我们在本地机器上拥有的少量
    **CPU** 和 **GPU**。
- en: Recall that our main objective for getting to this state is to be able to develop
    a classification model to correctly classify proteins based on some input features.
    The dataset we are working with is known as a `real-world` dataset in the sense
    that it is not well organized, has missing values, may contain too much data,
    and will need some preprocessing prior to the development of any model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们达到这个状态的主要目标是能够开发一个分类模型，根据一些输入特征正确地分类蛋白质。我们正在处理的数据集被称为 `真实世界` 数据集，因为它组织得不好，有缺失值，可能包含过多的数据，并且在开发任何模型之前需要一些预处理。
- en: 'Let''s go ahead and start by importing a few necessary libraries:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入一些必要的库：
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let''s now import our dataset from BigQuery. We can do that directly
    here in the notebook by instantiating a client using the BigQuery class of the
    Google Cloud library:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，现在让我们从 BigQuery 导入我们的数据集。我们可以通过使用 Google Cloud 库中的 BigQuery 类来实例化一个客户端，直接在这里的笔记本中完成：
- en: '[PRE23]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we can go ahead and query our data using a `SELECT` command in the **SQL**
    language. We can simply begin by querying all the data in our dataset. In the
    following code snippet, we will query the data using SQL, and convert the results
    to a dataframe:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用SQL语言的`SELECT`命令查询我们的数据。我们可以简单地从查询我们数据集中的所有数据开始。在以下代码片段中，我们将使用SQL查询数据，并将结果转换为数据框：
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once converted to a more manageable dataframe, we can see that the dataset
    we are working with is quite extensive, with nearly 140,000 rows and 14 columns
    of data. Immediately, we notice that one of the columns is called `classification`.
    Let''s take a look at the unique number of classes in this dataset using the `n_unique()`
    function:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦转换为一个更易于管理的数据框，我们就可以看到我们正在处理的数据集相当庞大，有近140,000行和14列的数据。立刻，我们注意到其中一列被称作`classification`。让我们使用`n_unique()`函数查看这个数据集中独特的类别数量：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We notice that there are 5,050 different classes! That is a lot for a dataset
    this size, indicating that we may need to reduce this quite heavily before any
    analysis. Before proceeding any further, let''s go ahead and drop any and all
    potential duplicates:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到有5,050个不同的类别！对于一个这样的数据集来说，这相当多，表明我们可能需要在任何分析之前大幅度缩减。在继续之前，让我们先删除任何可能的重复项：
- en: '[PRE26]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s now take a closer look at the top 10 classes in our dataset by count:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在更仔细地查看数据集中按计数排序的前10个类别：
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following figure is yielded from the code, showing us the top 10 classes
    from this dataset:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图形是由代码生成的，展示了这个数据集中的前10个类别：
- en: '![Figure 7.32 – The top 10 most frequent labels in the dataset ](img/B17761_07_032.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图7.32 – 数据集中最频繁的10个标签](img/B17761_07_032.jpg)'
- en: Figure 7.32 – The top 10 most frequent labels in the dataset
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.32 – 数据集中最频繁的10个标签
- en: 'Immediately, we notice that there are two or three classes or proteins that
    account for the vast majority of this data: hydrolase, transferase, and oxidoreductase.
    This will be problematic for two reasons:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 立刻，我们注意到有两到三个类别或蛋白质占用了大部分数据：水解酶、转移酶和氧化还原酶。这将会带来两个问题：
- en: Data should always be **balanced** in the sense that each of the classes should
    have a roughly equal number of rows.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据应该始终在意义上保持**平衡**，即每个类别应该有大致相等的行数。
- en: As a general rule of thumb, the ratio of classes to observations should be around
    50:1, meaning that with 5,050 classes, we would require around 252,500 observations,
    which we do not currently have.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一般规则，类别与观察值的比率应该大约是50:1，这意味着在5,050个类别的情况下，我们需要大约252,500个观察值，而我们目前并没有这么多。
- en: 'Given these two constraints, we can account for both by simply focusing on
    developing a model using the first three classes. For now, we notice that there
    are quite a few features available to us regardless of the classes at hand. We
    can go ahead and take a closer look at the completeness of our features of interest
    using the `msno` library:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个约束条件下，我们可以通过专注于使用前三个类别来开发模型来同时解决这两个问题。目前，我们注意到无论手头上的类别如何，我们都有相当多的特征可用。我们可以继续使用`msno`库来更仔细地查看我们感兴趣的特征的完整性：
- en: '[PRE28]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following screenshot, representing the completeness of the dataset, is
    then generated. Notice that a substantial number of rows for the `crystallizationTempK`
    feature are missing:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图，表示数据集的完整性，随后生成。请注意，`crystallizationTempK`特征的行数大量缺失：
- en: '![Figure 7.33 – A graphical representation showing the completeness of the
    dataset ](img/B17761_07_033.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图7.33 – 一个图形表示，展示了数据集的完整性](img/B17761_07_033.jpg)'
- en: Figure 7.33 – A graphical representation showing the completeness of the dataset
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.33 – 一个图形表示，展示了数据集的完整性
- en: 'So far within this dataset, we have noted the fact that we will need to reduce
    the number of classes to the top two classes to avoid an imbalanced dataset, and
    we will also need to address the many rows of data we are missing. Let''s go ahead
    and prepare our dataset for the development of a few classification models based
    on our observations. First, we can go ahead and reduce the dataset using a simple
    `groupby` function:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这个数据集中，我们已经注意到我们需要将类别数量缩减到前两个类别，以避免数据不平衡，同时我们还需要解决缺失的许多行数据。让我们继续准备我们的数据集，以便基于我们的观察开发几个分类模型。首先，我们可以使用简单的`groupby`函数来缩减数据集：
- en: '[PRE29]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If we run a quick check on the dataframe using the `value_counts()` function,
    we notice that we were able to reduce it down to the top two labels.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`value_counts()`函数对数据框进行快速检查，我们会发现我们能够将其缩减到前两个标签。
- en: 'Alternatively, we can run this same command in `SELECT` the classification
    and `COUNT` the `residueCount` feature and the `GROUP BY` classification. Next,
    we `INNER JOIN` that query with the original table setting, classification against
    classification, but filtering using our `WHERE` clause:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以在 `SELECT` 中运行相同的命令，对分类进行 `COUNT`，并按 `GROUP BY` 分类。接下来，我们将该查询与原始表设置进行
    `INNER JOIN`，分类对分类，但使用我们的 `WHERE` 子句进行过滤：
- en: '[PRE30]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we can go ahead and remove the rows of data with missing values using
    the `dropna()` function:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `dropna()` 函数删除含有缺失值的行数据：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Immediately, we observe that the size of the dataset has been reduced down
    to 24,179 observations. This will be a sufficient dataset to work with when developing
    our models. In order to avoid having to process it again, we can write the contents
    of the dataframe to a new table in the same BigQuery dataset:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 立即观察到数据集的大小已减少到 24,179 个观测值。这将是我们开发模型时的一个足够大的数据集。为了避免再次处理，我们可以将数据框的内容写入同一 BigQuery
    数据集中新的表中：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'With the data now prepared, let''s go ahead and develop a model. We can go
    ahead and split the input and output data, scale the data using the `StandardScaler`
    class, and split the data into test and training sets:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已准备就绪，让我们继续开发一个模型。我们可以继续分割输入和输出数据，使用 `StandardScaler` 类缩放数据，并将数据分割成测试集和训练集：
- en: '[PRE33]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For the automation section, we will use a library called `autosklearn`, which
    can be installed using the command line via `pip`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动化部分，我们将使用一个名为 `autosklearn` 的库，该库可以通过命令行使用 `pip` 安装：
- en: '[PRE34]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'With the library installed, we can go ahead and import the library and instantiate
    a new instance of that model. We will then set a few parameters relating to the
    time we wish to dedicate to this process and give the model a temporary directory
    to operate in:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了库之后，我们可以继续导入库并实例化该模型的新实例。然后我们将设置一些与我们要投入此过程的时间相关的参数，并为模型提供一个临时目录来操作：
- en: '[PRE35]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we can go ahead and fit the model on our data. This process will take
    a few minutes to run:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以继续将模型拟合到我们的数据上。这个过程将需要几分钟才能运行：
- en: '[PRE36]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'When the model is complete, we can take a look at the results by printing the
    leader board:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型完成时，我们可以通过打印排行榜来查看结果：
- en: '[PRE37]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Upon printing the leaderboard, we retrieve the following results:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 打印排行榜后，我们检索到以下结果：
- en: '![Figure 7.34 – Results of the auto-sklearn model ](img/B17761_07_034.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.34 – auto-sklearn 模型的结果](img/B17761_07_034.jpg)'
- en: Figure 7.34 – Results of the auto-sklearn model
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34 – auto-sklearn 模型的结果
- en: 'We can also take a look at the top-performing `random_forest` model using the
    `get_models_with_weights()` function:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `get_models_with_weights()` 函数查看表现最佳的 `random_forest` 模型：
- en: '[PRE38]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We can also go ahead and get a few more metrics by making some predictions
    using the model and the `classification_report()` function:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用模型和 `classification_report()` 函数进行一些预测来获取更多指标：
- en: '[PRE39]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Upon printing the report, this yields the following results:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 打印报告后，得到以下结果：
- en: '![Figure 7\. 35 – Results of the top-performing auto-sklearn model ](img/B17761_07_035.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.35 – 表现最佳的 auto-sklearn 模型的结果](img/B17761_07_035.jpg)'
- en: Figure 7\. 35 – Results of the top-performing auto-sklearn model
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.35 – 表现最佳的 auto-sklearn 模型的结果
- en: 'With that, we managed to develop a machine learning model successfully and
    automatically for our dataset. However, the model has not yet been fined-tuned
    or optimized for this task. One challenge that I recommend you complete is the
    process of tuning the various parameters in this model in an attempt to increase
    our metrics. In addition, another challenge would be to try and explore some of
    the other models we learned about and see whether any of them can beat `autosklearn`.
    Hint: **XGBoost** has always been a great model for structured datasets.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们成功地为我们的数据集开发了一个成功且自动化的机器学习模型。然而，该模型尚未针对此任务进行微调或优化。我推荐你完成的一个挑战是调整模型中的各种参数，以尝试提高我们的指标。此外，另一个挑战将是尝试探索我们学到的其他一些模型，看看它们是否能够击败
    `autosklearn`。提示：**XGBoost** 一直是一个针对结构化数据集的优秀模型。
- en: Using the AutoML application in GCP
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 GCP 中的 AutoML 应用
- en: 'In the previous section, we used an open source library called `auto-sklearn`
    that automates the process in which models can be selected using the `sklearn`
    library. However, as we have seen with the `XGBoost` library, there are many other
    models out there outside of the `sklearn` API. GCP offers a robust tool, similar
    to that of `auto-sklearn`, that iterates over a large selection of models and
    methods to find the most optimal model for a given dataset. Let''s go ahead and
    give this a try:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了一个名为`auto-sklearn`的开源库来自动化使用`sklearn`库选择模型的过程。然而，正如我们通过`XGBoost`库所看到的，还有许多其他模型存在于`sklearn`
    API之外。GCP提供了一个强大的工具，类似于`auto-sklearn`，它会遍历大量模型和方法，以找到给定数据集的最优模型。让我们继续尝试一下：
- en: In the navigation menu in GCP, scroll down to the **ARTIFICIAL INTELLIGENCE**
    section, hover over **Tables**, and select **Datasets**:![Figure 7.36 – Selecting
    Datasets from the ARTIFICIAL INTELLIGENCE menu ](img/B17761_07_036.jpg)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GCP的导航菜单中，滚动到**人工智能**部分，将鼠标悬停在**表格**上，然后选择**数据集**：![图7.36 – 从人工智能菜单中选择数据集](img/B17761_07_036.jpg)
- en: Figure 7.36 – Selecting Datasets from the ARTIFICIAL INTELLIGENCE menu
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.36 – 从人工智能菜单中选择数据集
- en: At the top of the page, select the **New Dataset** option. At the time this
    book was written, the beta implementation of the model was available. Some of
    the steps will likely have changed in future implementations:![Figure 7.37 – A
    screenshot of the button to create a new dataset ](img/B17761_07_037.jpg)
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面顶部，选择**新建数据集**选项。在本书编写时，该模型的beta版本可用。一些步骤在未来实现中可能会发生变化：![图7.37 – 创建新数据集按钮的截图](img/B17761_07_037.jpg)
- en: Figure 7.37 – A screenshot of the button to create a new dataset
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.37 – 创建新数据集按钮的截图
- en: Go ahead and give the dataset a name and region and click **Create Dataset**.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给数据集命名并选择区域，然后点击**创建数据集**。
- en: We have the option to import our dataset of interest either as a raw CSV file
    or using BigQuery. Go ahead and import our cleaned version of the proteins dataset
    by specifying `projectID`, `datasetID`, and the table name, and then click **Import**.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以选择将我们感兴趣的数据库集以原始CSV文件或使用BigQuery的方式导入。请指定`projectID`、`datasetID`和表名，然后点击**导入**来导入我们清理过的蛋白质数据集版本。
- en: In the **TRAIN** section, you will have the ability to see the tables within
    this dataset. Go ahead and specify the **Classification** column as the target
    column and then click **TRAIN MODEL**:![Figure 7.38 – An example of the training
    menu ](img/B17761_07_038.jpg)
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**训练**部分，你将能够看到这个数据集内的表格。指定**分类**列为目标列，然后点击**训练模型**：![图7.38 – 训练菜单的示例](img/B17761_07_038.jpg)
- en: Figure 7.38 – An example of the training menu
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.38 – 训练菜单的示例
- en: The model selection process will take some time to complete. Upon completion,
    you will be able to see the results of the model under the **Evaluate** tab. Here
    you will get a sense of the classification metrics we have been working with,
    as well as a few others.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型选择过程需要一些时间来完成。完成后，你将在**评估**选项卡下看到模型的结果。在这里，你将了解我们一直在使用的分类指标，以及一些其他指标。
- en: '![Figure 7.39 – Results of the trained model showing the metrics ](img/B17761_07_039.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图7.39 – 训练模型结果的截图，显示了指标](img/B17761_07_039.jpg)'
- en: Figure 7.39 – Results of the trained model showing the metrics
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.39 – 训练模型结果的截图，显示了指标
- en: '**GCP** **AutoML** is a powerful tool that you can use to your advantage when
    handling a difficult dataset. You will find that the implementation of the model
    is quite robust and generally comprehensive relative to the many options that
    we, as data scientists, can explore. One of the downsides of **AutoML** is the
    fact that the final model is not shared with the user; however, the user does
    have the ability to test new data and use the model later on. We will explore
    another option similar to **AutoML** in the following section known as **AutoPilot**
    in **AWS**. Now that we have explored quite a few different models and methods
    relating to classification, let''s go and explore their respective counterparts
    when it comes to regression.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCP** **AutoML**是一个强大的工具，当处理困难的数据集时，你可以利用它。你会发现模型的实现相当稳健，相对于我们作为数据科学家可以探索的许多选项，通常比较全面。**AutoML**的一个缺点是最终模型不会与用户共享；然而，用户确实有测试新数据和稍后使用模型的能力。在下一节中，我们将探讨一个类似于**AutoML**的选项，在**AWS**中称为**AutoPilot**。现在我们已经探索了许多与分类相关的不同模型和方法，让我们去探索回归方面的相应对应物。'
- en: Understanding regression in supervised machine learning
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解监督机器学习中的回归
- en: '**Regressions** are models generally used to determine the relationship or
    **correlation** between dependent and independent variables. Within the context
    of machine learning, we define regressions as supervised machine learning models
    that allow for the identification of correlations between two or more variables
    in order to **generalize** or learn from historical data to make predictions on
    new observations.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**回归**是一般用于确定依赖变量和独立变量之间关系或**相关性**的模型。在机器学习的背景下，我们将回归定义为监督机器学习模型，允许识别两个或多个变量之间的相关性，以便**泛化**或从历史数据中学习，对新观测进行预测。'
- en: Within the confines of the **biotechnology** space, we use regression models
    to predict values in many different areas.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物技术领域的限制范围内，我们使用回归模型来预测许多不同领域的值。
- en: Predicting the LCAP of a compound ahead of time
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前预测化合物的LCAP
- en: Predicting titer results further upstream
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上游预测滴度结果
- en: Predicting the isoelectric point of a monoclonal antibody
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测单克隆抗体的等电点
- en: Predicting the decomposition percentages of compounds
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测化合物的分解百分比
- en: 'Correlations are generally established between two columns. Two columns within
    a dataset are said to have a strong **correlation** when a dependence is observed.
    The specific relationship can be better understood using a linear regression model
    such that:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在两列之间建立相关性。当观察到依赖关系时，数据集中的两列被认为具有强烈的**相关性**。可以使用线性回归模型更好地理解具体的关系，如下所示：
- en: '![](img/B17761_Formula_07_029.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17761_Formula_07_029.jpg)'
- en: In which ![](img/B17761_Formula_07_030.png) is the first feature, ![](img/B17761_Formula_07_031.png)
    is the second feature, ![](img/B17761_Formula_07_032.png) is a small error term,
    with ![](img/B17761_Formula_07_033.png) and ![](img/B17761_Formula_07_034.png)
    as constants. Using this simple equation, we can understand our data more effectively,
    and calculate any correlation. For example, recall earlier we observed a correlation
    in the toxicity dataset, specifically between the `MolWt` and `HeavyAtoms` features.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中 ![](img/B17761_Formula_07_030.png) 是第一个特征，![](img/B17761_Formula_07_031.png)
    是第二个特征，![](img/B17761_Formula_07_032.png) 是一个小的误差项，其中 ![](img/B17761_Formula_07_033.png)
    和 ![](img/B17761_Formula_07_034.png) 是常数。使用这个简单的方程，我们可以更有效地理解我们的数据，并计算任何相关性。例如，回想一下我们之前在毒性数据集中观察到的相关性，特别是`MolWt`和`HeavyAtoms`特征之间的相关性。
- en: The main idea behind any given regression model, unlike its classification counterparts,
    is to output a continuous value rather than a class or category. For example,
    we could use a number of columns in the toxicity dataset in an attempt to predict
    other columns in the same dataset.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 任何给定的回归模型背后的主要思想，与它的分类对应物不同，是输出一个连续值而不是一个类别或类别。例如，我们可以在毒性数据集中使用多个列来尝试预测同一数据集中的其他列。
- en: 'There are many different types of regression models commonly used in the data
    science space. There are linear regressions that focus on **optimizing** a linear
    relationship between a set of variables, logistic regression that acts more as
    a binary classifier rather than regressors, and ensemble models that combine the
    predictive power of several base estimators, among many others. We can see some
    examples in *Figure 7.40*:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学领域，常用的回归模型有很多种。这些模型包括专注于优化一组变量之间线性关系的线性回归，更像是二元分类器而不是回归器的逻辑回归，以及结合多个基础估计器的预测能力的集成模型，等等。我们可以在*图7.40*中看到一些例子：
- en: '![Figure 7.40 – Different types of regression models ](img/B17761_07_040.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图7.40 – 不同类型的回归模型](img/B17761_07_040.jpg)'
- en: Figure 7.40 – Different types of regression models
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.40 – 不同类型的回归模型
- en: Over the course of this section, we will explore a few of these models as we
    investigate the application of a few regression models using the toxicity dataset.
    Let's go ahead and prepare our data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的其余部分，我们将探讨这些模型中的一些，当我们调查使用毒性数据集应用一些回归模型时。让我们继续准备我们的数据。
- en: 'We can begin by importing our libraries of interest:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从导入我们感兴趣的库开始：
- en: '[PRE40]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we can go ahead and import our dataset and drop the missing rows. For
    practice, I suggest you upload this dataset to `SELECT` statement:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以继续导入我们的数据集并删除缺失的行。为了练习，我建议你将此数据集上传到`SELECT`语句：
- en: '[PRE41]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we can split our data into input and output values, and scale our data
    using the `MinMaxScaler()` class from `sklearn`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将我们的数据分为输入值和输出值，并使用`MinMaxScaler()`类从`sklearn`中进行数据缩放：
- en: '[PRE42]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we can split the dataset into training and test data:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将数据集分为训练数据和测试数据：
- en: '[PRE43]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: With our dataset prepared, we are now ready to go ahead and start exploring
    a few regression models.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集准备就绪后，我们现在可以继续前进并开始探索一些回归模型。
- en: Exploring different regression models
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索不同的回归模型
- en: 'There are many types of regression methods that can be used with a given dataset.
    We can think of regression as falling into four main categories: linear regressions,
    logistic regressions, ensemble regressions, and finally, boosted regressions.
    Throughout the next section, we will be exploring examples from each of these
    categories, starting with linear regression.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的数据集，可以使用许多类型的回归方法。我们可以将回归视为分为四个主要类别：线性回归、逻辑回归、集成回归，最后是提升回归。在下一节中，我们将探索每个这些类别中的示例，从线性回归开始。
- en: Single and multiple linear regression
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单变量和多元线性回归
- en: In many of the datasets you will likely encounter in your career, you oftentimes
    find that some of the features exhibit some type of correlation vis-à-vis one
    another. Earlier in this chapter, we discussed the idea of a correlation between
    two features as a dependence of one feature upon another, which can be calculated
    using the Pearson correlation metric known as **R2**. Over the last few chapters,
    we have looked at the idea of correlations in a few different ways, including
    heats maps and pairplots.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在你职业生涯中可能遇到的大多数数据集中，你通常会发现在某些特征之间存在某种类型的相关性。在本章早期，我们讨论了两个特征之间相关性的概念，即一个特征对另一个特征的依赖性，这可以通过称为**R2**的皮尔逊相关系数来计算。在过去的几章中，我们以不同的方式探讨了相关性的概念，包括热图和pairplot。
- en: 'Using the dataset we just prepared, we can take a look at a few correlations
    using `seaborn`:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚准备好的数据集，我们可以使用`seaborn`查看一些相关性：
- en: '[PRE44]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This yields the following figure:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图示：
- en: '![Figure 7.41 – Results of the pairplot function using the toxicity dataset
    ](img/B17761_07_041.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图7.41 – 使用毒性数据集的pairplot函数的结果](img/B17761_07_041.jpg)'
- en: Figure 7.41 – Results of the pairplot function using the toxicity dataset
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.41 – 使用毒性数据集的pairplot函数的结果
- en: 'We can see that there are a few correlations in our dataset already. Using
    **simple linear regression**, we can take advantage of this correlation in the
    sense that we can use one variable to predict what the other will most likely
    be. For example, if X was the independent variable, and Y was the dependent variable,
    we can define the linear relationship between the two as:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的数据集中已经存在一些相关性。使用**简单线性回归**，我们可以利用这种相关性，即我们可以使用一个变量来预测另一个变量最可能是什么。例如，如果X是自变量，Y是因变量，我们可以定义两者之间的线性关系为：
- en: '![](img/B17761_Formula_07_035.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17761_Formula_07_035.jpg)'
- en: In which *m* is the slope and *c* is the *y* intercept. This equation should
    be familiar to you based on some of the earlier content in this book, as well
    as your math classes in high school. Using this relationship, our objective will
    be to optimize this line relative to our data in order to determine the values
    for *m* and *c* using a method known as the least squares method.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*m*是斜率，*c*是*y*截距。根据本书前面的一些内容以及你在高中数学课上学到的知识，这个方程应该对你来说很熟悉。使用这种关系，我们的目标将是相对于我们的数据优化这条线，以确定*m*和*c*的值，使用的方法称为最小二乘法。
- en: 'Before we can discuss the **least squares method**, let''s first discuss the
    idea of a **loss function**. A loss function within the context of machine learning
    is a measure of the difference between our calculated and expected values. For
    example, let''s examine the **quadratic loss function**, commonly used to calculate
    loss within a regression model, which we can define as:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论**最小二乘法**之前，让我们首先讨论**损失函数**的概念。在机器学习的背景下，损失函数是计算值和预期值之间差异的度量。例如，让我们考察**二次损失函数**，它通常用于回归模型中的损失计算，我们可以将其定义为：
- en: '![](img/B17761_Formula_07_036.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17761_Formula_07_036.jpg)'
- en: By now, I would hope you recognize this function from our discussions in the
    *Measuring success* section. Can you tell me where we last used this?
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，我希望你能从我们在*衡量成功*部分中的讨论中认出这个函数。你能告诉我我们上次在哪里使用了它吗？
- en: Now that we have discussed the idea of loss functions, let's take a closer look
    at the **least squares method**. The main idea behind this mathematical method
    is to determine a line of best fit for a given set of data, as demonstrated by
    the correlations we just saw, by minimizing the loss. To fully explain the concepts
    behind this equation we would need to discuss partial derivatives and what not.
    For the purposes of simplicity, we will simply define the least squares method
    as a process for **minimizing** loss in order to determine a line of best fit
    for a given dataset.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了损失函数的概念，让我们更深入地了解一下**最小二乘法**。这个数学方法背后的主要思想是通过最小化损失来确定一组数据的最佳拟合线，正如我们刚才看到的关联所展示的那样。为了完全解释这个方程背后的概念，我们需要讨论偏导数等。为了简化起见，我们将简单地将最小二乘法定义为一种**最小化**损失的过程，以便确定一组数据的最佳拟合线。
- en: 'We can divide linear regressions into two main categories: **simple linear
    regression** and **multiple linear regression**. The main idea here concerns the
    number of features we will be training the model with. If we are simply training
    the model based on one feature, we will be developing a simple linear regression.
    On the other hand, if we train the model using multiple features, we would be
    training a multiple regression model.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将线性回归分为两大类：**简单线性回归**和**多元线性回归**。这里的主要思想是我们将用多少个特征来训练模型。如果我们仅仅基于一个特征来训练模型，我们将开发一个简单线性回归。另一方面，如果我们使用多个特征来训练模型，我们将训练一个多元回归模型。
- en: 'Whether you are training a simple or multiple regression model, the process
    and desired outcomes are generally the same. Ideally, the output of our models
    when plotted against the actual values should result in a linear line showing
    a strong correlation in our data, as depicted in *Figure 7.42*:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是训练简单回归模型还是多元回归模型，过程和期望的结果通常是相同的。理想情况下，我们的模型在绘制实际值时应该显示出一条线性线，显示出数据中的强相关性，如图*图7.42*所示：
- en: '![Figure 7.42 – A simple linear line showing the ideal correlation ](img/B17761_07_042.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图7.42 – 显示理想相关性的简单线性线](img/B17761_07_042.jpg)'
- en: Figure 7.42 – A simple linear line showing the ideal correlation
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.42 – 显示理想相关性的简单线性线
- en: 'Let''s go ahead and explore the development of a multiple linear regression
    model. With the data imported in the previous section, we can import the `LinearRegression`
    class for `sklearn`, fit it with our training data, and make a prediction using
    the test dataset:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探索多元线性回归模型的发展。在上一节导入的数据中，我们可以导入`sklearn`的`LinearRegression`类，用我们的训练数据对其进行拟合，并使用测试数据集进行预测：
- en: '[PRE45]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we can go ahead and use the actual and predicted values to both calculate
    the **R2** value and visualize on the graph. In addition, we will also capture
    the **MSE** metric:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用实际值和预测值来计算**R2**值，并在图上可视化。此外，我们还将捕捉**MSE**指标：
- en: '[PRE46]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The code will then yield the following figure:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将生成以下图：
- en: '![Figure 7.43 – Results of the linear regression model ](img/B17761_07_043.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图7.43 – 线性回归模型的结果](img/B17761_07_043.jpg)'
- en: Figure 7.43 – Results of the linear regression model
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.43 – 线性回归模型的结果
- en: Immediately, we notice that this simple linear regression model was quite effective
    in making predictions on our dataset. Notice that this model not only used one
    feature, but used all of the features to make its prediction. We notice from the
    graph that the vast majority of our data is localized at the bottom left. This
    is not ideal for a regression model as we would prefer the values to be equally
    dispersed across all bounds; however, it is important to remember that in the
    biotechnology space, you will almost always encounter real-world data in which
    you will observe items such as these.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 立即，我们注意到这个简单的线性回归模型在预测我们的数据集方面非常有效。请注意，这个模型不仅使用了一个特征，而且还使用了所有特征来进行预测。从图中我们可以看出，我们的大部分数据都集中在左下角。这对于回归模型来说并不理想，因为我们更希望值在所有边界上均匀分布；然而，重要的是要记住，在生物技术领域，你几乎总是会遇到真实世界的数据，其中你会观察到这些项目。
- en: If you are following along in **Jupyter Notebooks**, go ahead and reduce the
    dataset to only one input feature, scale and split the data, train a simple linear
    regression, and compare the results to the multiple linear regression. Does the
    correlation of our predictions and actual values increase or decrease?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用**Jupyter Notebooks**进行操作，请将数据集减少到只有一个输入特征，对数据进行缩放和拆分，训练一个简单线性回归，并将结果与多元线性回归进行比较。我们的预测值和实际值之间的相关性是增加还是减少？
- en: Logistic regression
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recall that in the linear regression section, we discussed the methodology
    in which a single linear line can be used to predict a value based on a correlated
    feature as input. We outlined the linear equation as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17761_Formula_07_037.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: 'In some instances, the relationship between the data and the desired output
    may not be best represented by a linear model, but rather a non-linear one:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.44 – A simple sigmoid curve ](img/B17761_07_044.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
- en: Figure 7.44 – A simple sigmoid curve
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Although known as **logistic regression**, this regression is mostly used as
    a **binary classification** algorithm. However, the main focus here is that the
    word *logistic* is referring to the **logistic function**, also known as the **Sigmoid**
    function, represented as:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17761_Formula_07_038.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: 'With this in mind, we will want to use this function to make predictions in
    our dataset. If we wanted to determine whether or not a compound was toxic given
    a specific input value, we could calculate the weighted sum of inputs such that:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17761_Formula_07_039.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
- en: 'This would allow us to calculate the probability of toxicity such that:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17761_Formula_07_040.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: Using this probability, a final prediction can be made, and an output value
    assigned. Go ahead and implement this model using the protein classification dataset
    in the previous section and compare the results that you find to those of the
    other classifiers.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees and random forest regression
  id: totrans-374
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to its classification counterpart, **Decision Tree Regressions** (**DTRs**)
    are commonly used machine learning models implementing nearly the same internal
    mechanisms that decision tree classifiers use. The only difference between the
    models is the fact that regressors output continuous numerical values, whereas
    classifiers output discrete classes.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, another model known as **Random Forest Regressors** (**RFRs**) also
    exists and operates similarly to its classification counterparts. This model is
    also an ensemble method in which each tree is trained as a separate model and
    subsequently averaged.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go ahead and implement an RFR using this dataset. Just as we have previously
    done, we will first create an instance of the model, fit it to our data, make
    a prediction, and visualize the results:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With the model developed, we can visualize the results using the following
    plot:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.45 – Results of the random forest regression model ](img/B17761_07_045.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
- en: Figure 7.45 – Results of the random forest regression model
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that while the `max_depth` parameter:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output for this code is shown below, indicating that a `max_depth` of `8`
    would likely be optimal given that it results in an `0.967` and an `248.133`:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.46 – Results of the random forest regression model with differing
    max_depth ](img/B17761_07_046.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: Figure 7.46 – Results of the random forest regression model with differing max_depth
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to classification, decision trees for regression tend to be excellent
    methods for allowing you to develop models while trying to avoid overfitting your
    data. Another great benefit of **DTR** models, when using the **sklearn** API,
    is gaining insights into feature importance directly from the model. Let''s go
    ahead and demonstrate this:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类类似，回归中的决策树通常是避免过度拟合数据的同时开发模型的好方法。当使用 **sklearn** API 时，**DTR** 模型的另一个巨大好处是能够直接从模型中获得特征重要性的见解。让我们继续演示这一点：
- en: '[PRE49]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With that complete, this yields the following diagram:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，得到以下图表：
- en: '![Figure 7.47 – Feature importance of the random forest regression model ](img/B17761_07_047.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.47 – 随机森林回归模型的特征重要性](img/B17761_07_047.jpg)'
- en: Figure 7.47 – Feature importance of the random forest regression model
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.47 – 随机森林回归模型的特征重要性
- en: Looking at this figure, we can see that the top three features that had the
    biggest impact on the development of this model were `HDonors`, `Heteroatoms`,
    and `HAcceptors`. Although this example of feature importance was developed using
    the RFR model, we can theoretically use this with many other models. One library
    in particular that has gained a great deal of importance in the field concerning
    the idea of feature importance is the `SHAP` library. It is highly recommended
    that you take a glance at this library and the many wonderful features (no pun
    intended) that it has to offer.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这张图，我们可以看到对模型发展影响最大的前三个特征是 `HDonors`、`Heteroatoms` 和 `HAcceptors`。尽管这个特征重要性的例子是使用
    RFR 模型开发的，但我们理论上可以使用它来与许多其他模型一起使用。在关于特征重要性的想法领域，有一个特别受到重视的库是 `SHAP` 库。强烈建议您浏览这个库及其提供的许多精彩特性（不是字面意义上的玩笑）。
- en: XGBoost regression
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XGBoost 回归
- en: 'Similar to the **XGBoost** classification model we investigated in the previous
    section, we also have regression implementation, which allows us to predict a
    value rather than a category. We can go ahead and implement this easily, just
    as we did in the previous section:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节中我们研究的 **XGBoost** 分类模型类似，我们也有回归实现，这使我们能够预测一个值而不是一个类别。我们可以像前一节那样轻松实现这一点：
- en: '[PRE50]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'With the code completed, this yields the following figure:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 代码完成后，得到以下图示：
- en: '![Figure 7.48 – Results of the XGBoost regression model ](img/B17761_07_048.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.48 – XGBoost 回归模型的结果](img/B17761_07_048.jpg)'
- en: Figure 7.48 – Results of the XGBoost regression model
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.48 – XGBoost 回归模型的结果
- en: You will notice that this implementation of the model gave us a very respectable
    **R2** value in the context of our actual and predicted values and managed to
    yield an **MSE** of 282.79, which is slightly less than some of the other models
    we've attempted in this chapter. With the model completed, let's now move on to
    see how we could use some of the automated machine learning functionality provided
    in AWS in the following tutorial.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，这个模型的实现给我们带来了一个非常令人尊重的 **R2** 值，在实际情况和预测值之间，并成功产生了 282.79 的 **MSE**，这略低于我们在本章尝试的一些其他模型。模型完成后，现在让我们继续看看我们如何在下一教程中使用
    AWS 提供的一些自动化机器学习功能。
- en: 'Tutorial: Regression for property prediction'
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教程：属性预测的回归
- en: Over the course of this chapter, we have reviewed some of the most common (and
    popular) regression models as they relate to the prediction of the `TPSA` feature
    using the `toxicity` dataset. In the previous section pertaining to classification,
    we created a GCP instance and used the `auto-sklearn` library to automatically
    identify one of the top machine learning models for a given dataset. In this tutorial,
    we will create an `auto-sklearn` library in a similar manner. In addition, we
    will also explore an even more powerful automated machine learning method using
    **AWS Autopilot**. In one of the earlier chapters, we used **AWS RDS** to launch
    a relation database to host our toxicity dataset. Using that same **AWS** account,
    we will now go ahead and get started.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的整个过程中，我们回顾了一些最常见（且最受欢迎）的回归模型，这些模型与使用毒性数据集预测 `TPSA` 特征相关。在前一节关于分类的部分，我们创建了一个
    GCP 实例，并使用 `auto-sklearn` 库自动识别给定数据集的最佳机器学习模型之一。在本教程中，我们将以类似的方式创建一个 `auto-sklearn`
    库。此外，我们还将探索一种更强大的自动化机器学习方法，即使用 **AWS Autopilot**。在早期的一些章节中，我们使用 **AWS RDS** 启动一个关系型数据库来托管我们的毒性数据集。使用相同的
    **AWS** 账户，我们现在将开始操作。
- en: Creating a SageMaker notebook in AWS
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 AWS 中创建 SageMaker 笔记本
- en: 'Similar to the creation of a notebook in **GCP**, we can create a **SageMaker**
    notebook in **AWS** in a few simple steps:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to the AWS Management Console on the front page. Click on the **Services**
    drop-down menu and select **Amazon SageMaker** under the **Machine Learning**
    section:![Figure 7.49 – The list of services provided by AWS ](img/B17761_07_049.jpg)
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.49 – The list of services provided by AWS
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the left-hand side of the page, click the **Notebook** drop-down menu and
    then select the **Notebook instances** button:![Figure 7.50 – A screenshot of
    the notebook menu ](img/B17761_07_050.jpg)
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.50 – A screenshot of the notebook menu
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Within the notebook instances menu, click on the orange button called **Create
    notebook instance**:![Figure 7.51 – A screenshot of the Create notebook instance
    button ](img/B17761_07_051.jpg)
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.51 – A screenshot of the Create notebook instance button
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's now go ahead and give the notebook instance a name, such as `biotech-machine-learning`.
    We can leave the instance type as the default selection of `ml.t2.medium`. This
    is a medium-tier instance and is more than enough for the purposes of our demo
    today:![Figure 7.52 – A screenshot of the notebook instance settings ](img/B17761_07_052.jpg)
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.52 – A screenshot of the notebook instance settings
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Under the **Permissions and encryption** section, select the **Create a new
    role** option for the IAM role section. The main idea behind IAM roles is the
    concept of granting certain users or roles the ability to interact with specific
    AWS resources. For example, we could allow this role to also have access to some
    but not all S3 buckets. For the purposes of this tutorial, let's go ahead and
    grant this role access to any S3 bucket. Go ahead and click on **Create role**:![Figure
    7.53 – Creating an IAM role in AWS ](img/B17761_07_053.jpg)
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.53 – Creating an IAM role in AWS
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Leaving all the other options as they are, go ahead and click on **Create notebook
    instance**. You will be redirected back to the **Notebook instance** menu where
    you will see your newly created instance in a **Pending** state. Within a few
    moments, you will notice that status change to **InService**. Click on the **Open
    JupyterLab** button to the right of the status:![Figure 7.54 – The options to
    open a Jupyter notebook or Jupyer lab in AWS ](img/B17761_07_054.jpg)
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.54 – The options to open a Jupyter notebook or Jupyer lab in AWS
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once again, you will find yourself in the familiar Jupyter environment you have
    been working in.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AWS SageMaker** is a great resource for you to use at a very low cost. Within
    this space, you will be able to run all of the Python commands and libraries you
    have learned throughout this book. You can create directories to organize your
    files and scripts and access them anywhere in the world without having to have
    your laptop with you. In addition, you will also have access to almost 100 SageMaker
    sample notebooks and starter code for you to use.'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.55 – An example list of the SageMaker starter notebooks ](img/B17761_07_055.jpg)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: Figure 7.55 – An example list of the SageMaker starter notebooks
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: With this final step complete, we now have a fully working notebook instance.
    In the following section, we will use SageMaker to import data and start running
    our models.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Creating a notebook and importing data in AWS
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given that we are once again working in our familiar Jupyter space, we can
    easily create a notebook by selecting one of the many options on the right-hand
    side and start running our code. Let''s go ahead and get started:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: We can begin by selecting the **conda_python3** option on the right-hand side,
    creating a new notebook for us in our current directory:![Figure 7.56 – A screenshot
    of conda_python3 ](img/B17761_07_056.jpg)
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.56 – A screenshot of conda_python3
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'With the notebook prepared, we will need to install a few libraries to get
    started. Go ahead and install `mysql-connector` and `pymysql` using `pip`:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we will need to import a few things:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, we can define some of the items we will need to query our data, as we
    did previously in [*Chapter 3*](B17761_03_Final_JM_ePub.xhtml#_idTextAnchor050),
    *Getting Started with SQL and Relational Databases*:'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we can go ahead and create a connection to our **RDS** instance:'
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-432
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we can go ahead and query our data using a basic `SELECT` statement:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: With that complete, we are now able to query our data directly from **AWS RDS**.
    As you begin to explore new models in the realm of data science, you will need
    a place to store and organize your data. Selecting a platform such as **AWS RDS**,
    **AWS S3**, or even **GCP** **BigQuery** will help you organize your data and
    studies.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Running auto-sklearn using the toxicity dataset
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have our data in a working notebook, let''s go ahead and use the
    `auto-sklean` library to identify a model best suited for our given dataset:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'We can begin by installing the `auto-sklearn` library in our **SageMaker**
    instance:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, we can isolate our input features and output values and scale them accordingly:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With the data scaled, we can now go ahead and separate our training and test
    datasets:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, we can import the regression implementation of `sklearn`, adjust the
    parameters, and fit the model to our dataset:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Once the model is done, we can take a look at the top-performing candidate
    model using the `get_models_with_weights()` function:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Lastly, we can go ahead and get a sense of the model''s performance using the
    **R2** and **MSE** metrics, as we have done previously:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Upon plotting the output, this yields the following results:'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.57 – Results of the AutoML model showing the correlation of 0.97
    ](img/B17761_07_057.jpg)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
- en: Figure 7.57 – Results of the AutoML model showing the correlation of 0.97
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: We can see from the figure above that the actual results and predicted results
    line up quite nicely, giving us an R2 value of approximately 0.97, showing a strong
    correlation. In the following section, we will explore the process of automating
    parts of the model development process using AWS Autopilot.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: Automated regression using AWS Autopilot
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many different tools and applications can be found in the **AWS Management
    Console**, offering solutions to many data science and computer science problems
    any developer will likely encounter. There is one tool in particular that has
    stood out and begun to grow in popularity among the data science community known
    as **AWS Autopilot**. The purpose of **AWS Autopilot** is to help automate some
    of the tasks generally undertaken in any given data science project. We can see
    a visual representation of this in *Figure 7.58*:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.58 – The Autopilot pipeline ](img/B17761_07_058.jpg)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: Figure 7.58 – The Autopilot pipeline
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: 'Users are able to load in their datasets, identify a few parameters, and let
    the model take it from there as it identifies the top-performing models, optimizes
    a few parameters, and even generates sample code for the user to take and optimize
    even further. Let''s go ahead and demonstrate the use of this model using the
    same dataset:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: We can begin by creating a SageMaker Studio instance by navigating to the SageMaker
    console and selecting the **Open SageMaker Studio** button on the right. Using
    the quick start option, the default settings, and a new **IAM role**, click the
    **Submit** button. After a few moments, the instance will provision. Click on
    the **Open Studio** button:![Figure 7.59 – The Open Studio option in AWS ](img/B17761_07_059.jpg)
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.59 – The Open Studio option in AWS
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While the instance is provisioning, let's upload our dataset to `biotech-machine-learning`
    while keeping all the other options at their default values.![Figure 7.60 – Creating
    a bucket in AWS ](img/B17761_07_060.jpg)
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.60 – Creating a bucket in AWS
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once created, open the bucket and click on the **Upload** button. Then, upload
    the CSV file of the reduced and cleaned proteins dataset.![Figure 7.61 – Uploading
    files in AWS ](img/B17761_07_061.jpg)
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.61 – Uploading files in AWS
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With the dataset uploaded, let's now head back to SageMaker. Using the navigation
    pane on the left, select the **SageMaker Components and Registries** tab. Using
    the drop-down menu, select **Experiments and trials**, and then click the **Create
    Autopilot Experiment** button:![Figure 7.62 – Creating SageMaker resources in
    AWS SageMaker Studio ](img/B17761_07_062.jpg)
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.62 – Creating SageMaker resources in AWS SageMaker Studio
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Let's move on and give the experiment a name, such as `dataset-pdb-nodups-cleaned`.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **CONNECT YOUR DATA** section, select the S3 bucket name you created
    earlier, as well as the dataset filename:![Figure 7.63 – Connecting data to the
    experiment  ](img/B17761_07_063.jpg)
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.63 – Connecting data to the experiment
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, select the target column, which in our case, is the classification column:![Figure
    7.64 – Selecting a target column for the model training process ](img/B17761_07_064.jpg)
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 7.64 – Selecting a target column for the model training process
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, you can now go ahead and disable the **Auto deploy** option and click
    **Create Experiment**. Similar to GCP's **AutoML**, the application will identify
    a set of models deemed to be the best fit for your given dataset. You have the
    option to select between **Pilot** or **Complete**.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A complete experiment will train and tune the model while allowing users to
    view the details and statistics in real time. It will go through various phases,
    such as preprocessing, candidate definition generation, feature engineering, model
    tuning, and report generation.
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upon completing the process, a dashboard with all the trained models and their
    associated metrics will be presented, as depicted in *Figure 7.65*. Users can
    explore the models and deploy them in a few simple clicks.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.65 – Results of the Autopilot model ](img/B17761_07_065.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
- en: Figure 7.65 – Results of the Autopilot model
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: AWS Autopilot is a robust and useful tool that every data scientist can utilize
    when facing a difficult dataset. It not only assists in identifying the best model
    for a given dataset, but can also help preprocess the data, tune the model, and
    provide sample code for users to use.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-478
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! We finally made it to the end of a very dense, yet very informative
    chapter. In this chapter, we learned quite a few different things. In the first
    half of this chapter, we explored the realm of classification and demonstrated
    the application of a number of models using the single-cell RNA dataset – a classical
    application in the field of biotechnology and life sciences. We learned about
    a number of different models, including KNNs, SVMs, decision trees, random forests,
    and XGBoost. We then moved our data and code to GCP, where we stored our data
    in BigQuery, and provisioned a notebook instance to run our code in. In addition,
    we learned how to automate some of the manual and labor-intensive parts of the
    model development process as it pertains to the protein classification dataset
    using auto-sklearn. Finally, we took advantage of GCP's AutoML application to
    develop a classification model for our dataset.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we explored the realm of regression as it
    pertains to the toxicity dataset. We explored the idea of correlation within data
    and learned a few important regression models too. Some of the models we looked
    at included simple and multiple linear regression, logistic regression, decision
    tree regressors, and an XGBoost regressor as well. We then moved our code to AWS's
    SageMaker platform and used the previously provisioned RDS to query our data and
    run auto-sklearn for regression as well. Finally, we implemented AWS Autopilot's
    automated machine learning model for the toxicity dataset.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have spent much of our time developing machine learning models using
    the `sklearn` library. However, not every dataset can be classified or regressed
    using machine learning – sometimes, a more powerful set of models will be needed.
    For datasets such as those, we can turn to the field of deep learning, which will
    be our focus for the next chapter.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
