- en: Ensemble Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to discuss some important algorithms that exploit
    different estimators to improve the overall performance of an ensemble or committee.
    These techniques work either by introducing a medium level of randomness in every
    estimator belonging to a predefined set or by creating a sequence of estimators
    where, each new model is forced to improve the performance of the previous ones.
    These techniques allow us to reduce both the bias and the variance (thereby increasing
    validation accuracy) when employing models with a limited capacity or more prone
    to overfit the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the topics covered in the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to ensemble learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief introduction to decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest and extra randomized forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaBoost (algorithms M1, SAMME, SAMME.R, and R2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembles of voting classifiers, stacking, and bucketing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble learning fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main concept behind ensemble learning is the distinction between strong
    and weak learners. In particular, a strong learner is a classifier or a regressor
    which has enough capacity to reach the highest potential accuracy, minimizing
    both bias and variance (thus achieving also a satisfactory level of generalization).
    More formally, if we consider a parametrized binary classifier *f(x; θ)*, we define
    it as a strong learner if the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e85388b3-c547-42a0-a706-4e9975ae1a92.png)'
  prefs: []
  type: TYPE_IMG
- en: This expression can appear cryptic; however, it's very easy to understand. It
    simply expresses the concept that a strong learner is theoretically able to achieve
    any non-null probability of misclassification with a probability greater than
    or equal to 0.5 (that is, the threshold for a binary random guess). All the models
    normally employed in Machine Learning tasks are normally strong learners, even
    if their domain can be limited (for example, a logistic regression cannot solve
    non-linear problems). On the other hand, a weak learner is a model that is generically
    able to achieve an accuracy slightly higher than a random guess, but whose complexity
    is very low (they can be trained very quickly, but can never be used alone to
    solve complex problems). There is a formal definition also in this case, but it's
    simpler to consider that the real main property of a weak learner is a limited
    ability to achieve a reasonable accuracy. In some very particular and small regions
    of the training space, a weak learner could reach a low probability of misclassification,
    but in the whole space its performance is only a little bit superior to a random
    guess. The previous one is more a theoretical definition than a practice one,
    because all the models currently available are normally quite better than a random
    oracle. However, an ensemble is defined as a set of weak learners that are trained
    together (or in a sequence) to make up a committee. Both in classification and
    regression problems, the final result is obtained by averaging the predictions
    or employing a majority vote.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, a reasonable question is—Why do we need to train many weak learners
    instead of a single strong one? The answer is double—in ensemble learning, we
    normally work with medium-strong learners (such as decision trees or **support
    vector machines** (**SVMs**)) and we use them as a committee to increase the overall
    accuracy and reduce the variance thanks to a wider exploration of the sample space.
    In fact, while a single strong learner is often able to overfit the training set,
    it's more difficult to keep a high accuracy over the whole sample subspace without
    saturating the capacity. In order to avoid overfitting, a trade-off must be found
    and the result is a less accurate classifier/regressor with a simpler separation
    hyperplane. The adoption of many weak learners (that are actually quite strong,
    because even the simplest models are more accurate than a random guess), allows
    us to force them to focus only on a limited subspace, so as to be able to reach
    a very high local accuracy with a low variance. The committee, employing an averaging
    technique, can easily find out which prediction is the most suitable. Alternatively,
    it can ask each learner to vote, assuming that a successful training process must
    always lead the majority to propose the most accurate classification or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common approaches to ensemble learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging (bootstrap aggregating)**: This approach trains *n* weak learners
    *fw1, fw2, ..., fwn* (very often they are decision trees) using *n* training sets
    (*D1, D2, ..., Dn*) created by randomly sampling the original dataset *D*. The
    sampling process (called **bootstrap sampling**) is normally performed with replacement,
    so as to determine different data distributions. Moreover, in many real algorithms,
    the weak learners are also initialized and trained using a medium degree of randomness.
    In this way, the probability of having clones becomes very small and, at the same,
    time it''s possible to increase the accuracy by keeping the variance under a tolerable
    threshold (thus avoiding overfitting).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: This is an alternative approach that builds an incremental ensemble
    starting with a single weak learner *fw1* and adding a new one *fwi* at each iteration.
    The goal is to reweight the dataset, so as to force the new learner to focus on
    the samples that were previously misclassified. This strategy yields a very high
    accuracy because the new learners are trained with a positively-biased dataset
    that allows them to adapt to the most difficult internal conditions. However,
    in this way, the control over the variance is weakened and the ensemble can more
    easily overfit the training set. It''s possible to mitigate this problem by reducing
    the complexity of the weak learners or imposing a regularization constraint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stacking**: This method can be implemented in different ways but the philosophy
    is always the same—use different algorithms (normally a few strong learners) trained
    on the same dataset and filter the final result using another classifier, averaging
    the predictions or using a majority vote. This strategy can be very powerful if
    the dataset has a structure that can be partially managed with different approaches.
    Each classifier or regressor should discover some data aspects that are peculiar;
    that''s why the algorithms must be structurally different. For example, it can
    be useful to mix a decision tree with a SVM or linear and kernel models. The evaluation
    performed on the test set should clearly show the prevalence of a classifier only
    in some cases. If an algorithm is finally the only one that produces the best
    prediction, the ensemble becomes useless and it''s better to focus on a single
    strong learner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A random forest is the bagging ensemble model based on decision trees. If the
    reader is not familiar with this kind of model, I suggest reading the *Introduction
    to Machine Learning*, *Alpaydin E.*, *The MIT Press*, where a complete explanation
    can be found. However, for our purposes, it''s useful to provide a brief explanation
    of the most important concepts. A decision tree is a model that resembles a standard
    hierarchical decision process. In the majority of cases, a special family is employed,
    called binary decision trees, as each decision yields only two outcomes. This
    kind of tree is often the simplest and most reasonable choice and the training
    process (which consists in building the tree itself) is very intuitive. The root
    contains the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/207960e3-1f7e-440d-9a30-14e364aae79c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each level is obtained by applying a selection tuple, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94702ff1-52eb-4f64-a05e-d6520cfe18fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first index of the tuple corresponds to an input feature, while the threshold
    *t[i]* is a value chosen in the specific range of each feature. The application
    of a selection tuple leads to a split and two nodes that contain each a non-overlapping
    subset of the input dataset. In the following diagram, there''s an example of
    a slip performed at the level of the root (initial split):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a04f6f3f-79da-4289-b0a7-db4ddb65353d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of initial split in a decision tree
  prefs: []
  type: TYPE_NORMAL
- en: The set *X* is split into two subsets defined as *X11* and *X12* whose samples
    have respectively the feature with *i=2* less or greater than the threshold *ti=0.8*.
    The intuition behind classification decision trees is to continue splitting until
    the leaves contain samples belonging to a single category *yi* (these nodes are
    defined as pure). In this way, a new sample *xj* can traverse the tree with a
    computation complexity *O(log(M))* and reach a final node that determines its
    category. In a very similar way, it's possible to build regression trees whose
    output is continuous (even if, for our purposes, we are going to consider only
    classification scenarios).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the main problem is how to perform each split. We cannot pick
    any feature and any threshold, because the final tree will be completely unbalanced
    and very deep. Our goal is to find the optimal selection tuple at each node considering
    the final goal, which is classification into discrete categories (the process
    is almost identical for regressions). The technique is very similar to a problem
    based on a cost function that must be minimized, but, in this case, we operate
    locally, applying an impurity measure proportional to the heterogeneity of a node.
    A high impurity indicates that samples belonging to many different categories
    are present, while an impurity equal to 0 indicates that a single category is
    present. As we need to continue splitting until a pure leaf appears, the optimal
    choice is based on a function that scores each selection tuple, allowing us to
    select the one that yields the lowest impurity (theoretically, the process should
    continue until all the leaves are pure, but normally a maximum depth is provided,
    so as to avoid excessive complexity). If there are p classes, the category set
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f687377f-feb0-482c-88b0-290fcb4a533f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A very common impurity measure is called **Gini impurity** and it''s based
    on the probability of a misclassification if a sample is categorized using a label
    randomly chosen from the node subset distribution. Intuitively, if all the samples
    belong to the same category, any random choice leads to a correct classification
    (and the impurity becomes *0*). On the other side, if the node contains samples
    from many categories, the probability of a misclassification increases. Formally,
    the measure is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/420ffd2d-0948-4cc2-99fe-6b3a5e64f1b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The subset is indicated by *Xk* and *p(j|k)* is obtained as the ratio of the
    samples belonging to the class *j* over the total number of samples. The selection
    tuple must be chosen so as to minimize the Gini impurity of the children. Another
    common approach is the cross-entropy impurity, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95e8f999-d39c-40ee-ad7e-405a812b0875.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The main difference between this measure and the previous one is provided by
    some fundamental information theory concepts. In particular, the goal we want
    to reach is the minimization of the uncertainty, which is measured using the *(Cross-)Entropy*.
    If we have a discrete distribution and all the samples belong to the same category,
    a random choice is can fully describe the distribution; therefore, the uncertainty
    is null. On the contrary, if, for example, we have a fair die, the probability
    of each outcome is 1/6 and the corresponding entropy is about 2.58 bits (if the
    base of the logarithm is 2). When the nodes become purer and purer, the cross-entropy
    impurity decreases and reaches 0 in an optimal scenario. Moreover, adopting the
    concept of mutual information, we can define the information gain obtained after
    a split has been performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcec7570-adf4-445f-b1db-51ee487b1e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Given a node, we want to create two children to maximize the information gain.
    In other words, by choosing the cross-entropy impurity we implicitly grow the
    tree until the information gain becomes null. Considering again the example of
    a fair die, we need 2.58 bits of information to decide which is the right outcome.
    If, instead, the die is loaded and the probability of an outcome is 1.0, we need
    no information to make a decision. In a decision tree, we'd like to resemble this
    situation, so that, when a new sample has completely traversed the tree, we don't
    need any further information to classify it. If a maximum depth is imposed, the
    final information gain cannot be null. This means that we need to pay an extra
    cost to finalize the classification. This cost is proportional to the residual
    uncertainty and should be minimized to increase the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Other methods can also be employed (even if Gini and cross-entropy are the most
    common) and I invite the reader to check the references for further information.
    However, at this point, a consideration naturally arises. Decision trees are simple
    models (they are not weak learners!), but the procedure for building them is more
    complex than, for example, training a logistic regression or a SVM. Why are they
    so popular? One reason is already clear—they represent a structural process that
    can be shown using a diagram; however, this is not enough to justify their usage.
    Two important properties allow the employment of decision trees without any data
    preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it's easy to understand that, contrary to other methods, there's no
    need for any scaling or whitening and it's possible to use continuous and categorical
    features at the same time. For example, if in a bidimensional dataset a feature
    has a variance equal to 1 and the other equal to 100, the majority of classifiers
    will achieve a low accuracy; therefore, a preprocessing step becomes necessary.
    In a decision tree, a selection tuple has the same effect also when the ranges
    are very different. It goes without saying that a split can be easily performed
    considering also categorical features and there's no need, for example, to use
    techniques such as one-hot encoding (which is necessary in most cases to avoid
    generalization errors). However, unfortunately, the separation hypersurface obtained
    with a decision tree is normally much more complex than the one obtained using
    other algorithms and this drives to a higher variance with a consequential loss
    of generalization ability.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the reason, it's possible to imagine a very simple bidimensional
    dataset made up of two blobs located in the second and fourth quarters. The first
    set is characterized by *(x < 0, y > 0)*, but the second one by *(x < 0, y < 0)*.
    Let's also suppose that we have a few outliers, but our knowledge about the data
    generating process is not enough to qualify them as noisy samples (the original
    distribution can have tails that are extended over the axes; for example, it may
    be a mixture of two Gaussians). In this scenario, the simplest separation line
    is a diagonal splitting the plane into two subplanes containing regions belonging
    also to the first and third quarters. However, this decision can be made only
    considering both coordinates at the same time. Using a decision tree, we need
    to split initially, for example, using the first feature and again with the second
    one. The result is a piece-wise separation line (for example, splitting the plane
    into the region corresponding to the second quarter and its complement), leading
    to a very high classification variance. Paradoxically, a better solution can be
    obtained with an incomplete tree (limiting the process, for example, to a single
    split) and with the selection of the *y*-axis as the separation line (this is
    why it's important to impose a maximum depth), but the price you pay is an increased
    bias (and a consequently worse accuracy).
  prefs: []
  type: TYPE_NORMAL
- en: Another important element to consider when working with decision trees (and
    related models) is the maximum depth. It's possible to grow the tree until the
    all leaves are pure, but sometimes it's preferable to impose a maximum depth (and,
    consequently, a maximum number of terminal nodes). A maximum depth equal to 1
    drives to binary models called **decision stumps**, which don't allow any interaction
    among the features (they can simply be represented as *If... Then* conditions).
    Higher values yield more terminal nodes and allow an increasing interaction among
    features (it's possible to think about a combination of many *If... Then* statements
    together with `AND` logical operators). The right value must be tuned considering
    every single problem and it's important to remember that very deep trees are more
    prone to overfitting than pruned ones.
  prefs: []
  type: TYPE_NORMAL
- en: In some contexts, it's preferable to achieve a slightly worse accuracy with
    a higher generalization ability and, in those case, a maximum depth should be
    imposed. The common tool to determine the best value is always a grid search together
    with a cross-validation technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Random forests provide us with a powerful tool to solve the bias-variance trade-off
    problem. They were proposed by L. Breiman (in *Breiman L.*, *Random Forests*,
    *Machine Learning*, *45*, *2001*) and their logic is very simple. As already explained
    in the previous section, the bagging method starts with the choice of the number
    of weak learners, *Nc*. The second step is the generation of Nc datasets (called
    bootstrap samples) *D1, D2, ..., DNc*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7223f375-7714-439a-99f1-c5927a22dd01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each decision tree is trained using the corresponding dataset using a common
    impurity criterion; however, in a random forest, in order to reduce the variance,
    the selection splits are not computed considering all the features, but only via
    a random subset containing a quite smaller number of features (common choices
    are the rounded square root, log2 or natural logarithm). This approach indeed
    weakens each learner, as the optimality is partially lost, but allows us to obtain
    a drastic variance reduction by limiting the over-specialization. At the same
    time, a bias reduction and an increased accuracy are a result of the ensemble
    (in particular for a large number of estimators). In fact, as the learners are
    trained with slightly different data distributions, the average of a prediction
    converges to the right value when *Nc → ∞* (in practice, it''s not always necessary
    to employ a very large number of decision trees, however, the correct value must
    be tuned using a grid search with cross-validation). Once all the models, represented
    with a function *di(x)*, have been trained, the final prediction can be obtained
    as an average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a7ab209-96a9-4ae5-bd03-3328a219e30f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alternatively, it''s possible to employ a majority vote (but only for classifications):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1db39b66-3c55-486d-946e-99d8a17d431f.png)'
  prefs: []
  type: TYPE_IMG
- en: These two methods are very similar and, in most cases, they yield the same result.
    However, averaging is more robust and allows an improved flexibility when the
    samples are almost on the boundaries. Moreover, it can be used for both classification
    and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests limit their randomness by picking the best selection tuple from
    a smaller sample subset. In some cases, for example, when the number of features
    is not very large, this strategy drives to a minimum variance reduction and the
    computational cost is no longer justified by the result. It's possible to achieve
    better performances with a variant called extra-randomized trees (or simply extra-trees).
    The procedure is almost the same; however, in this case, before performing a split,
    *n* random thresholds are computed (for each feature) and the one which leads
    to the least impurity is chosen. This approach further weakens the learners but,
    at the same time, reduces residual variance and prevents overfitting. The dynamic
    is not very different from many techniques such as regularization or dropout (we're
    going to discuss this approach in the next chapter); in fact, the extra-randomness
    reduces the capacity of the model, forcing it to a more linearized solution (which
    is clearly sub-optimal). The price to pay for this limitation is a consequent
    bias worsening, which, however, is compensated by the presence of many different
    learners. Even with random splits, when *Nc* is large enough, the probability
    of a wrong classification (or regression prediction) becomes smaller and smaller
    because both the average and the majority vote tend to compensate the outcome
    of trees whose structure is strongly sub-optimal in particular regions. This result
    is easier to obtain, in particular, when the number of training samples is large.
    In this case, in fact, sampling with replacement leads to slightly different distributions
    that could be considered (even if this is not formally correct) as partially and
    randomly boosted. Therefore, every weak learner will implicitly focus on the whole
    dataset with extra-attention to a smaller subset that, however, is randomly selected
    (differently from actual boosting).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete random forest algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the number of decision trees *Nc*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a dataset *Di* sampling with replacements from the original dataset *X*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of features to consider during each split *Nf* (for example,
    *sqrt(n)*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set an impurity measure (for example, Gini impurity)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an optional maximum depth for each tree
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Random forest:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the decision tree *di(x)* using the dataset *Di* and selecting the best
    split among *Nf* features randomly sampled
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Extra-trees:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the decision tree *di(x)* using the dataset *Di*, computing before each
    split *n* random thresholds and selecting the one that yields the least impurity
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Define an output function averaging the single outputs or employing a majority
    vote
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of random forest with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to use the famous Wine dataset (178 13-dimensional
    samples split into three classes) that is directly available in Scikit-Learn.
    Unfortunately, it''s not so easy to find good and simple datasets for ensemble
    learning algorithms, as they are normally employed with large and complex sets
    that require too long a computational time. As the Wine dataset is not particularly
    complex, the first step is to assess the performances of different classifiers
    (logistic regression, decision tree, and polynomial SVM) using a k-fold cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the performances are quite good, with a top value of average cross-validation
    accuracy equal to about 96% achieved by the polynomial (the default degree is
    3) SVM. A very interesting element is the performance of the decision tree, the
    worst of the set (with Gini impurity it''s lower). Even if it''s not correct,
    we can define this model as the weakest of the group and it''s a perfect candidate
    for our bagging test. We can now fit a Random Forest by instantiating the class
    `RandomForestClassifier` and selecting `n_estimators=50` (I invite the reader
    to try different values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the average cross-validation accuracy is the highest, about 98.3%.
    Therefore, the random forest has successfully found a global configuration of
    decision trees, so as to specialize them in almost any region of the sample space.
    The parameter `n_jobs=cpu_count()` tells Scikit-Learn to parallelize the training
    process using all of the CPU cores available in the machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the dynamics of this model, it''s useful to plot the cross-validation
    accuracy as a function of the number of trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/458c8030-ceab-46d5-89e5-8183299e051d.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation accuracy of a random forest as a function of the number of
    trees
  prefs: []
  type: TYPE_NORMAL
- en: It's not surprising to observe some oscillations and a plateau when the number
    of trees becomes greater at about 320\. The effect of the randomness can cause
    a performance loss, even increasing the number of learners. In fact, even if the
    training accuracy grows, the validation accuracy on different folds can be affected
    by an over-specialization. Moreover, in this case, it's very interesting to notice
    that the top accuracy is achievable with 50 trees instead of 400 or more. For
    this reason, I always suggest performing at least a grid search, in order not
    only to achieve the best accuracy but also to minimize the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important element to consider when working with decision trees and
    random forests is feature importance (also called Gini importance when this criterion
    is chosen), which is a measure proportional to the impurity reduction that a particular
    feature allows us achieve. For a decision tree, it is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8f75f6a-5542-415c-91c0-43f2cd907dd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous formula, *n(j)* denotes the number of samples reaching the
    node *j* (the sum must be extended to all nodes where the feature is chosen) and
    *ΔIi* is the impurity reduction achieved at node *j* after splitting using the
    feature *i*. In a random forest, the importance must be computed by averaging
    over all trees:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc2e83bd-307a-45cc-a990-135ad97e7518.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After fitting a model (decision tree or random forest), Scikit-Learn outputs
    the feature importance vector in the `feature_importances_ instance` variable.
    In the following graph, there''s a plot showing the importance of each feature
    (the labels can be obtained with the command `load_wine()[''feature_names''])`
    in descending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22a4f588-560e-44c4-af5e-1dcdee9ed168.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature importances for Wine dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t want to analyze the chemical meaning of each element, but it''s clear
    that, for example, the presence of proline and the color intensity are much more
    important than the presence of non-flavonoid phenols. As the model is working
    with features that are semantically independent (it''s not the same for the pixels
    of an image), it''s possible to reduce the dimensionality of a dataset by removing
    all those features whose importance doesn''t have a high impact on the final accuracy.
    This process, called **feature selection**, should be performed using more complex
    statistical techniques, such as Chi-squared, but when a classifier is able to
    produce an importance index, it''s also possible to use a Scikit-Learn class called
    `SelectFromModel`. Passing an estimator (that can be fitted or not) and a threshold,
    it''s possible to transform the dataset by filtering out all the features whose
    value is below the threshold. Applying it to our model and setting a minimum importance
    equal to `0.02`, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The new dataset now contains 10 features instead of the 13 of the original Wine
    dataset (for example., it's easy to verify that ash and non-flavonoid phenols
    have been removed). Of course, as for any other dimensionality reduction method,
    it's always suggested you verify the final accuracy with a cross-validation and
    make decisions only if the trade-off between loss of accuracy and complexity reduction
    is reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we have seen that sampling with a replacement leads
    to datasets where the samples are randomly reweighted. However, if *M* is very
    large, most of the samples will appear only once and, moreover, all the choices
    are totally random. AdaBoost is an algorithm proposed by Schapire and Freund that
    tries to maximize the efficiency of each weak learner by employing adaptive boosting
    (the name derives from this). In particular, the ensemble is grown sequentially
    and the data distribution is recomputed at each step so as to increase the weight
    of those samples that were misclassified and reduce the weight of the ones that
    were correctly classified. In this way, every new learner is forced to focus on
    those regions that were more problematic for the previous estimators. The reader
    can immediately understand that, contrary to random forests and other bagging
    methods, boosting doesn''t rely on randomness to reduce the variance and improve
    the accuracy. Rather, it works in a deterministic way and each new data distribution
    is chosen with a precise goal. In this paragraph, we are going to consider a variant
    called **Discrete AdaBoost** (formally *AdaBoost.M1*), which needs a classifier
    whose output is thresholded (for example, *-1* and *1*). However, real-valued
    versions (whose output behaves like a probability) have been developed (a classical
    example is shown in *Additive Logistic Regression: a Statistical View of Boosting*,* Friedman
    J.*, *Hastie T.*, *Tibshirani R.*,* Annals of Statistics*, 28/1998). As the main
    concepts are always the same, the reader interested in the theoretical details
    of other variants can immediately find them in the referenced papers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, the training dataset of **AdaBoost.M1** is defined as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cca3c68-157b-4ca9-ab53-baeee3d284f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This choice is not a limitation because, in multi-class problems, a one-versus-the-rest
    strategy can be easily employed, even if algorithms like **AdaBoost.SAMME** guarantee
    a much better performance. In order to manipulate the data distribution, we need
    to define a weight set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/532490e8-b784-4762-ac4c-74c24c91f4db.png)'
  prefs: []
  type: TYPE_IMG
- en: The weight set allows defining an implicit data distribution *D(t)(x)*, which
    initially is equivalent to the original one but that can be easily reshaped by
    changing the values *wi*. Once the family and the number of estimators, *Nc*,
    have been chosen, it's possible to start the global training process. The algorithm
    can be applied to any kind of learner that is able to produce thresholded estimations
    (while the real-valued variants can work with probabilities, for example, obtained
    through the Platt scaling method).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first instance *d1(x)* is trained with the original dataset, which means
    with the data distribution *D(1)(x)*. The next instances, instead, are trained
    with the reweighted distributions *D(2)(x), D(3)(x), ..., D(Nc)(x)*. In order
    to compute them, after each training process, the normalized weighted error sum
    is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/95ea5305-93ea-4421-a85a-25ad479a6bc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This value is bounded between *0* (no misclassifications) and *1* (all samples
    have been misclassified) and it''s employed to compute the estimator weight *α(t)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15597875-3154-412d-a332-0d145c51c7ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand how this function works, it''s useful to consider its plot (shown
    in the following diagram):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3267ea61-5b68-4668-b290-b59ca43cd89d.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimator weight plot as a function of the normalized weighted error sum
  prefs: []
  type: TYPE_NORMAL
- en: 'This diagram unveils an implicit assumption: the worst classifier is not the
    one that misclassifies all samples (*ε(t) = 1*), but a totally random binary guess
    (corresponding to *ε(t) = 0.5*). In this case, *α(t)* is null and, therefore,
    the outcome if the estimator is completely discarded. When *ε(t) < 0.5*, a boosting
    is applied (between about 0.05 and 0.5, the trend is almost linear), but it becomes
    greater than 1 only when *ε(t)* < about 0.25 (larger values drive to a penalty
    because the weight is smaller than 1). This value is a threshold to qualify an
    estimator as trusted or very strong and *α(t) → +∞* in the particular case of
    a perfect estimator (no errors).'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, an upper bound should be imposed in order to avoid overflows or
    divisions by zero. Instead, when *ε(t) > 0.5*, the estimator is unacceptably weak,
    because it's worse than a random guess and the resulting boosting would be negative.
    To avoid this problem, real implementations must invert the output of such estimators,
    transforming them de facto into learners with *ε(t) < 0.5* (this is not an issue,
    as the transformation is applied to all output values in the same way). It's important
    to consider that this algorithm shouldn't be directly applied to multi-class scenarios
    because, as pointed out in *Multi-class AdaBoost*,* Zhu J.*, *Rosset S.*, *Zou
    H.*, *Hastie T.*,* 01/2006*, the threshold 0.5 corresponds to a random guess accuracy
    only for binary choices. When the number of classes is larger than two, a random
    estimator outputs a class with a probability *1/Ny* (where *Ny* is the number
    of classes) and, therefore, AdaBoost.M1 will boost the classifiers in a wrong
    way, yielding poor final accuracies (the real threshold should be *1 - 1/Ny*,
    which is larger than 0.5 when *Ny > 2*). The AdaBoost.SAMME algorithm (implemented
    by Scikit-Learn) has been proposed to solve this problem and exploit the power
    of boosting also in multi-class scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'The global decision function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/621e3cfe-d929-4bf7-93c0-31b9cb09cf09.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, as the estimators are added sequentially, the importance of each
    of them will decrease while the accuracy of *di(x)* increases. However, it's also
    possible to observe a plateau if the complexity of *X* is very high. In this case,
    many learners will have a high weight, because the final prediction must take
    into account a sub-combination of learners in order to achieve an acceptable accuracy.
    As this algorithm specializes the learners at each step, a good practice is to
    start with a small number of estimators (for example, 10 or 20) and increase the
    number until no improvement is achieved. Sometimes, a minimum number of good learners
    (like SVM or decision trees) is sufficient to reach the highest possible accuracy
    (limited to this kind of algorithm), but in some other cases, the number of estimators
    can be some thousands. Grid search and cross-validation are again the only good
    strategies to make the right choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'After each training step it is necessary to update the weights in order to
    produce a boosted distribution. This is achieved using an exponential function
    (based on bipolar outputs *{-1, 1}*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/affbe3ea-ac1a-433d-8490-f2260c52d34b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Given a sample *x[i]*, if it has been misclassified, its weight will be increased
    considering the overall estimator weight. This approach allows a further adaptive
    behavior because a classifier with a high *α(t)* is already very accurate and
    it''s necessary a higher attention level to focus only on the (few) misclassified
    samples. On the contrary, if *α(t)* is small, the estimator must improve its overall
    performance and the over-weighting process must be applied to a large subset (therefore,
    the distribution won''t peak around a few samples, but will penalize only the
    small subset that has been correctly classified, leaving the estimator free to
    explore the remaining space with the same probability). Even if not present in
    the original proposal, it''s also possible to include a learning rate *η* that
    multiplies the exponent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/065c5ca6-a718-4d26-94df-93c1bc6ba57d.png)'
  prefs: []
  type: TYPE_IMG
- en: A value *η = 1* has no effect, while smaller values have been proven to increase
    the accuracy by avoiding a premature specialization. Of course, when *η << 1*,
    the number of estimators must be increased in order to compensate the minor reweighting
    and this can drive to a training performance loss. As for the other hyperparameters,
    the right value for *η* must be discovered using a cross-validation technique
    (alternatively, if it's the only value that must be fine-tuned, it's possible
    to start with one and proceed by decreasing its value until the maximum accuracy
    has been reached).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete AdaBoost.M1 algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the family and the number of estimators *Nc*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial weights *W(1)* equal to *1/M*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the learning rate *η* (for example, *η = 1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the normalized weighted error sum *ε(i)*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *ε(i) > 0.5*, invert all estimator outputs
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the estimator weight *α(i)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights using the exponential formula (with or without the learning
    rate)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the weights
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the global estimator applying the sign(•) function to the weighted sum *α(i)di(x)*
    (for *i=1* to *Nc*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaBoost.SAMME
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This variant, called **Stagewise Additive Modeling using a Multi-class Exponential
    loss** (**SAMME**), was proposed by Zhu, Rosset, Zou, and Hastie in *Multi-class
    AdaBoost*,* Zhu J.*, *Rosset S.*, *Zou H.*, *Hastie T.*,* 01/2006*. The goal is
    to adapt AdaBoost.M1 in order to work properly in multi-class scenarios. As this
    is a discrete version, its structure is almost the same, with a difference in
    the estimator weight computation. Let''s consider a label dataset, *Y*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93ea80f7-59b0-413b-9c59-1b65285610d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, there are *p* different classes and it''s necessary to consider that a
    random guess estimator cannot reach an accuracy equal to 0.5; therefore, the new
    estimator weights are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b32c95f1-abcb-4c3d-b36c-cfba45033445.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this way, the threshold is pushed forward and *α(t)* will be zero when the
    following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c30ed323-a649-4852-b175-540e93085a9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows the plot of *α(t)* with *p = 10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dbe4459-d05f-4e36-a488-70cf4fb0ca60.png)'
  prefs: []
  type: TYPE_IMG
- en: Estimator weight plot as a function of the normalized weighted error sum when
    p = 10
  prefs: []
  type: TYPE_NORMAL
- en: Employing this correction, the boosting process can successfully cope with multi-class
    problems without the bias normally introduced by AdaBoost.M1 when *p > 2 (α(t) >
    0* when the error is less than an actual random guess, which is a function of
    the number of classes). As the performance of this algorithm is clearly superior,
    the majority of AdaBoost implementations aren't based on the original algorithm
    anymore (as already mentioned, for example, Scikit-Learn implements AdaBoost.SAMME
    and the real-valued version AdaBoost.SAMME.R). Of course, when *p = 2*, AdaBoost.SAMME
    is exactly equivalent to AdaBoost.M1.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost.SAMME.R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaBoost.SAMME.R is a variant that works with classifiers that can output prediction
    probabilities. This is normally possible employing techniques such as Platt scaling,
    but it's important to check whether a specific classifier implementation is able
    to output the probabilities without any further action. For example, SVM implementations
    provided by Scikit-Learn don't compute the probabilities unless the parameter
    `probability=True` (because they require an extra step that could be useless in
    some cases).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we assume that the output of each classifier is a probability
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f489c04a-b017-42e0-9615-0d5b81d71d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each component is the conditional probability that the *j^(th)* class is output
    given the input xi. When working with a single estimator, the winning class is
    obtained through the argmax(•) function; however, in this case, we want to re-weight
    each learner so as to obtain a sequentially grown ensemble. The basic idea is
    the same as AdaBoost.M1, but, as now we manage probability vectors, we also need
    an estimator weighting function that depends on the single sample *xi* (this function
    indeed wraps every single estimator that is now expressed as a probability vectorial
    function *pi(t)(y=i|x))*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d766343-dccd-48c3-8891-3188e83156b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the properties of logarithms, the previous expression is equivalent
    to a discrete *α(t)*; however, in this case, we don''t rely on a weighted error
    sum (the theoretical explanation is rather complex and is beyond the scope of
    this book. The reader can find it in the aforementioned paper, even if the method
    presented in the next chapter discloses a fundamental part of the logic). To better
    understand the behavior of this function, let''s consider a simple scenario with
    *p = 2*. The first case is a sample that the learner isn''t able to classify (*p=(0.5,
    0.5)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1d2fce3-0e5c-4ac9-9955-d38081974cd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the uncertainty is maximal and the classifier cannot be trusted
    for this sample, so the weight becomes null for all output probabilities. Now,
    let''s apply the boosting, obtaining the probability vector *p=(0.7, 0.3)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f301973-7a85-426d-8907-c2d27deee853.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first class will become positive and its magnitude will increase when *p →
    1*, while the other one is the opposite value. Therefore, the functions are symmetric
    and allow working with a sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cc89b32f-6c44-49fc-9ac7-65811bb4cb08.png)'
  prefs: []
  type: TYPE_IMG
- en: This approach is very similar to a weighted majority vote because the winning
    class *yi* is computed taking into account not only the number of estimators whose
    output is *yi* but also their relative weight and the negative weight of the remaining
    classifiers. A class can be selected only if the strongest classifiers predicted
    it and the impact of the other learners is not sufficient to overturn this result.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to update the weights, we need to consider the impact of all probabilities.
    In particular, we want to reduce the uncertainty (which can degenerate to a purely
    random guess) and force a superior attention focused on all those samples that
    have been misclassified. To achieve this goal, we need to define the *yi* and
    *p(t)(xi)* vectors, which contain, respectively, the one-hot encoding of the true
    class (for example, *(0, 0, 1, ..., 0)*) and the output probabilities yielded
    by the estimator (as a column vector). Hence, the update rule becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf93290c-bb10-4179-9b2c-268f90f047dc.png)'
  prefs: []
  type: TYPE_IMG
- en: If, for example, the true vector is (1, 0) and the output probabilities are
    (0.1, 0.9), with η=1, the weight of the sample will be multiplied by about 3.16\.
    If instead, the output probabilities are (0.9, 0.1), meaning the sample has been
    successfully classified, the multiplication factor will become closer to 1\. In
    this way, the new data distribution *D(t+1)*, analogously to AdaBoost.M1, will
    be more peaked on the samples that need more attention. All implementations include
    the learning rate as a hyperparameter because, as already explained, the default
    value equal to 1.0 cannot be the best choice for specific problems. In general,
    a lower learning rate allows reducing the instability when there are many outliers
    and improves the generalization ability thanks to a slower convergence towards
    the optimum. When *η < 1*, every new distribution is slightly more focused on
    the misclassified samples, allowing the estimators to search for a better parameter
    set without big jumps (that can lead the estimator to skip an optimal point).
    However, contrary to Neural Networks that normally work with small batches, AdaBoost
    can often perform quite well also with *η=1* because the correction is applied
    only after a full training step. As usual, I recommend performing a grid search
    to select the right values for each specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete AdaBoost.SAMME.R algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the family and the number of estimators *Nc*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial weights *W(1)* equal to *1/M*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the learning rate *η* (for example, *η = 1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output probabilities for each class and each training sample
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the estimator weights *αj(i)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights using the exponential formula (with or without the learning
    rate)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the weights
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the global estimator applying the argmax(•) function to the sum *αj(i)* (for
    *i=1* to *Nc*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaBoost.R2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A slightly more complex variant has been proposed by Drucker (in *Improving
    Regressors using Boosting Techniques*,* Drucker H.*,* ICML 1997*) to manage regression
    problems. The weak learners are commonly decision trees and the main concepts
    are very similar to the other variants (in particular, the re-weighting process
    applied to the training dataset). The real difference is the strategy adopted
    in order to choose the final prediction *yi* given the input sample *xi*. Assuming
    that there are Nc estimators and each of them is represented as function *dt(x)*,
    we can compute the absolute residual *ri(t)* for every input sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebdc8cc2-5fe8-4a61-92bb-27b556ce0f13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the set *Ri* containing all the absolute residuals has been populated,
    we can compute the quantity *Sr = sup Ri* and compute the values of a cost function
    that must be proportional to the error. The common choice that is normally implemented
    (and suggested by the author himself) is a linear loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ef3a9e6-36f9-47f8-b553-0aeb08f9e2fb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This loss is very flat and it''s directly proportional to the error. In most
    cases, it''s a good choice because it avoids premature over-specialization and
    allows the estimators to readapt their structure in a gentler way. The most obvious
    alternative is the square loss, which starts giving more importance to those samples
    whose prediction error is larger. It is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9257d79a-f66f-45c2-a3df-1c03dcb51383.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The last cost function is strictly related to AdaBoost.M1 and it''s exponential:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e10feb45-b692-4c0d-aa25-ccb2a6ba9ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: This is normally a less robust choice because, as we are also going to discuss
    in the next section, it penalizes small errors in favor of larger ones. Considering
    that these functions are also employed in the re-weighting process, an exponential
    loss can force the distribution to assign very high probabilities to samples whose
    misclassification error is high, driving the estimators to become over-specialized
    with effect from the first iterations. In many cases (such as in neural networks),
    the loss functions are normally chosen according to their specific properties
    but, above all, to the ease to minimize them. In this particular scenario, loss
    functions are a fundamental part of the boosting process and they must be chosen
    considering the impact on the data distribution. Testing and cross-validation
    provide the best tool to make a reasonable decision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the loss function has been evaluated for all training samples, it''s possible
    to build the global cost function as the weighted average of all losses. Contrary
    to many algorithms that simply sum or average the losses, in this case, it''s
    necessary to consider the structure of the distribution. As the boosting process
    reweights the samples, also the corresponding loss values must be filtered to
    avoid a bias. At the iteration *t*, the cost function is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7adb16a-f916-4e5e-916d-bb2b39af3039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This function is proportional to the weighted errors, which can be either linearly
    filtered or emphasized using a quadratic or exponential function. However, in
    all cases, a sample whose weight is lower will yield a smaller contribution, letting
    the algorithm focus on the samples more difficult to be predicted. Consider that,
    in this case, we are working with classifications; therefore, the only measure
    we can exploit is the loss. Good samples yield low losses, hard samples yield
    proportionally higher losses. Even if it''s possible to use *C(t)* directly, it''s
    preferable to define a confidence measure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f8193e2-f1a9-48c6-9d13-bb99d2ad7077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This index is inversely proportional to the average confidence at the iteration
    *t*. In fact, when *C(t) → 0*, *γ(t) → 0* and when *C(t) → ∞*, *γ(t) → 1*. The
    weight update is performed considering the overall confidence and the specific
    loss value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3958f502-bacd-4402-a368-9a88e4802cab.png)'
  prefs: []
  type: TYPE_IMG
- en: A weight will be decreased proportionally to the loss associated with the corresponding
    absolute residual. However, instead of using a fixed base, the global confidence
    index is chosen. This strategy allows a further degree of adaptability, because
    an estimator with a low confidence doesn't need to focus only on a small subset
    and, considering that *γ(t)* is bounded between 0 and 1 (worst condition), the
    exponential becomes ineffective when the cost function is very high (1x = 1),
    so that the weights remain unchanged. This procedure is not very dissimilar to
    the one adopted in other variants, but it tries to find a trade-off between global
    accuracy and local misclassification problems, providing an extra degree of robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most complex part of this algorithm is the approach employed to output
    a global prediction. Contrary to classification algorithms, we cannot easily compute
    an average, because it''s necessary to consider the global confidence at each
    iteration. Drucker proposed a method based on the weighted median of all outputs.
    In particular, given a sample xi, we define the set of predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dacfd8db-452f-45d5-88ae-f1edff8371b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As weights, we consider the *log(1 / γ(t))*, so we can define a weight set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dc09994-0ed6-4fd1-be70-75bbb1f51c47.png)'
  prefs: []
  type: TYPE_IMG
- en: The final output is the median of *Y* weighted according to *Γ* (normalized
    so that the sum is 1.0). As *γ(t) → 1* when the confidence is low, the corresponding
    weight will tend to 0\. In the same way, when the confidence is high (close to
    1.0), the weight will increase proportionally and the chance to pick the output
    associated with it will be higher. For example, if the outputs are *Y = {1, 1.2,
    1.3, 2.0, 2.2, 2.5, 2.6}* and the weights are *Γ = { 0.35, 0.15, 0.12, 0.11, 0.1,
    0.09, 0.08 }*, the weighted median corresponds to the second index, therefore
    the global estimator will output 1.2 (which is, also intuitively, the most reasonable
    choice).
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure to find the median is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: The *yi(t)* must be sorted in ascending order, so that *yi(1) < yi(2) < ...
    < yi(Nc)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The set *Γ* is sorted accordingly to the index of *yi(t)* (each output *yi(t)*
    must carry its own weight)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The set *Γ* is normalized, dividing it by its sum
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The index corresponding to the smallest element that splits *Γ* into two blocks
    (whose sums are less than or equal to 0.5) is selected
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output corresponding to this index is chosen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The complete AdaBoost.R2 algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the family and the number of estimators *Nc*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial weights *W(1)* equal to *1/M*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial distribution *D(1)* equal to the dataset *X*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a loss function *L*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *D(i)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the absolute residuals, the loss values, and the confidence measures
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the global cost function
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights using the exponential formula
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the global estimator using the weighted median
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of AdaBoost with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's continue using the Wine dataset in order to analyze the performance of
    AdaBoost with different parameters. Scikit-Learn, like almost all algorithms,
    implements both a classifier `AdaBoostClassfier` (based on the algorithm SAMME
    and SAMME.R) and a regressor `AdaBoostRegressor` (based on the algorithm R2).
    In this case, we are going to use the classifier, but I invite the reader to test
    the regressor using a custom dataset or one of the built-in toy datasets. In both
    classes, the most important parameters are `n_estimators` and `learning_rate`
    (default value set to `1.0`). The default underlying weak learner is always a
    decision tree, but it's possible to employ other models creating a base instance
    and passing it through the parameter `base_estimator`. As explained in the chapter,
    real-valued AdaBoost algorithms require an output based on a probability vector.
    In Scikit-Learn, some classifiers/regressors (such as SVM) don't compute the probabilities
    unless it is explicitly required (setting the parameter `probability=True`); therefore,
    if an exception is raised, I invite you to check the documentation in order to
    learn how to force the algorithm to compute them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples we are going to discuss have only a didactic purpose because they
    focus on a single parameter. In a real-world scenario, it''s always better to
    perform a grid search (which is more expensive), so as to analyze a set of combinations.
    Let''s start analyzing the cross-validation score as a function of the number
    of estimators (the vectors *X* and *Y* are the ones defined in the previous example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have considered a range starting from 10 trees and ending with 200 trees
    with steps of 10 trees. The learning rate is kept constant and equal to 0.8\.
    The resulting plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ac290ef-4dc9-44c8-96e3-90f49af5d90d.png)'
  prefs: []
  type: TYPE_IMG
- en: 10-fold cross-validation accuracy as a function of the number of estimators
  prefs: []
  type: TYPE_NORMAL
- en: The maximum is reached with 50 estimators. Larger values cause performance worsening
    due to the over-specialization and a consequent variance increase. As explained
    also in other chapters, the capacity of a model must be tuned according to the
    Occam's Razor principle, not only because the resulting model can be faster to
    train, but also because a capacity excess is normally saturated, overfitting the
    training set and reducing the scope for generalization. Cross-validation can immediately
    show this effect, which, instead, can remain hidden when a standard training/test
    set split is done (above all when the samples are not shuffled).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now check the performance with different learning rates (keeping the
    number of trees fixed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The final plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0341b161-a7b8-4bbe-9fbc-855f635049ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 10-fold Cross-validation accuracy as a function of the learning rate (number
    of estimators = 50)
  prefs: []
  type: TYPE_NORMAL
- en: Again, different learning rates yield different accuracies. The choice of *η
    = 0.8* seems to be the most effective, as higher and lower values lead to performance
    worsening. As explained, the learning rate has a direct impact on the re-weighting
    process. Very small values require a larger number of estimators because subsequent
    distributions are very similar. On the other side, large values can lead to a
    premature over-specialization. Even if the default value is `1.0`, I always suggest
    checking the accuracy also with smaller values. There's no golden rule for picking
    the right learning rate in every case, but it's important to remember that lower
    values allow the algorithm to smoothly adapt to fit the training set in a gentler
    way, while higher values reduce the robustness to outliers, because the samples
    that have been misclassified are immediately boosted and the probability of sampling
    them increases very rapidly. The result of this behavior is a constant focus on
    those samples that may be affected by noise, almost forgetting the structure of
    the remaining sample space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last experiment we want to make is analyzing the performance after a dimensionality
    reduction performed with **Principal Component Analysis** (**PCA**) and **Factor
    Analysis** (**FA**) (with 50 estimators and `η = 0.8`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00546deb-0510-444a-b198-e697936e69d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 10-fold cross-validation accuracy as a function of the number of components
    (PCA and factor analysis)
  prefs: []
  type: TYPE_NORMAL
- en: This exercise confirms some important features analyzed in [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml),
    *EM Algorithm and Applications*. First of all, performances are not dramatically
    affected even by a 50% dimensionality reduction. This consideration is further
    confirmed by the feature importance analysis performed in the previous example.
    Decision trees can perform quite a good classification considering only 6/7 features
    because the remaining ones offer a marginal contribution to the characterization
    of a sample. Moreover, FA is almost always superior to PCA. With 7 components,
    the accuracy achieved using the FA algorithm is higher than 0.95 (very close to
    the value achieved with no reduction), while a PCA reaches this value with 12
    components. The reader should remember that PCA is a particular case of FA, with
    the assumption of homoscedastic noise. The diagram confirms that this condition
    is not acceptable with the Wine dataset. Assuming different noise variances allows
    remodeling the reduced dataset in a more accurate way, minimizing the cross-effect
    of the missing features. Even if PCA is normally the first choice, with large
    datasets, I suggest you always compare the performance with a Factor Analysis
    and choose the technique that guarantees the best result (given also that FA is
    more expensive in terms of computational complexity).
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we can introduce a more general method of creating boosted ensembles.
    Let''s choose a generic algorithm family, represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d3d5c8e-4f1c-4184-9ac3-9a275caf6f87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Each model is parametrized using the vector *θi* and there are no restrictions
    on the kind of method that is employed. In this case, we are going to consider
    decision trees (which is one of the most diffused algorithms when this boosting
    strategy is employed—in this case, the algorithm is known as gradient tree boosting),
    but the theory is generic and can be easily applied to more complex models, such
    as neural networks. In a decision tree, the parameter vector *θi* is made up of
    selection tuples, so the reader can think of this method as a pseudo-random forest
    where, instead of randomness, we look for extra optimality exploiting the previous
    experience. In fact, as with AdaBoost, a gradient boosting ensemble is built sequentially,
    using a technique that is formally defined as **Forward Stage-wise Additive Modeling**.
    The resulting estimator is represented as a weighted sum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a543a7f1-1bdb-4f5d-b624-8e37fd1b852a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore the variables to manage are the single estimator weights *αi* and
    the parameter vectors *θi*. However, we don''t have to work with the whole set,
    but with a single tuple *(αi, θi)*, without modifying the values already chosen
    during the previous iterations. The general procedure can be summarized with a
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: The estimator sum is initialized to a null value
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The best *tuple(αi, θi)* is chosen and the estimator *f(x; θi)* is trained
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*di(x) = di-1(x) + αif(x; θi)*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The final estimator *d(x)* is output
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How is it possible to find out the best tuple? We have already presented a
    strategy for improving the performance of every learner through boosting the dataset.
    In this case, instead, the algorithm is based on a cost function that we need
    to minimize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/184477c8-9617-4b24-90e9-29f44aafd30c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In particular, the generic optimal tuple is obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65fb3164-1039-4186-92f9-63c2bfc83488.png)'
  prefs: []
  type: TYPE_IMG
- en: As the process is sequential, each estimator is optimized to improve the previous
    one's accuracy. However, contrary to AdaBoost, we are not constrained to impose
    a specific loss function (it's possible to prove that AdaBoost.M1 is equivalent
    to this algorithm with an exponential loss but the proof is beyond the scope of
    this book). As we are going to discuss, other cost functions can yield better
    performances in several different scenarios, because they avoid the premature
    convergence towards sub-optimal minima.
  prefs: []
  type: TYPE_NORMAL
- en: The problem could be considered as solved by employing the previous formula
    to optimize each new learner; however, the `argmin(•)` function needs a complete
    exploration of the cost function space and, as `C(•)` depends on each specific
    model instance and, therefore, on *θi*, it's necessary to perform several retraining
    processes in order to find the optimal solution. Moreover, the problem is generally
    non-convex and the number of variables can be very high. Numerical algorithms
    such as L-BFGS or other quasi-Newton methods need too many iterations and a prohibitive
    computational time. It's clear that such an approach is not affordable in the
    vast majority of cases and the Gradient Boosting algorithm has been proposed as
    an intermediate solution. The idea is to find a sub-optimal solution with a gradient
    descent strategy limited to a single step for each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to present the algorithm, it''s useful to rewrite the additive model
    with an explicit reference to the optimal goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5ca1d40-bc8e-4be4-b5f4-81ba1e81e284.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the cost function is computed carrying on all the previously trained
    models; therefore, the correction is always incremental. If the cost function
    *L* is differentiable (a fundamental condition that is not difficult to meet),
    it''s possible to compute the gradient with respect to the current additive model
    (at the *i^(th)* iteration, we need to consider the additive model obtained summing
    all the previous *i-1* models):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b61fc950-e403-4b93-a086-4ecd6d187c32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, a new classifier can be added by moving the current additive
    model into the negative direction of the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f2f22b0-32fa-43b4-abc7-08f1946018df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We haven''t considered the parameter αi yet (nor the learning rate *η*, which
    is a constant), however the reader familiar with some basic calculus can immediately
    understand the effect of an update is to reduce the value of the global loss function
    by forcing the next model to improve its accuracy with respect to its predecessors.
    However, a single gradient step isn''t enough to guarantee an appropriate boosting
    strategy. In fact, as discussed previously, we also need to weight each classifier
    according to its ability to reduce the loss. Once the gradient has been computed,
    it''s possible to determine the best value for the weight αi with a direct minimization
    of the loss function (using a line search algorithm) computed considering the
    current additive model with *α* as an extra variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b76af08-36a2-41e9-b6d7-2ad04db82758.png)'
  prefs: []
  type: TYPE_IMG
- en: When using the gradient tree boosting variant, an improvement can be achieved
    by splitting the weight *αi* into *m* sub-weights *αi(j)* associated with each
    terminal node of the tree. The computational complexity is slightly increased,
    but the final accuracy can be higher than the one obtained with a single weight.
    The reason derives from the functional structure of a tree. As the boosting forces
    a specialization in specific regions, a single weight could drive to an over-estimation
    of a learner also when a specific sample cannot be correctly classified. Instead,
    using different weights, it's possible to operate a fine-grained filtering of
    the result, accepting or discarding an outcome according to its value and to the
    properties of the specific tree.
  prefs: []
  type: TYPE_NORMAL
- en: This solution cannot provide the same accuracy of a complete optimization, but
    it's rather fast and it's possible to compensate for this loss using more estimators
    and a lower learning rate. Like many other algorithms, gradient boosting must
    be tuned up in order to yield the maximum accuracy with a low variance. The learning
    rate is normally quite smaller than 1.0 and its value should be found by validating
    the results and considering the total number of estimators (it's better to reduce
    it when more learners are employed). Moreover, a regularization technique could
    be added in order to prevent overfitting. When working with specific classifier
    families (such as logistic regression or neural networks), it's very easy to include
    an *L1* or *L2* penalty, but it's not so easy with other estimators. For this
    reason, a common regularization technique (implemented also by Scikit-Learn) is
    the downsampling of the training dataset. Selecting *P < N* random samples allows
    the estimators to reduce the variance and prevent overfitting. Alternatively,
    it's possible to employ a random feature selection (for gradient tree boosting
    only) as in a random forest; picking a fraction of the total number of features
    increases the uncertainty and avoids over-specialization. Of course, the main
    drawback to these techniques is a loss of accuracy (proportional to the downsampling/feature
    selection ratio) that must be analyzed in order to find the most appropriate trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving to the next section, it''s useful to briefly discuss the main
    cost functions that are normally employed with this kind of algorithms. In the
    first chapter, we have presented some common cost functions, like mean squared
    error, Huber Loss (very robust in regression contexts), and cross-entropy. They
    are all valid examples, but there are other functions that are peculiar to classification
    problems. The first one is Exponential Loss, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90bfefe0-decf-497b-aab8-9add8054ad98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As pointed out by Hastie, Tibshirani and, Friedman, this function transforms
    the gradient boosting into an AdaBoost.M1 algorithm. The corresponding cost function
    has a very precise behavior that sometimes is not the most adequate to solve particular
    problems. In fact, the result of an exponential loss has a very high impact when
    the error is large, yielding distributions that are strongly peaked around a few
    samples. The subsequent classifiers can be consequently driven to over-specialize
    their structure to cope only with a small data region, with a concrete risk of
    losing the ability to correctly classify other samples. In many situations, this
    behavior is not dangerous and the final bias-variance trade-off is absolutely
    reasonable; however, there are problems where a softer loss function can allow
    a better final accuracy and generalization ability. The most common choice for
    real-valued binary classification problems is Binomial Negative Log-Likelihood
    Loss (deviance), defined as follows (in this case we are assuming that the classifier
    *f(•)* is not thresholded, but outputs a positive-class probability):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd2ed604-b82b-4fe5-9585-c1585c579e3a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This loss function is the same employed in Logistic Regressions and, contrary
    to Exponential Loss, doesn''t yield peaked distributions. Two misclassified samples
    with different probabilities are boosted proportionally to the error (not the
    exponential value), so as to force the classifiers to focus on all the misclassified
    population with almost the same probability (of course, a higher probability assigned
    to samples whose error is very large is desirable, assuming that all the other
    misclassified samples have always a good chance to be selected). The natural extension
    of the Binomial Negative Log-Likelihood Loss to multi-class problems is the Multinomial
    Negative Log-Likelihood Loss, defined as follows (the classifier *f(•)* is represented
    as probability vector with *p* components):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0dd27751-38b1-48d8-bb41-3f25a9466aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: In the previous formula, the notation *Iy=j* must be interpreted as an indicator
    function, which is equal to 1 when *y=j* and 0 otherwise. The behavior of this
    loss function is perfectly analogous to the binomial variant and, in general,
    it is the default choice for classification problems. The reader is invited to
    test the examples with both exponential loss and deviance and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete gradient boosting algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the family and the number of estimators *Nc*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select a loss function *L* (for example, deviance)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the base estimator *d0(x)* as a constant (such as 0) or using another
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the learning rate *η* (such as *η = 1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *Nc*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the gradient *∇d L(•)* using the additive model at the step *i-1*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the *i^(th)* estimator *di(x)* with the data distribution *{ (xi, ∇d L(yi, di-1(xi))
    }*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a line search to compute *αi*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the estimator to the ensemble
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Example of gradient tree boosting with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we want to employ a gradient tree boosting classifier (class
    `GradientBoostingClassifier`) and check the impact of the maximum tree depth (`parameter
    max_depth`) on the performance. Considering the previous example, we start by
    setting `n_estimators=50` and `learning_rate=0.8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5252e92-7501-42d6-911c-b59617e11481.png)'
  prefs: []
  type: TYPE_IMG
- en: 10-fold Cross-validation accuracy as a function of the maximum tree depth
  prefs: []
  type: TYPE_NORMAL
- en: 'As explained in the first section, the maximum depth of a decision tree is
    strictly related to the possibility of interaction among features. This can be
    a positive or negative aspect when the trees are employed in an ensemble. A very
    high interaction level can create over-complex separation hyperplanes and reduce
    the overall variance. In other cases, a limited interaction results in a higher
    bias. With this particular (and simple) dataset, the gradient boosting algorithm
    can achieve better performances when the max depth is 2 (consider that the root
    has a depth equal to zero) and this is partially confirmed by both the feature
    importance analysis and dimensionality reductions. In many real-world situations,
    the result of such a research could be completely different, with increased performance,
    therefore I suggest you cross-validate the results (it''s better to employ a grid
    search) starting from a minimum depth and increasing the value until the maximum
    accuracy has been achieved. With `max_depth=2`, we want now to tune up the learning
    rate, which is a fundamental parameter in this algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0af543ef-0cee-4572-ba36-71ce1ba7e228.png)'
  prefs: []
  type: TYPE_IMG
- en: 10-fold Cross-validation accuracy as a function of the learning rate (max depth
    equal to 2)
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, gradient tree boosting outperforms AdaBoost with *η ≈ 0.9*,
    achieving a cross-validation accuracy slightly lower than 0.99\. The example is
    very simple, but it clearly shows the power of this kind of techniques. The main
    drawback is the complexity. Contrary to single models, ensembles are more sensitive
    to changes to the hyperparameters and more detailed research must be conducted
    in order to optimize the models. When the datasets are not excessively large,
    cross-validation remains the best choice. If, instead, we are pretty sure that
    the dataset represents almost perfectly the underlying data generating process,
    it's possible to shuffle it and split it into two (training/test) or three blocks
    (training/test/validation) and proceed by optimizing the hyperparameters and trying
    to overfit the test set (this expression can seem strange, but overfitting the
    test set means maximizing the generalization ability while learning perfectly
    the training set structure).
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles of voting classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A simpler but no less effective way to create an ensemble is based on the idea
    of exploiting a limited number of strong learners whose peculiarities allow them
    to yield better performances in particular regions of the sample space. Let''s
    start considering a set of *Nc* discrete-valued classifiers *f1(x), f2(x), ...,
    fNc(x)*. The algorithms are different, but they are all trained with the same
    dataset and output the same label set. The simplest strategy is based on a hard-voting
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81f2b833-ad97-48b4-9fa1-893d280000be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the function *n(•)* counts the number of estimators that output
    the label *yi*. This method is rather powerful in many cases, but has some limitations.
    If we rely only on a majority vote, we are implicitly assuming that a correct
    classification is obtained by a large number of estimators. Even if, *Nc/2 + 1*
    votes are necessary to output a result, in many cases their number is much higher.
    Moreover, when k is not very large, also *Nc/2 + 1* votes imply a symmetry that
    involves a large part of the population. This condition often drives to the training
    of useless models that could be simply replaced by a single well-fitted strong
    learner. In fact, let''s suppose that the ensemble is made up of three classifiers
    and one of them is more specialized in regions where the other two can easily
    be driven to misclassifications. A hard-voting strategy applied to this ensemble
    could continuously penalize the more complex estimator in favor of the other classifiers.
    A more accurate solution can be obtained by considering real-valued outcomes.
    If each estimator outputs a probability vector, the confidence of a decision is
    implicitly encoded in the values. For example, a binary classifier whose output
    is *(0.52, 0.48)* is much more uncertain than another classifier outputting *(0.95,
    0.05)*. Applying a threshold is equivalent to flattening the probability vectors
    and discarding the uncertainty. Let''s consider an ensemble with three classifiers
    and a sample that is hard to classify because it''s very close to the separation
    hyperplane. A hard-voting strategy decides for the first class because the thresholded
    output is *(1, 1, 2)*. Then we check the output probabilities, obtaining *(0.51,
    0.49)*, *(0.52, 0.48)*, *(0.1, 0.9)*. After averaging the probabilities, the ensemble
    output becomes about (0.38, 062) and by applying `argmax(•)`, we get the second
    class as the final decision. In general, it''s also a good practice to consider
    a weighted average, so that the final class is obtained as follows (assuming the
    output of the classifier is a probability vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d745df4d-35df-4a4e-b758-3be4cfde5cc3.png)'
  prefs: []
  type: TYPE_IMG
- en: The weights can be simply equal to 1.0 if no weighting is required or they can
    reflect the level of trust we have for each classifier. An important rule is to
    avoid the dominance of a classifier in the majority of cases because it would
    be an implicit fallback to a single estimator scenario. A good voting example
    should always allow a minority to overturn a result when their confidence is quite
    higher than the majority. In this strategies, the weights can be considered as
    hyperparameters and tuned up using a grid search with cross-validation. However,
    contrary to other ensemble methods, they are not fine-grained, therefore the optimal
    value is often a compromise among some different possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: A slightly more complex technique is called **stacking** and consists of using
    an extra classifier as a post-filtering step. The classical approach consists
    of training the classifiers separately, then the whole dataset is transformed
    into a prediction set (based on class labels or probabilities) and the combining
    classifier is trained to associate the predictions to the final classes. Using
    even very simple models like Logistic Regressions or Perceptrons, it's possible
    to mix up the predictions so as to implement a dynamic reweighting that is a function
    of the input values. A more complex approach is feasible only when a single training
    strategy can be used to train the whole ensemble (including the combiner). For
    example, it could be employed with neural networks that, however, have already
    an implicit flexibility and can often perform quite better than complex ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Example of voting classifiers with Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to employ the MNIST handwritten digits dataset.
    As the concept is very simple, our goal is to show how to combine two completely
    different estimators to improve the overall cross-validation accuracy. For this
    reason, we have selected a Logistic Regression and a decision tree, which are
    structurally different. In particular, while the former is a linear model that
    works with the whole vectors, the latter is a feature-wise estimator that can
    support the decision only in particular cases (images are not made up of semantically
    consistent features, but the over-complexity of a Decision Tree can help with
    particular samples which are very close to the separation hyperplane and, therefore,
    more difficult to classify with a linear method). The first step is loading and
    normalizing the dataset (this operation is not important with a Decision Tree,
    but has a strong impact on the performances of a Logistic Regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we need to evaluate the performances of both estimators individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, the Logistic Regression (∼94% accuracy) outperforms the decision
    tree (83% accuracy); therefore, a hard-voting strategy is not the best choice.
    As we trust the Logistic Regression more, we can employ soft voting with a weight
    vector set to *(0.9, 0.1)*. The class `VotingClassifier` accepts a list of tuples
    (name of the estimator, instance) that must be supplied through the `estimators`
    parameter. The strategy can be specified using parameter voting (it can be either
    "soft" or "hard") and the optional weights, using the parameter with the same
    name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using a soft-voting strategy, the estimator is able to outperform Logistic Regression
    by reducing the global uncertainty. I invite the reader to test this algorithm
    with other datasets, using more estimators, and try to find out the optimal combination
    using both the hard and soft voting strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning as model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is not a proper ensemble learning technique, but it is sometimes known
    as **bucketing**. In the previous section, we have discussed how a few strong
    learners with different peculiarities can be employed to make up a committee.
    However, in many cases, a single learner is enough to achieve a good bias-variance
    trade-off but it''s not so easy to choose among the whole Machine Learning algorithm
    population. For this reason, when a family of similar problems must be solved
    (they can differ but it''s better to consider scenarios that can be easily compared),
    it''s possible to create an ensemble containing several models and use cross-validation
    to find the one whose performances are the best. At the end of the process, a
    single learner will be used, but its choice can be considered like a grid search
    with a voting system. Sometimes this technique can unveil important differences
    even using similar datasets. For example, during the development of a system,
    a first dataset (*X1, Y1*) is provided. Everybody expects that it is correctly
    sampled from an underlying data generating process p[data] and, so, a generic
    model is fitted and evaluated. Let''s imagine that a SVM achieves a very high
    validation accuracy (evaluated using a k-fold cross-validation) and, therefore,
    it is chosen as the final model. Unfortunately, a second, larger dataset (*X2,
    Y2*) is provided and the final mean accuracy worsens. We might simply think that
    the residual variance of the model cannot let it generalize correctly or, as sometimes
    happens, we can say the second dataset contains many outliers which are not correctly
    classified. The real situation is a little more complex: given a dataset, we can
    only suppose that it represents a complete data distribution. Even when the number
    of samples is very high or we use data augmentation techniques, the population
    might not represent some particular samples that will be analyzed by the system
    we are developing. Bucketing is a good way to create a security buffer that can
    be exploited whenever the scenario changes. The ensemble can be made up of completely
    different models, models belonging to the same family but differently parametrized
    (for example, different kernel SVMs) or a mixture of composite algorithms (like
    PCA + SVM, PCA + decision trees/random forests, and so on). The most important
    element is the cross-validation. As explained in the first chapter, splitting
    the dataset into training and test sets can be an acceptable solution only when
    the number of samples and their variability is high enough to justify the belief
    that it correctly represents the final data distribution. This often happens in
    deep learning, where the dimensions of the datasets are quite large and the computational
    complexity doesn''t allow retraining the model too many times. Instead, in classical
    Machine Learning contexts, cross-validation is the only way to check the behavior
    of a model when trained with a large random subset and tested on the remaining
    samples. Ideally, we''d like to observe the same performances, but it can also
    happen that the accuracy is higher in some folds and quite lower in other. When
    this phenomenon is observed and the dataset is the final one, it probably means
    that the model is not able to manage one or more regions of the sample space and
    a boosting approach could dramatically improve the final accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the main concepts of ensemble learning, focusing
    on both bagging and boosting techniques. In the first section, we explained the
    difference between strong and weak learners and we presented the big picture of
    how it's possible to combine the estimators to achieve specific goals.
  prefs: []
  type: TYPE_NORMAL
- en: The next topic focused on the properties of decision trees and their main strengths
    and weaknesses. In particular, we explained that the structure of a tree causes
    a natural increase in the variance. The bagging technique called random forests
    allow mitigating this problem, improving at the same time the overall accuracy.
    A further variance reduction can be achieved by increasing the randomness and
    employing a variant called **extra randomized trees**. In the example, we have
    also seen how it's possible to evaluate the importance of each input feature and
    perform dimensionality reduction without involving complex statistical techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the third section, we presented the most famous boosting techniques, AdaBoost,
    which is based on the concept of creating a sequential additive model, when each
    new estimator is trained using a reweighted (boosted) data distribution. In this
    way, every learner is added to focus on the misclassified samples without interfering
    with the previously added models. We analyzed the original M1 discrete variant
    and the most effective alternatives called SAMME and SAMME.R (real-valued), and
    R2 (for regressions), which are implemented in many Machine Learning packages.
  prefs: []
  type: TYPE_NORMAL
- en: After AdaBoost, we extended the concept to a generic Forward Stage-wise Additive
    Model, where the task of each new estimator is to minimize a generic cost function.
    Considering the complexity of a full optimization, a gradient descent technique
    was presented that, combined with an estimator weight line search, can yield excellent
    performances both in classification and in regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: The final topics concerned how to build ensembles using a few strong learners,
    averaging their prediction or considering a majority vote. We discussed the main
    drawback of thresholded classifiers and we showed how it's possible to build a
    soft-voting model that is able to trust the estimator that show less uncertainty.
    Other useful topics are the Stacking method, which consists of using an extra
    classifier to process the prediction of each member of the ensemble and how it's
    possible to create candidate ensembles that are evaluated using a cross-validation
    technique to find out the best estimator for each specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to begin discussing the most important deep
    learning techniques, introducing the fundamental concepts regarding neural networks
    and the algorithms involved in their training processes.
  prefs: []
  type: TYPE_NORMAL
