<html><head></head><body>
        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Fair Value of House and Property</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we are going to expand our knowledge and skills in building regression <strong class="calibre4">machine learning</strong> (<strong class="calibre4">ML</strong>) models in C#. In the last chapter, we built a linear regression and linear support vector machine model on a foreign exchange rate dataset, where all the features were continuous variables. However, we are going to be dealing with a more complex dataset, where some features are categorical variables and some others are continuous variables.</p>
<p class="calibre2">In this chapter, we will be using a house prices dataset that contains numerous attributes of houses with mixed variable types. Using this data, we will start looking at the two common types of categorical variables (ordinal versus non-ordinal) and the distributions of some of the categorical variables in the housing dataset. We will also look at the distributions of some of the continuous variables in the dataset and the benefits of using log transformations for variables that show skewed distributions. Then, we are going to learn how to encode and engineer such categorical features so that we can fit machine learning models. Unlike the last chapter, where we explored the basics of <strong class="calibre4">Support Vector Machine</strong> (<strong class="calibre4">SVM</strong>), we are going to apply different Kernel methods for our SVM models and see how it affects the model performances.</p>
<p class="calibre2">Similar to the last chapter, we will be using <strong class="calibre4">root mean squared error</strong> (<strong class="calibre4">RMSE</strong>), R<sup class="calibre64">2</sup>, and a plot of actual versus predicted values to evaluate the performances of our ML models. By the end of this chapter, you will have a better understanding of how to handle categorical variables, how to encode and engineer such features for regression models, how to apply various kernel methods for building SVM models, and how to build models that predict the fair values of houses.</p>
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Problem definition for the fair value of house/property project</li>
<li class="calibre11">Data analysis for categorical versus continuous variables</li>
<li class="calibre11">Feature engineering and encoding</li>
<li class="calibre11">Linear regression versus Support Vector Machine with kernels</li>
<li class="calibre11">Model validations using RMSE, R<sup class="calibre64">2</sup>, and actual versus predicted plot</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Problem definition</h1>
                
            
            <article>
                
<p class="calibre2">Let's start this chapter by understanding <span class="calibre5">exactly </span>what ML models we are going to build. When you are looking for a house or a property to purchase, you consider numerous attributes of those houses or properties that you look at. You might be looking at the number of bedrooms and bathrooms, how many cars you can park in your garage, the neighborhoods, the materials or finishes of the house, and so forth. All of these attributes of a house or property go into how you decide the price you want to pay for the given property or how you negotiate the price with the seller. However, it is very difficult to understand and estimate what the fair value of a property is. By having a model that predicts the fair value or the final price of each property, you can make better informed decisions when you are negotiating with the seller.</p>
<p class="calibre2">In order to build such models for fair value of a house predictions, we are going to use a dataset that contains 79 explanatory variables that cover almost all attributes of residential homes in Ames, Iowa, U.S.A. and their final sale prices from 2006 to 2010. This dataset was compiled by Dean De Cock (<a href="https://ww2.amstat.org/publications/jse/v19n3/decock.pdf" target="_blank" class="calibre9">https://ww2.amstat.org/publications/jse/v19n3/decock.pdf</a>) at the Truman State University and can be downloaded from this link: <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" target="_blank" class="calibre9">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>. With this data, we are going to build features that contain information about square footage or sizes of different parts of the houses, the styles and materials used for the houses, the conditions and finishes of different parts of the houses, and various other attributes that further describe the information of each house. Using these features, we are going to explore different regression machine learning models, such as linear regression, Linear Support Vector Machine, and <strong class="calibre4">Support Vector Machines</strong> (<strong class="calibre4">SVMs</strong>) with polynomial and Gaussian kernels. Then, we will evaluate these models by looking at RMSE, R<sup class="calibre64">2</sup>, and a plot of actual versus predicted values.</p>
<p class="calibre2">To summarize our problem definition for the fair value of house and property project:</p>
<ul class="calibre10">
<li class="calibre11">What is the problem? We need a regression model that predicts the fair values of residential homes in Ames, Iowa, U.S.A., so that we can understand and make better informed decisions when purchasing houses.</li>
<li class="calibre11">Why is it a problem? Due to the complex nature and numerous moving parts in deciding the fair value of a house or a property, it is advantageous to have a machine learning model that can predict and inform home buyers what the expected values of houses that they are looking at are.</li>
<li class="calibre11">What are some of the approaches to solving this problem? We are going to use a pre-compiled dataset that contains 79 explanatory variables that contain information of residential homes in Ames, Iowa, U.S.A., and build and encode features of mixed types (both categorical and continuous). Then, we will explore linear regression and support vector machines with different Kernels for making predictions of fair values of houses. We will evaluate the model candidates by looking at RMSE, R<sup class="calibre64">2</sup>, and an actual versus predicted values plot.</li>
<li class="calibre11">What are the success criteria? As we want our predictions of house prices to be as close to the actual house sale prices as possible, we want to gain as low an RMSE as possible, without hurting our goodness of fit measure, R<sup class="calibre64">2</sup>, and the plot of actual versus predicted values.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Categorical versus continuous variables</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now let's start looking at the actual dataset. You can follow this link: <a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" target="_blank" class="calibre9">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a> and download the <kbd class="calibre12">train.csv</kbd> and <kbd class="calibre12">data_description.txt</kbd> files. We are going to build models using the <kbd class="calibre12">train.csv</kbd> file, and the <kbd class="calibre12">data_description.txt</kbd> file will help us better understand the structure of the dataset, especially concerning the categorical variables we have.</span></p>
<p class="calibre2">If you look at the train data file and the description file, you can easily find that there are some variables with certain names or codes that represent specific types of each house's attributes. For example, the <kbd class="calibre12">Foundation</kbd> variable can take one of the values among <kbd class="calibre12">BrkTil</kbd>, <kbd class="calibre12">CBlock</kbd>, <kbd class="calibre12">PConc</kbd>, <kbd class="calibre12">Slab</kbd>, <kbd class="calibre12">Stone</kbd>, and <kbd class="calibre12">Wood</kbd>, where each of those values or codes represents the type of foundation that a house is built with—Brick and Tile, Cinder Block, Poured Contrete, Slab, Stone, and Wood respectively. On the other hand, if you look at the <kbd class="calibre12">TotalBsmtSF</kbd> variable in the data, you can see that it can take any numerical values and the values are continuous. As mentioned previously, this dataset contains mixed types of variables and we need to approach carefully when we are dealing with a dataset with both categorical and continuous variables.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Non-ordinal categorical variables</h1>
                
            
            <article>
                
<p class="calibre2">Let's first look at some categorical variables and their distributions. The first house attribute that we are going to look at is the building type. The code to build a bar chart that shows the distributions of the building type is as follows:</p>
<pre class="calibre19">// Categorical Variable #1: Building Type<br class="title-page-name"/>Console.WriteLine("\nCategorical Variable #1: Building Type");<br class="title-page-name"/>var buildingTypeDistribution = houseDF.GetColumn&amp;lt;string&amp;gt;(<br class="title-page-name"/>    "BldgType"<br class="title-page-name"/>).GroupBy&amp;lt;string&amp;gt;(x =&amp;gt; x.Value).Select(x =&amp;gt; (double)x.Value.KeyCount);<br class="title-page-name"/>buildingTypeDistribution.Print();<br class="title-page-name"/><br class="title-page-name"/>var buildingTypeBarChart = DataBarBox.Show(<br class="title-page-name"/>    buildingTypeDistribution.Keys.ToArray(),<br class="title-page-name"/>    buildingTypeDistribution.Values.ToArray()<br class="title-page-name"/>);<br class="title-page-name"/>buildingTypeBarChart.SetTitle("Building Type Distribution (Categorical)");</pre>
<p class="calibre2"><span class="calibre5">When you run this code, it will display a bar chart like the following:</span></p>
<div class="mce-root"><img src="../images/00066.jpeg" class="calibre72"/></div>
<p class="calibre2"><span class="calibre5">As you can tell from this bar chart, the majority of the building types in our dataset is <span class="calibre5">1Fam</span>, which represents the <em class="calibre13">Single-family Detached</em> building type. The second most common building type is <span class="calibre5">TwnhsE</span>, which represents the <em class="calibre13">Townhouse End Unit</em> building type.</span></p>
<p class="calibre2">Let's take a look at one more categorical variable, Lot Configuration (<kbd class="calibre12">LotConfig</kbd> field in the dataset). The code to build a bar chart for lot configuration distributions is as follows:</p>
<pre class="calibre19">// Categorical Variable #2: Lot Configuration<br class="title-page-name"/>Console.WriteLine("\nCategorical Variable #1: Building Type");<br class="title-page-name"/>var lotConfigDistribution = houseDF.GetColumn&amp;lt;string&amp;gt;(<br class="title-page-name"/>    "LotConfig"<br class="title-page-name"/>).GroupBy&amp;lt;string&amp;gt;(x =&amp;gt; x.Value).Select(x =&amp;gt; (double)x.Value.KeyCount);<br class="title-page-name"/>lotConfigDistribution.Print();<br class="title-page-name"/><br class="title-page-name"/>var lotConfigBarChart = DataBarBox.Show(<br class="title-page-name"/>    lotConfigDistribution.Keys.ToArray(),<br class="title-page-name"/>    lotConfigDistribution.Values.ToArray()<br class="title-page-name"/>);<br class="title-page-name"/>lotConfigBarChart.SetTitle("Lot Configuration Distribution (Categorical)");</pre>
<p class="calibre2">When you run this code, it will display the following bar chart:</p>
<div class="mce-root"><img src="../images/00067.jpeg" class="calibre73"/></div>
<p class="calibre2"><span class="calibre5">As you can see from this bar chart, i</span>nside lot <span class="calibre5">is the most common lot configuration in our dataset, and</span> corner lot <span class="calibre5">is the second most common log configuration.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Ordinal categorical variable</h1>
                
            
            <article>
                
<p class="calibre2">The two categorical variables that we just looked at have no natural ordering. One type does not come before another or one type does not have more weight than another. However, there are some categorical variables that have natural ordering, and we call such categorical variables ordinal categorical variables. For example, when you rank a quality of a material from <span class="calibre5">1</span> to <span class="calibre5">10</span>, where <span class="calibre5">10</span> represents the best and <span class="calibre5">1</span> represents the worst, there is a natural ordering. Let's look at some of the ordinal categorical variables in this dataset.</p>
<p class="calibre2">The first ordinal categorical variable that we are going to look at is the <kbd class="calibre12">OverallQual</kbd> attribute, which represents the overall material and finish of the house. The code to look at the distributions of this variable is as follows:</p>
<pre class="calibre19">// Ordinal Categorical Variable #1: Overall material and finish of the house<br class="title-page-name"/>Console.WriteLine("\nOrdinal Categorical #1: Overall material and finish of the house");<br class="title-page-name"/>var overallQualDistribution = houseDF.GetColumn&amp;lt;string&amp;gt;(<br class="title-page-name"/>    "OverallQual"<br class="title-page-name"/>).GroupBy&amp;lt;int&amp;gt;(<br class="title-page-name"/>    x =&amp;gt; Convert.ToInt32(x.Value)<br class="title-page-name"/>).Select(<br class="title-page-name"/>    x =&amp;gt; (double)x.Value.KeyCount<br class="title-page-name"/>).SortByKey().Reversed;<br class="title-page-name"/>overallQualDistribution.Print();<br class="title-page-name"/><br class="title-page-name"/>var overallQualBarChart = DataBarBox.Show(<br class="title-page-name"/>    overallQualDistribution.Keys.Select(x =&amp;gt; x.ToString()),<br class="title-page-name"/>    overallQualDistribution.Values.ToArray()<br class="title-page-name"/>);<br class="title-page-name"/>overallQualBarChart.SetTitle("Overall House Quality Distribution (Ordinal)");</pre>
<p class="calibre2"><span class="calibre5">When you run this code, it will display the following bar chart in order from <span class="calibre5">10</span> to <span class="calibre5">1</span>:</span></p>
<div class="mce-root"><img src="../images/00068.jpeg" class="calibre74"/></div>
<p class="calibre2"><span class="calibre5">As expected, there is a smaller number of houses in the <em class="calibre13">Very Excellent, </em>encoded as <span class="calibre5">10</span>, or <em class="calibre13">Excellent</em>,<em class="calibre13"> </em>encoded as <span class="calibre5">9</span>, categories than there are in the <em class="calibre13">Above Average</em>, encoded as <span class="calibre5">6</span>, or <em class="calibre13">Average</em> categories, encoded as <span class="calibre5">5</span>.</span></p>
<p class="calibre2">Another ordinal categorical variable that we will be looking at is the <kbd class="calibre12">ExterQual</kbd> variable, which represents the exterior quality. The code to look at the distributions of this variable is as follows:</p>
<pre class="calibre19">// Ordinal Categorical Variable #2: Exterior Quality<br class="title-page-name"/>Console.WriteLine("\nOrdinal Categorical #2: Exterior Quality");<br class="title-page-name"/>var exteriorQualDistribution = houseDF.GetColumn&amp;lt;string&amp;gt;(<br class="title-page-name"/>    "ExterQual"<br class="title-page-name"/>).GroupBy&amp;lt;string&amp;gt;(x =&amp;gt; x.Value).Select(<br class="title-page-name"/>    x =&amp;gt; (double)x.Value.KeyCount<br class="title-page-name"/>)[new string[] { "Ex", "Gd", "TA", "Fa" }];<br class="title-page-name"/>exteriorQualDistribution.Print();<br class="title-page-name"/><br class="title-page-name"/>var exteriorQualBarChart = DataBarBox.Show(<br class="title-page-name"/>    exteriorQualDistribution.Keys.Select(x =&amp;gt; x.ToString()),<br class="title-page-name"/>    exteriorQualDistribution.Values.ToArray()<br class="title-page-name"/>);<br class="title-page-name"/>exteriorQualBarChart.SetTitle("Exterior Quality Distribution (Ordinal)");</pre>
<p class="calibre2"><span class="calibre5">When you run this code, it will display the following bar chart:</span></p>
<div class="mce-root"><img src="../images/00069.jpeg" class="calibre75"/></div>
<p class="calibre2"><span class="calibre5">Unlike the <kbd class="calibre12">OverallQual</kbd> variable, the <kbd class="calibre12">ExterQual</kbd> variable does not have numerical values for the ordering. In our dataset, it has one of the following values: <kbd class="calibre12">Ex</kbd>, <kbd class="calibre12">Gd</kbd>, <kbd class="calibre12">TA</kbd>, and <kbd class="calibre12">FA</kbd>, and these represent excellent, good, average/typical, and fair respectively. Although this variable does not have numerical values, it clearly has a natural ordering, where the e</span>xcellent category <span class="calibre5">(</span><span class="calibre5">Ex</span>) represents the best quality of material on the exterior and the good category (<span class="calibre5">Gd</span>) represents the second best quality of material on the exterior. In the feature engineering step, we will discuss how we can encode this type of variable for our future model building step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Continuous variable</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">We have so far looked at two types of categorical variables in our dataset. However, there is another type of variable in the dataset; the continuous variable. Unlike categorical variables, continuous variables have no limited number of values they can take. For example, square footage for basement area of a house can be any positive number. A house can have a 0 square foot basement area (or no basement) or a house can have a 1,000 square feet basement area. The first continuous variable that we are going to look at is <kbd class="calibre12">1stFlrSF</kbd>, which represents the first floor square feet. The following code shows how we can build a histogram for <kbd class="calibre12">1stFlrSF</kbd>:</span></p>
<pre class="calibre19">// Continuous Variable #1-1: First Floor Square Feet<br class="title-page-name"/>var firstFloorHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["1stFlrSF"].ValuesAll.ToArray(),<br class="title-page-name"/>    title: "First Floor Square Feet (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, the following histogram will be displayed:</span></p>
<div class="mce-root"><img class="alignnone16" src="../images/00070.gif"/></div>
<p class="calibre2"><span class="calibre5">One thing that is obvious from this chart is that it has a long tail in the positive direction, or in other words, the distribution is right skewed. The skewness in the data can adversely affect us when we build ML models. One way to handle this skewness in the dataset is to apply some transformations. One frequently used transformation is the log transformation, where you take log values of a given variable. In this example, the following code shows how we can apply log transformation to the <kbd class="calibre12">1stFlrSF</kbd> variable and show a histogram for the transformed variable:</span></p>
<pre class="calibre19">// Continuous Variable #1-2: Log of First Floor Square Feet<br class="title-page-name"/>var logFirstFloorHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["1stFlrSF"].Log().ValuesAll.ToArray(),<br class="title-page-name"/>    title: "First Floor Square Feet - Log Transformed (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, you will see the following histogram:</span></p>
<div class="mce-root"><img class="alignnone17" src="../images/00071.jpeg"/></div>
<p class="calibre2"><span class="calibre5">As you can see from this chart, the distribution looks more symmetric and closer to the bell shape that we are familiar with, compared to the previous histogram that we looked at for the same variable. Log transformation is frequently used to handle skewness in the dataset and make the distribution closer to the normal distribution. Let's look at another continuous variable in our dataset. The following code is used to show the distribution of the</span> <kbd class="calibre12">GarageArea</kbd> <span class="calibre5">variable, which represents the size of the garage in square feet:</span></p>
<pre class="calibre19">// Continuous Variable #2-1: Size of garage in square feet<br class="title-page-name"/>var garageHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["GarageArea"].ValuesAll.ToArray(),<br class="title-page-name"/>    title: "Size of garage in square feet (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, you will see the following histogram:</span></p>
<div class="mce-root"><img class="alignnone18" src="../images/00072.jpeg"/></div>
<p class="calibre2"><span class="calibre5">Similar to the previous case of <kbd class="calibre12">1stFlrSF</kbd>, it is also right skewed, although it seems the degree of skewness is less than <kbd class="calibre12">1stFlrSF</kbd>. We used the following code to apply log transformation for the <kbd class="calibre12">GarageArea</kbd> variable:</span></p>
<pre class="calibre19">// Continuous Variable #2-2: Log of Value of miscellaneous feature<br class="title-page-name"/>var logGarageHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["GarageArea"].Log().ValuesAll.ToArray(),<br class="title-page-name"/>    title: "Size of garage in square feet - Log Transformed (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2">The following histogram chart will be displayed when you run this code:</p>
<div class="mce-root"><img class="alignnone19" src="../images/00073.jpeg"/></div>
<p class="calibre2"><span class="calibre5">As expected, the distribution looks closer to the normal distribution when the log transformation is applied to the variable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Target variable – sale price</h1>
                
            
            <article>
                
<p class="calibre2">There is one last variable we need to take a look at before we move onto the feature engineering step; the target variable. In this fair value of a house project, our target variable for predictions is <kbd class="calibre12">SalePrice</kbd>, which represents the final sale price in U.S. dollar amounts for each residential home sold in Ames, Iowa, U.S.A. from 2006 to 2010. Since the sale price can take any positive numerical value, it is a continuous variable. Let's first look at how we built a histogram for the sale price variable:</p>
<pre class="calibre19">// Target Variable: Sale Price<br class="title-page-name"/>var salePriceHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["SalePrice"].ValuesAll.ToArray(),<br class="title-page-name"/>    title: "Sale Price (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, the following histogram chart will be shown:</span></p>
<div class="mce-root"><img class="alignnone20" src="../images/00074.gif"/></div>
<p class="calibre2">Similar to the previous cases of continuous variables, the distribution of <em class="calibre13">SalePrice</em> has a long right tail and it's heavily skewed to the right. This skewness often adversely affects the regression models, as some of those models, such as the linear regression model, assume that variables are normally distributed.  As discussed previously, we can fix this issue by applying log transformation. The following code shows how we log transformed the sale price variable and built a histogram chart:</p>
<pre class="calibre19">// Target Variable: Sale Price - Log Transformed<br class="title-page-name"/>var logSalePriceHistogram = HistogramBox<br class="title-page-name"/>.Show(<br class="title-page-name"/>    houseDF.DropSparseRows()["SalePrice"].Log().ValuesAll.ToArray(),<br class="title-page-name"/>    title: "Sale Price - Log Transformed (Continuous)"<br class="title-page-name"/>)<br class="title-page-name"/>.SetNumberOfBins(20);</pre>
<p class="calibre2"><span class="calibre5">When you run this code, you will see the following histogram for the log-transformed sale price variable:</span></p>
<div class="mce-root"><img class="alignnone16" src="../images/00075.gif"/></div>
<p class="calibre2">As expected, the distribution of the <kbd class="calibre12">SalePrice</kbd> variable looks much closer to the normal distribution. We are going to use this log-transformed <kbd class="calibre12">SalePrice</kbd> variable as the target variable for our future model building steps.</p>
<p class="calibre2">The full code for this data analysis step can be found at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/DataAnalyzer.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/DataAnalyzer.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feature engineering and encoding</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that we have looked at our dataset and the distributions of the categorical, continuous, and target variables, let's start building features for our ML models. As we discussed previously, categorical variables in our dataset have certain string values to represent each type of variable. However, as it might already be clear to you, we cannot use string types to train our ML models. All the values of variables need to be numerical to be able to used for fitting the models. One way to handle categorical variables with multiple types or categories is to create dummy variables.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Dummy variables</h1>
                
            
            <article>
                
<p class="calibre2">A dummy variable is a variable that takes a <span class="calibre5">value of </span>0 or 1 to indicate whether a given category or type exists or not. For example, in the case of <kbd class="calibre12">BldgType</kbd> variable, where it has the five different categories <kbd class="calibre12">1Fam</kbd>, <kbd class="calibre12">2FmCon</kbd>, <kbd class="calibre12">Duplx</kbd>, <kbd class="calibre12">TwnhsE</kbd>, and <kbd class="calibre12">Twnhs</kbd>, we will create five dummy variables, where each dummy variable represents the existence or absence of each of those five categories in a given record. The following shows an example of how dummy variable encoding works:</p>
<div class="mce-root"><img class="alignnone21" src="../images/00076.gif"/></div>
<p class="calibre2">As you can see from this example, the absence and existence of each category of the building types is encoded into a separate dummy variable as <kbd class="calibre12">0</kbd> or <kbd class="calibre12">1</kbd>. For example, for the record with the ID <kbd class="calibre12">1</kbd>, the building type is <kbd class="calibre12">1Fam</kbd> and this is encoded with the value 1 for the new variable, <kbd class="calibre12">BldgType_1Fam</kbd>, and 0 for the other four new variables, <span class="calibre5"><kbd class="calibre12">BldgType_2fmCon</kbd>, <kbd class="calibre12">BldgType_Duplex</kbd>, <kbd class="calibre12">BldgType_TwnhsE</kbd>, and</span> <kbd class="calibre12">BldgType_Twnhs</kbd><span class="calibre5">. On the other hand, for the record with the ID <kbd class="calibre12">10</kbd>, the building type is</span> <kbd class="calibre12">2fmCon</kbd> <span class="calibre5">and this is encoded with the value 1 for the variable <kbd class="calibre12">BldgType_2fmCon</kbd> and 0 for the other four new variables, <kbd class="calibre12">BldgType_1Fam</kbd>, <kbd class="calibre12">BldgType_Duplex</kbd>, <kbd class="calibre12">BldgType_TwnhsE</kbd>, and <kbd class="calibre12">BldgType_Twnhs</kbd>.</span></p>
<p class="calibre2">For this chapter, we created dummy variables for the following list of categorical variables:</p>
<pre class="calibre19">string[] categoricalVars = new string[]<br class="title-page-name"/>{<br class="title-page-name"/>    "Alley", "BldgType", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2",<br class="title-page-name"/>    "BsmtQual", "CentralAir", "Condition1", "Condition2", "Electrical", "ExterCond",<br class="title-page-name"/>    "Exterior1st", "Exterior2nd", "ExterQual", "Fence", "FireplaceQu", "Foundation",<br class="title-page-name"/>    "Functional", "GarageCond", "GarageFinish", "GarageQual", "GarageType",<br class="title-page-name"/>    "Heating", "HeatingQC", "HouseStyle", "KitchenQual", "LandContour", "LandSlope", <br class="title-page-name"/>    "LotConfig", "LotShape", "MasVnrType", "MiscFeature", "MSSubClass", "MSZoning", <br class="title-page-name"/>    "Neighborhood", "PavedDrive", "PoolQC", "RoofMatl", "RoofStyle", <br class="title-page-name"/>    "SaleCondition", "SaleType", "Street", "Utilities"<br class="title-page-name"/>};</pre>
<p class="calibre2">The following code shows a method we wrote to create and encode dummy variables:</p>
<pre class="calibre19">private static Frame&amp;lt;int, string&amp;gt; CreateCategories(Series&amp;lt;int, string&amp;gt; rows, string originalColName)<br class="title-page-name"/>{<br class="title-page-name"/><br class="title-page-name"/>    var categoriesByRows = rows.GetAllValues().Select((x, i) =&amp;gt;<br class="title-page-name"/>    {<br class="title-page-name"/>        // Encode the categories appeared in each row with 1<br class="title-page-name"/>        var sb = new SeriesBuilder&amp;lt;string, int&amp;gt;();<br class="title-page-name"/>        sb.Add(String.Format("{0}_{1}", originalColName, x.Value), 1);<br class="title-page-name"/><br class="title-page-name"/>        return KeyValue.Create(i, sb.Series);<br class="title-page-name"/>    });<br class="title-page-name"/><br class="title-page-name"/>    // Create a data frame from the rows we just created<br class="title-page-name"/>    // And encode missing values with 0<br class="title-page-name"/>    var categoriesDF = Frame.FromRows(categoriesByRows).FillMissing(0);<br class="title-page-name"/><br class="title-page-name"/>    return categoriesDF;<br class="title-page-name"/>}</pre>
<p class="calibre2">As you can see from line 8 of this method, we prefix the newly created dummy variables with the original categorical variable's names and append them with each category. For example, <kbd class="calibre12">BldgType</kbd> variables in the <kbd class="calibre12">1Fam</kbd> category will be encoded as <span class="calibre5"><kbd class="calibre12">BldgType_1Fam</kbd>. Then, in line 15 of the <kbd class="calibre12">CreateCategories</kbd> method, we are encoding all the other values with 0s to indicate the absence of such categories in the given categorical variable.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Feature encoding</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now that we know which categorical variables to encode and have created a method for dummy variable encoding for those categorical variables, it is time to build a data frame with features and their values. Let's first take a look at how we went about creating a features d</span>ata frame <span class="calibre5">in the following code snippet:</span></p>
<pre class="calibre19">var featuresDF = Frame.CreateEmpty&amp;lt;int, string&amp;gt;();<br class="title-page-name"/><br class="title-page-name"/>foreach(string col in houseDF.ColumnKeys)<br class="title-page-name"/>{<br class="title-page-name"/>    if (categoricalVars.Contains(col))<br class="title-page-name"/>    {<br class="title-page-name"/>        var categoryDF = CreateCategories(houseDF.GetColumn&amp;lt;string&amp;gt;(col), col);<br class="title-page-name"/><br class="title-page-name"/>        foreach (string newCol in categoryDF.ColumnKeys)<br class="title-page-name"/>        {<br class="title-page-name"/>            featuresDF.AddColumn(newCol, categoryDF.GetColumn&amp;lt;int&amp;gt;(newCol));<br class="title-page-name"/>        }<br class="title-page-name"/>    }<br class="title-page-name"/>    else if (col.Equals("SalePrice"))<br class="title-page-name"/>    {<br class="title-page-name"/>        featuresDF.AddColumn(col, houseDF[col]);<br class="title-page-name"/>        featuresDF.AddColumn("Log"+col, houseDF[col].Log());<br class="title-page-name"/>    }<br class="title-page-name"/>    else<br class="title-page-name"/>    {<br class="title-page-name"/>        featuresDF.AddColumn(col, houseDF[col].Select((x, i) =&amp;gt; x.Value.Equals("NA")? 0.0: (double) x.Value));<br class="title-page-name"/>    }<br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code snippet, we are starting with an empty Deedle data frame,</span> <kbd class="calibre12">featuresDF</kbd><em class="calibre13"> </em><span class="calibre5">(in line 1), and start adding in features one by one. For those categorical variables for which we are going to create dummy variables, we are calling the encoding method,</span> <kbd class="calibre12">CreateCategories</kbd><span class="calibre5">, that we wrote previously and then adding the newly created dummy variable columns to the <kbd class="calibre12">featuresDF</kbd> data frame (in lines 5-12).  For the <kbd class="calibre12">SalePrice</kbd> variable, which is the target variable for this project, we are applying log transformation and adding it to the <kbd class="calibre12">featuresDF</kbd> data frame (in lines 13-17). Lastly, we append all the other continuous variables, after replacing the <kbd class="calibre12">NA</kbd> values with 0s, to the <kbd class="calibre12">featuresDF</kbd> data frame (in lines 18-20).</span></p>
<p class="calibre2">Once we have created and encoded all the features for our model training, we then export this <kbd class="calibre12">featuresDF</kbd> data frame into a <kbd class="calibre12">.csv</kbd> file. The following code shows how we export the data frame into a <kbd class="calibre12">.csv</kbd> file:</p>
<pre class="calibre19">string outputPath = Path.Combine(dataDirPath, "features.csv");<br class="title-page-name"/>Console.WriteLine("Writing features DF to {0}", outputPath);<br class="title-page-name"/>featuresDF.SaveCsv(outputPath);</pre>
<p class="calibre2"><span class="calibre5">We now have all the necessary features that we can use to start building machine learning models to predict fair values of houses. The full code for feature encoding and engineering can be found in this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/FeatureEngineering.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/FeatureEngineering.cs</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Linear regression versus SVM with kernels</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The first thing we need to do before we start training our machine learning models is to split our dataset into train and test sets. In this section, we will split the sample set into train and test sets by randomly sub-selecting and dividing the indexes at a pre-defined proportion. The code we used to split the dataset into train and test sets is as follows:</span></p>
<pre class="calibre19">// Split the sample set into train and test sets<br class="title-page-name"/>double trainProportion = 0.8;<br class="title-page-name"/><br class="title-page-name"/>int[] shuffledIndexes = featuresDF.RowKeys.ToArray();<br class="title-page-name"/>shuffledIndexes.Shuffle();<br class="title-page-name"/><br class="title-page-name"/>int trainSetIndexMax = (int)(featuresDF.RowCount * trainProportion);<br class="title-page-name"/>int[] trainIndexes = shuffledIndexes.Where(i =&amp;gt; i &amp;lt; trainSetIndexMax).ToArray();<br class="title-page-name"/>int[] testIndexes = shuffledIndexes.Where(i =&amp;gt; i &amp;gt;= trainSetIndexMax).ToArray();<br class="title-page-name"/><br class="title-page-name"/>var trainSet = featuresDF.Where(x =&amp;gt; trainIndexes.Contains(x.Key));<br class="title-page-name"/>var testSet = featuresDF.Where(x =&amp;gt; testIndexes.Contains(x.Key));<br class="title-page-name"/><br class="title-page-name"/>Console.WriteLine("\nTrain Set Shape: ({0}, {1})", trainSet.RowCount, trainSet.ColumnCount);<br class="title-page-name"/>Console.WriteLine("Test Set Shape: ({0}, {1})", testSet.RowCount, testSet.ColumnCount);</pre>
<p class="calibre2"><span class="calibre5">You can choose to have different proportions for training and testing; however, in this example, we reserved 80% of our dataset to be used for training and the remaining 20% for testing. In lines 4-5 of the code snippet, we first randomly shuffle the indexes of our dataset. Then, in lines 7-8, we sub-select indexes for the train set and the test set, and in lines 10-11, we split the <kbd class="calibre12">featuresDF</kbd> data frame that we created in the previous feature engineering and encoding step into train and test sets.</span></p>
<p class="calibre2">Once we have these train and test data frames ready, we need to filter out unnecessary columns from the data frames, since the train and test data frames currently have values for columns, such as <kbd class="calibre12">SalePrice</kbd> and <kbd class="calibre12">Id</kbd>. Then, we will have to cast the two data frames into arrays of double arrays, which will be input to our learning algorithms. The code to filter out unwanted columns from the train and test data frames and to cast the two data frames into arrays of arrays is as follows:</p>
<pre class="calibre19">string targetVar = "LogSalePrice";<br class="title-page-name"/>string[] features = featuresDF.ColumnKeys.Where(<br class="title-page-name"/>    x =&amp;gt; !x.Equals("Id") &amp;&amp; !x.Equals(targetVar) &amp;&amp; !x.Equals("SalePrice")<br class="title-page-name"/>).ToArray();<br class="title-page-name"/><br class="title-page-name"/>double[][] trainX = BuildJaggedArray(<br class="title-page-name"/>    trainSet.Columns[features].ToArray2D&amp;lt;double&amp;gt;(),<br class="title-page-name"/>    trainSet.RowCount,<br class="title-page-name"/>    features.Length<br class="title-page-name"/>);<br class="title-page-name"/>double[][] testX = BuildJaggedArray(<br class="title-page-name"/>    testSet.Columns[features].ToArray2D&amp;lt;double&amp;gt;(),<br class="title-page-name"/>    testSet.RowCount,<br class="title-page-name"/>    features.Length<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>double[] trainY = trainSet[targetVar].ValuesAll.ToArray();<br class="title-page-name"/>double[] testY = testSet[targetVar].ValuesAll.ToArray();</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Linear regression</h1>
                
            
            <article>
                
<p class="calibre2">The first ML model we are going to explore for this chapter's housing price prediction project is the linear regression model. You should already be familiar with building linear regression models in C# using the Accord.NET framework. We use the following code to build a linear regression model:</p>
<pre class="calibre19">Console.WriteLine("\n**** Linear Regression Model ****");<br class="title-page-name"/>// OLS learning algorithm<br class="title-page-name"/>var ols = new OrdinaryLeastSquares()<br class="title-page-name"/>{<br class="title-page-name"/>    UseIntercept = true,<br class="title-page-name"/>    IsRobust = true<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>// Fit a linear regression model<br class="title-page-name"/>MultipleLinearRegression regFit = ols.Learn(<br class="title-page-name"/>    trainX,<br class="title-page-name"/>    trainY<br class="title-page-name"/>);<br class="title-page-name"/><br class="title-page-name"/>// in-sample predictions<br class="title-page-name"/>double[] regInSamplePreds = regFit.Transform(trainX);<br class="title-page-name"/>// out-of-sample predictions<br class="title-page-name"/>double[] regOutSamplePreds = regFit.Transform(testX);</pre>
<p class="calibre2"><span class="calibre5">The only difference between this chapter's linear regression model code and the previous chapter's code is the <kbd class="calibre12">IsRobust</kbd> parameter to the <kbd class="calibre12">OrdinaryLeastSquares</kbd> learning algorithm. As the name suggests, it makes the learning algorithm fit a more robust linear regression model, meaning it is less sensitive to outliers. When we have variables that are not normally distributed, as is the case for this project, it often causes problems when fitting a linear regression model as traditional linear regression models are sensitive to outliers from non-normal distributions. Setting this parameter to <kbd class="calibre12">true</kbd> helps resolve this issue.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Linear SVM</h1>
                
            
            <article>
                
<p class="calibre2">The second learning algorithm we are going to experiment with in this chapter is the linear SVM. The following code shows how we build a linear SVM model:</p>
<pre class="calibre19">Console.WriteLine("\n**** Linear Support Vector Machine ****");<br class="title-page-name"/>// Linear SVM Learning Algorithm<br class="title-page-name"/>var teacher = new LinearRegressionNewtonMethod()<br class="title-page-name"/>{<br class="title-page-name"/>    Epsilon = 0.5,<br class="title-page-name"/>    Tolerance = 1e-5,<br class="title-page-name"/>    UseComplexityHeuristic = true<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>// Train SVM<br class="title-page-name"/>var svm = teacher.Learn(trainX, trainY);<br class="title-page-name"/><br class="title-page-name"/>// in-sample predictions<br class="title-page-name"/>double[] linSVMInSamplePreds = svm.Score(trainX);<br class="title-page-name"/>// out-of-sample predictions<br class="title-page-name"/>double[] linSVMOutSamplePreds = svm.Score(testX);</pre>
<p class="calibre2"><span class="calibre5">As you might have noticed, and similar to the previous chapter, we used</span> <kbd class="calibre12">LinearRegressionNewtonMethod</kbd> <span class="calibre5">as a learning algorithm to fit a linear SVM.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SVM with a polynomial kernel</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">The next model we are going to experiment with is an SVM with a polynomial kernel. We will not go into too much detail about the kernel methods, but simply put, kernels are functions of input feature variables that can transform and project the original variables into a new feature space that is more linearly separable. The polynomial kernel looks at the combinations of input features, on top of the original input features. These combinations of input feature variables are often called <strong class="calibre4">interaction variables</strong> in regression analysis. Using different kernel methods will make SVM models learn and behave differently with the same dataset.</span></p>
<p class="calibre2"><span class="calibre5">The following code shows how you can build a SVM model with a polynomial kernel:</span></p>
<pre class="calibre19">Console.WriteLine("\n**** Support Vector Machine with a Polynomial Kernel ****");<br class="title-page-name"/>// SVM with Polynomial Kernel<br class="title-page-name"/>var polySVMLearner = new FanChenLinSupportVectorRegression&amp;lt;Polynomial&amp;gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Epsilon = 0.1,<br class="title-page-name"/>    Tolerance = 1e-5,<br class="title-page-name"/>    UseKernelEstimation = true,<br class="title-page-name"/>    UseComplexityHeuristic = true,<br class="title-page-name"/>    Kernel = new Polynomial(3)<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>// Train SVM with Polynomial Kernel<br class="title-page-name"/>var polySvm = polySVMLearner.Learn(trainX, trainY);<br class="title-page-name"/><br class="title-page-name"/>// in-sample predictions<br class="title-page-name"/>double[] polySVMInSamplePreds = polySvm.Score(trainX);<br class="title-page-name"/>// out-of-sample predictions<br class="title-page-name"/>double[] polySVMOutSamplePreds = polySvm.Score(testX);</pre>
<p class="calibre2"><span class="calibre5">We are using the <kbd class="calibre12">FanChenLinSupportVectorRegression</kbd> learning algorithm for a support vector machine with a polynomial kernel. In this example, we used a degree 3 polynomial, but you can experiment with different degrees. However, the higher the degrees are, the more likely it is to overfit to the training data. So, you will have to take cautious steps when you are using high degree polynomial kernels.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">SVM with a Gaussian kernel</h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Another commonly used kernel method is the Gaussian kernel. Simply put, the Gaussian kernel looks at the distance between the input feature variables and results in higher values for close or similar features and lower values for more distanced features. The Gaussian kernel can help transform and project a linearly inseparable dataset into a more linearly separable feature space and can improve the model performances.</span></p>
<p class="calibre2"><span class="calibre5">The following code shows how you can build a SVM model with a Gaussian kernel:</span></p>
<pre class="calibre19">Console.WriteLine("\n**** Support Vector Machine with a Gaussian Kernel ****");<br class="title-page-name"/>// SVM with Gaussian Kernel<br class="title-page-name"/>var gaussianSVMLearner = new FanChenLinSupportVectorRegression&amp;lt;Gaussian&amp;gt;()<br class="title-page-name"/>{<br class="title-page-name"/>    Epsilon = 0.1,<br class="title-page-name"/>    Tolerance = 1e-5,<br class="title-page-name"/>    Complexity = 1e-4,<br class="title-page-name"/>    UseKernelEstimation = true,<br class="title-page-name"/>    Kernel = new Gaussian()<br class="title-page-name"/>};<br class="title-page-name"/><br class="title-page-name"/>// Train SVM with Gaussian Kernel<br class="title-page-name"/>var gaussianSvm = gaussianSVMLearner.Learn(trainX, trainY);<br class="title-page-name"/><br class="title-page-name"/>// in-sample predictions<br class="title-page-name"/>double[] guassianSVMInSamplePreds = gaussianSvm.Score(trainX);<br class="title-page-name"/>// out-of-sample predictions<br class="title-page-name"/>double[] guassianSVMOutSamplePreds = gaussianSvm.Score(testX);</pre>
<p class="calibre2">Similar to the case of the polynomial kernel, we used the <kbd class="calibre12">FanChenLinSupportVectorRegression</kbd> learning algorithm, but replaced the kernel with the <kbd class="calibre12">Gaussian</kbd> method.</p>
<p class="calibre2">We have discussed how we can use different kernel methods for SVMs so far. We will now compare the performances of these models on the housing price dataset. You can find the full code we used for building and evaluating models at this link: <a href="https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/Modeling.cs" target="_blank" class="calibre9">https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.5/Modeling.cs</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Model validations </h1>
                
            
            <article>
                
<p class="calibre2"><span class="calibre5">Before we start looking into the performances of the linear regression and SVM models that we built in the previous section, let's refresh our memory on the metrics and the diagnostics plot we discussed in the previous chapter. We are going to look at</span> RMSE, <span class="calibre5">R<sup class="calibre64">2</sup>, and a plot of actual versus predicted values to evaluate the performances of our models. The code we are going to use throughout this section for model evaluation is as follows:</span></p>
<pre class="calibre19">private static void ValidateModelResults(string modelName, double[] regInSamplePreds, double[] regOutSamplePreds, double[][] trainX, double[] trainY, double[][] testX, double[] testY)<br class="title-page-name"/>{<br class="title-page-name"/>    // RMSE for in-sample <br class="title-page-name"/>    double regInSampleRMSE = Math.Sqrt(new SquareLoss(trainX).Loss(regInSamplePreds));<br class="title-page-name"/>    // RMSE for out-sample <br class="title-page-name"/>    double regOutSampleRMSE = Math.Sqrt(new SquareLoss(testX).Loss(regOutSamplePreds));<br class="title-page-name"/><br class="title-page-name"/>    Console.WriteLine("RMSE: {0:0.0000} (Train) vs. {1:0.0000} (Test)", regInSampleRMSE, regOutSampleRMSE);<br class="title-page-name"/><br class="title-page-name"/>    // R^2 for in-sample <br class="title-page-name"/>    double regInSampleR2 = new RSquaredLoss(trainX[0].Length, trainX).Loss(regInSamplePreds);<br class="title-page-name"/>    // R^2 for out-sample <br class="title-page-name"/>    double regOutSampleR2 = new RSquaredLoss(testX[0].Length, testX).Loss(regOutSamplePreds);<br class="title-page-name"/><br class="title-page-name"/>    Console.WriteLine("R^2: {0:0.0000} (Train) vs. {1:0.0000} (Test)", regInSampleR2, regOutSampleR2);<br class="title-page-name"/><br class="title-page-name"/>    // Scatter Plot of expected and actual<br class="title-page-name"/>    var scatterplot = ScatterplotBox.Show(<br class="title-page-name"/>        String.Format("Actual vs. Prediction ({0})", modelName), testY, regOutSamplePreds<br class="title-page-name"/>    );<br class="title-page-name"/>    <br class="title-page-name"/>}</pre>
<p class="calibre2"><span class="calibre5">The way we use this method for our models is as follows:</span></p>
<pre class="calibre19">ValidateModelResults("Linear Regression", regInSamplePreds, regOutSamplePreds, trainX, trainY, testX, testY);<br class="title-page-name"/>ValidateModelResults("Linear SVM", linSVMInSamplePreds, linSVMOutSamplePreds, trainX, trainY, testX, testY);<br class="title-page-name"/>ValidateModelResults("Polynomial SVM", polySVMInSamplePreds, polySVMOutSamplePreds, trainX, trainY, testX, testY);<br class="title-page-name"/>ValidateModelResults("Guassian SVM", guassianSVMInSamplePreds, guassianSVMOutSamplePreds, trainX, trainY, testX, testY);</pre>
<p class="calibre2"><span class="calibre5">As you can see from this code snippet, we pass the in-sample and out-of-sample predictions by the models, along with the train and test sets, onto the <kbd class="calibre12">ValidateModelResults</kbd> method. When you run this code, you will see the following output on your console:</span></p>
<div class="mce-root"><img class="alignnone22" src="../images/00077.gif"/></div>
<p class="calibre2"><span class="calibre5">When looking at the values of the goodness of fit, R<sup class="calibre64">2</sup>, and the RMSE values, the linear SVM model seems to have the best fit to the dataset, and the SVM model with the Gaussian kernel seems to have the second best fit to the dataset. Looking at this output, the SVM model with the polynomial kernel does not seem to work well for predicting the fair values of house prices. Now, let's look at the diagnostic plots to evaluate how well our models predict the house prices.</span></p>
<p class="calibre2">The following plot shows the diagnostic plot for the linear regression model:</p>
<div class="mce-root"><img src="../images/00078.jpeg" class="calibre76"/></div>
<p class="calibre2"><span class="calibre5">This diagnostic plot for the linear regression model looks good. Most of the points seem to be aligned on a diagonal line, which suggests that the linear regression model's predictions are well aligned with the actual values.</span></p>
<p class="calibre2"><span class="calibre5">The following plot shows the diagnostic plot for the linear SVM model:</span></p>
<div class="mce-root"><img src="../images/00079.jpeg" class="calibre77"/></div>
<p class="calibre2"><span class="calibre5">As expected from the previous R<sup class="calibre64">2</sup> metrics value, the goodness of fit for the linear SVM model looks good, even though there seems to be one prediction that is far off from the actual value. Most of the points seem to be aligned on a diagonal line, which suggests that the linear SVM model's predictions are well aligned with the actual values.</span></p>
<p class="calibre2"><span class="calibre5">The following plot shows the diagnostic plot for the SVM model with the polynomial kernel:</span></p>
<div class="mce-root"><img src="../images/00080.jpeg" class="calibre78"/></div>
<p class="calibre2"><span class="calibre5">This diagnostic plot for the SVM model with the polynomial kernel suggests that the goodness of fit for this model is not so good. Most of the predictions lie on a straight line at around 12. This is well aligned with the other metrics, where we have seen that RMSE and R<sup class="calibre64">2</sup> measures were the worst among the four models we tried.</span></p>
<p class="calibre2"><span class="calibre5">The following plot shows the diagnostic plot for the SVM model with the Gaussian kernel:</span></p>
<div class="mce-root"><img src="../images/00081.jpeg" class="calibre79"/></div>
<p class="calibre2"><span class="calibre5">This diagnostic plot result for the SVM model with the Gaussian kernel is rather surprising. From the RMSE and R<sup class="calibre64">2</sup> measures, we expected the model fit using SVM with Gaussian kernel will be good. However, most of the predictions by this model are on a straight line, without showing any patterns of a diagonal line. Looking at this diagnostic plot, we cannot conclude that the model fit for the SVM model with the Gaussian kernel is good, even though the R<sup class="calibre64">2</sup> metrics showed a strong positive sign of the goodness of model fit.</span></p>
<p class="calibre2"><span class="calibre5">By looking at both the metrics numbers and the diagnostic plots, we can conclude that the linear regression model and the linear SVM model seem to work the best for predicting the fair values of house prices. This project shows us a good example of the importance of looking at the diagnostic plots. Looking at and optimizing for single metrics might be tempting, but it is always better to evaluate models with more than one validation metric, and looking at diagnostic plots, such as the plot of actual values against predicted values, is especially helpful for regression models.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="calibre2">In this chapter, we expanded our knowledge and skills regarding building regression models. We built prediction models using the sale price data of residential homes in Ames, Iowa, U.S.A. Unlike other chapters, we had a more complex dataset, where the variables had mixed types, categorical and continuous. We looked at the categorical variables, where there were no natural orderings (non-ordinal) and where there were natural orderings (ordinal) in the categories. We then looked at continuous variables, whose distributions had long right tails. We also discussed how we can use log transformations on such variables with high skewness in the data to mediate the skewness and make those variables' distributions closer to normal distributions.</p>
<p class="calibre2">We discussed how to handle categorical variables in our dataset. We learned how to create and encode dummy variables for each type of categorical variable. Using these features, we experimented with four different machine learning models—linear regression, linear support vector machine, SVM with a polynomial kernel, and SVM with a Gaussian kernel. We briefly discussed the purpose and usage of kernel methods and how they can be used for linearly inseparable datasets. Using RMSE, R<sup class="calibre64">2</sup>, and the plot of the actual values against the predicted values, we evaluated the performances of those four models we built for predicting the fair values of house prices in Ames, Iowa, U.S.A. During our model validation step, we saw a case where the validation metrics results contradict with the diagnostic plots results and we have learned the importance of looking at more than one metric and diagnostic plots to be sure of our model's performance.</p>
<p class="calibre2">In the next chapter, we are going to switch gear again. So far, we have been learning how to use and build supervised learning algorithms. However, in the next chapter, we are going to learn unsupervised learning and more specifically clustering algorithms. We will discuss how to use clustering algorithms to gain insights on the customer segments using an online retail dataset.</p>


            </article>

            
        </section>
    </body></html>