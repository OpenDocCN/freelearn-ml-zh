<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Model Creation</h1>
            </header>

            <article>
                
<p>We have now created several data sources based on the original <kbd>Titanic</kbd> dataset in S3. We are ready to train and evaluate an Amazon ML prediction model. In Amazon ML, creating a model consists of the following:</p>
<ul>
<li>Selecting the training datasource</li>
<li>Defining a recipe for data transformation</li>
<li>Setting the parameters of the learning algorithm</li>
<li>Evaluating the quality of the model</li>
</ul>
<p>In this chapter, we will start by exploring the data transformations available in Amazon ML, and we will compare different recipes for the <kbd>Titanic</kbd> dataset. Amazon ML defines recipes by default depending on the nature of the data. We will investigate and challenge these default transformations.</p>
<p>The model-building step is simple enough, and we will spend some time examining the available parameters. The model evaluation is where everything converges. The evaluation metrics are dependent on the type of the prediction at hand, regression, binary or multi-class classification. We will look at how these different evaluations are carried out. We will also download the model training logs to better understand what goes on under the Amazon ML hood when training the model. We will conclude the chapter by comparing the model evaluation for several data recipes and regularization strategies.</p>
<p>The chapter is organized as follows:</p>
<ul>
<li>Recipes</li>
<li>Model parameters</li>
<li>Evaluations</li>
<li>Log analysis</li>
<li>Feature engineering, recipes, and regularization</li>
</ul>
<p>At the end of <a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Loading and Preparing the Dataset,</em> we modified the schema to exclude three variables: <kbd>boat</kbd>, <kbd>body</kbd>, and <kbd>home.dest</kbd> from the original dataset and created a new datasource based on this schema. We will use this datasource to train the model.</p>
<p>Go to your Amazon ML datasource dashboard; you should see three datasources:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="206" src="assets/B05028_05_01.png" width="484"/></div>
<ul>
<li><span class="packt_screen">Titanic train set</span>: It is the original raw dataset with 14 variables</li>
<li><span class="packt_screen">Titanic train set 11 variables</span>: Has 11 variables; <kbd>boat</kbd>, <kbd>body</kbd> and <kbd>home.dest</kbd> have been removed from the schema</li>
<li><span class="packt_screen">Titanic train set extended</span>: It is the cleaned up and extended dataset we obtained through SQL-based feature engineering.</li>
</ul>
<p>We will work with the <kbd>Titanic train set 11 variables</kbd> datasource. Before starting with the model creation, let’s first review what types of data transformations are available in Amazon ML.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Transforming data with recipes</h1>
            </header>

            <article>
                
<p>A crucial element of the data science workflow is feature engineering. Amazon ML offers certain data transformations via its data recipes. Note that although transformations are conceptually part of the ETL or data preparation phase of a predictive analytics workflow, in Amazon ML, data recipes are part of the model-building step and not of the initial datasource creation step. In this section, we start by reviewing the available data transformations in Amazon ML, and then we apply some of them to the <kbd>Titanic</kbd> dataset using the <kbd>Titanic train set 11 variables</kbd> datasource.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Managing variables</h1>
            </header>

            <article>
                
<p>Recipes are JSON-structured scripts that contains the following three sections in the given order:</p>
<ul>
<li>Groups</li>
<li>Assignments</li>
<li>Outputs</li>
</ul>
<p>An empty recipe instructing Amazon ML to take all the dataset variables into account for model training will be as follows:</p>
<pre>
{<br/><span>    "groups" : {},</span><br/><span>    "assignments" : { },</span><br/><span>    "outputs":["ALL_INPUTS"]</span><br/>}
</pre>
<p>The recipe does not transform the data in any way.</p>
<div class="packt_infobox">The complete Amazon ML recipes documentation is available at <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html" target="_blank">http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html</a>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Grouping variables</h1>
            </header>

            <article>
                
<p>Groups enable grouping of multiple variables to facilitate applying the same transformations to several variables. The groups section of the recipe has a naming function. Group definition follows this syntax:</p>
<pre>
"group_name": "group('first_variable',<span>'second_variable'</span> )"
</pre>
<p>Amazon ML has defined a set of default groups based on the type of the variables: <kbd>ALL_TEXT</kbd>, <kbd>ALL_NUMERIC</kbd>, <kbd>ALL_CATEGORICAL</kbd>, <kbd>ALL_BINARY</kbd>, and the <kbd>ALL_INPUTS</kbd> group for all the variables at once. Let's look at a couple of examples.</p>
<p>Consider the following example where we want to apply the same transformation (normalization) on the <kbd>age</kbd> and <kbd>fare</kbd> variables. We can define a group and name it <kbd>TO_BE_NORMALIZED</kbd>:</p>
<pre>
"groups" : {<br/><span>    "TO_BE_NORMALIZED" : "group('age','fare')",</span><br/>},
</pre>
<p>Similarly, consider an e-mail spam detection context where for each e-mail, we have a header, a subject, and a body. We want to create N-grams of the e-mail title and body but not of the header; we can define a group composed of all text variables with the exception of specifically excluded ones. Here we create a group named <kbd>N_GRAM_TEXT</kbd> that combines all text variables except the header:</p>
<pre>
"groups" : {<br/><span>    "N_GRAM_TEXT" : "group_remove(ALL_TEXT, 'header')",</span><br/>},
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Naming variables with assignments</h1>
            </header>

            <article>
                
<p>The main purpose of assignments is naming facilitation. You can choose to name the transformed variable or group of variables in the assignments section or directly in the output section. Assignments are only for convenience and readability.<br/>
Assignments follow this syntax:</p>
<pre>
"assignment_name": "transformation('group_name<span>'</span> )"
</pre>
<p>For instance, you could rename and normalize the numeric variables as follows:</p>
<pre>
"assignments": {<br/><span>    "normalized_numeric": "normalize(TO_BE_NORMALIZED)",</span><br/>}
</pre>
<p>Or rename and process the subject and body of your e-mails:</p>
<pre>
"assignments": {<br/><span>    "bigrams": "ngram(N_GRAM_TEXT,2)",</span><br/>}
</pre>
<p>You can also leave the assignments section empty and apply the transformations to the variables groups in the output section. In the end, it’s more a question of style and readability than anything else.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Specifying outputs</h1>
            </header>

            <article>
                
<p>The outputs section is where you explicitly list all the variables that will be used for the model training. If you have defined a group with some of the variables but you still want the original variables to be accounted for, you need to explicitly list them. The assignment section declares a list composed of the following: </p>
<ul>
<li>Groups</li>
<li>Assignments</li>
<li>Variables</li>
<li>Transformation (variable)</li>
</ul>
<p>For instance, if you wanted the original body and subject of the e-mails as well as the <kbd>bigrams</kbd> you defined in assignments, you would need to declare the outputs as follows:</p>
<pre>
"outputs": [<br/><span>    "header",<br/><span>    "subject",<br/><span>    "body",<br/><span>    "bigrams"</span><br/>]</span></span></span>
</pre>
<p>The following outputs declaration declares all the text variables and adds the bigrams assignment defined earlier on:</p>
<pre>
"outputs": [<br/>    "ALL_TEXT",<br/>    "bigrams"<br/>    ]
</pre>
<p>The recipe format reference page has other examples of combining groups, assignments, and outputs to create recipes: <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html" target="_blank">http://docs.aws.amazon.com/machine-learning/latest/dg/recipe-format-reference.html</a>. We will now look at the available transformations.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data processing through seven transformations</h1>
            </header>

            <article>
                
<p>Amazon ML offers the following seven transformations. Four transformations for text variables are as follows:</p>
<ul>
<li>Lowercase transformation</li>
<li>Remove punctuation transformation</li>
<li>N-gram transformation</li>
<li><strong>Orthogonal sparse bigram (OSB)</strong> transformation</li>
</ul>
<p>Two <span>transformations</span> for numeric variables are as follows:</p>
<ul>
<li>Normalization transformation</li>
<li>Quantile binning transformation</li>
</ul>
<p>And one <span>transformation </span>for coupling text with categorical variables:</p>
<ul>
<li>Cartesian product transformation</li>
</ul>
<p>These transformations are well explained on the Amazon ML documentation (<a href="http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html" target="_blank">http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html</a>).</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using simple transformations</h1>
            </header>

            <article>
                
<p>The lowercase transformation takes a text variable as input and returns the text in lowercase: <kbd>Amazon ML is great for Predictive Analytics</kbd> is returned as <kbd>amazon ml is great for predictive analytics</kbd>. Syntax for lowercase transformation is <kbd>lowercase(text_variable)</kbd> or <kbd>lowercase(group_of_text_variables)</kbd>.</p>
<p>The remove punctuation transformation also takes a text variable as input and removes all punctuation signs, with the exception of hyphens within words (<kbd>seat-belts</kbd> will remain as <kbd>seat-belts</kbd>). It is not possible to define your own set of punctuation signs. Syntax for the remove punctuation transformation is <kbd>no_punct(text_variable)</kbd> or <kbd>no_punct(group_of_text_variables)</kbd>.</p>
<p>The normalization transformation normalizes numeric variables to have a mean of zero and a variance of one. This is a useful transformation when numeric variables vary significantly in range. This transformation corresponds to the <strong>z-score</strong> normalization also known as  standardization and not to the min-max normalization (see <a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span>,</a> <em>Machine Learning Definitions and Concepts</em>). Syntax for normalization transformation is <kbd>normalize(numeric_variable)</kbd> or <kbd>normalize(group_of_numeric_variables)</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Text mining</h1>
            </header>

            <article>
                
<p>The N-gram and the <strong>orthogonal sparse bigram</strong> (<strong>OSB</strong>) transformations are the main text-mining transformations available in Amazon ML.</p>
<p>In text mining, the classic approach is called the <strong>bag-of-words</strong> approach. This approach boils down to discarding the order of the word in a given text and only considering the relative frequency of the words in the documents. Although it may seem to be overly simplistic, since the order of the words is essential to understand a message, this approach has given satisfying results in all types of natural language processing problems. A key part of the bag-of-words method, is driven by the need to extract the words from a given text. However, instead of considering single words as the only elements holding information, we could extract sequences of words. These sequences are called N-grams. Sequences of two words are called bigrams, for three words trigrams, and so forth. Single words are also called unigrams. N-grams are also called tokens and the process of extracting words, and N-grams from a text is called tokenization.</p>
<p>For instance, consider the sentence: <em>The brown fox jumps over the dog</em></p>
<ul>
<li><strong>Unigrams</strong> are {<em>The, brown, fox, jumps, over, the, dog</em>}</li>
<li><strong>Bigrams</strong> are {<em>The brown, brown fox, fox jumps, jumps over, over the, the dog</em>}</li>
<li><strong>Trigrams</strong> are {<em>The brown fox, brown fox jumps, fox jumps over, jumps over the, over the dog</em>}</li>
</ul>
<p>There is no rule or heuristic that would let you know if you need N-grams in your model or what order of N-grams would be the most beneficial for your model. It depends on the type of text you are dealing with. Only experimentation can tell.</p>
<p>Amazon ML offers two tokenization transformations: N-gram and OSB.</p>
<p>The <strong>N-gram</strong> transformation: Takes a text variable and an integer from 2 to 10 and returns expected N-grams. Note that all text variables are, by default, tokenized as unigrams in Amazon ML. There is no need to explicitly specify unigrams in the recipe. <kbd>ngram(text_variable, n)</kbd> will produce bigrams for <em>n= 2</em>, trigrams for <em>n=3</em> and so forth.</p>
<p>The <strong>OSB</strong> or orthogonal sparse bigram transformation is an extension on the bigram transformation (N<em>-gram</em> with <em>n=2</em>). Given a word in a text, compose pairs of words by associating the other words separated by <em>1,2, …, N</em> words from the initial word. <em>N</em> being the size of the OSB window.<br/>
For instance, in the sentence <em>this is a limited time offer</em>, first consider the word <em>offer</em>. The OSBs for a window of four are: <em>time_offer</em>, <em>limited_&lt;skip&gt;_offer</em>, <em>a_&lt;skip&gt;_&lt;skip&gt;_offer</em>, <em>is_&lt;skip&gt;_&lt;skip&gt;_&lt;skip&gt;_offer</em>, <em>this_&lt;skip&gt;_&lt;skip&gt;_&lt;skip&gt;_&lt;skip&gt;_offer</em>. We build word pairs by skipping 1,2,..., N words each time.</p>
<p>The OSB transformation allows us to extract information about the context surrounding each word. For instance, the OSB <em>is_&lt;skip&gt;_&lt;skip&gt;_offer</em>, could be used to detect strings such as <em>is a special offer</em> as well as <em>is our best offer</em>. OSB extraction has been found to generally improve the performance of spam filtering algorithms. Syntax for OSB transformation is <kbd>osb(text_variable, N)</kbd>, with <kbd>N</kbd> the size of the window ranging from 2 to 10.</p>
<p>It’s worth noting that some very standard text transformations are absent from Amazon ML recipes. Stemming and <strong>Lemmatization</strong> are used to regroup words with different endings to a common base form (for instance, <em>walking</em>, <em>walker</em> and <em>walked</em> would all be accounted for as <em>walk</em>) and are not offered in Amazon ML.</p>
<p>Similarly, removing very common words, known as <em>stopwords</em>, such as articles or prepositions (the, a, but, in, is, are, and so on) from a text is also a very standard text-mining transformation but is not an option in Amazon ML recipes. It is nonetheless possible that Amazon ML carries out similar transformations in the background without explicitly stating so. However, nothing in the available documentation indicates that to be the case.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Coupling variables</h1>
            </header>

            <article>
                
<p>The Cartesian product transformation combines two categorical or text variables into one. Consider, for instance, a dataset of books and for each book, their title and genre. We could imagine that the title of a book has some correlation with its genre, and creating a new <kbd>title_genre</kbd> variable would bring forth that relation.</p>
<p>Consider the following four books, their titles, and genres. Coupling the words in the title with the genre of the book adds extra information to the words in the title. Information that the model could use effectively. This is illustrated in the <kbd>title_genre</kbd> column in the following table:</p>
<table>
<tbody>
<tr>
<td><strong>Title</strong></td>
<td><strong>Genre</strong></td>
<td><strong>title_genre</strong></td>
</tr>
<tr>
<td><em>All the Birds in the Sky</em></td>
<td><span>scifi</span></td>
<td><span>{<kbd>all_scifi</kbd>, <kbd>birds_scifi</kbd>, <kbd>sky_scifi</kbd>}</span></td>
</tr>
<tr>
<td><em>Robots and Empire</em></td>
<td><span>scifi</span></td>
<td><span><kbd>{robots_scifi</kbd></span>, <span><kbd>emprire_scifi</kbd>}</span></td>
</tr>
<tr>
<td><em>The Real Cool Killers</em></td>
<td><span>crime</span></td>
<td><span>{<kbd>real_crime</kbd>, <kbd>cool_crime</kbd>, <kbd>killers_crime</kbd>}</span></td>
</tr>
<tr>
<td><em>Bullet in the Sky</em></td>
<td><span>crime</span></td>
<td><span>{<kbd>bullet_crime</kbd>, <kbd>sky_crime</kbd>}</span></td>
</tr>
</tbody>
</table>
<p>The word sky now takes a different meaning if it's in the title of a crime novel: <kbd>sky_crime</kbd> or in the title of a SciFi novel: <kbd>sky_scifi</kbd>.</p>
<p>In the case of the <kbd>Titanic</kbd> dataset, we could couple the <kbd>sibsp</kbd> and <kbd><em>parch</em></kbd> variables (number of siblings and number of parents) by taking their cartesian products: <kbd>sibsp*parch</kbd> and come up with a new variable that distinguishes between passengers with (without) parents and few or no siblings from those with (without) parents and many siblings. Syntax is <kbd>cartesian(var1, var2)</kbd>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Binning numeric values</h1>
            </header>

            <article>
                
<p>The final and most important transformation is quantile binning. The goal with quantile binning is to transform a numeric variable into a categorical one in order to better extract the relation between the variable and the prediction target. This is particularly useful in the presence of nonlinearities between a variable and the target. By splitting the original numeric variables values into <em>n</em> bins of equal size, it is possible to substitute each value by a corresponding bin. Since the number of bins is finite (from 2 to 1,000), the variable is now categorical. Syntax is <kbd>quantile_bin(var, N)</kbd> with <kbd>N</kbd> the number of bins.</p>
<p>There are two types of unsupervised binning, equal frequency and equal width binning. In equal frequency, each bin has the same number of samples, whereas in equal width binning, the variable range is split into N smaller ranges of equal width. Quantile binning usually refers to equal frequency binning.</p>
<p>Categorizing continuous data is not always a good approach as you are, by definition, throwing away information that could be useful for the model. This page lists several other problems associated with binning: <a href="http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous" target="_blank">http://biostat.mc.vanderbilt.edu/wiki/Main/CatContinuous</a>. However, Amazon ML seems to be quite fond of the quantile binning technique. In fact, of all the datasets we considered, Amazon ML always applied quantile binning to all the numeric variables, in the suggested recipe and often used large, sometimes very large, number of bins. For instance, the default transformation for the <kbd>fare</kbd> variable in the <kbd>Titanic</kbd> dataset was quantile binning with 500 bins although the variable only ranged from 0 to 512. We compare the evaluations obtained by keeping the original numeric values versus applying quantile binning at the end of this chapter, <em>Keeping variables as numeric or applying quantile binning?</em> section Now that we’ve explored the available recipes, let’s look at how Amazon ML suggests we transform our <kbd>Titanic</kbd> dataset</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a model</h1>
            </header>

            <article>
                
<p>Amazon ML always suggests a recipe based on your datasource when you create a model. You can choose to use that recipe or to modify it. We will now create our first model and during that process analyze the recipe Amazon ML has generated for us.</p>
<p>Go to the model dashboard, and click on the <span class="packt_screen">Create new...</span> | <span class="packt_screen">ML model</span> button.</p>
<p>You will go through three screens:</p>
<ol>
<li>Select the datasource, choose the <kbd>Titanic train set with 11 variables</kbd>.</li>
<li>Amazon ML will validate the datasource and present a summary.</li>
<li>Choose the default or <span class="packt_screen">Custom</span> model creation; choose the custom path:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="148" src="assets/B05028_05_02.png" width="233"/></div>
<p>The next screen is split between the attributes, their type and a sample of values on the left side, and the suggested recipe on the right side, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class=" image-border" height="471" src="assets/B05028_05_03.png" width="577"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Editing the suggested recipe</h1>
            </header>

            <article>
                
<p>This is where you can edit the recipe and replace it with a recipe of your own creation.</p>
<div class="packt_tip">You can find all the JSON in this chapter in the book's GitHub repository, properly formatted and indented at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json" target="_blank">https://github.com/alexperrier/packt-aml/blob/master/ch5/recipes.json</a>.</div>
<p>The recipe is validated while you type. Anything not respecting the JSON format will result in the following error message: <kbd>Recipe must be valid JSON</kbd>. Some common errors include the following:</p>
<ul>
<li>Indentation is not respected</li>
<li>The last element between braces should not be followed by a comma</li>
<li>All strings must be between double quotes</li>
</ul>
<div class="packt_tip">Manually formatting JSON text is not fun. This online JSON editor is very helpful: <a href="http://www.cleancss.com/json-editor/" target="_blank">http://www.cleancss.com/json-editor/</a>. </div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Applying recipes to the Titanic dataset</h1>
            </header>

            <article>
                
<p>The recipe generated by Amazon ML for our dataset is as follows:</p>
<pre>
{<br/>    "groups": {<br/>        "NUMERIC_VARS_QB_50": "group('sibsp','parch')",<br/>        "NUMERIC_VARS_QB_100": "group('age')",<br/>        "NUMERIC_VARS_QB_10": "group('fare')"<br/>    },<br/>    "assignments": {},<br/>    "outputs": [<br/>        "ALL_BINARY",<br/>        "ALL_CATEGORICAL",<br/>        "quantile_bin(NUMERIC_VARS_QB_50,50)",<br/>        "quantile_bin(NUMERIC_VARS_QB_100,100)",<br/>        "quantile_bin(NUMERIC_VARS_QB_10,10)",<br/>        "ALL_TEXT"<br/>    ]<br/>}
</pre>
<p>All numeric values are quantile binned. No further processing is done on the text, binary, or categorical variables. The output section of the recipe shows that the numeric variables are replaced by the binned equivalent.</p>
<p>Further comments can be made on this recipe:</p>
<ul>
<li>The <kbd>sibsp</kbd> and <kbd>parch</kbd> variables are grouped together. First of all, both <kbd>sibsp</kbd> and <kbd>parch</kbd> have similar ranges, 0 to 9 and 0 to 8 respectively. It makes sense to have the same number of bins for both variables.</li>
<li>Why Amazon ML chose 50 bins for <kbd>sibsp</kbd> and <kbd>parch</kbd>, 100 bins for <kbd>age</kbd>, and 10 bins for <kbd>fare</kbd> is less clear.</li>
</ul>
<p>We found that the number of bins was very sensitive to the data in the training set. Several versions of the initial datasets produced very different binning numbers. One constant in all our trials was that all the numeric values went through quantile binning with a rather high number of bins. In one instance, Amazon ML suggested 500 bins for the <kbd>fare</kbd> variable and 200 for the <kbd>age</kbd> variable. In both cases, we would have ended with a very small number of samples per bin since our total number of training sample consists of just 1,047 passengers. How Amazon ML calculates the optimal number of bins is not clear.</p>
<p>There are other transformations Amazon ML could decide to apply to our <kbd>Titanic</kbd> dataset such as the following:</p>
<ul>
<li>Extracting bigrams or OSBs from the passengers' titles</li>
<li>Coupling <kbd>sibsp</kbd> and <kbd>parch</kbd> with cartesian product transformation</li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing between recipes and data pre-processing.</h1>
            </header>

            <article>
                
<p>So far we have transformed our initial dataset via scripts and Amazon ML recipes. The two techniques are complementary. Some transformation and data manipulation can only be done by preprocessing the data. We did so in <a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 4</span>,</a> <em>Loading and Preparing the Dataset</em> with Athena and SQL. We could have achieved similar data processing with other scripting languages such as Python or R, which are most fruitful for creative feature engineering. SQL and scripts can also better deal with outliers and missing values — corrections that are not available with Amazon ML recipes.</p>
<p>The goal of the Amazon ML transformations is to prepare the data for consumption by the Amazon ML algorithm, whereas scripted feature engineering is about cleaning up the data and creating new variables out of the original dataset. </p>
<p>Although Amazon ML recipes are quite restrained, they offer an easy way to fiddle around with the dataset and quickly compare models based on different recipes. Creating a new model and associated evaluation from a given datasource and schema only takes a few clicks. And by choosing to write different recipes for each model, it becomes possible to experiment with a wide range of datasets. Recipes allow us to create a fast try-fail loop. The associated workflow becomes the following:</p>
<ol>
<li>Specify the datasource.</li>
<li>Experiment with different recipes.</li>
<li>Create or remove variables.</li>
<li>Train and select the model associated to that recipe.</li>
<li>Once the best recipe is found, then start optimizing the model parameters, regularization, passes, and memory.</li>
</ol>
<p>We can compare how we transformed the data with scripting (Athena and SQL) and with recipes:</p>
<p><strong>Recipes:</strong></p>
<ul>
<li>Removing features (<kbd>boat</kbd>, <kbd>body</kbd>, and <kbd>home.dest</kbd>). This can also be done via the schema or directly by removing the columns from the dataset CSV file.</li>
<li>Cartesian product for an indication of family by aggregating <kbd>parch</kbd> and <kbd>sibsp</kbd>.</li>
<li>Normalization of numeric values (a possibility).</li>
<li>Tokenization of all text variables, names, destinations, and so on.</li>
<li>Quantile binning of all numeric values; although the number of bins were large this transformation produced good results.</li>
</ul>
<p><strong>Scripting (SQL):</strong></p>
<ul>
<li>Handling missing values for <kbd>age</kbd>: We replaced all missing values by the mean of the <em>age</em></li>
<li>Text processing: We extracted <kbd>titles</kbd> from the <kbd>name</kbd> variables</li>
<li>Created a new feature, the <kbd>family_size</kbd> as the sum of <kbd>parch</kbd> and <kbd>sibsp</kbd></li>
<li>Extraction of the <kbd>deck</kbd> from the cabin number</li>
</ul>
<p>Both approaches are very complementary.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Parametrizing the model</h1>
            </header>

            <article>
                
<p>Now that our data has been prepared for the SGD algorithm, we are ready to set the parameters of our experiment. In a way similar to scientific experimentation, we will want to try out several sets of parameters to test several models and pick up the best one. The next screenshot shows where we actually specify our model parameters:</p>
<ul>
<li>Model memory</li>
<li>Data passes</li>
<li>Shuffling</li>
<li>Regularization</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="395" src="assets/B05028_05_04.png" width="542"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting model memory</h1>
            </header>

            <article>
                
<p>Model memory is related to the memory Amazon ML must set aside to build and evaluate your model. It is set, by default, to 100Mb. In the case of the <kbd>Titanic</kbd> dataset, the model memory was always below 1Mb as shown by the logs. Model memory is also used to set aside memory when dealing with streaming data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting the number of data passes</h1>
            </header>

            <article>
                
<p>Amazon ML will use the training set of samples several times, each time shuffling it and using the new sequence to increase prediction. It's similar to squeezing a wet piece of cloth — each time you wring it, more water comes out of it. Set by default to 10 passes, it does not hurt to set it to the maximum value of a 100 at the expense of a longer training time for the model and a higher cost of operation. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Choosing regularization</h1>
            </header>

            <article>
                
<p>As seen in <a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span>,</a> <em>Machine Learning Definitions and Concepts</em>, regularization makes your model more robust and allows it to better handle previously unseen data by reducing overfitting. The rule of thumb is to lower regularization if your evaluation score is poor (underfitting) and increase it if your model shows great performance on the training set but poor results on the evaluation set (overfitting). </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating an evaluation</h1>
            </header>

            <article>
                
<p>Evaluations and models are independent in Amazon ML. You can train a model and carry out several evaluations by specifying different evaluation datasets. The evaluation page, shown in the following screenshot, lets you name and specify how the model will be evaluated:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class=" image-border" height="397" src="assets/B05028_05_05.png" width="724"/></div>
<p>As you know by now, to evaluate a model, you need to split your dataset into two parts, the training and the evaluation sets with a 70/30 split. The training part is used to train your model, while the evaluation part is used to evaluate the model. At this point, you can let Amazon ML split the dataset into training and evaluation or specify a different datasource for evaluation.</p>
<p>Recall that the initial <kbd>Titanic</kbd> file was ordered by class and passenger alphabetical order. Using this ordered dataset and splitting it without shuffling, that is, taking sequentially the first 70% samples, would give the model a very different data for the training and the evaluation sets. The evaluation would not be relevant. However, if your data is not already shuffled, you can tell Amazon ML to shuffle it. It is a good practice to let Amazon ML reshuffle your data by default just in case your own randomizing left some sequential patterns in the dataset.</p>
<p>Amazon ML will make some verifications regarding your training and validation sets, checking that there is enough data for the validation, that the two sets follow similar distributions, and that the evaluation set has valid samples. Take a look at <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html" target="_blank">http://docs.aws.amazon.com/machine-learning/latest/dg/evaluation-alerts.html</a> for more information on Evaluation Alerts.</p>
<p>Note that if you choose to let Amazon ML split the data, it will create two new datasources titled in a way that lets you see how the split was performed. You can reuse these new datasources if you decide to test another model with different recipes or model parameters such as regularization. </p>
<p>For instance:</p>
<ul>
<li><kbd>Titanic.csv_[percentBegin=0, percentEnd=70, strategy=sequential]</kbd></li>
<li><kbd>Titanic.csv_[percentBegin=70, percentEnd=100, strategy=random]</kbd></li>
</ul>
<p>Click on <span class="packt_screen">Review</span>, make sure your model is as expected, and click on the final <em><span class="packt_screen">Create ML model</span></em> button. Creating the model usually takes a few minutes.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating the model</h1>
            </header>

            <article>
                
<p>At this point, Amazon ML will use the training set to train several models and the evaluation sets to select the best one. </p>
<p>Amazon ML runs several model training in parallel, each time trying new parameters and shuffling the training set at each new pass. Once the number of passes initially set has been exhausted or the algorithm has converged, whichever comes first, the model is considered trained. For each model it trains, Amazon ML uses it for prediction on the validation subset to obtain an evaluation score per model. Once all the models have been trained and evaluated this way, Amazon ML simply selects the one with the best evaluation score.</p>
<p>The evaluation metric depends on the type of prediction at hand. AUC and <kbd>F1</kbd> score for classification (binary and multiclass), and RMSE for regression. How the evaluation results are displayed by Amazon ML also depends on the type of prediction at hand.</p>
<p>We’ll start with evaluation for binary classification for our Titanic prediction, followed by the regression case with a new dataset related to Air traffic delays, and finally perform multiclass classification with the classic <kbd>Iris</kbd> dataset.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating binary classification</h1>
            </header>

            <article>
                
<p>Once your model is ready, click on the model's title from the service dashboard to access the model's result page, which contains the summary of the model, its settings and the evaluation results.</p>
<p>The following screenshot shows that we obtained an <em>AUC</em> score of <kbd>0.880</kbd>, which is considered very good for most machine-learning applications. <strong>AUC</strong> stands for the <strong>Area under the Curve</strong> and was introduced in <span class="ChapterrefPACKT"><a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank">Chapter 2</a>, <em>Machine Learning Definitions and Concepts</em></span>. It is the de-facto metric for classification problems.</p>
<p>The baseline for Binary classification is an AUC of 0.5, which is the score for a model that would randomly predict 0 or 1:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="399" src="assets/B05028_05_06.png" width="607"/></div>
<p>Amazon ML validates the model by checking the following conditions and raising alerts in case the conditions are not met:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="321" src="assets/B05028_05_07.png" width="482"/></div>
<p>In our case, no alerts were raised:</p>
<ul>
<li>The training and validation datasets were separate</li>
<li>The validation dataset had a sufficient number of samples</li>
<li>The validation and training sets shared the same schema</li>
<li>All samples of the validation set were valid and used for the evaluation, implying that the target was not missing for one or more samples</li>
<li>The distribution of the target variable was similar in the training and validation sets</li>
</ul>
<p>Most of these alerts will not happen if we let Amazon ML handle the training validation data split, but they might be more frequent if we provide the validation set ourselves.</p>
<p>The AUC score is not the only element Amazon ML gives us to evaluate the quality of our model. By clicking on the <em><span class="packt_screen">Explore performance</span></em> link, we can analyze further the performance of our model.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Exploring the model performances</h1>
            </header>

            <article>
                
<p>You may recall from <span class="ChapterrefPACKT"><a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank">Chapter 2</a>: <em>Machine Learning Definitions and Concepts,</em></span> that in a binary classification context, a logistic regression model calculates for each sample to be predicted a probability — the probability of belonging to one class or the other. The model will not directly output the class of the sample to be predicted. The sample is assigned to one class or the other depending on whether the probability is below or above a certain threshold. By default, this threshold is set to 0.5. Although the AUC score given by the evaluation does not depend on the value of the decision threshold, other classification metrics do. We can change the value of the threshold and see how that impacts our predictions.</p>
<p>The <strong>Explore performance</strong> page of the evaluation shows several other classification metrics as well as the confusion matrix of the model. The vertical bar in the graph below is a cursor that can slide left or right. By sliding the cursor, we increase or decrease the decision threshold used to classify a prediction sample as belonging to one class or another. As we move that cursor, the following metrics vary accordingly.</p>
<ul>
<li><span class="packt_screen">False positive rate</span></li>
<li><span class="packt_screen">Precision</span>: proportion of predicted positives that are truly positives</li>
<li><span class="packt_screen">Recall</span> (the proportion of positives that are correctly identified)</li>
<li><span class="packt_screen">Accuracy</span></li>
</ul>
<p>For a threshold of <em>0.5</em>, we have the following sceenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="289" src="assets/B05028_05_08.png" width="586"/></div>
<p>If we lower the threshold to <em>0.4</em>, accuracy decreases while recall increases, as you can see in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="285" src="assets/B05028_05_09.png" width="571"/></div>
<p>And if we raise the threshold to <em>0.7</em>, accuracy increases slightly while recall decreases:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="302" src="assets/B05028_05_10.png" width="595"/></div>
<p>In our context, the predictions are quite clearly separated between survived and did not survive values. Slightly changing the threshold does not have a huge impact on the metrics.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating linear regression</h1>
            </header>

            <article>
                
<p>Amazon ML uses the standard metric RMSE for linear regression. RMSE is defined as the sum of the squares of the difference between the real values and the predicted values:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="66" src="assets/image_03_015.png" width="214"/></div>
<p>Where <em>ŷ</em> are the predicted values and <em>y</em> the real values. The closer the predictions are to the real values, the lower the RMSE is; therefore, a lower RMSE is interpreted as a better predictive model.</p>
<p>To demonstrate the evaluation in the regression context, we will consider a simplified version of the <strong>Airlines delay</strong> dataset available on Kaggle at <a href="https://www.kaggle.com/giovamata/airlinedelaycauses" target="_blank">https://www.kaggle.com/giovamata/airlinedelaycauses</a>. The full dataset is quite large (<em>~250Mb</em>). We extracted roughly <em>19,000</em> rows from the year 2008, filtering out cancelled flights. We also removed several variables that were too correlated with our target, which is the <kbd>Airdelay</kbd> variable. The resulting dataset and schema are available on GitHub at <a href="https://github.com/alexperrier/packt-aml/tree/master/ch5" target="_blank">https://github.com/alexperrier/packt-aml/tree/master/ch5</a>.  </p>
<p>We upload the dataset to S3, create a datasource, train and evaluate a model and finally obtain an RMSE of <span class="packt_screen">7.0557</span> with a baseline of <span class="packt_screen">31.312</span>. The baseline for regression is given by a model that always predicts the average of the target:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="76" src="assets/B05028_05_14.png" width="596"/></div>
<p>Exploring further, we obtain the following histograms of residuals. As we can see in the next screenshot, the errors are roughly bell-shaped and centered around <em>0</em>, meaning that our errors are half the time overestimating/underestimating the real values. All the information available in the dataset has been consumed by the model:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="463" src="assets/B05028_05_15.png" width="605"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Evaluating multiclass classification</h1>
            </header>

            <article>
                
<p>The classic dataset for multiclass classification is the <kbd>Iris</kbd> dataset composed of three types of Iris flowers. This dataset is quite simple, very popular and using it to illustrate the performance of a platform as powerful as Amazon ML seems overkill. Luckily, there are another three class datasets composed of seeds. The seeds dataset is available at <a href="https://archive.ics.uci.edu/ml/datasets/seeds">https://archive.ics.uci.edu/ml/datasets/seeds</a> and of course on the GitHub repository accompanying this book (as well as the schema).</p>
<p>The seed dataset has 210 samples distributed evenly among three different <kbd>seedTypes</kbd> and seven attributes. The dataset has an ID, which must be set to categorical, all attributes are NUMERIC, and the target is the <kbd>seedType.</kbd> We upload the dataset to S3, and create a datasource and a model.</p>
<p>The metric for multiclass classification is the <em>F1</em> score defined as the harmonic mean of precision and recall:</p>
<div class="CDPAlignCenter CDPAlign"><img height="72" src="assets/B05028_05_13.png" width="411"/></div>
<p>The baseline for a multiclass classification problem is the macro average <em>F1</em> score for a model that would always predict the most common class. In the case of the seed dataset, we obtain a <em>F1</em> score of <span class="packt_screen">0.870</span> for baseline of <span class="packt_screen">0.143</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="183" src="assets/B05028_05_11.png" width="643"/></div>
<p>Performance exploration is not as developed as in the binary classification case. Amazon ML gives us the confusion matrix which shows, for each class, the ratio of correctly predicted samples over the real number of samples in that class:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="341" src="assets/B05028_05_12.png" width="394"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Analyzing the logs</h1>
            </header>

            <article>
                
<p>For every operation it carries out, Amazon ML gives us access to the related logs. We can download and analyze the model training logs and infer a few things on how Amazon ML trains and selects the best model.</p>
<p>Go back to the last Titanic model, and in the summary part, click on the <span class="packt_screen">Download Log</span> link. The log file is too long to be reproduced here but is available at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log" target="_blank">https://github.com/alexperrier/packt-aml/blob/master/ch5/titanic_training.log</a>:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="218" src="assets/B05028_05_16.png" width="419"/></div>
<p>Amazon ML launches five versions of the SGD algorithm in parallel. Each version is called a learner and corresponds to a different value for the learning rate: 0.01, 0.1,1, 10, and 100. The following five metrics are calculated at each new pass of the algorithm:</p>
<ul>
<li>Accuracy</li>
<li>Recall</li>
<li>Precision</li>
<li>F1-score</li>
<li>AUC</li>
</ul>
<p>The <kbd>negative-log-likelihood</kbd> is also calculated to assess whether the last iterations have brought significant improvement in reducing the residual error.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Optimizing the learning rate</h1>
            </header>

            <article>
                
<p>If you recall from <span class="ChapterrefPACKT"><a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank">Chapter 2</a>, <em>Machine Learning Definitions and Concepts</em></span>, under the section <em>Regularization on linear models</em>, the <strong>Stochastic Gradient Descent (SGD)</strong> algorithm has a parameter called the learning rate.</p>
<p>The SGD is based on the idea of taking each new (block of) data sample to make little corrections to the linear regression model coefficients. At each iteration, the input data samples are used either on a sample-by-sample basis or on a block-by-block basis to estimate the best correction (the so-called gradient) to make to the linear regression coefficients to further reduce the estimation error. It has been shown that the SGD algorithm converges to an optimal solution for the linear regression weights. These corrections are multiplied by a parameter called the <kbd>learning rate</kbd> , which drives the amount of correction brought to the coefficients at each iteration.</p>
<p>SGD calculations are low in computation costs. It's a fascinating yet simple algorithm that is used in many applications.</p>
<p>Imagine a marble in a bowl. Set the marble on the rim of the bowl and let it drop into the bowl with a circular movement. It will circle around the bowl while falling to the bottom. At the end of its descent, it will tend to circle around the bottom of the bowl and finally come to rest at the lowest point of the bowl. The SGD behaves similarly when you consider the marble as the prediction error at each iteration and the bottom of the bowl as the ultimate and most optimal coefficients that could be estimated. At each iteration, the prediction error becomes smaller on average. The error will not follow the most direct path to the bottom of the bowl like the marble does, nor will it reach the lowest most optimal solution, but on average, the predictions get better and the error decreases iteration after iteration. After a certain number of iterations, the error will approach its potential optimal minimum. How fast and how close it gets to the minimum error and the best coefficients depends directly on the value of the learning rate.</p>
<p>The learning rate controls how much the weights are corrected at each iteration. The learning rate drives the convergence of the algorithm. The larger the learning rate, the faster the convergence and potentially, the larger the residual error once converged.</p>
<p>Thus, choosing an optimal learning rate will be a balance between the following:</p>
<ul>
<li>A faster convergence and poorer estimation</li>
<li>A slower convergence and more accurate estimation</li>
</ul>
<p>However, if the learning rate is too small, the convergence can be too slow and take too long to reach an optimal solution. One standard strategy is to decrease the learning rate as the algorithm converges, thus ensuring a fast convergence at the beginning, which will slow down as the prediction error becomes slower. As the learning rate decreases, the coefficient estimation becomes more accurate. Small learning rates mean that the algorithm converges slowly, while higher values mean each new sample has a bigger impact on the correcting factor. Amazon ML does not use that strategy and keeps the learning rate constant. In Amazon ML, the learning rate is set for you. You cannot choose a value.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Visualizing convergence </h1>
            </header>

            <article>
                
<p>By parsing the logs, we can extract the following convergence plots for our Titanic model:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="580" src="assets/B05028_05_17.png" width="580"/></div>
<p>The previous screenshot of different plots shows four metrics: <span class="packt_screen">Accuracy</span>, <span class="packt_screen">AUC</span>, <span class="packt_screen">F1</span> <span class="packt_screen">score</span>, and <span class="packt_screen">Precision</span> for the five different values of the learning rate. The model was set to 50 passes with mild (10^-6) L2 regularization on the Titanic training dataset. We can see that, for all metrics, the best value for the learning rate is either 10 or 100, with a slight advantage for learning rate=100. These values converge faster and reach better scores. The smallest learning rate (0.01) converges far slower. In our context, faster convergence and large learning rate values beat smaller rate values.</p>
<p>The default number of passes when creating a model is <em>10</em>. We can see that 10 iterations would not have been sufficient for the score to stabilize and converge. At the 10th iteration, the curves are barely out of the chaotic initialization phase.</p>
<p>Looking at the negative log likelihood graph extracted from the logs, we also see that the best learner corresponds to a learning rate of 100 shown here by the curve with diamond shapes:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="411" src="assets/B05028_05_18.png" width="411"/></div>
<p>One conclusion that can be made from these graphs is that you should not limit your model to the default 10 passes.</p>
<p>These two convergence graphs are entirely dependent on the problem at hand. For a different dataset, we would have ended with entirely different graphs in terms of convergence rate, learning rate, and score achieved.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Impact of regularization</h1>
            </header>

            <article>
                
<p>The following graph compares AUC for three different models:</p>
<ul>
<li>No regularization</li>
<li>Mild regularization (10^-6)</li>
<li>Aggressive regularization (10^-2)</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="198" src="assets/B05028_05_19.png" width="593"/></div>
<p>We notice that there is no significant difference between having no regularization and having mild regularization. Aggressive regularization, however, has a direct impact on the model performance. The algorithm converges to a lower AUC, and the optimal learning rate is no longer 100 but 1.</p>
<p>Comparing the performance graph given by Amazon ML for mild and aggressive regularization, we see that although the scores (AUC, accuracy, and so on) are very similar in both cases, the difference lies with the certainty of the predictions. In the mild regularization case (left graph), the predictions are far apart. The probabilities or predictions that a sample is zero or one are very distinct. In the aggressive regularization case (right graph), this separation is far less obvious. The probabilities for samples to belong to one class versus the other are much closer. The decision boundary is less clear:</p>
<div class="packt_figref CDPAlignCenter CDPAlign">
<table style="width: 687px;height: 294px">
<tbody>
<tr>
<td><strong>Mild regularization</strong></td>
<td><strong>Aggressive regularization</strong></td>
</tr>
<tr>
<td><img class=" image-border" height="196" src="assets/B05028_05_20.png" width="305"/></td>
<td><img class=" image-border" height="203" src="assets/B05028_05_21.png" width="318"/></td>
</tr>
</tbody>
</table>
</div>
<p>The goal of regularization being to decouple the performance of the model from the training data in order to reduce overfitting, it may well be that, on the held-out dataset on previously unseen data, heavy regularization would give better results and no regularization would perform worse than mild regularization. Less optimal performance in the training-validation phase is sometimes more robust during the real prediction phase. It’s important to keep in mind that performance in the validation phase does not always translate into performance in the prediction phase.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Comparing different recipes on the Titanic dataset</h1>
            </header>

            <article>
                
<p>In this last section, we would like to compare several recipes and see if our SQL, based feature engineering drives a better model performance. In all our experimentation, the one thing that stood out with regards to the recipes Amazon ML suggested was that all the numeric variables ended up being categorized via quantile binning. The large number of bins was also in question. We compare the following scenarios on the <kbd>Titanic</kbd> dataset:</p>
<ul>
<li>Suggested Amazon ML recipe</li>
<li>Numeric values are kept as numeric. No quantile binning is involved in the recipe.</li>
<li>The extended Titanic datasource we created in <span class="ChapterrefPACKT"><a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank">Chapter 4</a>,</span> <em>Loading and Preparing the Dataset</em> is used with the suggested Amazon ML recipe</li>
</ul>
<p>We slightly modified the extended Titanic dataset that was used in <span class="ChapterrefPACKT"><a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank">Chapter 4</a>,</span><span> </span><em>Loading and Preparing the Dataset:</em></p>
<ul>
<li>There was no need to have both <kbd>fare</kbd> and <kbd>log_fare</kbd>. We removed <kbd>fare</kbd>.</li>
<li>We manually corrected some titles that were not properly extracted from the names.</li>
<li>The new extended dataset is available in the GitHub repository for his chapter as <kbd>ch5_extended_titanic_training.csv</kbd>.</li>
</ul>
<p>In all three cases, we apply L2 mild regularization.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Keeping variables as numeric or applying quantile binning?</h1>
            </header>

            <article>
                
<p>We found that keeping all numeric variables as numeric and avoiding any quantile binning had a very direct and negative effect on the model performance. The overall score was far lower in the numeric case than in the quantile binning case: <kbd>AUC: 0.81</kbd> for all numeric versus  <kbd>AUC: 0.88</kbd> for QB.</p>
<p>Looking at the convergence graph for the <em>All Numeric</em> model, it appears that the algorithm converged much more slowly than it had for the quantile binning model. It obviously had not converged after 50 passes, so we increased the number of passes to 100. We also noticed that in the <span><em>All Numeric</em> case</span>, the best learning rate was equal to 0.01, whereas in the quantile binning model, the best learning rate was much larger (10 or 100). A smaller learning rate induces a slower convergence rate:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="282" src="assets/B05028_05_22.png" width="563"/></div>
<p>We also see on the following performance charts that the quantile binning model separates the classes much better than the All Numeric model:</p>
<table>
<tbody>
<tr>
<td><strong>Quantile Binning 50 passes</strong></td>
<td><strong>All Numeric 100 Passes</strong></td>
</tr>
<tr>
<td><img class=" image-border" height="185" src="assets/B05028_05_23.png" width="302"/></td>
<td><img class=" image-border" height="191" src="assets/B05028_05_24.png" width="315"/></td>
</tr>
</tbody>
</table>
<p>So quantile binning is definitely preferable to no quantile binning. What about our efforts to extend the initial dataset with new features? Well, somehow, our extended model did not produce better results than the initial dataset. Extracting the <kbd>title</kbd> from the <kbd>name</kbd>, replacing missing values for the <kbd>age</kbd>, and extracting the <kbd>deck</kbd> from the <kbd>cabin</kbd> did not generate an obviously better model:</p>
<ul>
<li>Original Titanic dataset: AUC 0.88</li>
<li>Extended Titanic dataset with feature engineering: AUC 0.82</li>
</ul>
<p>Convergence and performance charts were similar for both models and are not reproduced here. Several factors can be at play here to explain why our improved dataset did not produce a better model, and further analysis would be required to understand which feature engineering had a positive impact on the model and which one did not. However, we will see in the next chapter that this may also have been dependent on the actual samples in the evaluation set. On average, the extended dataset generates better performances but for this particular trial, the associated model performed roughly the same as the one trained on the original dataset. The conclusion being that it is worth the effort to run several trials to assess the quality and performance of a model, and not rely on a unique trial where the particularities of the evaluation set may influence the comparison between models. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Parsing the model logs</h1>
            </header>

            <article>
                
<p>The convergence plots were obtained by parsing the Amazon ML model logs to extract the data into a CSV file that could be used later on to create plots. The process is simple and mostly based on command line scripting using the <kbd>grep</kbd> and the <kbd>sed</kbd> commands. We want to extract and parse the following lines from the log file:</p>
<pre>
16/12/25 13:54:03 INFO: learner-id=4202 model-performance:         accuracy=0.6562 recall=0.5000 precision=0.5909 f1-score=0.5417 auc=0.7095
</pre>
<p>And convert them into a CSV format as follows:</p>
<div>
<table>
<tbody>
<tr>
<td>iteration</td>
<td>alpha</td>
<td>learner</td>
<td>accuracy</td>
<td>recall</td>
<td>precision</td>
<td>f1</td>
<td>auc</td>
</tr>
<tr>
<td>1</td>
<td>0.01</td>
<td>1050</td>
<td>0.5937</td>
<td>0.56</td>
<td>0.4828</td>
<td>0.5185</td>
<td>0.6015</td>
</tr>
</tbody>
</table>
</div>
<p>The first step is to extract the right lines from the log file. We notice that they all contain the string <kbd>model-performance:</kbd>. We use grep to extract all the lines containing this string into a temporary file that we name <kbd>model_performance.tmp</kbd>.</p>
<p>Copy-paste the log data from the Amazon ML Model page into a log file (<kbd>model.log</kbd>) and in the terminal run the following:</p>
<pre>
<strong>grep "model-performance:" model.log &gt;&gt; model_performance.tmp</strong>
</pre>
<p>The trick then is to replace the right sub-strings by commas using the <kbd>sed</kbd> command. The <kbd>sed</kbd> command follows this syntax:</p>
<pre>
<strong>sed -i.bak 's/STRING_TO_REPLACE/STRING_TO_REPLACE_IT/g' filename</strong>
</pre>
<p>The <kbd>-i.bak</kbd> option makes it possible to replace the string within the file itself without the need to create a temporary file.</p>
<p>So, for instance, replacing the string <kbd>INFO: learner-id=</kbd> by a comma in the <kbd>model_performance.tmp</kbd> file is obtained by running the following line in a terminal:</p>
<pre>
sed -i.bak 's/ INFO: learner-id=/,/g' model_performance.tmp
</pre>
<p>With the following commands, most of the original log file will have been transformed into a CSV formatted file, which you can use as a base for visualizing the convergence of the Amazon ML model. The rest of the file cleaning can be done in a spreadsheet editor:</p>
<pre>
sed -i.bak 's/ INFO: learner-id=/,,/g' model_performance.tmp<br/>sed -i.bak 's/ model-performance:         accuracy=/,/g' model_performance.tmp<br/>sed -i.bak 's/ recall=/,/g' model_performance.tmp<br/>sed -i.bak 's/ precision=/,/g' model_performance.tmp<br/>sed -i.bak 's/ f1-score=/,/g' model_performance.tmp<br/>sed -i.bak 's/ auc=/,/g' model_performance.tmp
</pre>
<p>A similar pattern can be used to extract the negative log likelihood data from the Amazon ML model logs:</p>
<pre>
sed -i.bak 's/ INFO: learner-id=/,,/g' filename<br/>sed -i.bak 's/ model-convergence:         negative-log-likelihood=/,/g' filename<br/>sed -i.bak 's/ (delta=1.000000e+00) is-converged=no//g' filename
</pre>
<p>We end up with a CSV file with a row for each iteration and a column for the learning rate and each metric.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we created predictive models in Amazon ML--from selecting the datasource, applying transformations to the initial data with recipes, and analyzing the performance of the trained model. The model performance exploration depends on the type of prediction problem at hand: binary, multi-classification, or regression. We also looked at the model logs for the Titanic dataset and learned how the SGD algorithm trains and selects the best model out of several different ones with different learning rates.</p>
<p>Finally, we compared several data transformation strategies and their impact on the model performance and algorithm convergence in the context of the Titanic dataset. We found out that quantile binning of numeric values is a key strategy in boosting the convergence speed of the algorithm, which overall generated much better models.</p>
<p>So far, these models and performance evaluation are all obtained on training data. That is data that is fully available to the model from the start. The raison <span>d'être</span> of these models is not to run on subsets of the training data, but to make robust predictions on previously unseen data.</p>
<p>In the next chapter, we will apply these models on the held-out datasets we created in <a href="08d9b49a-a25c-4706-8846-36be9538b087.xhtml" target="_blank">Chapter 4</a>, <em>Loading and preparing the dataset,</em> to make real predictions.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>