<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-73"><em class="italic"><a id="_idTextAnchor076"/>Chapter 4</em>: LIME for Model Interpretability</h1>
			<p>In the previous chapters, we discussed the various technical concepts of <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) that are needed to build trustworthy AI systems. Additionally, we looked at certain practical examples and demonstrations using various Python frameworks to implement the concepts of practical problem solving, which are given in the GitHub code repository of this chapter. XAI has been an important research topic for quite some time, but it is only very recently that all organizations have started to adopt XAI as a part of the solution life cycle for solving business problems using AI. One such popular approach is <strong class="bold">Local Interpretable Model-Agnostic Explanations</strong> (<strong class="bold">LIME</strong>), which has been widely adopted to provide model-agnostic local explainability. The LIME Python library is a robust framework that provides human-friendly explanations to tabular, text, and image data and helps in interpreting black-box supervised machine learning algorithms.</p>
			<p>In this chapter, you will be introduced to the LIME framework, which has made a significant impact in the field of XAI. We will discuss the workings of the LIME algorithm for global and local model explainability. Also, I will demonstrate an example in which the LIME Python framework can be used in practice. I will cover the limitations of this framework that you should be aware of. </p>
			<p>So, in this chapter, we will discuss the following main topics:</p>
			<ul>
				<li>An intuitive understanding of LIME</li>
				<li>What makes LIME a good model explainer?</li>
				<li>Submodular pick (SP-LIME)</li>
				<li>A practical example of using LIME for classification problems</li>
				<li>Potential pitfalls</li>
			</ul>
			<p>Without further ado, let's get started.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor077"/>Technical requirements </h1>
			<p>This chapter is slightly more technical than the previous chapters covered in this book. The code and dataset resources can be downloaded or cloned from the GitHub repository for this chapter, which is located at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04</a>. Similar to the previous chapters, we will be using Python and Jupyter notebooks to run the code and generate the necessary outputs. Other important Python frameworks that are necessary to run the code will be mentioned in the notebooks with further relevant details to understand the code implementation of these concepts.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor078"/>Intuitive understanding of LIME</h1>
			<p>LIME <a id="_idIndexMarker314"/>is a novel, model-agnostic, local explanation technique used for interpreting black-box models by learning a local model around the predictions. LIME provides an intuitive global understanding of the model, which is helpful for non-expert users, too. The technique was first proposed in the research paper <em class="italic">"Why Should I Trust You?" Explaining the Predictions of Any Classifier</em> by <em class="italic">Ribeiro et al</em>. (https://arxiv.org/abs/1602.04938). The Python library can be installed from the GitHub repository at <a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a>. The algorithm does a pretty good job of interpreting any classifier or regressor in faithful ways by using approximated local interpretable models. It provides a global perspective to establish trust for any black-box model; therefore, it allows you to identify interpretable models over human-interpretable representation, which is locally faithful to the algorithm. So, it mainly functions by <em class="italic">learning interpretable data representations</em>, <em class="italic">maintaining a balance in a fidelity-interpretability trade-off</em>, and <em class="italic">searching for local explorations</em>. Let's look at each one of them in detail.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor079"/>Learning interpretable data representations</h2>
			<p>LIME does a <a id="_idIndexMarker315"/>pretty good job in differentiating between impactful features and choosing interpretable data representations that are understandable to any non-expert user regardless of the actual complex features used by the algorithm. For example, when explaining models trained on unstructured data such as images, the actual algorithm might use complex numerical feature vectors for its decision-making process, but these numerical feature values are incomprehensible to any non-technical end user. In comparison, if the explainability is provided in terms of the presence or absence of a region of interest or superpixel (that is, a continuous patch of pixels) within the image, that is a human-interpretable way of providing explainability. </p>
			<p>Similarly, for text data, instead of using word-embedding vector values to interpret models, a better way to provide a human-interpretable explanation is by using examples of the presence or absence of certain words used to describe the target outcome of the model. So, mathematically speaking, the original representation of a data instance being explained is denoted by <img src="image/B18216_04_001.png" alt=""/>, where <em class="italic">d</em> is the entire dimension of data. A binary vector of interpretable data representations is mathematically denoted by <img src="image/B18216_04_002.png" alt=""/>. Intuitively speaking, the algorithm tries to denote the presence or absence of human-interpretable data representations to explain any black-box model.</p>
			<p><em class="italic">Figure 4.1</em> shows how <a id="_idIndexMarker316"/>LIME tries to divide the input image data into human-interpretable components that are later used to explain black-box models in a manner that is understandable to any non-technical user:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B18216_04_001.jpg" alt="Figure 4.1 – How LIME transforms an image into human-interpretable components&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – How LIME transforms an image into human-interpretable components</p>
			<p> Next, let's discuss how to maintain the fidelity-interpretability trade-off.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor080"/>Maintaining a balance in the fidelity-interpretability trade-off</h2>
			<p>LIME <a id="_idIndexMarker317"/>makes use of inherently interpretable models such as decision trees, linear models, and rule-based heuristic models to provide explanations to non-expert users with visual or textual artifacts. Mathematically speaking, this explanation is a model that can be denoted by <img src="image/B18216_04_003.png" alt=""/>, where <img src="image/B18216_04_004.png" alt=""/> is the entire set of potentially interpretable models and the domain of <img src="image/B18216_04_005.png" alt=""/> is represented with another binary vector, <img src="image/B18216_04_006.png" alt=""/>, which represents the presence or absence of interpretable components. Additionally, the algorithm tries to measure the <em class="italic">complexity</em> of an explanation along with its <em class="italic">interpretability</em>. For example, even in interpretable models such as decision trees, the depth of the tree is a measure of its complexity. </p>
			<p>Mathematically speaking, the complexity of an interpretable model is denoted by <img src="image/B18216_04_007.png" alt=""/>. LIME tries to maintain <strong class="bold">local fidelity</strong> while <a id="_idIndexMarker318"/>providing explanations. This means that the algorithm tries to replicate the behavior of the model in proximity to the individual data instance being predicted. So, mathematically, the inventors of this algorithm used a function, <img src="image/B18216_04_008.png" alt=""/>, to measure the proximity between any data instances, <img src="image/B18216_04_009.png" alt=""/>, thus defining the locality around the original representation, <img src="image/B18216_04_010.png" alt=""/>. Now, if the probability function, <img src="image/B18216_04_011.png" alt=""/>, defines the probability that <img src="image/B18216_04_012.png" alt=""/> belongs to a certain class, then to approximate <img src="image/B18216_04_013.png" alt=""/>, the LIME algorithm tries to measure how unfaithful <img src="image/B18216_04_014.png" alt=""/> is with a proximity function, <img src="image/B18216_04_015.png" alt=""/>. This entire operation is denoted by the <img src="image/B18216_04_016.png" alt=""/> function. Therefore, the algorithm tries to minimize the locality-aware loss function, <img src="image/B18216_04_017.png" alt=""/>, while maintaining <img src="image/B18216_04_018.png" alt=""/> to be a low value. This is so that it is easily explainable to any non-expert user. The measure of an interpretability local fidelity trade-off is approximated by the following mathematical function:</p>
			<p class="figure-caption"> <img src="image/B18216_04_019.png" alt=""/></p>
			<p>Hence, this trade-off <a id="_idIndexMarker319"/>measure depends on the interpretable models, <img src="image/B18216_04_020.png" alt=""/>, the fidelity function, <img src="image/B18216_04_021.png" alt=""/>, and the complexity measure, <img src="image/B18216_04_022.png" alt=""/>.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor081"/>Searching for local explorations</h2>
			<p>The<a id="_idIndexMarker320"/> LIME algorithm is <em class="italic">model-agnostic</em>. This means when we try to minimize the <em class="italic">locality-aware loss function</em>, <img src="image/B18216_04_023.png" alt=""/>, without any assumption about <em class="italic">f</em>. Also, LIME maintains local fidelity by taking samples that are weighted by <img src="image/B18216_04_024.png" alt=""/> while approximating <img src="image/B18216_04_025.png" alt=""/>. Nonzero samples of <img src="image/B18216_04_026.png" alt=""/> are drawn uniformly at random to sample instances around <img src="image/B18216_04_0261.png" alt=""/>. Let's suppose there is a perturbed sample containing fractions of nonzero elements of <img src="image/B18216_04_027.png" alt=""/>, which is denoted by <img src="image/B18216_04_029.png" alt=""/>. The algorithm tries to recover samples from the original representation, <img src="image/B18216_04_030.png" alt=""/>, to approximate <img src="image/B18216_04_031.png" alt=""/>. Then, <img src="image/B18216_04_032.png" alt=""/> is used as a label for the explanation model, <img src="image/B18216_04_033.png" alt=""/>. </p>
			<p><em class="italic">Figure 4.2</em> represents an example presented in the original paper of the LIME framework at <a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a>, which intuitively explains the working of the algorithm using a visual representation:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18216_04_002.jpg" alt="Figure 4.2 – Explaining the working of the LIME algorithm intuitively &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Explaining the working of the LIME algorithm intuitively </p>
			<p>In <em class="italic">Figure 4.2</em>, the curve separating the light blue and pink backgrounds is considered a complex <img src="image/B18216_04_034.png" alt=""/> decision function of a black-box model. Since the decision function is not linear, approximating it using linear models is not efficient. The crosses and the dots represent training data belonging to two different classes. The bold cross represents the inference data instance being explained. The algorithm functions by sampling instances to get predictions using <em class="italic">f</em>. Then, the algorithm assigns weight by the proximity to the data instance being explained. In the preceding diagram, based on the proximity of the data instance, the sizes of the red crosses and blue dots are varied. So, the instances that are sampled are both in closer proximity to <em class="italic">x</em>, having a higher weight from <img src="image/B18216_04_035.png" alt=""/>, and far away from it, thus having a lower weight of <img src="image/B18216_04_036.png" alt=""/>. The original black-box model might be too complex to provide a global explanation, but the LIME framework can provide explanations that are appropriate for the local data instance, <img src="image/B18216_04_037.png" alt=""/>. The learned explanation is<a id="_idIndexMarker321"/> illustrated by the dashed line, which is locally faithful with a global perspective. </p>
			<p><em class="italic">Figure 4.3</em> illustrates a far more intuitive understanding of the LIME algorithm. From the original image, the algorithm generates a set of perturbed data instances:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18216_04_003.jpg" alt="Figure 4.3 – Predictions being explained using LIME &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Predictions being explained using LIME </p>
			<p>The perturbed instances, as shown in <em class="italic">Figure 4.3</em>, are created by switching some of the interpretable components off. In the case of images, as shown in the preceding diagram, it is done by turning certain components gray. Then, the black-box model is applied to each of the perturbed instances that are generated, and the probability of the instance being predicted as the final outcome of the model is calculated. Then, an interpretable model (such as a simple locally weighted linear model) is learned on the dataset, and finally, the<a id="_idIndexMarker322"/> superpixels having the maximum positive weights are considered for the final explanation. </p>
			<p>In the next section, let's discuss why LIME is a good model explainer.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor082"/>What makes LIME a good model explainer?</h1>
			<p>LIME enables non-expert users to understand the working of untrustworthy black-box models. The following properties of LIME make it a good model explainer:</p>
			<ul>
				<li><strong class="bold">Human interpretable</strong>: As <a id="_idIndexMarker323"/>discussed in the previous section, LIME provides explanations that are easy<a id="_idIndexMarker324"/> to understand, as it provides a qualitative way to compare the components of the input data with the model outcome.</li>
				<li><strong class="bold">Model-agnostic</strong>: In the <a id="_idIndexMarker325"/>previous chapters, although you have learned about various model-specific explanation methods, it is always an advantage if the explanation method can be used to provide explainability for any black-box model. LIME does not make any assumptions about the model while providing the explanations and can work with any model.</li>
				<li><strong class="bold">Local fidelity</strong>: LIME <a id="_idIndexMarker326"/>tries to replicate the behavior of the entire model by exploring the proximity of the data instance being predicted. So, it provides local explainability to the data instance being used for prediction. This is important for any non-technical user to understand the exact reason for the model's decision-making process.</li>
				<li><strong class="bold">Global intuition</strong>: Although the<a id="_idIndexMarker327"/> algorithm provides local explainability, it does try to explain a representative set to the end users, thereby providing a global perspective to the functioning of the model. SP-LIME provides a global understanding of the model by explaining a collection of data instances. This will be covered in more<a id="_idIndexMarker328"/> detail in the next section.</li>
			</ul>
			<p>Now that we understand the key advantages of the LIME framework, in the next section, let's discuss the submodular pick algorithm of LIME, which is used for extracting global explainability.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor083"/>SP-LIME</h1>
			<p>In order<a id="_idIndexMarker329"/> to make explanation methods more trustworthy, providing an explanation to a single data instance (that is, a local explanation) is not always sufficient, and the end user might want a global understanding of the model to have higher reliability on the robustness of the model. So, the SP-LIME algorithm tries to run the explanations on multiple diverse, yet carefully selected, sets of instances and returns non-redundant explanations. </p>
			<p>Now, let me provide an intuitive understanding of the SP-LIME algorithm. The algorithm considers that the time required to go through all the individual local explanations is limited and is a constraint. So, the number of explanations that the end users are willing to examine to explain a model is the budget of the algorithm denoted by <em class="italic">B</em>. Let's suppose that <em class="italic">X</em> denotes the set of instances; the task of selecting <em class="italic">B</em> instances for the end user to analyze for model explainability is defined as the <strong class="bold">pick step</strong>. The <a id="_idIndexMarker330"/>pick step is independent of the existence of the explanation and it needs to provide <em class="italic">non-redundant explanations</em> by picking up a diverse representative set of instances to explain how the model is behaving considering a global perspective. Therefore, the algorithm tries to avoid picking up instances with similar explanations. </p>
			<p>Mathematically, this idea is represented using the <em class="italic">Explanation Matrix</em> (<em class="italic">W</em>), in which <em class="italic">W</em> <em class="italic">= n * d'</em>, such that <em class="italic">n</em> is the number of samples and <em class="italic">d'</em> is the human interpretable features. The algorithm also uses a <em class="italic">Global importance component matrix</em> (<em class="italic">I</em>), in which for each component of <em class="italic">j</em>, <em class="italic">I(j)</em> represent the global importance in the explanation space. Intuitively speaking, <em class="italic">I</em> is formulated in a way to assign higher scores to features, which explains many instances of the data. The set of important features that are considered for the explanations is denoted by <em class="italic">V</em>. So, combining all these parameters, the algorithm tries to learn a <em class="italic">non-redundant coverage intuition function</em>, <em class="italic">c(V,W,I)</em>. The non-redundant coverage intuition tries to compute the collective importance of all features that appear in at least one instance in set <em class="italic">V</em>. However, the <em class="italic">pick problem</em> is about <em class="italic">maximizing the weighted coverage function</em>. This is denoted by the following equation:</p>
			<p class="figure-caption"><img src="image/B18216_04_038.png" alt=""/></p>
			<p>The<a id="_idIndexMarker331"/> details about the algorithm that we just covered in this section might be slightly overwhelming to understand for certain readers. However, intuitively, the algorithm tries to cover the following steps:</p>
			<ol>
				<li>The explanation model is run on all instances (<em class="italic">x</em>).</li>
				<li>The global importance of all individual components is computed.</li>
				<li>Then, the algorithm tries to maximize the non-redundant coverage intuition function (<em class="italic">c</em>) by iteratively adding instances with the highest maximum coverage gain.</li>
				<li>Finally, the algorithm tries to obtain the representative non-redundant explanation set (<em class="italic">V</em>) and return it.</li>
			</ol>
			<p>In the next section, we will cover how the LIME Python framework can be used for classification problems using code examples. </p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor084"/>A practical example of using LIME for classification problems</h1>
			<p>So far, we <a id="_idIndexMarker332"/>have covered most of the in-depth conceptual understanding that is needed regarding the LIME algorithm. In this section, we will try to explore the LIME Python framework for explaining classification problems. The framework is available as an open source project on GitHub at <a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a>. Installing LIME in Python can be done easily using the <strong class="source-inline">pip</strong> installer inside the Jupyter notebook:</p>
			<pre class="source-code">!pip install lime</pre>
			<p>The complete notebook version of the tutorial is accessible from the GitHub repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter04/Intro_to_LIME.ipynb</a>. However, for now, I will try to walk you through the entire code so that you understand the code in detail. Once the LIME framework has been installed, quickly verify whether the installation was successful or not by importing the library:</p>
			<pre class="source-code">import lime</pre>
			<p>If the import was successful, you can easily proceed with the next steps; otherwise, you need to check what went wrong while installing the framework. But usually, you should not face any errors or any dependency conflicts as installing the library is quite straightforward. For this tutorial, we will use the <em class="italic">Titanic dataset </em>(<a href="https://www.openml.org/search?type=data&amp;sort=runs&amp;id=40945&amp;status=active">https://www.openml.org/search?type=data&amp;sort=runs&amp;id=40945&amp;status=active</a>). This is one of the classic machine learning datasets used for predicting the survival of passengers on the Titanic. So, this is a binary classification problem that can be solved using machine learning. Although this is a classic dataset that is not very complex, it contains all types of features such as <em class="italic">Categorical</em>, <em class="italic">Ordinal</em>, <em class="italic">Continuous</em>, and even certain <em class="italic">identifiers</em> that are not relevant for the classification, thereby making this an interesting dataset to work with. To make it easier for you to execute notebooks, I have downloaded and provided the dataset after some slight modifications in the code repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter04/dataset</a>.</p>
			<p class="callout-heading">Titanic dataset</p>
			<p class="callout">The original Titanic dataset, describing the survival status of individual passengers on the Titanic. The titanic data does not contain information from the crew, but it does contain actual ages of half of the passengers. The principal source for data about Titanic passengers is the Encyclopedia Titanica. The datasets used here were begun by a variety of researchers. One of the original sources is Eaton &amp; Haas (1994) Titanic: Triumph and Tragedy, Patrick Stephens Ltd, which includes a passenger list created by many researchers and edited by Michael A. Findlay.</p>
			<p class="callout">Thomas Cason of UVa has greatly updated and improved the titanic data frame using the Encyclopedia Titanica and created the dataset here. Some duplicate passengers have been dropped, many errors corrected, many missing ages filled in, and new variables created.</p>
			<p>After installing and importing all the required modules, first, we will start by loading the dataset from the directory as a pandas DataFrame:</p>
			<pre class="source-code">data = pd.read_csv('dataset/titanic.csv')</pre>
			<p>When you try to visualize the DataFrame using the <strong class="source-inline">head</strong> method from pandas, you will get a glimpse of the dataset, as shown in <em class="italic">Figure 4.4</em>. Often, this step helps you to get a quick idea about how to understand your data:</p>
			<pre class="source-code">data.head()</pre>
			<p>The following diagram shows a glimpse of the pandas DataFrame used for this example:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18216_04_004.jpg" alt="Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side) and a data dictionary (right-hand side)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Displaying the dataset as a pandas DataFrame (left-hand side) and a data dictionary (right-hand side)</p>
			<p>For this<a id="_idIndexMarker333"/> particular example, we are not concerned about getting a highly efficient machine learning model, but rather our focus is on using LIME to produce human-friendly explanations in a few lines of code. So, we will skip doing rigorous <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>) or feature engineering steps. However, I do highly encourage all of you to perform these steps as a good practice. As we can see from the dataset, certain features such as <em class="italic">Passenger ID</em> and <em class="italic">Ticket Number</em> are identifiers that can be ignored. The <em class="italic">Cabin Number</em> feature is an interesting feature, especially as it could indicate a certain wing, floor, or side of the ship that is more vulnerable. But this feature is a sparse categorical feature, which, alone, will not be very helpful and might require some advanced transformation or feature engineering. So, to build a simple model, we will drop this feature. Also, the <em class="italic">passenger names</em> are not useful for the predictive model, and hence, we can remove them. There are some categorical features that need to be transformed for better model results. If you want to try out some more ideas for feature engineering, the following article might be helpful: <a href="https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/">https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/</a>.</p>
			<p>Here <a id="_idIndexMarker334"/>are the lines of code for data preparation before the model training:</p>
			<pre class="source-code"># Dropping all irrelevant columns</pre>
			<pre class="source-code">data.drop(columns=['PassengerId', 'Name', 'Cabin', 'Ticket'], inplace = True)</pre>
			<pre class="source-code"># Handling missing values</pre>
			<pre class="source-code">data.dropna(inplace=True)</pre>
			<pre class="source-code"># Ensuring that Age and Fare is of type float </pre>
			<pre class="source-code">data['Age'] = data['Age'].astype('float')</pre>
			<pre class="source-code">data['Fare'] = data['Fare'].astype('float')</pre>
			<pre class="source-code"># Label Encoding features </pre>
			<pre class="source-code">categorical_feat = ['Sex']</pre>
			<pre class="source-code"># Using label encoder to transform string categories to integer labels</pre>
			<pre class="source-code">le = LabelEncoder()</pre>
			<pre class="source-code">for feat in categorical_feat:</pre>
			<pre class="source-code">    data[feat] = le.fit_transform(data[feat]).astype('int')</pre>
			<pre class="source-code"># One-Hot Encoding Categorical features</pre>
			<pre class="source-code">data = pd.get_dummies(data, columns=['Embarked'])</pre>
			<p>The transformed DataFrame is shown in <em class="italic">Figure 4.5</em>:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18216_04_005.jpg" alt="Figure 4.5 – DataFrame display after basic preprocessing and feature engineering&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – DataFrame display after basic preprocessing and feature engineering</p>
			<p>Now, for<a id="_idIndexMarker335"/> the model training part, we will use an XGBoost classifier. This is an ensemble learning algorithm and is not inherently interpretable. Based on the number of estimators, the complexity of the algorithm can vary. It can also be installed easily using the <strong class="source-inline">pip</strong> installer:</p>
			<pre class="source-code">!pip install xgboost</pre>
			<p>The code to train the model after dividing into training and testing is as follows:</p>
			<pre class="source-code">features = data.drop(columns=['Survived'])</pre>
			<pre class="source-code">labels = data['Survived']</pre>
			<pre class="source-code"># Dividing into training-test set with 80:20 split ratio</pre>
			<pre class="source-code">x_train,x_test,y_train,y_test = train_test_split(</pre>
			<pre class="source-code">    features,labels,test_size=0.2, random_state=123)</pre>
			<pre class="source-code">model = XGBClassifier(n_estimators = 300, </pre>
			<pre class="source-code">                      random_state = 123)</pre>
			<pre class="source-code">model.fit(x_train, y_train)</pre>
			<p>Next, let's define <img src="image/B18216_04_039.png" alt=""/> as the prediction probability score, which will be later utilized by the LIME framework:</p>
			<pre class="source-code">predict_fn = lambda x: model.predict_proba(x)</pre>
			<p>To provide model explanations, we can define the LIME object and explain the required data instance with just a few lines of code:</p>
			<pre class="source-code">explainer = lime.lime_tabular.LimeTabularExplainer(</pre>
			<pre class="source-code">    data[features.columns].astype(int).values, </pre>
			<pre class="source-code">    mode='classification', </pre>
			<pre class="source-code">    training_labels=data['Survived'],</pre>
			<pre class="source-code">    feature_names=features.columns)</pre>
			<pre class="source-code">exp = explainer.explain_instance(</pre>
			<pre class="source-code">    data.loc[i,features.columns].astype(int).values, </pre>
			<pre class="source-code">    predict_fn, num_features=5)</pre>
			<pre class="source-code">exp.show_in_notebook(show_table=True)</pre>
			<p>The following diagram shows the visualizations provided by LIME for model explainability:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18216_04_006.jpg" alt="Figure 4.6 – Visualizations provided by the LIME framework to explain the model outcome&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Visualizations provided by the LIME framework to explain the model outcome</p>
			<p>From <em class="italic">Figure 4.6</em>, we can<a id="_idIndexMarker336"/> see the explanations provided by the LIME framework with only a few lines of code. Now, let's try to understand what the visualization is telling us:</p>
			<ul>
				<li>The leftmost bar plot is showing us the prediction probabilities, which can be treated as the model's confidence level in making the prediction. In <em class="italic">Figure 4.6</em>, for the selected data instance, the model is 100% confident that the particular passenger would <em class="italic">survive</em>.</li>
				<li>The second visualization from the left is probably the most important visualization that provides maximum explainability. It tells us that the most important feature, with a feature importance score of 38%, is the <strong class="source-inline">Sex</strong> feature, followed by <strong class="source-inline">Age,</strong> with a feature importance score of 26%. However, as illustrated in <em class="italic">Figure 4.6</em>, for the selected data instance, the <strong class="source-inline">Sex</strong>, <strong class="source-inline">Pclass</strong> (Passenger Class), <strong class="source-inline">Fare</strong>, and <strong class="source-inline">Embarked_C</strong> (Port of Embarkation as Cherbourg) features contribute toward the model outcome of <em class="italic">survival</em> along with their threshold scores learned from the entire dataset. In comparison, the <strong class="source-inline">Age</strong> feature, which is highlighted in blue, was more inclined toward predicting the outcome as <em class="italic">Did not Survive</em> as the particular passenger's age was 38 and, usually, passengers above the age of 38 have lower chances of surviving the disaster. The threshold feature values learned by the LIME model are also in alignment with our own common sense and <em class="italic">a prior</em> knowledge. Even in the case of the actual incident of the sinking of the Titanic, which happened over 100 years ago, women and children were given the first preference to escape the sinking ship using the limited lifeboats. </li>
			</ul>
			<p>Similarly, first-class passengers who had paid higher ticket fares got a higher preference to take the lifeboats and, therefore, had higher chances of survival. So, the model explanation provided is human-friendly and consistent with our prior beliefs.</p>
			<ul>
				<li>The third visualization from the left shows the top five features and their respective values. Here, the features highlighted in orange are contributing toward class 1, while features highlighted in blue are contributing toward class 0.</li>
				<li>The rightmost visualization is almost the same as the second visualization, except that it is presented in a different format, and it also provides local explanations for the particular data instance selected.</li>
			</ul>
			<p>As we<a id="_idIndexMarker337"/> discussed in the previous section, LIME also provides a global understanding of the model alongside the local explanations. This is provided using the SP-LIME algorithm. This can be implemented using the following lines of code:</p>
			<pre class="source-code">sp_exp = submodular_pick.SubmodularPick(</pre>
			<pre class="source-code">   explainer, data[features.columns].values, predict_fn,</pre>
			<pre class="source-code">   num_features=5, num_exps_desired=10)</pre>
			<pre class="source-code">[exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_exp.sp_explanations]</pre>
			<p> <em class="italic">Figure 4.7</em> shows the visualizations obtained using SP-LIME:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18216_04_007.jpg" alt="Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME to get a global understanding of the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Visualizations of diverse explanations obtained from SP-LIME to get a global understanding of the model</p>
			<p><em class="italic">Figure 4.7</em> shows <a id="_idIndexMarker338"/>the output of the SP-LIME code. SP-LIME provides a diverse representative sample set of local explanations considering different instances of the model to get a global perspective of the black-box model. These visualizations show us the important features, the feature-important scores, and even the range of values for each of those features and how these features contribute toward either of the classes. All these properties and features of the entire LIME framework make it a powerful approach in which to provide model-agnostic human-understandable model interpretability to black-box models. Additionally, the framework is also very robust so the entire algorithm can be implemented with only a few lines of code. </p>
			<p>Although LIME has many advantages, unfortunately, there are some drawbacks of this algorithm that we should be aware of. Let's discuss them in the next section.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor085"/>Potential pitfalls</h1>
			<p>In the previous <a id="_idIndexMarker339"/>section, we learned how easily the LIME Python framework can be used to explain black-box models for a classification problem. But unfortunately, the algorithm does have certain limitations, and there are a few scenarios in which the algorithm is not effective:</p>
			<ul>
				<li>While providing interpretable explanations, a particular choice of interpretable data representation and interpretable model might still have a lot of limitations. While the underlying trained model might still be considered a black-box model, there is no assumption about the model that is made during the explanation process. However, certain representations are not powerful enough to represent some complex behaviors of the model. For example, if we are trying to build an image classifier to distinguish between black and white images and colored images, then the presence or absence of superpixels will not be useful to provide the explanations. </li>
				<li>As<a id="_idIndexMarker340"/> discussed earlier, LIME learns an interpretable model to provide local explanations. Usually, these interpretable models are linear and non-complex. However, suppose that the underlying black-box model is not linear, even in the locality of the prediction, so the LIME algorithm is not effective. </li>
				<li>LIME explanations are highly sensitive to any change in input data. Even a slight change in the input data can drastically alter the explanation instance provided by LIME.</li>
				<li>For certain datasets, LIME explanations are not robust as, even for similar data instances, the explanations provided can be completely different. This might prevent end users from completely relying on the explanations provided by LIME.</li>
				<li>The algorithm is extremely prone to data drifts and label drifts. A slight drift between the training and the inference data can completely produce inconsistent explanations. The authors of the paper named <em class="italic">A study of data and label shift in the LIME framework</em>, <em class="italic">Rahnama</em> and <em class="italic">Boström</em> (https://arxiv.org/abs/1910.14421), mention certain experiments that can be used to evaluate the impact of data drift in the LIME framework. Due to this limitation, the goodness of approximation of the LIME explanations (also referred to as <em class="italic">fidelity</em>) is considered to be low. This is not expected in a good explanation method.</li>
				<li>Explanations provided by LIME depend on the choice of the hyperparameters of the algorithm. Similar to most of the algorithms, even for the LIME algorithm, the choice of the hyperparameters can determine the quality of the explanations provided. Hyperparameter tuning is also difficult for the LIME algorithm as, usually, qualitative methods are adopted to evaluate the quality of the LIME <a id="_idIndexMarker341"/>explanations. </li>
			</ul>
			<p>There are many research works that indicate the other limitations of the LIME algorithm. I have mentioned some of these research works in the <em class="italic">References </em>section. I would strongly recommend that you go through those papers to get more details about certain limitations of the algorithm.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor086"/>Summary</h1>
			<p>This brings us to the end of the chapter. In this chapter, we discussed LIME, one of the most widely adopted frameworks in XAI. Throughout this chapter, we discussed the intuition behind the workings of the algorithm and some important properties of the algorithm that make the generated explanations human-friendly. Additionally, we saw an end-to-end tutorial on how to use LIME for a practical use case to provide explainability to a black-box classification model. Even though we discussed some limitations of the LIME algorithm, due to its simplicity, LIME is still one of the most popular and widely used XAI frameworks. Hence, it is very important for us to discuss this algorithm and have a thorough understanding of the workings of the framework. </p>
			<p>In the next chapter, we will apply the LIME framework to solve other types of machine learning problems using different types of datasets.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor087"/>References</h1>
			<p>For additional information, please refer to the following resources:</p>
			<ul>
				<li><em class="italic">"Why Should I Trust You?" Explaining the Predictions of Any Classifier</em> by <em class="italic">Ribeiro et al</em>: <a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a></li>
				<li><em class="italic">LIME - Local Interpretable Model-Agnostic Explanations</em>: <a href="https://homes.cs.washington.edu/~marcotcr/blog/lime/">https://homes.cs.washington.edu/~marcotcr/blog/lime/</a></li>
				<li>The LIME GitHub project: <a href="https://github.com/marcotcr/lime">https://github.com/marcotcr/lime</a></li>
				<li><em class="italic">A study of data and label shift in the LIME framework</em> by <em class="italic">Rahnama</em> and <em class="italic">Boström</em>: <a href="https://arxiv.org/abs/1910.14421">https://arxiv.org/abs/1910.14421</a></li>
				<li><em class="italic">What's Wrong with LIME</em>: <a href="https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612">https://towardsdatascience.com/whats-wrong-with-lime-86b335f34612</a></li>
				<li><em class="italic">Why model why? Assessing the strengths and limitations of LIME</em> by <em class="italic">Dieber</em> and <em class="italic">Kirrane</em> (2020): <a href="https://arxiv.org/pdf/2012.00093.pdf">https://arxiv.org/pdf/2012.00093.pdf</a></li>
			</ul>
		</div>
	</body></html>