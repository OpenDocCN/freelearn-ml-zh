<html><head></head><body>
		<div id="_idContainer116">
			<h1 id="_idParaDest-61" class="chapter-number"><a id="_idTextAnchor061"/>5</h1>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor062"/>Machine Learning System Design</h1>
			<p>In the last chapter, we delved into the different machine learning concepts and the packages and libraries used to create these models. Using that information, we will begin to discuss the design process when building a machine learning pipeline and the different components found in most machine <span class="No-Break">learning pipelines.</span></p>
			<p>We will cover the following areas in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Machine learning <span class="No-Break">system components</span></li>
				<li>Fit and <span class="No-Break">transform interfaces</span></li>
				<li>Train and <span class="No-Break">serve interfaces</span></li>
				<li><span class="No-Break">Orchestration</span></li>
			</ul>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Machine learning system components</h1>
			<p>There are many<a id="_idIndexMarker326"/> moving parts required in order to build a robust machine learning system. Starting from gathering data to deploying your model to the user, each plays a vital role in keeping the system dynamic and scalable. Here, we will briefly discuss the different stages in the machine learning system life cycle and the role they play. These stages can be edited in order to suit the model or application <span class="No-Break">at hand.</span></p>
			<p>The majority of machine learning systems include the following stages, with some other stages depending <a id="_idIndexMarker327"/>on <span class="No-Break">business needs:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Data collection</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Date preprocessing</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Model training</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Model testing</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Model serving</strong></span></li>
			</ul>
			<p>Realistically, the majority of the time spent building machine learning systems is spent on the data. This is a key element in the process that can decide the effectiveness of your system since the model is dependent on the data it uses during training. Just like the human body, if you feed the model poor data or not enough data, it will output <span class="No-Break">poor results.</span></p>
			<p>The first part when it comes to data is the collection process. Understanding the application and the goal of the task can assist in the process of deciding how to collect data and what data to collect. We then determine the target value that we want to predict, such as the price of a home or the presence of a certain disease. These target values can be collected explicitly or implicitly. A target variable is explicit when we can directly determine the value of the variable we are trying to capture, while an implicit target value is found by using contextual data to determine the <span class="No-Break">target value.</span></p>
			<p>Depending on the task, we usually store the data in a database (for either metadata or tabular data) such <a id="_idIndexMarker328"/>as MySQL or cloud storage (for images, video, or audio) such as <span class="No-Break">Amazon S3:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18934_05_1.jpg" alt="Figure 5.1: Data collection"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1: Data collection</p>
			<p>Once we set up continuous data collection, we must devise a procedure for cleaning and processing the data. Not everything we collect will be perfect. You will always find missing data and certain outliers, which can negatively impact our model. No matter how intuitive your model is, it will always perform poorly with <span class="No-Break">garbage data.</span></p>
			<p>Some practices to deal with unclean data include removing outliers, normalizing certain features, or imputing missing data depending on the amount of data you have collected. Once the data has gone through the cleaning process, the next step is the feature <span class="No-Break">selection/engineering process.</span></p>
			<p>Understanding the different features your data contains plays an important role when your model tries to find the relationship <a id="_idIndexMarker329"/>in its data. <strong class="bold">Exploratory Data Analysis </strong>(<strong class="bold">EDA</strong>) is the common process used when it comes to understanding the data you have collected and how the data is structured. This helps when it comes to determining which features to use in your model. As we previously mentioned in <a href="B18934_04.xhtml#_idTextAnchor051"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, when we include more features in our models, it allows them to map to more complex problems. However, adding too many features can lead to overfitting, so it is important to research the most important features for <span class="No-Break">your model.</span></p>
			<p>While most machine learning models can find patterns and relationships in data, the best way of understanding the data you collect is via the experts in the field of the task you are trying to solve. Subject matter experts can provide the best insight into what features to focus <a id="_idIndexMarker330"/>on when creating your model. Some unsupervised machine learning models, such as PCA and t-SNE, can group and find features that can provide the most valuable information for <span class="No-Break">your model.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Having domain knowledge of the problem you are trying to solve is the most effective way of understanding your data and determining which features to use for training your machine <span class="No-Break">learning model.</span></p>
			<p>Once you have set up the processes to collect and clean the data, the next step is creating and training your model. Thanks to most machine learning libraries, you can import prebuilt models and even use weights from already trained models to use on your own model. Here, it is common practice to use different models and techniques to see which produces the best result, and from there, you can choose the best model and begin to fine-tune it by updating the hyperparameters. This process can take time depending on the amount of data <span class="No-Break">you use.</span></p>
			<p>Testing your model is a critical element in your system’s pipeline. Depending on the application, a poor model can negatively impact your business and give your users a bad experience. To prevent that, you need to determine the different metrics and thresholds that need to be met for the model to be production-ready. If the model can’t meet these expectations, then you need to go back and understand the weaknesses of the model and address them before <span class="No-Break">training again.</span></p>
			<p>After performing tests and getting solid results from your model, you can now deploy your model to the user application. This varies from application to application. From then, the whole process can start from the beginning, where new data is inserted and follows the machine<a id="_idIndexMarker331"/> learning pipeline so it can dynamically grow based on <span class="No-Break">user actions:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B18934_05_2.jpg" alt="Figure 5.2: The machine learning pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2: The machine learning pipeline</p>
			<p>In the following sections, we will look into the details of the different interfaces that constitute our machine <span class="No-Break">learning pipeline.</span></p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>Fit and transform interfaces</h1>
			<p>Now that we have looked at the entire pipeline process, we will look in detail at the different interfaces that make up the machine learning system. The majority of the systems include<a id="_idIndexMarker332"/> the <span class="No-Break">following interfaces:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Fit</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Transform</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Train</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Serve</strong></span></li>
			</ul>
			<p>When it comes to the data and creating the model, we come across the fit and transform interfaces. We will start by looking at the <span class="No-Break">transform interface.</span></p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Transform</h2>
			<p>The transform<a id="_idIndexMarker333"/> interface is the process of taking in the collected data and preprocessing the data so that the model can train properly and extract <span class="No-Break">meaningful information.</span></p>
			<p>It is common for the data we collect to have missing values or outliers, which can cause bias in our model. To remove this bias, we can apply certain techniques that help remove the skew in the data and produce meaningful machine learning models. Some of the following techniques <a id="_idIndexMarker334"/>we will learn about fall into the following three types <span class="No-Break">of transformations:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Scaling</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Clipping</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Log</strong></span></li>
			</ul>
			<p>Log transformation is the most common and simple transformation technique we can apply to our data. A lot of the time, our data is skewed in one direction, which can introduce bias. To help mitigate the skewed distribution, we can simply apply the log function to our data, and this shifts our data into more of a normal distribution, which allows the data to be <span class="No-Break">more balanced.</span></p>
			<p>We can perform this transformation by using the <span class="No-Break">following code:</span></p>
			<pre class="console">import numpy as np
dataframe_log = np.log(dataframe["House Price"])</pre>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B18934_05_3.jpg" alt="Figure 5.3: Performing log transformation on skewed data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3: Performing log transformation on skewed data</p>
			<p>Once we apply the log transformation, we can start looking at the other transformations. The second transformation we can use is the clipping transformation. The more we make our data follow a normal distribution, the better, but we may encounter outliers that can skew our data. To help reduce the impact that outliers have on our data, we can apply<a id="_idIndexMarker335"/> a quantile function. The most common quantile range that people use is the 0.05 and 0.95 percentile. This means that any data below the 0.05 percentile will be rounded up to the lower bound while any data above the 0.95 percentile will be rounded down to the upper bound. This allows us to retain the majority of the data while reducing the impact that outliers have on the model. The upper and lower ranges can also be modified based on what makes sense for the distribution of <span class="No-Break">the data.</span></p>
			<p>This transformation can be performed using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import QuantileTransformer
quantile = QuantileTransformer(output_distribution='normal', random_state=0)
x_clipped = quantile.fit_transform("House Price")</pre>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B18934_05_4.jpg" alt="Figure 5.4: Clipping transformation on data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4: Clipping transformation on data</p>
			<p>The last major transformation technique is scaling transformations. A lot of the time, the data we collect have different types of metrics and values, which can skew our data and confuse our model. For example, one feature measures the revenue of companies in the millions while <a id="_idIndexMarker336"/>another feature measures the employee count in the thousands, and when using these features to train the model, the discrepancy may put more emphasis on one feature over another. To prevent these kinds of problems, we<a id="_idIndexMarker337"/> can apply scaling transformations, which can be of the <span class="No-Break">following types:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">MinMax</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Standard</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Max Abs</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Robust</strong></span></li>
				<li><span class="No-Break"><strong class="bold">Unit Vector</strong></span></li>
			</ul>
			<p>The MinMax scaler is the simplest scaling transformation. It works best when the data is not distorted. This scales the data between 0 and 1. It can be calculated using <span class="No-Break">this formula:</span></p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/Formula_05_001.jpg" alt=""/>
				</div>
			</div>
			<p>We can perform this scaling transformation using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_scaled = scaler.fit_transform("House Price")</pre>
			<p>The MaxAbs scaler is similar to MinMax but rather than scaling the data between 0 to 1, it scales the data <a id="_idIndexMarker338"/>from -1 to 1. This can be calculated using the <span class="No-Break">following formula:</span></p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Formula_05_002.jpg" alt=""/>
				</div>
			</div>
			<p>We can perform this scaling transformation using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
x_scaled = scaler.fit_transform("House Price")</pre>
			<p>The Standard scaler is another popular scaling transformation. Rather than using the min and max like the MinMax scaler, this scales the data so that the mean is 0 and the standard deviation is 1. This scaler works on the assumption that the data is normally distributed. This can be calculated using the <span class="No-Break">following formula:</span></p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/Formula_05_003.jpg" alt=""/>
				</div>
			</div>
			<p>We can perform this scaling transformation using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_scaled = scaler.fit_transform("House Price")</pre>
			<p>The MinMax, MaxAbs, and Standard scalers, while powerful, can suffer from outliers and skewed distribution. To remedy this issue, we can use the Robust scaler. Rather than using the mean or max, this scaler works by removing the median from the data and then scaling the data using the interquartile range. This can be calculated using the <span class="No-Break">following formula:</span></p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/Formula_05_004.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/Formula_05_005.jpg" alt=""/>
				</div>
			</div>
			<p>We can perform this scaling transformation using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import RobustScalar
scaler = RobustScalar()
x_scaled = scaler.fit_transform("House Price")</pre>
			<p>Finally, we have the Unit Vector scaler, also known as a normalizer. While the other scaler functions work <a id="_idIndexMarker339"/>based on columns, this scaler normalizes based on rows. It uses the MinMax scaler formula and converts positive values between 0 and 1 and negative values between -1 and 1. There are two ways of performing <span class="No-Break">this scaling:</span></p>
			<ul>
				<li>L1 norm – values in the column are converted so that the sum of their absolute value in the row <span class="No-Break">equals 1</span></li>
				<li>L2 norm – values in the column are squared and added so that the sum of their absolute value in the row is equal <span class="No-Break">to 1</span></li>
			</ul>
			<p>We can perform this scaling transformation using the <span class="No-Break">following code:</span></p>
			<pre class="console">from sklearn.preprocessing import Normalizer
scaler = Normalizer()
x_scaled = scaler.fit_transform("House Price")</pre>
			<p>There are many more scaling and transforming techniques, but these are the most commonly used, as they provide stable and <span class="No-Break">consistent results.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Much of the development process takes place in the transformation stage. Understanding how the data is structured and distributed helps dictate which transformation methods you will perform on your data. No matter how advanced your model is, poorly structured data will produce <span class="No-Break">weak models.</span></p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Fit</h2>
			<p>Now, we will look at<a id="_idIndexMarker340"/> the fit interface. This interface refers to the process of creating the machine learning model that will be used in training. With today’s technology, not much work or effort is needed to create the model used for training in the machine learning pipeline. There are already prebuilt models ready to be imported and used for any type <span class="No-Break">of application.</span></p>
			<p>Here is a small example of creating a KNN classification model using the <span class="No-Break">scikit-learn library.</span></p>
			<p>First, we import all the <span class="No-Break">required libraries:</span></p>
			<pre class="console">from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KneighborsClassifier
from sklearn.datasets import load_iris</pre>
			<p>We then import the data, split the data into training and testing batches, and apply a standard <span class="No-Break">scaler transformation:</span></p>
			<pre class="console">iris = load_iris()
X = iris.data
y = iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)</pre>
			<p>We then initialize a KNN model with k = <strong class="source-inline">3</strong> and then perform training on <span class="No-Break">the model:</span></p>
			<pre class="console">classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, y_train)</pre>
			<p>The main effort when using the fit interface is setting up the models that will be used for the training phase of the machine learning pipeline. Due to the simplicity of importing multiple prebuilt models, it is common practice to import multiple types of machine learning models and train all of them at once. This way, we are able to test different types of models and determine which one of them performs the best. Once we decide which model to<a id="_idIndexMarker341"/> use, we can then start to experiment with different hyperparameters to further fine-tune <span class="No-Break">our model.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/>Train and serve interfaces</h1>
			<p>The transform and fit interfaces are responsible for preparing the data and setting up our machine learning models for our pipeline. Now that we have preprocessed the data, we need to begin looking at how we can begin the actual training process and take our trained models and deploy them for our clients <span class="No-Break">to use.</span></p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>Training</h2>
			<p>Now that we<a id="_idIndexMarker342"/> have preprocessed the data and created our models, we can begin the training process. This stage can vary from time to time depending on the quality of data being trained on or the type of model being used <span class="No-Break">during training.</span></p>
			<p>Once we preprocess the data, we need to split the dataset into training and testing sets. This is done to prevent overfitting. We need the model to be able to generalize the data, and using all the data for training would defeat <span class="No-Break">the purpose.</span></p>
			<p>A common practice is to split your data into 70% training and 30% testing. This way, the model has enough data to learn the relationships and uses the testing data to self-correct its <span class="No-Break">training process.</span></p>
			<p>There is a more robust approach to splitting the data, which is called <strong class="bold">K-Fold Cross-Validation</strong>. This <a id="_idIndexMarker343"/>process works best in cases where there may not be enough training data. To perform this, we split the data into <em class="italic">k</em> number of subsets and then we train on all subsets except for one. We then iterate through this process where a new subset is selected to be the test data. Finally, we measure the performance of the model by averaging the metrics for each iteration. This way, we can train and test using all the data without leaving any important features that may be useful when it comes to learning <span class="No-Break">the data.</span></p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18934_05_5.jpg" alt="Figure 5.5: K-Cross Validation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5: K-Cross Validation</p>
			<p>Once we have<a id="_idIndexMarker344"/> split the data, now comes the actual training part. This part is as simple as setting up the function used to train the model. This part depends on the type of library you use and the different APIs <span class="No-Break">it offers.</span></p>
			<p>We can create a simple example using the <span class="No-Break">scikit-learn library:</span></p>
			<pre class="console">from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics
diabetes = load_diabetes()
features = diabetes.data
target = diabetes.target
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=1)
linear_regression = LinearRegression()
linear_regression.fit(features, target)
y_pred = linear_regression.predict(x_test)
print("Linear Regression model MSE:", metrics.mean_squared_error(y_test, y_pred))</pre>
			<p>After training your model, you must measure its performance. To prevent poor models from being <a id="_idIndexMarker345"/>deployed to users, it is a common practice to measure certain metrics and set certain thresholds that need to be met before a model is considered ready <span class="No-Break">for production.</span></p>
			<p>Depending on the type of model you create, certain metrics need to be evaluated. For example, a <a id="_idIndexMarker346"/>regression <a id="_idIndexMarker347"/>model <a id="_idIndexMarker348"/>will <a id="_idIndexMarker349"/>typically look at the <span class="No-Break">following metrics:</span></p>
			<ul>
				<li><strong class="bold">Mean Absolute </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MAE</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Mean Squared </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MSE</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Root Mean Squared </strong><span class="No-Break"><strong class="bold">Error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">RMSE</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break"><strong class="bold">R-Squared</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">R2</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>For classification models, you will monitor the following metrics to determine the <span class="No-Break">model’s strength:</span></p>
			<ul>
				<li><span class="No-Break">Accuracy</span></li>
				<li>Precision <span class="No-Break">and recall</span></li>
				<li><span class="No-Break">The F1-score</span></li>
				<li>The <strong class="bold">Area Under the Receiver Operating Characteristics </strong><span class="No-Break"><strong class="bold">Curve</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AUROC</strong></span><span class="No-Break">)</span></li>
			</ul>
			<p>Having domain<a id="_idIndexMarker350"/> knowledge helps immensely when determining what thresholds are applicable to the model you are training. In some cases, such as with cancer detection models, it is important to avoid false negatives, so it is important to set stricter thresholds for<a id="_idIndexMarker351"/> what models can be <span class="No-Break">used confidently.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Before serving your model, you need to make sure the model is viable for production. Setting up the metric thresholds that the model needs to pass is a fundamental way of validating your models before deploying them. If your model fails to pass these criteria, then there should be a process to redo the data transformation and model training phases until it can pass <span class="No-Break">the thresholds.</span></p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Serving</h2>
			<p>When it comes <a id="_idIndexMarker352"/>to serving our model, this is open and flexible depending on the user’s needs. In most cases, we are deploying our model into one of two types <span class="No-Break">of systems:</span></p>
			<ul>
				<li><strong class="bold">Model serving</strong>, where <a id="_idIndexMarker353"/>we deploy our model as <span class="No-Break">an API</span></li>
				<li><strong class="bold">Model embedding</strong>, where <a id="_idIndexMarker354"/>we deploy our model straight into an application <span class="No-Break">or device</span></li>
			</ul>
			<p>Model embedding is the simplest way of deploying your model. You create a binary file containing your model and you embed the file into your application code. This simplicity provides the best performance when making predictions, but this comes at a cost. Because you directly embed the file into your application, it is difficult to scale your model since you will have to recreate and reupload the file every time you make an update to your model. As such, this is not considered a <span class="No-Break">recommended practice.</span></p>
			<p>Model serving is the most commonly used method on today’s market. This separation between the application and the model makes it easy for a developer to maintain and update the model without having to change the application itself. You simply create an API service that a user can access to make calls and predictions. Due to the separation, you can continuously update the model without having to redeploy the <span class="No-Break">whole application.</span></p>
			<p>An alternative to model embedding that includes model serving is creating a microservice that<a id="_idIndexMarker355"/> includes the binary file of the model, which could be accessed by <span class="No-Break">other applications:</span></p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18934_05_6.jpg" alt="Figure 5.6: Serving machine learning models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6: Serving machine learning models</p>
			<p>One of the more intuitive approaches is creating your own package or library that includes all the models that you have trained. That way, you can scale efficiently by allowing multiple applications to access the different models you <span class="No-Break">have created.</span></p>
			<p>Everything we’ve seen so far is what it takes to build a simple machine learning pipeline. While this is doable for most applications, to be dynamic and robust, we need to look at orchestration and what it can offer us to support more advanced applications <span class="No-Break">and problems.</span></p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/>Orchestration</h1>
			<p>Now that we<a id="_idIndexMarker356"/> understand the different interfaces and the roles they play in the machine learning pipeline, the next step is understanding how to wrap everything together into one seamless system. To understand the holistic system, we must first understand automation <span class="No-Break">and orchestration.</span></p>
			<p><strong class="bold">Automation</strong> refers <a id="_idIndexMarker357"/>to the process of automating small or simple tasks, such as uploading files to a server or deploying an application, without human intervention. Rather than having a person perform these repetitive tasks, we can program our system to handle these simple tasks, thus reducing wasted time <span class="No-Break">and resources.</span></p>
			<p>This is useful for most systems due to the linear nature of the pipeline. This highlights a common limitation of automation though – the lack of flexibility. Most systems today require a more dynamic process to be able to adapt to certain applications and processes, and automation alone <span class="No-Break">isn’t enough:</span></p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B18934_05_7.jpg" alt="Figure 5.7: A linear system pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7: A linear system pipeline</p>
			<p>This is where orchestration comes into action. <strong class="bold">Orchestration</strong> is the configuration and coordination of automated tasks to create a whole workflow. We can create a system to perform certain jobs or tasks based on a certain set of rules. It takes some planning and understanding to create a comprehensive orchestration workflow since the user determines what actions the system needs to take for <span class="No-Break">certain cases.</span></p>
			<p>A simple example would be deploying an application to users. There can be many moving parts in the system, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Connecting to <span class="No-Break">a server</span></li>
				<li>Uploading certain files to <span class="No-Break">certain servers</span></li>
				<li>Handling <span class="No-Break">user requests</span></li>
				<li>Storing data or logs in <span class="No-Break">a database</span></li>
			</ul>
			<p>Let’s say that after the recent changes have been deployed, the app has suffered critical errors, which may bring down the application. The system admin could set up rules for recovering and restoring the system, such as rolling back to a stable version. With the system able to self-recover, the developers can spend more time in development rather than dealing with overhead when it comes <span class="No-Break">to recovery.</span></p>
			<p>Depending on certain outcomes, not all tasks may need to be performed. There may be backup actions that need to take place, or different paths that the system needs to go through to maintain a stable workflow. This way, the system can adapt to its environment and<a id="_idIndexMarker358"/> self-sustain without much <span class="No-Break">human intervention:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B18934_05_8.jpg" alt="Figure 5.8: A dynamic system pipeline (orchestration)"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8: A dynamic system pipeline (orchestration)</p>
			<p>The different tasks in the machine learning system that can be automated are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Gathering and preprocessing <span class="No-Break">the data</span></li>
				<li>Training the machine <span class="No-Break">learning model</span></li>
				<li>Running tests and diagnostics on the trained model to evaluate <span class="No-Break">its performance</span></li>
				<li>Serving the machine <span class="No-Break">learning model</span></li>
				<li>Monitoring the model <span class="No-Break">in production</span></li>
			</ul>
			<p>With these automated tasks, the system admin needs to orchestrate the stages of the pipeline to be dynamic and sustainable. The following components help create a <span class="No-Break">robust system:</span></p>
			<ul>
				<li><strong class="bold">Scheduling</strong>: The system <a id="_idIndexMarker359"/>must be able to schedule and run different automated tasks in the pipeline individually while maintaining <span class="No-Break">system dependencies.</span></li>
				<li><strong class="bold">CI/CD Testing</strong>: After model training is complete, it is imperative to do automated testing on your model to measure its performance. If it fails to pass certain metrics, you must repeat the training process from the beginning to address the weaknesses of the model; otherwise, it cannot be deployed <span class="No-Break">to production.</span></li>
				<li><strong class="bold">Deployment</strong>: Depending on where you will deploy your model to production, setting up an automated process can help reduce the time spent on deployment and still maintain an updated version of <span class="No-Break">the model.</span></li>
				<li><strong class="bold">Monitoring</strong>: After deploying your model, continuously monitoring the model’s performance in production is needed to maintain the model’s health without it decaying. This will give us an indication of when we need to update our pipeline or our model in order to <span class="No-Break">stay efficient.</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">Understanding what your business needs are and how your model functions gives you a good picture of how <a id="_idIndexMarker360"/>you want to orchestrate your machine learning pipeline. Setting up backup phases to address certain pitfalls in your system allows it to be more dynamic and adaptable to <span class="No-Break">industry demands.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>Summary</h1>
			<p>In this chapter, we looked at the different key components that make up a machine <span class="No-Break">learning pipeline.</span></p>
			<p>From there, we looked in detail at the interfaces that make up the components. We started with the transform interface, which is responsible for the data aspect of the pipeline. It takes the data and applies different types of data transformation that allow us to maintain clean and stable data, which we can later use in our machine <span class="No-Break">learning model.</span></p>
			<p>After our transformation stage, we start creating our model in the fit interface. Here, we can use the prebuilt models that the libraries and packages offer to initialize our models. Due to the ease of creating models, it is a good practice to test different types of models to see which model performs the best based on <span class="No-Break">our data.</span></p>
			<p>Once we have created our model, we can begin the actual training of our model. We need to split our data into training and test sets to allow our model to understand the relationship in our data. From there, we can measure the different metrics in our model to validate the <span class="No-Break">model’s performance.</span></p>
			<p>Once we feel comfortable with our model’s performance, we can start to deploy our application to production. There are two major ways of deploying our model, whether it be embedded into our application or deployed as a service for our clients <span class="No-Break">to use.</span></p>
			<p>Finally, wrapping everything together, we learned what orchestration consists of when it comes to machine learning. We learned what concepts need to be considered when orchestrating your machine learning pipeline and how to keep your system dynamic and robust to keep up with <span class="No-Break">everyday demands.</span></p>
			<p>As time passes and data changes, it is important that we adjust and maintain our models to handle certain situations that may arise in the real world. In the next chapter, we will look at how we can maintain our machine learning models when our data starts to shift <span class="No-Break">and change.</span></p>
		</div>
</body></html>