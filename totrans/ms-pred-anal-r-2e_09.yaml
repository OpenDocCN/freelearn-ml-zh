- en: Chapter 9. Ensemble Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we take a step back from learning new models and instead think
    about how several trained models can work together as an ensemble, in order to
    produce a single model that is more powerful than the involved models, individually.
  prefs: []
  type: TYPE_NORMAL
- en: The first type of ensemble that we will study uses different samples of the
    same dataset in order to train multiple versions of the same model. These models
    then vote on the correct answer for a new observation and an average or majority
    decision is made, depending on the type of problem. This process is known as bagging,
    which is short for bootstrap aggregation. Another approach to combine models is
    boosting. This essentially involves training a chain of models and assigning weights
    to observations that were incorrectly classified or fell far from their predicted
    value so that successive models are forced to prioritize them.
  prefs: []
  type: TYPE_NORMAL
- en: As methods, bagging and boosting are fairly general and have been applied with
    a number of different types of models. Decision trees, studied in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-Based Methods*, are particularly
    suited to ensemble methods. So much so, that a particular type of tree-based ensemble
    model has its own name--the random forest. Random forests offer significant improvements
    over the single decision tree and are generally considered to be very powerful
    and flexible models, as we shall soon discover. In this chapter, we'll revisit
    some of the datasets we analyzed in previous chapters and see if we can improve
    performance by applying some of the principles we learn here.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus of this chapter is on combining the results from different models
    in order to produce a single model that will outperform individual models on their
    own. **Bagging** is essentially an intuitive procedure for combining multiple
    models trained on the same dataset, by using majority voting for classification
    models and average value for regression models. We'll present this procedure for
    the classification case, and later show how this is easily extended to handle
    regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Bagging procedure for binary classification**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*data*: The input data frame containing the input features and a column with
    the binary output label.'
  prefs: []
  type: TYPE_NORMAL
- en: '*M*: An integer, representing the number of models that we want to train.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*models*: A set of Μ trained binary classifier models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method**:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Create a random sample of size *n*, where *n* is the number of observations
    in the original dataset, with replacement. This means that some of the observations
    from the original training set will be repeated and some will not be chosen at
    all. This process is known as **bootstrapping**, **bootstrap sampling**, or **bootstrap
    resampling**.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Train a classification model using this sampled dataset. Typically, we opt
    not to use regularization or shrinkage methods designed to reduce overfitting
    in this step, because the aggregating process used at the end will be used to
    smooth out overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. For each observation in the sampled dataset, record the class assigned by
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Repeat this process *M* times in order to train *M* models.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. For every observation in the original training dataset, compute the predicted
    class via a majority vote across the different models. For example, suppose M
    = 61 and through bootstrap sampling, a particular observation appears in the training
    data for 50 of the models. If 37 of these predict class 1 for this observation
    and 13 predict class -1, by majority vote the overall prediction will be class
    1.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Compute the model's accuracy using the labels provided by the training set.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, all we are effectively doing is training the same model on *M*
    different versions of the input training set (created through sampling with replacement)
    and averaging the result.
  prefs: []
  type: TYPE_NORMAL
- en: A legitimate question to ask would be, how many distinct observations do we
    get each time we sample with replacement? On average, we end up with 63 percent
    of the distinct observations in every sample that we make. To understand where
    this comes from, consider that because we are sampling with replacement, the probability
    of not picking out a particular observation, *x[1]*, during sampling is just the
    result of *n* failed Bernoulli trials.
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/00158.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This number also happens to be the average proportion of observations that are
    not selected across the entire training dataset because we multiply and divide
    the previous expression by *n* to compute this quantity. The numerical result
    of this expression can be approximated by *e-1*, which is roughly 37 percent.
    Consequently, the average proportion of observations that are selected is around
    63 percent. This number is just an average, of course, and is more accurate for
    larger values of *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Margins and out-of-bag observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's imagine that for a particular observation, *x[1]*, 85 percent of our models
    predict the correct class and the remaining 15 percent predict the incorrect class.
    Let's also imagine that we have another observation, *x[2]*, for which the analogous
    percentages are 53 percent and 47 percent. Clearly, our intuition suggests that
    we should be more confident about the classification of the former observation
    compared to the latter observation. Put differently, the difference between the
    classification proportions, also known as the **margin** (similar to but not to
    be confused with the margin used for support vector machines) is a good indicator
    of the confidence of our classification.
  prefs: []
  type: TYPE_NORMAL
- en: The 70 percent margin of observation *x[1]* is much larger than the 6 percent
    margin of observation *x[2]* and thus, we believe more strongly in our ability
    to correctly classify the former observation. In general, what we are hoping for
    is a classifier that has a large margin for all the observations. We are less
    optimistic about the generalization abilities of a classifier that has a small
    margin for more than a handful of observations.
  prefs: []
  type: TYPE_NORMAL
- en: One thing the reader may have noticed here is that in generating the set of
    predicted values for each model, we are using the same data on which the model
    was trained. If we look closely at *step 3* of the procedure, we are classifying
    the same sampled data that we used in *step 2* to train the model. Even though
    we are eventually relying on using an averaging process at the end in order to
    obtain the estimated accuracy of the bagged classifier for unseen data, we haven't
    actually used any unseen data at any step along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in *step 1* we constructed a sample of the training data with
    which to train our model. From the original dataset, we refer to the observations
    that were not chosen for a particular iteration of the procedure as the **out-of-bag**
    (**OOB**) observations. These observations are therefore not used in the training
    of the model at that iteration. Consequently, instead of relying on the observations
    used to train the models at every step, we can actually use the OOB observations
    to record the accuracy of a particular model.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, we average over all the OOB accuracy rates to obtain an average
    accuracy. This average accuracy is far more likely to be a realistic and objective
    estimate of the performance of the bagged classifier on unseen data. For a particular
    observation, the assigned class is thus decided as the majority vote over all
    classifiers for which the observation was not picked in their corresponding training
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: The samples generated from sampling the original dataset with replacement, known
    as **bootstrapped samples**, are similar to drawing multiple samples from the
    same distribution. As we are trying to estimate the same target function using
    a number of different samples instead of just one, the averaging process reduces
    the variance of the result. To see this, consider trying to estimate the mean
    of a set of observations drawn from the same distribution and all mutually independent
    of each other. More formally, these are known as **independent and identically
    distributed** (**iid**) observations. The variance of the mean of these observations
    is ![Margins and out-of-bag observations](img/00159.jpeg).
  prefs: []
  type: TYPE_NORMAL
- en: This shows that as the number of observations increases, the variance decreases.
    Bagging tries to achieve the same behavior for the function we are trying to model.
    We don't have truly independent training samples, and are instead forced to use
    bootstrapped samples, but this thought experiment should be enough to convince
    us that, in principle, bagging has the potential to reduce the variance of the
    model. At the same time, this averaging process is a form of smoothing over any
    localized bumps in the function that we are trying to estimate. Assuming that
    the target regression function or classification boundary that we are trying to
    estimate is actually smooth, then bagging may also reduce the bias of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting complex skill learning with bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging and boosting are both very popular with the tree-based models that we
    studied in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-Based Methods*. There are many notable
    implementations to apply these approaches to methodologies such as CART for building
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: The `ipred` package, for example, contains an implementation to build a bagged
    predictor for trees built with `rpart()`. We can experiment with the `bagging()`
    function that this package provides. To do this, we specify the number of bagged
    trees to make using the `nbagg` parameter (default is `25`) and indicate that
    we want to compute accuracy using the OOB samples by setting the `coob` parameter
    to `TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will do this for our complex skill learning dataset from the previous chapter,
    using the same training data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the SSE on the test set is less than the lowest SSE that we saw
    when tuning a single tree. Increasing the number of bagged iterations, however,
    does not seem to improve this performance substantially. We will revisit this
    dataset again later.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting heart disease with bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The prototypical use case for bagging is the decision tree; however, it is
    important to remember that we can use this method with a variety of different
    models. In this section, we will show how we can build a bagged logistic regression
    classifier. We built a logistic regression classifier for the Statlog Heart dataset
    in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*. Now, we will repeat that
    experiment but use bagging in order to see if we can improve our results. To begin
    with, we''ll draw our samples with replacement and use these to train our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In our code, the data frames `heart_train` and `heart_test` are referring to
    the same data frames that we prepared in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*. We begin by deciding on
    the number of models that we will train and setting the appropriate value of `M`.
    Here, we have used an initial value of `11`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is a good idea to use an odd number of models with bagging, so
    that during the majority voting process there can never be a tie with binary classification.
    For reproducibility, we set a vector of seeds that we will use. This is simply
    a counter from an arbitrarily chosen starting seed value of `70000`. The `sample_vectors`
    matrix in our code contains a matrix where the columns are the indexes of randomly
    selected rows from the training data with replacement. Note that the rows are
    numbered 1 through 230 in the training data, making the sampling process easy
    to code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll define a function that creates a single logistic regression model
    given a sampling vector of indices to use with our training data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the last line of the preceding code, we iterate through the columns of the
    `sample_vectors` matrix we produced earlier and supply them as an input to our
    logistic regression model training function, `train_1glm()`. The resulting models
    are then stored in our final list variable, `models`. This now contains 11 trained
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first method of evaluating our models, we are going to use the data
    on which each individual model was trained. To that end, we''ll construct the
    `bags` variable that is a list of these data frames, this time with unique indexes,
    as we don''t want to use any duplicate rows from the bootstrap sampling process
    in the evaluation. We''ll also add a new column called `ID` to these data frames
    that stores the original row names from the `heart_train` data frame. We''ll see
    why we do this shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We now have a list of models and a list of data frames that they were trained
    on, the latter without duplicate observations. From these two, we can create a
    list of predictions. For each training data frame, we will tack on a new column
    called `PREDICTIONS {m}`, where `{m}` will be the number of the model being used
    to make the predictions. Consequently, the first data frame in the bags list will
    have a predictions column called `PREDICTIONS 1`. The second data frame will have
    a predictions column called `PREDICTIONS 2`, the third will have one called `PREDICTIONS
    3`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following call produces a new set of data frames as just described, but
    only keeping the `PREDICTIONS{m}` and `ID` columns, and these data frames are
    stored as a list in the variable `training_predictions`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we want to merge all of these data frames onto a single data frame, where
    the rows are the rows of the original data frame (and thus, correspond to the
    observations in the dataset) and the columns are the predictions made by each
    model on the observations. Where a particular row (observation) was not selected
    by the sampling process to train a particular model, it will have an `NA` value
    in the column corresponding to the predictions that that model makes.
  prefs: []
  type: TYPE_NORMAL
- en: Just to be clear, recall that each model is making predictions only on the observations
    that were used to train it and so the number of predictions that each model makes
    is smaller than the total number of observations available in our starting data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have stored the original row numbers of the `heart_train` data frame
    in the `ID` column of every data frame created in the previous step, we can merge
    using this column. We use the `Reduce()` function along with the `merge()` function
    in order to merge all the data frames in our `training_predictions` variable into
    one new data frame. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s have a look at the first few lines and columns of this aggregated data
    frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first column is the `ID` row that was used to merge the data frame. The
    numbers in this column are the row numbers of the observations from the starting
    training data frame. The `PREDICTIONS 1` column contains the predictions that
    the first model makes. We can see that this model had rows `1`, `2`, `5`, and
    `6` as part of its training data. For the first row, the model predicts class
    `1` and for the other three rows, it predicts class `0`. Rows `3` and `4` were
    not part of its training data and so there are two `NA` values. This reasoning
    can be used to understand the remaining columns, which correspond to the next
    three models trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this data frame constructed, we can now produce our training data predictions
    for the whole bagged model using a majority vote across each row of the preceding
    data frame. Once we have these, we merely need to match the predictions with the
    labeled values of the corresponding rows of the original `heart_train` data frame
    and compute our accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We now have our first accuracy measure for our bagged model--91.7 percent. This
    is analogous to measuring the accuracy on our training data. We will now repeat
    this process using the OOB observations for each model to compute the OOB accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: There is one caveat here, however. In our data, the ECG column is a factor with
    three levels, one of which, level 1, is very rare. As a result of this, when we
    draw bootstrap samples from the original training data, we may encounter samples
    in which this factor level never appears. When that happens, the `glm()` function
    will think this factor only takes two levels, and the resulting model will be
    unable to make predictions when it encounters an observation with a value for
    the ECG factor that it has never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: 'To handle this situation, we need to replace the level 1 value of this factor
    with an `NA` value for the OOB observations, if the model they correspond to did
    not have at least one observation with an ECG factor level of 1 in its training
    data. Essentially, for simplicity, we will just not attempt to make a prediction
    for these problematic observations when they arise. With this in mind, we will
    define a function to compute the OOB observations for a particular sample and
    then use this to find the OOB observations for all our samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use our `glm_predictions()` function to compute predictions using
    our out-of-bag samples. The remainder of the process is identical to what we did
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As expected, we see that our OOB accuracy, which is a better measure of performance
    on unseen data, is lower than the training data accuracy. In the last line of
    the previous code sample, we excluded `NA` values when computing the out-of-bag
    accuracy. This is important because it is possible that a particular observation
    may appear in all the bootstrap samples and therefore will never be available
    for an OOB prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Equally, our fix for the rare level of the ECG factor means that even if an
    observation is not selected by the sampling process, we may still not be able
    to make a prediction for it. The reader should verify that only one observation
    happens to produce an `NA` value because of the combination of the two phenomena
    just described.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ll repeat this process a third time using the `heart_test` data
    frame to obtain the test set accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy on the test set seems lower than what we found without a bagged
    model. This is not necessarily bad news for us, since the test set is very small.
    In fact, the difference between the performance of this bagged model and the original
    model trained in [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*, is 32/40 compared to 36/40,
    which is to say it is only worse by four observations in 40.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world situation, we generally want to have a much larger test set
    to estimate our unseen accuracy. In fact, because of this, we are more inclined
    to believe our OOB accuracy measurement, which is done over a larger number of
    observations and averaged over many models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging is actually very useful for us in this scenario as it gives us a model
    for which we can have a better estimate of the test accuracy, using the OOB observations
    because the test set is so small. As a final demonstration, we run the previous
    code a number of times with different values of *M* and store the results in a
    data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This table shows us that the test accuracy fluctuates around 80 percent. This
    isn't that surprising given the small size of our test set of only 40 observations.
    For the training accuracy, we see that we are fluctuating around 91 percent. The
    OOB accuracy, which is far more stable as an accuracy measure, shows us that the
    expected performance of the model is around 85 percent. As the number of models
    increases, we don't see much of an improvement over 11 models, though for most
    real-world datasets, we would usually see some improvement before tapering off.
  prefs: []
  type: TYPE_NORMAL
- en: Although our example focused exclusively on bagging for classification problems,
    the move to regression problems is relatively straightforward. Instead of using
    majority votes for a particular observation, we use the average value of the target
    function predicted by the individual models. Bagging is not always guaranteed
    to provide a performance improvement on a model. For starters, we should note
    that it makes sense to use bagging only when we have a nonlinear model. As the
    bagging process is performing an average (a linear operation) over the models
    generated, we will not see any improvements with linear regression, for example,
    because we aren't increasing the expressive power of our model. The next section
    talks about some other limitations of bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on bagging, consult the original paper of *Leo Breiman*
    titled *Bagging Predictors*, published in 1996 in the journal *Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've only explored the upside of using bagging, but in some cases it
    may turn out not to be a good idea. Bagging involves taking the average across
    predictions made by several models, which are trained on bootstrapped samples
    of the training data. This averaging process smoothens the overall output, which
    may reduce bias when the target function is smooth. Unfortunately, if the target
    function is not smooth, we may actually introduce bias by using bagging.
  prefs: []
  type: TYPE_NORMAL
- en: Another way that bagging introduces bias is when one of the output classes is
    very rare. Under those circumstances, the majority voting system tends to be biased
    towards the more common class. Other problems may arise in relation to the sampling
    process itself. As we have already learned, when some categorical features include
    values that are rare, these may not appear at all in some of the bootstrap samples.
    When this happens, the models built for these samples will be unable to make a
    prediction when they encounter this new feature level in their test set.
  prefs: []
  type: TYPE_NORMAL
- en: High leverage points, which are highly influential in determining the model's
    output function compared to other points, can also be a problem. If a bootstrap
    sample is drawn that does not include one or more high leverage points, the resulting
    trained model will be quite different compared to when they are included. Therefore,
    bagging performance depends on how often these particular observations are sampled
    in order to win the majority vote. Due to this fact, our ensemble model will have
    a high variance in the presence of high leverage points. For a given dataset,
    we can often predict if we are in this situation by looking for outliers and highly
    skewed features.
  prefs: []
  type: TYPE_NORMAL
- en: We must also remember that the different models we build are not truly independent
    of each other in the strict sense because they still use the same set of input
    features. The averaging process would have been more effective if the models were
    independent. Also, bagging does not help when the type of model that we are using
    predicts a functional form that is very far from the true form of the target function.
    When this happens, training multiple models of this type merely reproduces the
    systematic errors across the different models. Put differently, bagging works
    better when we have low bias and high variance models as the averaging process
    is primarily designed to reduce the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, and this applies to ensemble models in general, we tend to lose the
    explanative power of our model. We saw an example of explanative power in linear
    regression where each model parameter (regression coefficient) corresponded to
    the amount of change in the output, for a unit increase in the corresponding feature.
    Decision trees are another example of a model with high explanatory power. Using
    bagging loses this benefit because of the majority voting process and so we cannot
    directly relate our inputs to the predicted output.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Boosting** offers an alternative take on the problem of how to combine models
    together to achieve greater performance. In particular, it is especially suited
    to **weak learners**. Weak learners are models that produce an accuracy that is
    better than a model that randomly guesses, but not by much. One way to create
    a weak learner is to use a model whose complexity is configurable.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can train a multilayer perceptron network with a very small
    number of hidden layer neurons. Similarly, we can train a decision tree, but only
    allow the tree to comprise a single node, resulting in a single split in the input
    data. This special type of decision tree is known as a **stump**.
  prefs: []
  type: TYPE_NORMAL
- en: When we looked at bagging, the key idea was to take a set of random bootstrapped
    samples of the training data and then train multiple versions of the same model
    using these different samples. In the classical boosting scenario, there is no
    random component, as all the models use all of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: For classification, boosting works by building a model on the training data
    and then measuring the classification accuracy on that training data. The individual
    observations that were misclassified by the model are given a larger weight than
    those that were correctly classified, and then the model is retrained again using
    these new weights. This is then repeated multiple times, each time adjusting the
    weights of individual observations based on whether they were correctly classified
    or not in the last iteration.
  prefs: []
  type: TYPE_NORMAL
- en: To combat overfitting, the ensemble classifier is built as a weighted average
    of all the models trained in this sequence, with the weights usually being proportional
    to the classification accuracy of each individual model. As we are using the entire
    training data, there are no OOB observations, so the accuracy in each case is
    measured using the training data itself. Regression with boosting is usually done
    by adjusting the weights of observation based on some measure of the distance
    between the predicted value and the labeled value.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Continuing our focus on classification problems, we now introduce **AdaBoost**,
    which is short for **adaptive boosting**. In particular, we will focus on **Discrete
    AdaBoost**, as it makes predictions on binary classes. We will use `-1` and `1`
    as the class labels. **Real AdaBoost** is an extension of AdaBoost, in which the
    outputs are the class probabilities. In our version of AdaBoost, all of the training
    data is used; however, there are other versions of AdaBoost in which the training
    data is also sampled. There are also multiclass extensions of AdaBoost as well
    as extensions that are suited to regression-type problems.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost for binary classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following outlines the specifics – inputs, outputs, and methods used by
    AdaBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*data*: The input data frame containing the input features and a column with
    the binary output label'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M*: An integer, representing the number of models that we want to train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '*models*: A series of *Μ* trained models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*alphas*: A vector of *M* model weights'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method**:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a vector of observation weights, *w*, of length *n* with entries
    *w[i]* *= 1/n*. This vector will be updated in every iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the current value of the observation weights and all the data in the training
    set, train a classifier model *G[m]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the weighted error rate as the sum of all misclassified observations
    multiplied by their observation weights, divided by the sum of the weight vector.
    Following our usual convention of using *x[i]* as an observation and *y[i]* as
    its label, we can express this using the following equation:![AdaBoost for binary
    classification](img/00160.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then set the model weight for this model, *a[m]*, as the logarithm of the
    ratio between the accuracy and error rates. In a formula, this is:![AdaBoost for
    binary classification](img/00161.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then update the observation weights vector, *w*, for the next iteration.
    Incorrectly classified observations have their weight multiplied by ![AdaBoost
    for binary classification](img/00162.jpeg), thereby increasing their weight for
    the next iteration. Correctly classified observations have their weight multiplied
    by ![AdaBoost for binary classification](img/00163.jpeg), thereby reducing their
    weight for the next iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Renormalize the weights vector so that the sum of the weights is 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps two through six *M* times in order to produce *M* models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define our ensemble classifier as the sign of the weighted sum of the outputs
    of all the boosted models:![AdaBoost for binary classification](img/00164.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Predicting atmospheric gamma ray radiation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to study boosting in action, in this section we'll introduce a new
    prediction problem from the field of atmospheric physics. More specifically, we
    will analyze the patterns made by radiation on a telescope camera in order to
    predict whether a particular pattern came from gamma rays leaking into the atmosphere,
    or from regular background radiation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gamma rays leave distinctive elliptical patterns and so we can create a set
    of features to describe these. The dataset we will use is the *MAGIC Gamma Telescope
    dataset*, hosted by the *UCI Machine Learning* repository at [http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope).
    Our data consists of 19,020 observations of the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column name | Type | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `FLENGTH` | Numerical | The major axis of the ellipse (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FWIDTH` | Numerical | The minor axis of the ellipse (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FSIZE` | Numerical | Logarithm to the base ten of the sum of the content
    of all pixels in the camera photo |'
  prefs: []
  type: TYPE_TB
- en: '| `FCONC` | Numerical | Ratio of the sum of the two highest pixels over `FSIZE`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `FCONC1` | Numerical | Ratio of the highest pixel over `FSIZE` |'
  prefs: []
  type: TYPE_TB
- en: '| `FASYM` | Numerical | Distance from the highest pixel to the center, projected
    onto the major axis (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FM3LONG` | Numerical | Third root of the third moment along the major axis
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FM3TRANS` | Numerical | Third root of the third moment along the minor axis
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FALPHA` | Numerical | Angle of the major axis with the vector to the origin
    (degrees) |'
  prefs: []
  type: TYPE_TB
- en: '| `FDIST` | Numerical | Distance from the origin to the center of the ellipse
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `CLASS` | Binary | Gamma rays (*g*) or Background Hadron Radiation (*b*)
    |'
  prefs: []
  type: TYPE_TB
- en: 'First, we will load the data into a data frame called `magic`, recoding the
    `CLASS` output variable to use classes `1` and `-1` for gamma rays and background
    radiation, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll split our data frame into a training data and a test data frame
    using our typical 80-20 split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The model that we are going to use with boosting is a simple multilayer perceptron
    with a single hidden layer. Harkening back to [Chapter 4](part0035_split_000.html#11C3M1-c6198d576bbb4f42b630392bd61137d7
    "Chapter 4. Generalized Linear Models"), *Neural Networks*, we know that the `nnet`
    package is perfect for this task. With neural networks, we often get superior
    accuracy when we normalize the inputs and so before training any models, we will
    carry out this preprocessing step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Boosting is designed to work best with weak learners, and for this reason,
    we are going to use a very small number of hidden neurons in our hidden layer.
    Concretely, we will begin with the simplest possible multilayer perceptron that
    uses a single hidden neuron. To understand the effect of using boosting, we will
    establish our baseline performance by training a single neural network and measuring
    its accuracy. We can do this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This establishes that we have a baseline accuracy of around 79.5 percent. This
    isn''t too bad, but we are going to use boosting to see if we can improve it.
    To that end, we are going to write our own function, `AdaBoostNN()`, which will
    take as input a data frame, the name of the output variable, the number of single
    hidden layer neural network models we want to build, and finally the number of
    hidden units these neural networks will have. This function will then implement
    the AdaBoost algorithm that we previously described and finally return a list
    of models and their weights. Here is the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Before proceeding, we will work our way through the function to understand what
    each line is doing. We first initialize empty lists of models and model weights
    (`alphas`). We also compute the number of observations in our training data, storing
    this in the variable `n`. The name of the output column provided is then used
    to create a formula that describes the neural network that we will build.
  prefs: []
  type: TYPE_NORMAL
- en: In our dataset, this formula will be `CLASS ~ .`, which means that the neural
    network will compute `CLASS` as a function of all the other columns as input features.
    We then initialize our weights vector, as we did in *step 1* of AdaBoost, and
    define our loop that will run for *M* iterations in order to build *M* models.
  prefs: []
  type: TYPE_NORMAL
- en: In every iteration, the first step is to use the current setting of the weights
    vector to train a neural network using as many hidden units as specified in the
    input, `hidden_units`. We then compute a vector of predictions that this model
    generates on the training data using the `predict()` function. By comparing these
    predictions to the output column of the training data, we calculate the errors
    that the current model makes on the training data. This then allows us to compute
    the error rate. According to *step 4* of the AdaBoost algorithm, this error rate
    is set as the weight of the current model. Finally, the observation weights to
    be used in the next iteration of the loop are updated according to whether each
    observation was correctly classified using *step 5* of the AdaBoost algorithm.
    The weight vector is then normalized and we are ready to begin the next iteration.
    After completing *M* iterations, we output a list of models and their corresponding
    model weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a function that is able to train our ensemble classifier using
    AdaBoost, but we also need a function to make predictions. This function will
    take in the output list produced by our training function, `AdaBoostNN()`, along
    with a test dataset. We''ve called this function `AdaBoostNN.predict()` and it
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In this function, we first extract the models and the model weights from the
    list produced by our previous function. We then create a matrix of predictions,
    where each column corresponds to the vector of predictions made by a particular
    model. Thus, we will have as many columns in this matrix as models that we used
    for boosting.
  prefs: []
  type: TYPE_NORMAL
- en: We then multiply the predictions produced by each model with their corresponding
    model weight. For example, every prediction from the first model is in the first
    column of the prediction matrix and will have its value multiplied by the first
    model weight *α1*. Finally, in the last step, we reduce our matrix of weighted
    observations into a single vector of observations by summing the weighted predictions
    for each observation and taking the sign of the result. This vector of predictions
    is then returned by our function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an experiment, we will train 10 neural network models with a single hidden
    unit and see if boosting improves accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Boosting 10 models seems to give us a marginal improvement in accuracy, but
    perhaps training more models might make more of a difference. We are also interested
    in the relationship between the complexity of our weak learner, as measured by
    the number of hidden neurons, and the performance benefits we can expect from
    boosting on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows the results of experimenting with our functions using
    different numbers of models as well as hidden neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Predicting atmospheric gamma ray radiation](img/00165.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: For the neural networks with one hidden unit, as the number of boosting models
    increases, we see an improvement in accuracy, but after 100 models, this tapers
    off and is actually slightly less for 200 models. The improvement over the baseline
    of a single model is substantial for these networks. When we increase the complexity
    of our learner by having a hidden layer with three hidden neurons, we get a much
    smaller improvement in performance. At 200 models, both ensembles perform at a
    similar level, indicating that at this point, our accuracy is being limited by
    the type of model we are training.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The original AdaBoost algorithm was presented by *Freund* and *Schapire* in
    the journal of *Computer and System Sciences* in a 1997 paper titled *A Decision-theoretic
    generalization of on-line learning and an application to boosting*. This is a
    good place to start learning more about AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting complex skill learning with boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will revisit our Skillcraft dataset in this section--this time in the context
    of another boosting technique known as **stochastic gradient boosting**. The main
    characteristic of this method is that in every iteration of boosting, we compute
    a gradient in the direction of the errors that are made by the model trained in
    the current iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gradient is then used in order to guide the construction of the model
    that will be added in the next iteration. Stochastic gradient boosting is commonly
    used with decision trees, and a good implementation in R can be found in the `gbm`
    package, which provides us with the `gbm()` function. For regression problems,
    we need to specify the `distribution` parameter to be `gaussian`. In addition,
    we can specify the number of trees we want to build (which is equivalent to the
    number of iterations of boosting) via the `n.trees` parameter, as well as a `shrinkage`
    parameter that is used to control the algorithm''s learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To learn more about how stochastic gradient boosting works, a good source to
    consult is the paper titled *Stochastic Gradient Boosting*. This was written by
    *Jerome H. Friedman* and appears in the February 2002 issue of the journal *Computational
    Statistics & Data Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to make predictions with this setup, we need to use the `gbm.perf()`
    function, whose job it is to take the boosted model we built and pick out the
    optimal number of boosting iterations. We can then provide this to our `predict()`
    function in order to make predictions on our test data. To measure the SSE on
    our test set, we will use the `compute_SSE()` function that we wrote in [Chapter
    6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7 "Chapter 6. Support
    Vector Machines"), *Tree-based Methods*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: A bit of experimentation has revealed that we can't get substantially better
    results than this by allowing the algorithm to iterate over more trees. Despite
    this, we are already performing better using this method than both the single
    and bagged tree classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting is a very powerful technique that continues to receive a lot of attention
    and research, but it is not without its limitations. Boosting relies on combining
    weak learners together. In particular, we can expect to get the most out of boosting
    when the models that are used are not already complex models themselves. We already
    saw an example of this with neural networks, by noting that the more complex architecture
    of three hidden neurons gives a better learner to begin with than the simpler
    architecture of a single hidden neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Combining weak learners may be a way to reduce overfitting, but this is not
    always effective. By default, boosting uses all of its training data and progressively
    tries to correct mistakes that it makes without any penalizing or shrinkage criterion
    (although the individual models trained may themselves be regularized). Consequently,
    boosting can sometimes overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, a very important limitation is that many boosting algorithms have a
    symmetric loss function. Specifically, there is no distinction that is made in
    classification between a false positive classification error and a false negative
    classification error. Every type of error is treated the same when the observation
    weights are updated.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, this might not be desirable, in that one of the two errors may
    be more costly. For example, on the website for our **Major Atmospheric Gamma
    Imaging Cherenkov** (**MAGIC**) telescope dataset, the authors state that a false
    positive of detecting gamma rays where there are none, is worse than a false negative
    of misclassifying gamma rays as background radiation. Cost-sensitive extensions
    of boosting algorithms have been proposed, however.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final ensemble model that we will discuss in this chapter is unique to tree-based
    models and is known as the **random forest**. In a nutshell, the idea behind random
    forests stems from an observation on bagging trees. Let's suppose that the actual
    relationship between the features and the target variable can be adequately described
    with a tree structure. It is quite likely that during bagging with moderately
    sized bootstrapped samples, we will keep picking the same features to split on
    high up in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in our Skillcraft dataset, we expect to see `APM` as the feature
    that will be chosen at the top of most of the bagged trees. This is a form of
    tree correlation that essentially impedes our ability to derive the variance reduction
    benefits from bagging. Put differently, the different tree models that we build
    are not truly independent of each other because they will have many features and
    split points in common. Consequently, the averaging process at the end will be
    less successful in reducing the ensemble variance.
  prefs: []
  type: TYPE_NORMAL
- en: To counteract this effect, the random forest algorithm introduces an element
    of randomization in the tree construction process. Just as with bagging, random
    forests involve building a number of trees with bootstrapped samples and using
    the average of their predictions to form the ensemble prediction. When we construct
    individual trees, however, the random forest algorithm imposes a constraint.
  prefs: []
  type: TYPE_NORMAL
- en: At each node in the tree, we draw a random sample of size *mtry* from the total
    number of input features. Whereas in regular tree construction, we consider all
    the features at each node to determine which one to split on, with random forests,
    we only consider features from the sample we created for that node. We can often
    use a relatively small number for *mtry*.
  prefs: []
  type: TYPE_NORMAL
- en: The number of trees we build, in combination with the fact that each tree has
    several nodes, is often enough to ensure that the more important features are
    sampled a sufficient number of times. Various heuristics have been proposed for
    choosing appropriate values for this parameter, such as one third or the square
    root of the total number of features available.
  prefs: []
  type: TYPE_NORMAL
- en: This sampling step effectively forces the structure of the bagged trees to be
    different from each other and offers a number of different benefits. Feature sampling
    allows us to consider input features that are successful in splitting the data
    for only a small range of the target variable. These locally relevant features
    are rarely chosen without the sampling constraint because we usually prefer features
    that form good overall splits of the data at a given node in the tree. Nonetheless,
    we may want to include these features in our model if we don't want to overlook
    local variations in the output.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, sampling input features is useful when we have correlated input features.
    Regular tree construction tends to favor only one of the features from a correlated
    set while ignoring the rest despite the fact that the resulting splits from even
    highly correlated features are not exactly the same. When we sample input features
    we are less likely to have correlated features compete with each other and so
    we can choose a wider range of features to use with our model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the randomized nature of the sampling process is designed to combat
    overfitting because we can think of this process as applying regularization on
    the impact of each input feature. Overfitting can still be a problem if we happen
    to have too many input features unrelated to the target variable compared to those
    that are related, but this is a fairly rare scenario. Random forests in general
    scale quite favorably with the number of input features, precisely because of
    this sampling process that doesn't require us to consider all the features when
    splitting at each node. In particular, this model is a good choice when the number
    of features exceeds the number of observations. Finally, the sampling process
    mitigates the cost of constructing a large number of trees again because we consider
    a subset of input features when deciding on how to split at each node. The number
    of trees is another tuning parameter that we must decide on in a random forest
    model; it is very common to build anywhere between several hundred and a few thousand
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'In R, we can use the `randomForest` package in order to train random forest
    models. The `randomForest()` function takes in a formula and a training data frame,
    as well as a number of other optional parameters. Of particular interest is the
    `ntree` parameter, which controls the number of trees that will be built for the
    ensemble, and the `mtry` parameter, which is the number of features sampled for
    use at each node for splitting. These parameters should be tuned by trying out
    different configurations, and we can use the `tune()` function from the `e1071`
    package to do just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The results show that on this dataset, the best parameter combination is to
    train 1,000 trees and use a value of 6 for `mtry`. This last value corresponds
    to one third of the number of input features, which is the typical heuristic for
    regression problems. The SSE value on our test set is almost identical to what
    we obtained using gradient boosting.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of variables in random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already discussed the fact that ensemble models do not, in general, have
    explanative power. For random forests, it turns out that we can still measure
    variable importance scores for the different input features by tallying and keeping
    track of the reductions in our error function across all the trees in the ensemble.
    In this way, we can obtain an analogous plot to the one we obtained for a single
    tree when we looked at this dataset in [Chapter 6](part0055_split_000.html#1KEEU2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 6. Support Vector Machines"), *Tree-based Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute variable importance, we use the `importance()` function and plot
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The importance of variables in random forests](img/00166.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Looking at this plot, we can see that `APM` and `ActionLatency` are once again
    the most important features, but their order is reversed. We also see that `TotalHours`
    is now third in importance, significantly higher than what we saw before in a
    single tree.
  prefs: []
  type: TYPE_NORMAL
- en: We have explored the Skillcraft dataset using a number of different methods,
    but each time we treated this as a regression problem and measured our accuracy
    using the SSE. Our target variable is the league index, which tells us the gaming
    league in which a player competes. As such, it is actually an ordered factor.
  prefs: []
  type: TYPE_NORMAL
- en: As we've already seen before, models whose output is an ordered factor can be
    tricky to train as well as assess. For example, perhaps a more appropriate method
    of assessing our model would be to first round our model's numerical output so
    that we obtain a prediction on an actual player league. Then, we could assess
    the model using a weighted classification error rate that more heavily penalizes
    a predicted league index that is very far from the actual league index. We leave
    this as an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: One of the issues that we often face when we model the problem as a regression
    problem is that we have no way to force the output to predict across the full
    range of the original levels. In our particular dataset, for example, we might
    never predict the lowest or highest league. For some suggestions on alternative
    ways to model ordered factors with regression trees, there is an insightful paper
    published in 2000 by *Kramer* and others, titled *Prediction of Ordinal Classes
    Using Regression Trees*. This appears in the *34th* issue of *Fundamentals Informaticae*
    by *IOS Press*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For random forests, the original reference is a 2001 paper by *Leo Breiman*,
    titled *Random Forests*, published in the journal *Machine Learning*. Besides
    this reference, a fantastic chapter with numerous examples appears in the book
    *Statistical Learning from a Regression Perspective*, *Richard A. Derk*, *Springer*.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along the same vein as AdaBoost, and with yet another twist on gradient boosting,
    is **Extreme Gradient Boosting** (**XGBoost**). XGBoost is a library of functions
    designed and optimized specifically for boosting trees algorithms. It is a generally
    advanced tool kit that yields impressive results, but does takes some time to
    understand.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is based upon the quite well known gradient boosting framework, but
    it is more efficient. Specifically, XGBoost leverages system optimization concepts
    such as out of core computations, parallelization, cache optimization, and distributed
    computing to create a faster and more flexible tool for learning tree ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Basically, XGBoost is built to perform the sequential process of *boosting*
    by utilizing all cores of a machine as it recursively divides data into parts,
    retaining the first part to be used as the test data, then reintegrating the first
    part to the dataset and retaining the second part, do a training and repeat, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, XGBoost has many parameters that can be customized and is extendable
    and therefore is much more flexible. Other advantages offered by XGBoost include
    regularization, customizable parameters, deeper tree pruning, and built-in cross
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we deviated from our usual pattern of learning a new type of
    model and instead focused on techniques to build ensembles of models that we have
    seen before. We discovered that there are numerous ways to combine models in a
    meaningful way, each with its own advantages and limitations. Our first technique
    for building ensemble models was bagging. The central idea behind bagging is that
    we build multiple versions of the same model using bootstrap samples of the training
    data. We then average the predictions made by these models in order to construct
    our overall prediction. By building many different versions of the model we can
    smooth out errors made due to overfitting and end up with a model that has reduced
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: A different approach to building model ensembles uses all of the training data
    and is known as boosting. Here, the defining characteristic is to train a sequence
    of models, but each time we weigh each observation with a different weight depending
    on whether we classified that observation correctly in the previous model. There
    are many variants of boosting and we presented two of the most well-known algorithms,
    AdaBoost and stochastic gradient boosting (as well as mentioning perhaps the newer,
    more efficient tree learner, XGBoost). The averaging process that operates over
    the predictions made by individual models to compute the final prediction often
    weighs each model by its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional texts that present bagging and boosting introduce them in the context
    of decision trees. There is good reason for this, as the decision tree is the
    prototypical model for which bagging and boosting have been applied. Boosting
    in particular works best on models that are weak learners and decision trees can
    easily be made into weak learners by significantly restricting their size and
    complexity during construction.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, however, this often leaves the reader with a view that ensemble
    methods only work for decision trees, or without any experience in how they can
    be applied to other methods. In this chapter, we emphasized how these techniques
    are general and how they can be used with a number of different types of models.
    Consequently, we applied these techniques to models that we have seen before,
    such as neural networks and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: The final type of ensemble model that we studied was the random forest. This
    is a very popular and powerful algorithm based on bagging decision trees. The
    key breakthrough behind this model is the use of an input feature sampling procedure,
    which limits the choice of features that are available to split on during the
    construction of each tree. In doing this, the model reduces the correlation between
    trees, captures significant localized variations in the output, and improves the
    degree of variance reduction in the final result. Another key benefit of this
    model is that it scales well with a larger number of input features. For our real-world
    Skillcraft dataset, we discovered that random forests and stochastic gradient
    boosting produced the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce another type of model with a distinct
    structure known as the probabilistic graphical model. These models use a graphical
    structure in order to explicitly represent the conditional independence between
    input features. Probabilistic graphical models find applications across a wide
    variety of predictive tasks from spam email identification to DNA sequence labeling.
  prefs: []
  type: TYPE_NORMAL
