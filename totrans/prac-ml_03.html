<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;An Introduction to Hadoop's Architecture and Ecosystem"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. An Introduction to Hadoop's Architecture and Ecosystem</h1></div></div></div><p>From this chapter onwards, we start with the implementation aspects of Machine learning. Let's start learning the platform of choice—a platform that can scale to Advanced Enterprise Data needs (big data needs of Machine learning in specific)—Hadoop.</p><p>In this chapter, we cover Hadoop platform and its capabilities in addressing large-scale loading, storage, and processing challenges for Machine learning. In addition to an overview of Hadoop Architecture, its core frameworks, and the other supporting ecosystem components, also included here is a detailed installation process with an example deployment approach. Though there are many commercial distributions of Hadoop, our focus in this chapter is to cover the open source, Apache distribution of Hadoop (latest version 2.x).</p><p>In this chapter, the following topics are covered in-depth:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An introduction to Apache Hadoop, its evolution history, the core concepts, and the ecosystem frameworks that comprise Hadoop</li><li class="listitem" style="list-style-type: disc">Hadoop distributions and specific offerings</li><li class="listitem" style="list-style-type: disc">Installation and set up of the Hadoop environment</li><li class="listitem" style="list-style-type: disc">Hadoop 2.0—HDFS and MapReduce (also<a id="id299" class="indexterm"/> <span class="strong"><strong>YARN</strong></span> (<span class="strong"><strong>Yet Another Resource Negotiator</strong></span>)) architectures with example implementation scenarios, using different components of the architecture</li><li class="listitem" style="list-style-type: disc">Understanding the purpose of the core ecosystem components, setting up and learning to build and run the programs using examples</li><li class="listitem" style="list-style-type: disc">Exploring Machine learning specific ecosystem extensions such as Mahout and R Connectors (<a class="link" href="ch04.html" title="Chapter 4. Machine Learning Tools, Libraries, and Frameworks">Chapter 4</a>, <span class="emphasis"><em>Machine Learning Tools, Libraries, and Frameworks</em></span>, covers the implementation details)</li></ul></div><div class="section" title="Introduction to Apache Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Introduction to Apache Hadoop</h1></div></div></div><p>Apache Hadoop<a id="id300" class="indexterm"/> is an open source, Java-based project from the Apache Software Foundation. The core purpose of this software has been to provide a platform that is scalable, extensible, and fault tolerant for the distributed storage and processing of big data. Please refer to <a class="link" href="ch02.html" title="Chapter 2. Machine learning and Large-scale datasets">Chapter 2</a>, <span class="emphasis"><em>Machine learning and Large-scale Datasets</em></span> for more information on what data qualifies as big data. The following image is the standard logo of Hadoop:</p><div class="mediaobject"><img src="graphics/B03980_03_01.jpg" alt="Introduction to Apache Hadoop"/></div><p>At the heart of it, it leverages clusters of nodes that can be commodity servers and facilitates parallel processing. The name Hadoop was given by its creator Doug Cutting, naming it after his child's yellow stuffed toy elephant. Till date, Yahoo! has been the largest contributor and an extensive user of Hadoop. More details of Hadoop, its architecture, and download links are <a id="id301" class="indexterm"/>accessible at <a class="ulink" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a>.</p><p>Hadoop is an industry standard platform for big data, and it comes with extensive support for all the popular Machine learning tools in the market. This platform is now used by several big firms such as Microsoft, Google, Yahoo!, and IBM. It is also used to address specific Machine learning requirements like sentiment analysis, search engines, and so on.</p><p>The following sections cover some key characteristics of the Hadoop platform that make it ideal for facilitating efficiency in large-scale data storage and processing capabilities.</p><div class="section" title="Evolution of Hadoop (the platform of choice)"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/>Evolution of Hadoop (the platform of choice)</h2></div></div></div><p>The following<a id="id302" class="indexterm"/> figure (Source Cloudera Inc.) explains the evolution of the Hadoop platform. With Doug Cutting and Mike Cafarella starting it all in 2002 to build a greatly scalable search engine that is open source and hence extensible and running over a bunch of machines. Some important milestones in this evolution phase have been by Google that released the <span class="strong"><strong>Google File System</strong></span> (<span class="strong"><strong>GFS</strong></span>) <a id="id303" class="indexterm"/>in October 2003, followed by the MapReduce framework in December 2004 that evolved to form the core frameworks HDFS and MapReduce/YARN respectively.</p><p>The other significant milestone had been Yahoo's contribution and adoption around February 2008 when <a id="id304" class="indexterm"/>Yahoo implemented a production version that had indexing of searches implemented over 10,000 Hadoop cluster nodes. The following table depicts the evolution of Hadoop:</p><div class="mediaobject"><img src="graphics/B03980_03_02.jpg" alt="Evolution of Hadoop (the platform of choice)"/></div></div><div class="section" title="Hadoop and its core elements"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/>Hadoop and its core elements</h2></div></div></div><p>The following concept map <a id="id305" class="indexterm"/>depicts<a id="id306" class="indexterm"/> the core elements and aspects of the Hadoop platform:</p><div class="mediaobject"><img src="graphics/B03980_03_03.jpg" alt="Hadoop and its core elements"/></div></div></div></div>
<div class="section" title="Machine learning solution architecture for big data (employing Hadoop)"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Machine learning solution architecture for big data (employing Hadoop)</h1></div></div></div><p>In this section, let us look <a id="id307" class="indexterm"/>at the essential architecture components for implementing a Machine learning solution considering big data requirements.</p><p>The proposed solution architecture should support the consumption of a variety of data sources in an efficient and cost-effective way. The following figure summarizes the core architecture components that should potentially be a part of the Machine learning solution technology stack. The choice of frameworks can either be open source or packaged license options. In the context of this book, we consider the latest version of open source (Apache) distribution of Hadoop and its ecosystem components.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Vendor specific frameworks and extensions are out of scope for this chapter.</p></div></div><div class="mediaobject"><img src="graphics/B03980_03_04.jpg" alt="Machine learning solution architecture for big data (employing Hadoop)"/></div><p>In the next sections, we'll<a id="id308" class="indexterm"/> discuss in detail each of these Reference Architecture layers and the required frameworks in each layer.</p><div class="section" title="The Data Source layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/>The Data Source layer</h2></div></div></div><p>The <a id="id309" class="indexterm"/>Data Source layer forms a<a id="id310" class="indexterm"/> critical part of the Machine learning Reference Architecture. There are many internal and external data feeds that form an input to solving a Machine learning problem. These feeds can be structured, unstructured, or semi-structured in nature. Moreover, in real-time, batch, or near real time mode, they need to be seamlessly integrated and consolidated for analytics engines and visualization tools.</p><p>Before ingesting this data into the system for further processing, it is important to remove the irrelevance or the noise in the data. Some unique techniques can be applied to clean and filter the data.</p><p>These consolidated datasets are also called data lakes in big data and the data aggregation context. Hadoop is one of the storage options of choice for Data Lakes.</p><p>The following diagram shows the variety of data sources that form a primary source of input.</p><div class="mediaobject"><img src="graphics/B03980_03_05.jpg" alt="The Data Source layer"/></div><p>Data architectures have always been designed to support some of the protocols such as JMS, HTTP, XML, and so on. However, now, the recent advancements in the field of big data have brought about significant changes. So now, the new age data sources include data streams from social networking sites, GPS data, machine-generated data such as user access logs, and other proprietary data formats.</p></div><div class="section" title="The Ingestion layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/>The Ingestion layer</h2></div></div></div><p>The <a id="id311" class="indexterm"/>Data Ingestion layer is responsible for <a id="id312" class="indexterm"/>bringing data in from multiple data sources into the system, with a primary responsibility to ensure data quality. This layer has the capability to filter, transform, integrate, and validate data. It is important that the choice of technology to implement this layer should be able to support high volumes and other characteristics of data. The following meta model shows the composition and flow of functions of the Ingestion Layer. An ingestion layer could potentially be an <span class="strong"><strong>ETL</strong></span> short for (<span class="strong"><strong>Extract, Transform, and Load</strong></span>) capability<a id="id313" class="indexterm"/> in the architecture.</p><p>Listed below are a set of basic requirements for an ingestion layer:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">High-speed transformation of data from any source system in any manner</li><li class="listitem" style="list-style-type: disc">Processing large volumes of records in minimal time</li><li class="listitem" style="list-style-type: disc">Producing output in a semantically rich format so that any target system can query for<a id="id314" class="indexterm"/> <span class="strong"><strong>smart data</strong></span></li></ul></div><p>The<a id="id315" class="indexterm"/> architecture framework for<a id="id316" class="indexterm"/> ingestion layer needs to provide the following capabilities; the upcoming model depicts various layers and compositions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An Adapter Framework—any product group or application should be able to use the Adapter Framework to quickly, reliably, and programmatically develop connectors to different data sources (Files, CSV format, and DB)</li><li class="listitem" style="list-style-type: disc">A high speed, parallel transformation execution engine</li><li class="listitem" style="list-style-type: disc">A job execution framework</li><li class="listitem" style="list-style-type: disc">Semantified output generator framework<div class="mediaobject"><img src="graphics/B03980_03_06.jpg" alt="The Ingestion layer"/></div></li></ul></div><p>The<a id="id317" class="indexterm"/> Ingestion layer loads the relevant <a id="id318" class="indexterm"/>data into the storage layer, which in our current context is the Hadoop Storage Layer that is primarily a file-based storage layer.</p><p>The concept map below lists ingestion core patterns (these patterns address performance and scalability needs of a Machine learning architecture):</p><div class="mediaobject"><img src="graphics/B03980_03_07.jpg" alt="The Ingestion layer"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Parallel Processing and Partitioning Patterns</strong></span>: The fundamental architecture for <a id="id319" class="indexterm"/>handling large <a id="id320" class="indexterm"/>volume ingestion requirements is to parallelize the execution. Running transformations on different input data in parallel and partitioning a single, large volume input into smaller batches for processing in parallel helps achieve parallelization.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pipeline design patterns</strong></span>: When<a id="id321" class="indexterm"/> designing the workflows for ingestion jobs, there are specific issues that need to be addressed, such as avoiding large sequential pipelines that enable parallel processing. Similarly, from the data reliability point of view, creating appropriate audit and execution logs is important to manage the entire ingestion execution.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transformation patterns</strong></span>: There are<a id="id322" class="indexterm"/> different categories of transformation. One of the main aspects of the transformation designs is to handle dependency. The patterns mentioned in the first category (parallelization) also handle dependency requirements. Other issues relate to the dependency on the past and historical data, which is especially significant when processing additional loads.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Storage Design</strong></span>: When<a id="id323" class="indexterm"/> loading data into the target data store, there are issues such as recovering from failed transformations or reloading data for specific feeds (for example, when there should be a fixed transformation rule).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data Load patterns</strong></span>: One of the<a id="id324" class="indexterm"/> biggest performance bottlenecks in data ingestion is the speed of loading data into the target data mart. Especially when the target is an <a id="id325" class="indexterm"/>RDBMS, parallelization strategies lead to concurrency issues while loading the data, limiting the throughput of the ingestion that is possible. The patterns present certain techniques of how to realize the data load and address performance and concurrency issues while loading data.</li></ul></div></div><div class="section" title="The Hadoop Storage layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec45"/>The Hadoop Storage layer</h2></div></div></div><p>Machine<a id="id326" class="indexterm"/> learning architecture has a<a id="id327" class="indexterm"/> distributed storage layer that supports parallel processing for running analytics or heavy computations over big data. The usage of distributed storage and processing large volumes in parallel is a fundamental change in the way an enterprise handles big data.</p><p>A typical distributed storage facilitates high performance by parallel processing the algorithms that run over petabyte scale data with fault-tolerance, reliability, and parallel processing capabilities.</p><p>In the current context of Hadoop architecture, <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>) is the core storage <a id="id328" class="indexterm"/>mechanism. In this section, let us have a brief look at HDFS and NoSQL (Not-only-SQL) storage options. The following sections cover HDFS and its architecture in more detail.</p><p>HDFS is one of the core components and acts as a database for Hadoop. It is a distributed file system that stores large-scale data across a cluster of nodes. It comes with a framework to ensure data reliability and fault tolerance. Applications can store files in parts or whole depending on the size, and it facilitates the write once read many times.</p><p>Since HDFS is a file system, access to data for consumption or manipulation is not simple and requires some complex file operation programs. Another way of bringing in easier data management is by using non-relational stores called NoSQL stores.</p><p>The following model represents various NoSQL data store categories that are available with examples for each of the categories. Every data store category caters to a particular business requirement, and it is important to understand the purpose of each of these categories of NoSQL store to make the right choice for a given requirement. The CAP theorem (that stands for consistency, availability, and partition tolerance) attributes are satisfied to a varying degree for each of the NoSQL stores, resulting in support for optimized storage systems that are expected to work for combinations of these attributes. In reality, these NoSQL stores may have to coexist with relational stores as they would need a system of record to sync up on a need basis, or a better case is where we would need to use a combination of relational and non-relational data.</p><p>The following figure depicts types of <a id="id329" class="indexterm"/>NoSQL databases<a id="id330" class="indexterm"/> and some of the products in the market:</p><div class="mediaobject"><img src="graphics/B03980_03_08.jpg" alt="The Hadoop Storage layer"/></div><p>Hadoop was originally <a id="id331" class="indexterm"/>meant for \batch processing where the data into HDFS is <a id="id332" class="indexterm"/>loaded in batch or a scheduled manner. Usually, the storage layer has data loaded in batch. Some of the core and ecosystem components that facilitate data loading or ingestion into HDFS are Sqoop, <a id="id333" class="indexterm"/>
<span class="strong"><strong>HIHO</strong></span> (<span class="strong"><strong>Hadoop-in Hadoop-out</strong></span>) MapReduce function, and ETL functions among others.</p></div><div class="section" title="The Hadoop (Physical) Infrastructure layer – supporting appliance"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec46"/>The Hadoop (Physical) Infrastructure layer – supporting appliance</h2></div></div></div><p>The difference<a id="id334" class="indexterm"/> between<a id="id335" class="indexterm"/> the traditional architectures and big data (for Machine learning) architecture is the importance that the underlying infrastructure grabs. Performance, Scalability, Reliability, Fault Tolerance, High Availability, and Disaster Recovery are some of the important quality attributes that this architecture is required to support. The underlying infrastructure of the platform handles these requirements.</p><p>The Hadoop Infrastructure is a distributed architecture or model where data is not stored in one place, but is distributed across multiple or a cluster of nodes. The data distribution strategy can be intelligent (as in the case of Greenplum) or can be simply mathematical (as in the case of Hadoop). The distributed file system nodes are linked over a network. This is referred to as<a id="id336" class="indexterm"/> <span class="strong"><strong>Shared Nothing Architecture</strong></span> (<span class="strong"><strong>SNA</strong></span>), and the big data solutions work on this reference architecture. Along with the data being distributed across multiple nodes, the processes run locally to the data nodes.</p><p>This is first cited in Michael Stonebraker's paper that can be accessed at <a class="ulink" href="http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf">http://db.cs.berkeley.edu/papers/hpts85-nothing.pdf</a>.</p><p>Nodes that have <a id="id337" class="indexterm"/>data stored are <a id="id338" class="indexterm"/>called data nodes and those where processing happens are called compute nodes. The data and compute nodes can be collocated or decoupled. The following figure represents an SNA context that has data and compute nodes collocated:</p><div class="mediaobject"><img src="graphics/B03980_03_09.jpg" alt="The Hadoop (Physical) Infrastructure layer – supporting appliance"/></div><p>Shared<a id="id339" class="indexterm"/> nothing data architecture supports parallel processing. Redundancy is a default expectation as it deals with a variety of data from diverse sources.</p><p>Hadoop and HDFS, over a<a id="id340" class="indexterm"/> grid infrastructure connected, over a fast gigabit network, or a virtual cloud infrastructure, forms the infrastructure layer that supports large-scale Machine learning architecture.</p><p>The following figure illustrates big data infrastructure setup using commodity servers:</p><div class="mediaobject"><img src="graphics/B03980_03_10.jpg" alt="The Hadoop (Physical) Infrastructure layer – supporting appliance"/></div></div><div class="section" title="Hadoop platform / Processing layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec47"/>Hadoop platform / Processing layer</h2></div></div></div><p>The platform or <a id="id341" class="indexterm"/>processing layer for Hadoop is the core data processing layer of the Machine learning architecture tools. This layer facilitates querying or accessing data stored in Hadoop's storage layer (NoSQL databases that use the HDFS storage file system typically), sitting at the top of the Hadoop infrastructure layer.</p><p>As learned in <a class="link" href="ch02.html" title="Chapter 2. Machine learning and Large-scale datasets">Chapter 2</a>, <span class="emphasis"><em>Machine learning, and Large-scale datasets</em></span>, technological advancements in the field of computing now facilitate handling large volumes of distributed computing and parallel processing.</p><p>The MapReduce framework of Hadoop helps to store and analyze large volumes of data efficiently and in an inexpensive way.</p><p>The key components of the Hadoop platform or processing layer are listed next; these components are a part of the ecosystem and are discussed in detail in the sections that follow in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>MapReduce</strong></span>: MapReduce <a id="id342" class="indexterm"/>is a programming paradigm that is used to efficiently execute a function over a larger volume of data, typically in a batch mode. The <span class="emphasis"><em>map</em></span> function is responsible for distributing the tasks across multiple systems, distributing the load equally and managing the processing in parallel. Post processing; the <span class="emphasis"><em>reduce</em></span> function assimilates and combines the elements to provide a result. A step-by-step implementation on Hadoop's native MapReduce architecture, MapReduce v2, and YARN is covered in the <span class="emphasis"><em>Hadoop ecosystem components</em></span> section.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hive</strong></span>: Hive<a id="id343" class="indexterm"/> is a data warehouse framework for Hadoop and is responsible for aggregating high volumes of data with SQL-like functions. Hive facilitates an efficient way of storing data that uses resources optimally. The configuration and implementation aspects of Hive are covered in the <span class="emphasis"><em>Hadoop ecosystem components</em></span> section.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pig</strong></span>: Pig <a id="id344" class="indexterm"/>is a simple scripting language that facilitates querying and manipulating data held on HDFS. It internally runs the functions in a MapReduce paradigm and is often perceived to simplify building MapReduce functions. A detailed step-by-step guide to configuring, learning the syntax, and building essential functions is covered in the <span class="emphasis"><em>Hadoop ecosystem components</em></span> section.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sqoop</strong></span>: Sqoop<a id="id345" class="indexterm"/> is a data import tool for Hadoop that has inbuilt functions to import data from specific tables, columns, or complete database onto the file system. Post processing, Sqoop supports extracting data from several Relational databases and NoSQL data stores.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>HBase</strong></span>: HBase<a id="id346" class="indexterm"/> is a Hadoop compliant NoSQL data store (a columnar NoSQL data store) that uses HDFS as the underlying file system. It supports distributed storage and automatic linear scalability.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>ZooKeeper</strong></span>: ZooKeeper <a id="id347" class="indexterm"/>is a monitoring and coordinating service that helps keep a check on the Hadoop instances and nodes. It is responsible for keeping the infrastructure synchronized and protects the distributed system from partial failures and ensures data consistency. The ZooKeeper framework can work standalone or outside Hadoop.</li></ul></div><p>More of these<a id="id348" class="indexterm"/> ecosystem components are discussed in depth in the following sections.</p></div><div class="section" title="The Analytics layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec48"/>The Analytics layer</h2></div></div></div><p>More often, enterprises <a id="id349" class="indexterm"/>have some real <span class="strong"><strong>Business Intelligence</strong></span> (<span class="strong"><strong>BI</strong></span>)<a id="id350" class="indexterm"/> tools that are<a id="id351" class="indexterm"/> responsible for running some analytical queries and producing some MIS reports or dashboards. There is a need for modern Machine learning or analytics tools and frameworks to coexist with them. There is now a need for the analytics to run either in a traditional way on the data warehouses or big data stores as well that can handle structured, semi-structured, and unstructured data.</p><p>In this case, we can expect the data flow between the traditional data stores and the big data stores using tools such as Sqoop.</p><p>NoSQL stores are known for low latency; they facilitate real-time analytics. Many open source analytics frameworks have simplified building models, and run complex statistical and mathematical algorithms using simple out-of-box functions. All that is required now is to understand the relevance of each of the algorithms and the ability to choose a suitable algorithm or approach, given a specific problem.</p><p>Let us look into the below listed open source Analytics and Machine learning frameworks in the chapters to follow.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">R</li><li class="listitem" style="list-style-type: disc">Apache Mahout</li><li class="listitem" style="list-style-type: disc">Python (scikit-learn distribution)</li><li class="listitem" style="list-style-type: disc">Julia</li><li class="listitem" style="list-style-type: disc">Apache Spark</li></ul></div><p>An introduction to one of <a id="id352" class="indexterm"/>the<a id="id353" class="indexterm"/> upcoming Spring projects called <span class="strong"><strong>Spring XD</strong></span>
<a id="id354" class="indexterm"/> is covered, as it looked like a comprehensive Machine learning solution that can run on Hadoop.</p></div><div class="section" title="The Consumption layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec49"/>The Consumption layer</h2></div></div></div><p>The<a id="id355" class="indexterm"/> insights generated from the <a id="id356" class="indexterm"/>analytics layer or the results of data processing are consumed in many ways by the end clients. Some of the ways this data can be made available for consumption are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Service APIs (for example, Web Service Interfaces (SOAP based or REST))</li><li class="listitem" style="list-style-type: disc">Web applications</li><li class="listitem" style="list-style-type: disc">Reporting engines and data marts</li><li class="listitem" style="list-style-type: disc">Dashboard and Visualization tools</li></ul></div><p>Of all the options, <span class="strong"><strong>Visualization</strong></span>
<a id="id357" class="indexterm"/> is core and not only an important way of distributing or communicating the results of Machine learning but also a good way of representing data in a way that helps in decision making. Very evidently data visualization is gaining traction in the field of big data and analytics. A visualization that best represents the data and the <a id="id358" class="indexterm"/>underlying patterns and the relationships is what is the key to decision making.</p><div class="mediaobject"><img src="graphics/B03980_03_11.jpg" alt="The Consumption layer"/></div><p>There are two types of <a id="id359" class="indexterm"/>visualizations; one is those that explain the data, and second is those that explore data and the underlying patterns. Visualization is now being looked at as a new language to communicate.</p><div class="section" title="Explaining and exploring data with Visualizations"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec33"/>Explaining and exploring data with Visualizations</h3></div></div></div><p>Visualizations for<a id="id360" class="indexterm"/> explaining and exploring data are unique and<a id="id361" class="indexterm"/> are used for different purposes.</p><p>The Visualizations for explaining are the typical ones we see in marketing and sales presentations. This is the case where data on the hand is clean to the maximum extent. The meaning of the data is clear, and communication is done by the final decision makers.</p><p>On the other hand, Visualizations for exploring help to correct data and link the related and useful attributes of data in the quest for understanding the data as such. Visualization for exploring sometimes can be inaccurate. The exploration usually happens iteratively, and there might be several rounds of refining the Visualizations before some sense is made out of the data on hand. There is a need to get rid of some irrelevant attributes in data or even the data itself (the one identified to be <span class="emphasis"><em>noise</em></span>). This step of data exploration using Visualization sometimes replaces running complex algorithms and often requires statistical acumen.</p><p>Some popular visualization tools in the market (both open source and commercial) are Highcharts JS, D3, Tableau, and others. Although we use some of these frameworks to demonstrate how to depict and communicate insights, we are not explicitly covering any of the visualization options in depth.</p><p>Another important <a id="id362" class="indexterm"/>aspect is that these visualization tools<a id="id363" class="indexterm"/> usually need to leverage, traditional data warehousing tools and big data analysis tools. The following figure depicts how the proposed Machine learning architecture can support having existing data warehouses or BI tools coexist with big data analysis tools. As explained in <a class="link" href="ch01.html" title="Chapter 1. Introduction to Machine learning">Chapter 1</a>, <span class="emphasis"><em>Introduction to Machine learning</em></span>, the aggregated data and the data lakes become the core input to any big data analysis tools that run the Machine learning tools. The new age data storage mantra is semantic data structures. More on semantic data architectures are covered as a part of emerging data architectures in <a class="link" href="ch14.html" title="Chapter 14. New generation data architectures for Machine learning">Chapter 14</a>, <span class="emphasis"><em>New generation data architectures for Machine learning</em></span>. The following figure depicts a high-level view of visualization in the context of data lakes and data warehouses:</p><div class="mediaobject"><img src="graphics/B03980_03_12.jpg" alt="Explaining and exploring data with Visualizations"/></div></div><div class="section" title="Security and Monitoring layer"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec34"/>Security and Monitoring layer</h3></div></div></div><p>When large volumes<a id="id364" class="indexterm"/> of data are being processed and consolidated across a variety of sources, security becomes of utmost importance and, in the case of sensitive data, the need for protecting data privacy is critical and sometimes the key compliance requirement too. The required authentication and authorization checks need to be implemented as a part of executing Machine learning algorithms. This is more of a prerequisite and cannot be an afterthought in the Machine learning architecture.</p><p>Data Ingestion and the processing function are the main areas that require strict security implementation, given the criticality of controlling data access.</p><p>By the virtue of distributed architecture, big data applications are inherently prone to security vulnerabilities; it is necessary that security implementation is taken care of, and it does not impact performance, scalability, or functionality with the ease of execution and maintenance of these applications.</p><p>The Machine<a id="id365" class="indexterm"/> learning architecture as such should support the following as a basic necessity for security:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Authentication for each node in the cluster with the support for standard protocols like Kerberos</li><li class="listitem" style="list-style-type: disc">Since it is a file system, there needs to be a minimum support for encryption</li><li class="listitem" style="list-style-type: disc">Communication with the nodes should always use<a id="id366" class="indexterm"/> <span class="strong"><strong>SSL</strong></span> (<span class="strong"><strong>Secure Socket Layer</strong></span>), TLS, or others that include NameNode</li><li class="listitem" style="list-style-type: disc">Secure keys and tokens and usage of standard key management systems</li><li class="listitem" style="list-style-type: disc">The implementation of distributed logging for tracking to trace any issues across layers easily</li></ul></div><p>The next significant requirement is monitoring. The distributed data architecture comes with robust monitoring and support tools that can handle large clusters of nodes that are connected in a federated model.</p><p>There are always SLAs for the downtime of an application, and it is important that the recovery mechanism adheres to these SLAs while ensuring the availability of the application.</p><p>It is important that these nodes and clusters communicate with the monitoring system in a machine independent way, and the usage of XML-like formats is key. The data storage needs for the monitoring systems should not impact the overall performance of the application.</p><p>Usually, every big data stack comes with an in-built monitoring framework or tool. Also, there are open source tools such as Ganglia and Nagios that can be integrated and used for monitoring the big data applications.</p></div><div class="section" title="Hadoop core components framework"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec35"/>Hadoop core components framework</h3></div></div></div><p>Apache Hadoop<a id="id367" class="indexterm"/> has<a id="id368" class="indexterm"/> two core components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hadoop Distributed File System also called HDFS</li><li class="listitem" style="list-style-type: disc">MapReduce (in the version 2.x of Hadoop, this is called YARN)</li></ul></div><p>The rest of the Hadoop components are represented in the Machine learning solution architecture. Using Hadoop we work around these two core components and form the eco-system components for Hadoop.</p><p>The focus of this chapter is Apache Hadoop 2.x distribution. There have been few architectural changes to HDFS and MapReduce in this version. We first cover the core architecture, and then the changes that have come in as a part of the 2.x architecture.</p><div class="section" title="Hadoop Distributed File System (HDFS)"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec04"/>Hadoop Distributed File System (HDFS)</h4></div></div></div><p>
<span class="strong"><strong>HDFS</strong></span> is<a id="id369" class="indexterm"/> inspired and built from <span class="strong"><strong>GFS</strong></span> (<span class="strong"><strong>Google File System</strong></span>). It is <a id="id370" class="indexterm"/>a distributed file system that is elastically scalable, with support load balancing and fault tolerance to ensure high availability. It has data redundancy built in to demonstrate reliability and consistency in data.</p><div class="mediaobject"><img src="graphics/B03980_03_13.jpg" alt="Hadoop Distributed File System (HDFS)"/></div><p>HDFS implements the Master-slave architecture. Here, the master node is called NameNode, and the slave nodes are called DataNodes. NameNode is the entry point for all client applications, and the distribution of data across the DataNodes happens via the NameNode. The actual data is not passed through NameNode server to ensure that NameNode does not become a bottleneck for any data distribution. Only the metadata is communicated to the client, and the actual data movement happens directly between the clients and DataNodes.</p><p>Both NameNode and DataNode are referred to as daemons in the Hadoop architecture. NameNode requires a high-end machine and is expected to run only the NameNode daemon. The following points justify the need for a high-end machine for NameNode:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The entire cluster's metadata is held in the memory for quicker access, and there is a need for more memory</li><li class="listitem" style="list-style-type: disc">NameNode is both the single point of entry and failure for the Hadoop cluster</li><li class="listitem" style="list-style-type: disc">The NameNode coordinates with several hundreds or thousands of DataNodes and manages batch jobs</li></ul></div><p>HDFS is <a id="id371" class="indexterm"/>built on the traditional hierarchical file system where the creation of new directories, adding new files, deletion directories or subdirectories, removal of files, renaming, and moving or updating a file are common tasks. Details of the directories, files, data nodes, and blocks created and stored in each of the DataNodes are stored as metadata in the NameNode.</p><p>There is another node in this architecture that NameNode communicates with, called secondary Namenode. The secondary Namenode is not a backup for NameNode and hence, does not failover to the secondary Namenode. Instead, it is used to store a copy of the metadata and log files from NameNode. NameNode holds the metadata for the data blocks and related distribution details in a file called <code class="literal">fsimage</code>. This image file is not updated for every data operation in the file system and is tracked periodically by logging them in separate log files. This ensures faster I/O and thus the efficiency of the data import or export operations.</p><p>The secondary Namenode has a specific function with this regard. It periodically downloads the image and log files, and creates a new image by appending the current operations from the log file into the fsimage, then uploading the new image file back to NameNode. This eliminates any overhead on NameNode. Any restart on NameNode happens very quickly, and the efficiency of the system is ensured. The following figure depicts the communication workflow between the client application and HDFS:</p><div class="mediaobject"><img src="graphics/B03980_03_14.jpg" alt="Hadoop Distributed File System (HDFS)"/></div><p>HDFS is<a id="id372" class="indexterm"/> built for reading and writing large volumes of data between DataNodes. These large files are split into blocks of smaller files, usually of a fixed size such as 64 MB or 128 MB, and these blocks are distributed across DataNodes. For each of these blocks, overall three copies are stored to ensure redundancy and support fault tolerance. The number of copies can be changed, which is a configuration of the system. More information on the HDFS architecture and specific functions is covered in the following section.</p><div class="section" title="Secondary Namenode and Checkpoint process"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec01"/>Secondary Namenode and Checkpoint process</h5></div></div></div><p>While <a id="id373" class="indexterm"/>defining the purpose and <a id="id374" class="indexterm"/>function of the secondary Namenode, we have learned one important function that takes care of updating or preparing the metadata for NameNode that is stored in a file called <code class="literal">fsimage</code>. This process of generating a new fsimage by merging the existing fsimage and the log file is called <a id="id375" class="indexterm"/>
<span class="strong"><strong>Checkpoint</strong></span>. The following figure depicts the checkpoint process:</p><div class="mediaobject"><img src="graphics/B03980_03_15.jpg" alt="Secondary Namenode and Checkpoint process"/></div><p>Some <a id="id376" class="indexterm"/>configurations<a id="id377" class="indexterm"/> changes are to be done to the <code class="literal">cross-site.XML</code> file related to checkpoint process.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Property</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.dir </code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the directory path where the temporary fsimage files are held to run the merge process.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.edits.dir </code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the directory path where the temporary edits are held to run the merge process. The default value for this parameter is same as <code class="literal">dfs.namenode.checkpoint.dir</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.period</code>
</p>
</td><td style="text-align: left" valign="top">
<p>The time gap between two checkpoint runs (in seconds).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.txns</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Irrespective of the time gap configurations, this property defines after how many transactions a checkpoint process needs to be triggered.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.check.period</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This property defines the frequency (in seconds) in which the NameNode is polled to check the un-checkpointed transactions.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.checkpoint.max-retries</code>
</p>
</td><td style="text-align: left" valign="top">
<p>In the case of failure, the secondary Namenode retry is checkpointing. This property defines the number of times a secondary Namenode attempts a retry for checkpointing before it gives up.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.num.checkpoints.retained</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This property represents the number of checkpoint files retained by both the NameNode and the secondary Namenode. </p>
</td></tr></tbody></table></div><p>The checkpoint <a id="id378" class="indexterm"/>process can be <a id="id379" class="indexterm"/>triggered by both NameNode and the secondary Namenode. Secondary Namenode is also responsible for taking backup of the <code class="literal">fsimage</code> files periodically, which will further help in recovery.</p></div><div class="section" title="Splitting large data files"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec02"/>Splitting large data files</h5></div></div></div><p>HDFS stores<a id="id380" class="indexterm"/> smaller chunks of huge files across the data nodes distributed over the cluster. Before the files are stored, HDFS internally splits the entire file content into multiple data blocks each of a fixed size (default 64 MB). This size is configurable. There is no specific business logic followed to split the files and build the data blocks; it is purely driven by the file size. These data blocks are then stored on the DataNodes for the data read and write to happen in parallel. Each data block is again a file in itself in the local file system.</p><p>The following figure depicts how a large file is split into smaller chunks or blocks of fixed size:</p><div class="mediaobject"><img src="graphics/B03980_03_16.jpg" alt="Splitting large data files"/></div><p>The size of each <a id="id381" class="indexterm"/>block can be controlled by the following configuration parameter in <code class="literal">hdfs-site.xml</code>. The cluster-wide block size is controlled by the <code class="literal">dfs.blocksize configuration</code> property in <code class="literal">hdfs-site.XML</code> The default value in Hadoop 1.0 is 64 MB and in Hadoop 2.x is 128 MB. The block size is determined by the effectiveness of the infrastructure and can get bigger with higher transfer speeds and the usage of the new age drives:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Property</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.blocksize </code>
</p>
</td><td style="text-align: left" valign="top">
<p>The value is 134217728.</p>
<p>The previous value in bytes represents 128 MB, alternatively any value suffixed by a measure can be defined. For example, 512m, 1g, 128k, and so on.</p>
</td></tr></tbody></table></div><p>Any update to the<a id="id382" class="indexterm"/> value in the block size will not be applied to the existing blocks; only new blocks are eligible.</p></div><div class="section" title="Block loading to the cluster and replication"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec03"/>Block loading to the cluster and replication</h5></div></div></div><p>Once the file is split, the data blocks are formed of a fixed block size and are configured for the environment.</p><p>By virtue <a id="id383" class="indexterm"/>of the distributed architecture, there is a strong need to store replicas of the data blocks to handle data reliability. By default, three copies of each data block are stored. The number of the copies configuration property is called replication factor. The following table lists all the configurations related to data loading and replication:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Property</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.replication </code>
</p>
</td><td style="text-align: left" valign="top">
<p>The value is 3.</p>
<p>This defines the number of replicas that need to be stored in each block.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.replication.max</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Maximal block replication.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">dfs.namenode.replication.min</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Minimal block replication.</p>
</td></tr></tbody></table></div><p>The NameNode is responsible for ensuring the block placement and replication as per the configuration is done. With these data blocks placed onto DataNodes, each DataNode in the cluster sends block status periodically to the NameNode. The fact that NameNode receives a signal from the DataNode implies that the DataNode is active and functioning properly.</p><p>HDFS uses a <span class="strong"><strong>default block placement policy</strong></span>
<a id="id384" class="indexterm"/> that is targeted to achieve load balancing across the available nodes. Following is the scope of this policy:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, the copy or replica is written to the DataNode that is creating the file; this facilitates a higher write performance</li><li class="listitem" style="list-style-type: disc">Second, the copy or replica is written to another DataNode from the same rack; this minimizes network traffic</li><li class="listitem" style="list-style-type: disc">Third, the replica is written to a DataNode in a different rack; this way even if a switch fails, there still is a copy of the data block available</li></ul></div><p>A default block<a id="id385" class="indexterm"/> placement policy is applied that uses all the nodes on the rack without compromising on the performance, data reliability, and availability. The following image depicts how three blocks of data are placed across four nodes with a replication strategy of two extra copies. Some of these nodes are located in the racks for optimal fault tolerance.</p><div class="mediaobject"><img src="graphics/B03980_03_17.jpg" alt="Block loading to the cluster and replication"/></div><p>Overall, the flow of <a id="id386" class="indexterm"/>loading data into HDFS is shown in the following flow diagram:</p><div class="mediaobject"><img src="graphics/B03980_03_18.jpg" alt="Block loading to the cluster and replication"/></div></div></div></div><div class="section" title="Writing to and reading from HDFS"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec36"/>Writing to and reading from HDFS</h3></div></div></div><p>While writing <a id="id387" class="indexterm"/>a file to HDFS, the client first contacts NameNode and<a id="id388" class="indexterm"/> passes the details of the file that needs to be written to HDFS. NameNode provides details on the replication configurations and other metadata details that specify where to place the data blocks. The following figure explains this flow:</p><div class="mediaobject"><img src="graphics/B03980_03_19.jpg" alt="Writing to and reading from HDFS"/></div></div><div class="section" title="Handling failures"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec37"/>Handling failures</h3></div></div></div><p>When the Hadoop cluster starts up, the NameNode gets into a safe-mode state and receives a heartbeat signal from all the data nodes. The fact that the NameNode receives a block report from DataNodes indicates that the DataNodes are up and functioning.</p><p>Let's now say that <span class="strong"><strong>Data Node 4</strong></span> goes down; this would mean that <span class="strong"><strong>Name Node</strong></span> does not receive any heartbeat signals from <span class="strong"><strong>Data Node 4</strong></span>. <span class="strong"><strong>Name Node</strong></span> registers the unavailability of <span class="strong"><strong>Name Node</strong></span> and hence, whatever <span class="strong"><strong>Data Node 4</strong></span> does is load balanced to the other nodes that have the replicas. This data is then updated in the metadata register by <span class="strong"><strong>Name Node</strong></span>. The following figure illustrates the same:</p><div class="mediaobject"><img src="graphics/B03980_03_20.jpg" alt="Handling failures"/></div></div><div class="section" title="HDFS command line"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec38"/>HDFS command line</h3></div></div></div><p>HDFS has a <a id="id389" class="indexterm"/>command line interface called<a id="id390" class="indexterm"/> <span class="strong"><strong>FS Shell</strong></span>. This facilitates the usage of shell commands to manage HDFS. The following screenshot shows the <code class="literal">Hadoop fs</code> command, and its usage/syntax:</p><div class="mediaobject"><img src="graphics/B03980_03_21.jpg" alt="HDFS command line"/></div></div><div class="section" title="RESTFul HDFS"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec39"/>RESTFul HDFS</h3></div></div></div><p>To have external<a id="id391" class="indexterm"/> applications, especially web applications or similar applications, have easy access to the data in HDFS over HTTP. HDFS supports an additional protocol called WebHDFS that is based on the RESTful standards that facilitate giving access to HDFS data over HTTP, without any need for Java binding or the availability of a complete Hadoop environment. Clients can use common tools such as curl/wget to access the HDFS. While providing web services-based access to data stored in HDFS, WebHDFS the built-in security and parallel processing capabilities of the platform, are well retained.</p><p>To enable WebHDFS, make the following configuration changes in <code class="literal">hdfs-site.xml</code>:</p><div class="informalexample"><pre class="programlisting">&lt;property&gt;
          &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
          &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;</pre></div><p>More details on WebHDFS REST API<a id="id392" class="indexterm"/> can be found at <a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/WebHDFS.html</a>.</p></div></div><div class="section" title="MapReduce"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec50"/>MapReduce</h2></div></div></div><p>MapReduce<a id="id393" class="indexterm"/> is similar to HDFS. The Hadoop MapReduce framework is inspired and built on Google's MapReduce framework. It is a distributed computing framework that facilitates processing gigantic amounts of data in parallel across clusters and has built-in fault tolerance mechanisms. It works on operating and processing the local data paradigm, where the processing logic is moved to the data instead of data moved to the processing logic.</p><div class="mediaobject"><img src="graphics/B03980_03_22.jpg" alt="MapReduce"/></div><div class="section" title="MapReduce architecture"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec40"/>MapReduce architecture</h3></div></div></div><p>The MapReduce framework <a id="id394" class="indexterm"/>is also based on Master-slave architecture. The master job is called JobTracker, and the slave jobs are called TaskTrackers. Unlike NameNode and DataNodes, these are not physical nodes, but are daemon processors that are responsible for running the processing logic across the DataNodes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>JobTracker</strong></span>: JobTracker <a id="id395" class="indexterm"/>schedules the execution of a job that comprises of multiple tasks. It is responsible for running the tasks or jobs on the task trackers and in parallel, monitors the status of processing. In the case of any failures, it is responsible for rerunning the failed tasks on the task tracker.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>TaskTracker</strong></span>: TaskTracker<a id="id396" class="indexterm"/> executes the tasks scheduled by the JobTracker and constantly communicates with JobTracker, working in cohesion.</li></ul></div><p>Now, let's draw the analogy between the Master-slave architecture on the HDFS and MapReduce. The NameNode runs the <a id="id397" class="indexterm"/>JobTracker and DataNodes run TaskTrackers.</p><p>In a typical multi-node cluster, the NameNode and DataNodes are separate physical nodes, but in the case of a single node cluster, where the NameNode and DataNode are infrastructure wise the same, JobTracker and TaskTracker functions run on the same node. Single node clusters are used in the development environment.</p><div class="mediaobject"><img src="graphics/B03980_03_23.jpg" alt="MapReduce architecture"/></div><p>There are two functions in a <a id="id398" class="indexterm"/>MapReduce process—<code class="literal">Map</code> and <code class="literal">Reduce</code>.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Mapper</strong></span>: Mapper <a id="id399" class="indexterm"/>job splits the file into multiple chunks<a id="id400" class="indexterm"/> in parallel, and runs some basic functions such as sorting, filtering, and any other specific business or analytics functions as needed. The output of the Mapper function is input to the Reducer function.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Reducer</strong></span>: Reducer <a id="id401" class="indexterm"/>job is used to consolidate the results <a id="id402" class="indexterm"/>across Mappers, and is additionally used to perform any business or analytics function as needed. The intermediate output from the Mapper and Reducer jobs are stored on the file system as key-value pairs. Both the input and output of the map and reduce jobs are stored in HDFS. Overall, the MapReduce framework takes care of scheduling the tasks, monitoring the status, and handling failures (if any). The following diagram depicts how the <code class="literal">Map</code> and the <code class="literal">Reduce</code> functions work and operate on the data held in HDFS:<div class="mediaobject"><img src="graphics/B03980_03_24.jpg" alt="MapReduce architecture"/></div></li></ul></div></div><div class="section" title="What makes MapReduce cater to the needs of large datasets?"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec41"/>What makes MapReduce cater to the needs of large datasets?</h3></div></div></div><p>Some of the advantages of MapReduce programming framework<a id="id403" class="indexterm"/> are listed as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Parallel execution</strong></span>: MapReduce<a id="id404" class="indexterm"/> programs are, by default, meant to be executed in parallel that can be executed on a cluster of nodes. Development teams need not focus on the internals of distributed computing and can just use the <a id="id405" class="indexterm"/>framework directly.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fault Tolerance</strong></span>: MapReduce <a id="id406" class="indexterm"/>framework works on Master-slave Architecture where, in case any node goes down, corrective actions are taken automatically by the framework.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scalability</strong></span>: MapReduce<a id="id407" class="indexterm"/> framework, having the ability to work distributed and with the ability to scale-out (horizontal scalability), with growing volumes new nodes, can be added to the cluster whenever needed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data Locality</strong></span>: One <a id="id408" class="indexterm"/>of the core premises that the MapReduce framework does is to take<a id="id409" class="indexterm"/> the program to the data as opposed to the traditional way of bringing data to the code. So to be precise, MapReduce always has local data to it, and this is one of the most important reasons for the performance.</li></ul></div></div><div class="section" title="MapReduce execution flow and components"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec42"/>MapReduce execution flow and components</h3></div></div></div><p>In this section, we <a id="id410" class="indexterm"/>will a take a deep dive into the execution flow of <a id="id411" class="indexterm"/>MapReduce and how each of the components function:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">A new job is submitted by the client to JobTracker (a MapReduce job) along with the input and the output file paths and required configurations. The job gets queued for execution and gets picked by the job scheduler.</li><li class="listitem">JobTracker gets the data at the place where the required data in context resides, and creates an execution plan that triggers TaskTrackers for the execution.</li><li class="listitem">JobTracker submits the job to the identified TaskTrackers.</li><li class="listitem">TaskTrackers execute the task using the data that is local to them. If the data is not available on the local Data Node, it communicates with other DataNodes.</li><li class="listitem">TaskTrackers reports the status back to JobTracker by the means of heartbeat signals. JobTracker is capable of handling any failure cases inherently.</li><li class="listitem">Finally, JobTracker reports the output to the Job client on the completion of the job.</li></ol></div><p>The steps just described are depicted in the following figure. There are two parts to the flow: the HDFS and the MapReduce with the Nodes and the Trackers respectively.</p><div class="mediaobject"><img src="graphics/B03980_03_25.jpg" alt="MapReduce execution flow and components"/></div><p>Let us focus on some <a id="id412" class="indexterm"/>core components of the MapReduce program and <a id="id413" class="indexterm"/>learn how to code it up. The following flow diagram details how the flow starts from the input data to the output data, and how each component or function of MapReduce framework kicks in to execute. The blocks in dotted red boxes are the components, and the blue boxes represent data being transitioned through the process.</p><div class="mediaobject"><img src="graphics/B03980_03_26.jpg" alt="MapReduce execution flow and components"/></div></div><div class="section" title="Developing MapReduce components"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec43"/>Developing MapReduce components</h3></div></div></div><p>The MapReduce framework<a id="id414" class="indexterm"/> of Hadoop comprises a set of Java APIs that need to be extended or implemented to incorporate a specific function that is targeted to be executed in parallel over the Hadoop cluster. Following are some API implementations that need to be done:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Input and output data format interfaces</li><li class="listitem" style="list-style-type: disc">Mapper implementation</li><li class="listitem" style="list-style-type: disc">Reducer implementation</li><li class="listitem" style="list-style-type: disc">Partitioner</li><li class="listitem" style="list-style-type: disc">Combiner</li><li class="listitem" style="list-style-type: disc">Driver</li><li class="listitem" style="list-style-type: disc">Context</li></ul></div><div class="section" title="InputFormat"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec05"/>InputFormat</h4></div></div></div><p>The <code class="literal">InputFormat</code> class <a id="id415" class="indexterm"/>is responsible <a id="id416" class="indexterm"/>for reading data from a file and making it available as input to the <code class="literal">map</code> function. Two core functions are performed by this process; one is splitting the input data into logical fragments called InputSplits, and the second is the reading of these splits as key value pairs to feed into the <code class="literal">map</code> function. There are two distinct interfaces to perform these two functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">InputSplit</li><li class="listitem" style="list-style-type: disc">RecordReader</li></ul></div><p>Splitting of the input file is not an essential function. In case we need to consider a complete file for processing, we will need to override the <code class="literal">isSplittable()</code> function and set the flag to <code class="literal">false</code>.</p></div><div class="section" title="OutputFormat"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec06"/>OutputFormat</h4></div></div></div><p>The <code class="literal">OutputFormat</code> API <a id="id417" class="indexterm"/>is responsible for validating that <a id="id418" class="indexterm"/>Hadoop has output data formats against output specification of the job. The RecordWriter implementation is responsible for writing the final output key value pairs to the file system. Every InputFormat API has a corresponding OutputFormat API. The following table lists some of the input and output format APIs of the MapReduce framework:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Input Format API</p>
</th><th style="text-align: left" valign="bottom">
<p>Corresponding Output Format API</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">TextInputFormat</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">TextOutputFormat</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">SequenceFileInputFormat</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">SequenceFileOutputFormat</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">DBInputFormat</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">DBOutputFormat</code>
</p>
</td></tr></tbody></table></div></div><div class="section" title="Mapper implementation"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec07"/>Mapper implementation</h4></div></div></div><p>All the Mapper <a id="id419" class="indexterm"/>implementations need to extend the <code class="literal">Mapper&lt;KeyIn, ValueIn, KeyOut, ValueOut&gt;</code> base class and importantly override the <code class="literal">map()</code> method to implement the specific business function. The Mapper implementation class takes key-value pairs as input and returns a set of key-value pairs as output. Any other interim output subsequently is taken by the shuffle and sort function.</p><p>There is one Mapper instance for each InputSplit generated by the InputFormat for a given MapReduce job.</p><p>Overall, there are four methods that the Mapper implementation class needs to extend from the base class. Following are the methods that are briefly described, along with the purpose of each method:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Method name and Syntax</p>
</th><th style="text-align: left" valign="bottom">
<p> Purpose</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">setup(Context)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This is the first method that is called back when a mapper is initiated for execution. It is not mandatory to override this method unless any specific initializations need to be done or any specific configuration setup needs to be done.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">map(Object, Object, Context)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Overriding this method is the key to mapper implementation as this method would be invoked as a part of executing the mapper logic. It takes key-value pairs as input, and the response can be a collection of key-value pairs </p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">clean (Context)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>This method is called at the end of the mapper function execution in the lifecycle and facilitates clearing any resources utilized by the mapper.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">run (Context)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Overriding this method provides additional capability to run multi-threaded mappers.</p>
</td></tr></tbody></table></div><p>Let's take an example from a given file; we want to find out how many times a word is repeated. In this case, <code class="literal">TextInputFormat</code> is used. In fact, this is the default InputFormat. The following diagram shows what the InputSplit function does. It splits every row and builds a key-value pair.</p><p>The diagram shows how the text is stored in multiple blocks on DataNode. <code class="literal">TextInputFormat</code> then reads these blocks and multiple InputSplits (we can see that there are two InputSplits, and hence there are two mappers). Each mapper picks an InputSplit and<a id="id420" class="indexterm"/> generates a key value pair for each occurrence of the word followed by the number 1.</p><div class="mediaobject"><img src="graphics/B03980_03_27.jpg" alt="Mapper implementation"/></div><p>The output of the mapper function is written onto the disk at the end of the processing, and none of the intermediate results are written to the file system. They are held in the memory. This helps in optimizing performance. It's possible because the key space is partitioned and each mapper only gets a fragment of the total dataset. Now, in terms of how much memory should be assigned for this purpose, by default, 100 MB is allocated and for any changes to this value, the <code class="literal">io.sort.mb</code> property will have to be set. There is usually a threshold set to this limit and, in case it exceeds this, there is a background process that starts writing onto the disk. The following program snippet demonstrates how to implement a <a id="id421" class="indexterm"/>mapper class.</p><div class="informalexample"><pre class="programlisting">public static class VowelMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;
{
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
public void map(Object key, Text value, Context context) throws IOException, InterruptedException
{
StringTokenizer itr = new StringTokenizer(value.toString());
while (itr.hasMoreTokens())
{
word.set(itr.nextToken());
context.write(word, one);
}
}
}</pre></div></div></div></div></div>
<div class="section" title="Hadoop 2.x"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Hadoop 2.x</h1></div></div></div><p>Until Hadoop 2.x, all the<a id="id422" class="indexterm"/> distributions were focused on addressing the limitations in Hadoop 1.x but did not deviate from the core architecture. Hadoop 2.x really changed the underlying architecture assumptions and turned out to be a real breakthrough; most importantly, the introduction of YARN. YARN was a new framework for managing Hadoop cluster, which introduced the ability to handle real-time processing needs in addition to the batch. Some important issues that were addressed are listed as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Single NameNode issues</li><li class="listitem" style="list-style-type: disc">Dramatic increase in the number of nodes in the cluster</li><li class="listitem" style="list-style-type: disc">Extension to the number of tasks that can be successfully addressed with Hadoop</li></ul></div><p>The following figure depicts the difference between the Hadoop 1.x and 2.x architectures and how YARN wires MapReduce and HDFS:</p><div class="mediaobject"><img src="graphics/B03980_03_28.jpg" alt="Hadoop 2.x"/></div><div class="section" title="Hadoop ecosystem components"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec51"/>Hadoop ecosystem components</h2></div></div></div><p>Hadoop <a id="id423" class="indexterm"/>has<a id="id424" class="indexterm"/> spawned a bunch of auxiliary and supporting frameworks. The following figure depicts the gamut of supporting frameworks contributed by the open source developer groups:</p><div class="mediaobject"><img src="graphics/B03980_03_29.jpg" alt="Hadoop ecosystem components"/></div><p>The following table <a id="id425" class="indexterm"/>lists all the frameworks and purposes of each <a id="id426" class="indexterm"/>framework. These frameworks work with the Apache distribution of Hadoop. There are many frameworks built by Vendors, who are commercially positioned and are not in the scope of this book:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Framework</p>
</th><th style="text-align: left" valign="bottom">
<p>URL</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose (in brief)</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>HDFS</strong></span> (<span class="strong"><strong>Hadoop Distributed File System</strong></span>)</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Hadoop<a id="id427" class="indexterm"/> File storage system is a core component of Hadoop, which has a built-in fault tolerance (refer to HDFS section for more details on the architecture and implementation specifics).</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>MapReduce</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a>
</p>
</td><td style="text-align: left" valign="top">
<p>MapReduce<a id="id428" class="indexterm"/> is a programming model and framework for processing large volumes of data on a distributed platform such as Hadoop. The latest version of Apache MapReduce extends another framework Apache YARN.</p>
<p>
<span class="strong"><strong>YARN</strong></span>: MapReduce<a id="id429" class="indexterm"/> has gone through a complete overhaul in Hadoop 2.0 and now it is called MapReduce 2. but the MapReduce programming model has not changed. YARN provides a new resource management and job scheduling model, along with its implementation to execute MapReduce jobs. In most cases, your existing MapReduce jobs run without any changes. In some instances, minor updates and recompilation might be needed.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Pig</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="https://pig.apache.org/">https://pig.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Pig is a <a id="id430" class="indexterm"/>framework to execute data flows in <a id="id431" class="indexterm"/>parallel. It comes <a id="id432" class="indexterm"/>with a scripting language, Pig Latin, that helps in developing the data flows. Pig Latin comes with a bunch of internal operations for data such as <a id="id433" class="indexterm"/>join, split, sort, and so on. Pig runs on Hadoop and utilizes both HDFS and MapReduce. The compiled Pig Latin scripts run their functions in parallel and internally.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hive</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="https://hive.apache.org/">https://hive.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Hive<a id="id434" class="indexterm"/> is a data warehouse framework for Hadoop. It supports<a id="id435" class="indexterm"/> querying and handling big datasets held in distributed stores. An SQL-like querying language called HiveQL can be used that allows plugging in the mapper and reducer programs.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Flume</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://flume.apache.org/">http://flume.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>The<a id="id436" class="indexterm"/> Flume <a id="id437" class="indexterm"/>framework is more of an efficient transport framework that facilitates<a id="id438" class="indexterm"/> aggregating, analyzing, processing, and moving huge volumes of log data. It comes with an extensible data model and supports online analytics.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Chukwa</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="https://chukwa.apache.org/">https://chukwa.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>The <a id="id439" class="indexterm"/>Chukwa framework<a id="id440" class="indexterm"/> comes with an API that helps in easily collecting, analyzing, and monitoring prominent collections of data. Chukwa runs at the top of the HDFS and MapReduce framework, thus inheriting Hadoop's ability to scale.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HBase</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://hbase.apache.org/">http://hbase.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>HBase is <a id="id441" class="indexterm"/>inspired from Google BigTable. It is a <a id="id442" class="indexterm"/>NoSQL, columnar data store built to complement the Hadoop<a id="id443" class="indexterm"/> platform, and supports real-time operations on the data. HBase is a Hadoop database that is responsible for backing MapReduce job outputs.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HCatalog</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="https://cwiki.apache.org/confluence/display/Hive/HCatalog">https://cwiki.apache.org/confluence/display/Hive/HCatalog</a>
</p>
</td><td style="text-align: left" valign="top">
<p>HCatalog<a id="id444" class="indexterm"/> is like a relational<a id="id445" class="indexterm"/> view of the data in HDFS. It doesn't matter where and how or what format the underlying data is stored. It is currently a part of Hive, and there are no separate distributions for the current distributions.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Avro</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://avro.apache.org/">http://avro.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>The <a id="id446" class="indexterm"/>Apache Avro framework<a id="id447" class="indexterm"/> is more of an interface to data. It supports modeling, serializing, and making<a id="id448" class="indexterm"/> <span class="strong"><strong>Remote Procedure Calls</strong></span> (<span class="strong"><strong>RPC</strong></span>). Every schema representation in Avro, also called the metadata definition, resides close to the data and on the same file, thus making the file self-describing.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>HIHO</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="https://github.com/sonalgoyal/hiho/wiki/About-HIHO">https://github.com/sonalgoyal/hiho/wiki/About-HIHO</a>
</p>
</td><td style="text-align: left" valign="top">
<p>HIHO<a id="id449" class="indexterm"/> stands for Hadoop-in Hadoop-out. This framework helps <a id="id450" class="indexterm"/>connecting multiple data stores with the Hadoop system and facilitate interoperability. HIHO supports several RDBMS and file systems, providing internal functions to load and off-load data between RDBMS and HDFS in parallel.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Sqoop</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://sqoop.apache.org/">http://sqoop.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Sqoop <a id="id451" class="indexterm"/>is a widely adopted <a id="id452" class="indexterm"/>framework for data transfer between HDFS and RDBMS in bulk or batch. It is very similar to Flume but operates with RDBMS. Sqoop is one of the<a id="id453" class="indexterm"/> <span class="strong"><strong>ETL</strong></span> (<span class="strong"><strong>Extract-Transform-Load</strong></span>) tools for Hadoop.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Tajo</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://tajo.apache.org/">http://tajo.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Tajo<a id="id454" class="indexterm"/> is a distributed data warehouse system for Apache<a id="id455" class="indexterm"/> Hadoop that is relational in nature. Tajo<a id="id456" class="indexterm"/> supports ad-hoc <a id="id457" class="indexterm"/>querying and online integration, and extract-transform-load functions on large datasets stored in HDFS or other data stores.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Oozie</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://oozie.apache.org/">http://oozie.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Oozie <a id="id458" class="indexterm"/>is a framework that <a id="id459" class="indexterm"/>facilitates workflow management. It acts as a scheduler system for MapReduce jobs using <a id="id460" class="indexterm"/>
<span class="strong"><strong>DAG</strong></span> (<span class="strong"><strong>Direct Acyclical Graph</strong></span>). Oozie can either be data aware or time aware while it schedules and executes jobs.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>ZooKeeper</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://zookeeper.apache.org/">http://zookeeper.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Zookeeper, as the name says it all, is more like an<a id="id461" class="indexterm"/> orchestration and coordination service for<a id="id462" class="indexterm"/> Hadoop. It provides <a id="id463" class="indexterm"/>tools to build, manage, and provide high availability for distributed applications.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ambari</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://ambari.apache.org/">http://ambari.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Ambari <a id="id464" class="indexterm"/>is an intuitive web UI for Hadoop management with RESTful APIs. Apache <a id="id465" class="indexterm"/>Ambari was a<a id="id466" class="indexterm"/> contribution from Hortonworks. It serves as an interface to many other Hadoop frameworks in the ecosystem. </p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mahout</p>
</td><td style="text-align: left" valign="top">
<p>
<a class="ulink" href="http://mahout.apache.org/">http://mahout.apache.org/</a>
</p>
</td><td style="text-align: left" valign="top">
<p>Apache Mahout is <a id="id467" class="indexterm"/>an open <a id="id468" class="indexterm"/>source library for Machine learning algorithms. The design focus for Mahout is to provide a scalable library for huge data sets distributed across multiple systems. Apache Mahout is a tool to derive useful information from raw data.</p>
</td></tr></tbody></table></div></div><div class="section" title="Hadoop installation and setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec52"/>Hadoop installation and setup</h2></div></div></div><p>There are three <a id="id469" class="indexterm"/>different ways to setup Hadoop:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Standalone operation</strong></span>: In this <a id="id470" class="indexterm"/>operation, Hadoop runs in a non-distributed mode. All the daemons run within a single Java process and help in easy debugging. This setup is also called single node installation.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pseudo-Distributed Operation</strong></span>: In this<a id="id471" class="indexterm"/> operation, Hadoop is configured to run on a single node, but in a pseudo-distributed mode that can run different daemon processes on different JVMs.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Fully-Distributed Operation</strong></span>: In this <a id="id472" class="indexterm"/>operation, Hadoop is configured to run on multiple nodes in a fully-distributed mode, and all Hadoop daemons such as NameNode, Secondary Namenode, and JobTracker in the Master node; and DataNode and TaskTracker in slave nodes (in short, run on a cluster of nodes).</li></ul></div><p>The Ubuntu-based Hadoop Installation prerequisites are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Java v1.7<a id="id473" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Creating dedicated Hadoop user</li><li class="listitem" style="list-style-type: disc">Configuring SSH access</li><li class="listitem" style="list-style-type: disc">Disable IPv6</li></ul></div><div class="section" title="Installing Jdk 1.7"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec44"/>Installing Jdk 1.7</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download<a id="id474" class="indexterm"/> Java using this command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget https://edelivery.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-linux-x64.tar.gz</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03980_03_30.jpg" alt="Installing Jdk 1.7"/></div></li><li class="listitem">Unpack the binaries using this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo tar xvzf jdk-7u45-linux-x64.tar.gz</strong></span>
</pre></div></li><li class="listitem">Create a directory to install Java with the help of the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir -P /usr/local/Java</strong></span>
<span class="strong"><strong>cd /usr/local/Java</strong></span>
</pre></div></li><li class="listitem">Copy the binaries into the newly created directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo cp -r jdk-1.7.0_45 /usr/local/java</strong></span>
</pre></div></li><li class="listitem">Configure the PATH parameters:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo nano /etc/profile</strong></span>
</pre></div><p>Or else, use this command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo gedit /etc/profile</strong></span>
</pre></div></li><li class="listitem">Include the following content at the end of the file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>JAVA_HOME=/usr/local/Java/jdk1.7.0_45</strong></span>
<span class="strong"><strong>PATH=$PATH:$HOME/bin:$JAVA_HOME/bin</strong></span>
<span class="strong"><strong>export JAVA_HOME</strong></span>
<span class="strong"><strong>export PATH</strong></span>
</pre></div></li><li class="listitem">In Ubuntu, configure the path for Java:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo update-alternatives --install "/usr/bin/javac" "javac" "/usr/local/java/jdk1.7.0_45/bin/javac" 1</strong></span>
<span class="strong"><strong>sudo update-alternatives --set javac /usr/local/Java/jdk1.7.0_45/bin/javac</strong></span>
</pre></div></li><li class="listitem">Check for installation completion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>java -version</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03980_03_31.jpg" alt="Installing Jdk 1.7"/></div></li></ol></div></div><div class="section" title="Creating a system user for Hadoop (dedicated)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec45"/>Creating a system user for Hadoop (dedicated)</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create/add a<a id="id475" class="indexterm"/> new group:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo addgroup hadoop</strong></span>
</pre></div></li><li class="listitem">Create/add a new user and attach it to the group:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo adduser –ingroup hadoop hduser</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03980_03_32.jpg" alt="Creating a system user for Hadoop (dedicated)"/></div></li><li class="listitem">Create/configure<a id="id476" class="indexterm"/> the SSH key access:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh-keygen -t rsa -P ""</strong></span>
<span class="strong"><strong>cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys</strong></span>
</pre></div></li><li class="listitem">Verify the SSH setup:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ssh hduser@localhost</strong></span>
</pre></div></li></ol></div></div><div class="section" title="Disable IPv6"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec46"/>Disable IPv6</h3></div></div></div><p>Open <code class="literal">sysctl.conf</code> using <a id="id477" class="indexterm"/>the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo gedit /etc/sysctl.conf</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div><p>Add the following lines at the end of the file. Reboot the machine to update the configurations correctly:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>#disable ipv6</strong></span>
<span class="strong"><strong>net.ipv6.conf.all.disable_ipv6 = 1</strong></span>
<span class="strong"><strong>net.ipv6.conf.default.disable_ipv6 = 1</strong></span>
<span class="strong"><strong>net.ipv6.conf.lo.disable_ipv6 = 1</strong></span>
</pre></div></div><div class="section" title="Steps for installing Hadoop 2.6.0"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec47"/>Steps for installing Hadoop 2.6.0</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download <a id="id478" class="indexterm"/>Hadoop 2.6.0 using this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget  http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz</strong></span>
</pre></div></li><li class="listitem">Unpack compressed Hadoop file using this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>tar –xvzf hadoop-2.6.0.tar.gz</strong></span>
</pre></div></li><li class="listitem">Move hadoop-2.6.0 directory (a new directory):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mv hadoop-2.6.0 hadoop</strong></span>
</pre></div></li><li class="listitem">Move Hadoop to a local folder (for convenience) with this command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo mv hadoop /usr/local/</strong></span>
</pre></div></li><li class="listitem">Change the owner of the folder:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>sudo chown -R hduser:hadoop Hadoop</strong></span>
</pre></div></li><li class="listitem">Next, update the configuration files.<p>There are <a id="id479" class="indexterm"/>three site-specific configuration files and one environment setup configuration file to communicate with the Master node (NameNode) and slave nodes (DataNodes):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">core-site.xml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">hdfs-site.xml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">mapred-site.xml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">yarn-site.xml</code></li></ul></div><p>Navigate to the path that has the configuration files:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cd /usr/local/Hadoop/etc/Hadoop</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03980_03_33.jpg" alt="Steps for installing Hadoop 2.6.0"/><div class="caption"><p>yarn-site.xml</p></div></div></li></ol></div><p>Th <code class="literal">core-site.XML</code> file <a id="id480" class="indexterm"/>has the details of the Master node IP or the hostname, Hadoop temporary directory path, and so on.</p><div class="mediaobject"><img src="graphics/B03980_03_34.jpg" alt="Steps for installing Hadoop 2.6.0"/><div class="caption"><p>core-site.xml</p></div></div><p>The <code class="literal">hdfs-site.xml</code> file has the details of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Local file system path where NameNode stores namespace and transactions logs</li><li class="listitem" style="list-style-type: disc">A list of local file system paths to store the blocks</li><li class="listitem" style="list-style-type: disc">Block size</li><li class="listitem" style="list-style-type: disc">Number of replications</li></ul></div><div class="mediaobject"><img src="graphics/B03980_03_35.jpg" alt="Steps for installing Hadoop 2.6.0"/><div class="caption"><p>hdfs-site.xml</p></div></div><p>The <code class="literal">mapred-site.xml</code> file has <a id="id481" class="indexterm"/>the details of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The host or IP and port where the JobTracker runs</li><li class="listitem" style="list-style-type: disc">The path on the HDFS where Map/Reduce stores the files</li><li class="listitem" style="list-style-type: disc">A list of paths on the local file system to store the intermediate MapReduce data</li><li class="listitem" style="list-style-type: disc">The maximum limit of Map/Reduce tasks for every task tracker</li><li class="listitem" style="list-style-type: disc">A list of DataNodes that need to be included or excluded</li><li class="listitem" style="list-style-type: disc">A list of TaskTrackers that need to be included or excluded<div class="mediaobject"><img src="graphics/B03980_03_38.jpg" alt="Steps for installing Hadoop 2.6.0"/><div class="caption"><p> mapred-site.xml</p></div></div></li></ul></div><p>Edit the <code class="literal">.bashrc</code> file <a id="id482" class="indexterm"/>as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03980_03_36.jpg" alt="Steps for installing Hadoop 2.6.0"/></div></div><div class="section" title="Starting Hadoop"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec48"/>Starting Hadoop</h3></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To<a id="id483" class="indexterm"/> start the NameNode:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ Hadoop-daemon.sh start namenode</strong></span>
<span class="strong"><strong>$ jps</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">To start the DataNode:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ Hadoop-daemon.sh start datanode</strong></span>
<span class="strong"><strong>$ jps</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">To start ResourceManager, use the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ yarn-daemon.sh start resourcemanager</strong></span>
<span class="strong"><strong>$ jps</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03980_03_37.jpg" alt="Starting Hadoop"/></div></li><li class="listitem" style="list-style-type: disc">To start NodeManager:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ yarn-daemon.sh start nodemanager</strong></span>
</pre></div></li><li class="listitem" style="list-style-type: disc">Check Hadoop Web interfaces:<p>NameNode: <code class="literal">http://localhost:50070</code>
</p><p>Secondary Namenode: <code class="literal">http://localhost:50090</code>
</p></li><li class="listitem" style="list-style-type: disc">To stop<a id="id484" class="indexterm"/> Hadoop, use this:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>stop-dfs.sh</strong></span>
<span class="strong"><strong>stop-yarn.sh</strong></span>
</pre></div></li></ul></div></div></div><div class="section" title="Hadoop distributions and vendors"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec53"/>Hadoop distributions and vendors</h2></div></div></div><p>With the Apache distribution for <a id="id485" class="indexterm"/>Hadoop being the open source and core version that the <a id="id486" class="indexterm"/>big data community is adopting, several vendors have their distributions of the open source adoption of Apache Hadoop. Some of them have purely added support while others have wrapped and extended the capabilities of Apache Hadoop and its ecosystem components. In many cases, they have their frameworks or libraries built over the core frameworks to add new functionality or features to the underlying core component.</p><p>In this section, let us cover some of the distributions of Apache Hadoop and some differentiating data facts that help the development teams or organizations to take a decision about the distribution that works best for their requirements.</p><p>Let us now consider the following vendors:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cloudera</li><li class="listitem" style="list-style-type: disc">Hortonworks</li><li class="listitem" style="list-style-type: disc">MapR</li><li class="listitem" style="list-style-type: disc">Pivotal / EMC</li><li class="listitem" style="list-style-type: disc">IBM<div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Category</p>
</th><th style="text-align: left" valign="bottom">
<p>Function/Framework</p>
</th><th style="text-align: left" valign="bottom">
<p>Cloudera</p>
</th><th style="text-align: left" valign="bottom">
<p>Hortonworks</p>
</th><th style="text-align: left" valign="bottom">
<p>MapR</p>
</th><th style="text-align: left" valign="bottom">
<p>Pivotal</p>
</th><th style="text-align: left" valign="bottom">
<p>IBM</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Performance and Scalability</p>
</td><td style="text-align: left" valign="top">
<p>Data Ingestion</p>
</td><td style="text-align: left" valign="top">
<p>Batch</p>
</td><td style="text-align: left" valign="top">
<p>Batch</p>
</td><td style="text-align: left" valign="top">
<p>Batch and Streaming</p>
</td><td style="text-align: left" valign="top">
<p>Batch and Streaming</p>
</td><td style="text-align: left" valign="top">
<p>Batch and Streaming</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Metadata architecture</p>
</td><td style="text-align: left" valign="top">
<p>Centralized</p>
</td><td style="text-align: left" valign="top">
<p>Centralized</p>
</td><td style="text-align: left" valign="top">
<p>Distributed</p>
</td><td style="text-align: left" valign="top">
<p>Centralized</p>
</td><td style="text-align: left" valign="top">
<p>Centralized</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>HBase performance</p>
</td><td style="text-align: left" valign="top">
<p>Spikes in latency</p>
</td><td style="text-align: left" valign="top">
<p>Spikes in latency</p>
</td><td style="text-align: left" valign="top">
<p>Low latency</p>
</td><td style="text-align: left" valign="top">
<p>Low latency</p>
</td><td style="text-align: left" valign="top">
<p>Spikes in latency</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>NoSQL Support</p>
</td><td style="text-align: left" valign="top">
<p>Mainly batch applications</p>
</td><td style="text-align: left" valign="top">
<p>Mainly batch applications</p>
</td><td style="text-align: left" valign="top">
<p>Batch and online systems</p>
</td><td style="text-align: left" valign="top">
<p>Batch and online systems</p>
</td><td style="text-align: left" valign="top">
<p>Batch and online systems</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Reliability</p>
</td><td style="text-align: left" valign="top">
<p>High <a id="id487" class="indexterm"/>Availability</p>
</td><td style="text-align: left" valign="top">
<p>Single failure recovery</p>
</td><td style="text-align: left" valign="top">
<p>Single <a id="id488" class="indexterm"/>failure recovery</p>
</td><td style="text-align: left" valign="top">
<p>Self-healing across multiple failures</p>
</td><td style="text-align: left" valign="top">
<p>Self-healing across multiple failures</p>
</td><td style="text-align: left" valign="top">
<p>Single failure recovery</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Disaster Recovery</p>
</td><td style="text-align: left" valign="top">
<p>File copy</p>
</td><td style="text-align: left" valign="top">
<p>N/A</p>
</td><td style="text-align: left" valign="top">
<p>Mirroring</p>
</td><td style="text-align: left" valign="top">
<p>Mirroring</p>
</td><td style="text-align: left" valign="top">
<p>File copy</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Replication</p>
</td><td style="text-align: left" valign="top">
<p>Data</p>
</td><td style="text-align: left" valign="top">
<p>Data</p>
</td><td style="text-align: left" valign="top">
<p>Data and metadata</p>
</td><td style="text-align: left" valign="top">
<p>Data and metadata</p>
</td><td style="text-align: left" valign="top">
<p>Data</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Snapshots</p>
</td><td style="text-align: left" valign="top">
<p>Consistent with closed files</p>
</td><td style="text-align: left" valign="top">
<p>Consistent with closed files</p>
</td><td style="text-align: left" valign="top">
<p>Point in time consistency</p>
</td><td style="text-align: left" valign="top">
<p>Consistent with closed files</p>
</td><td style="text-align: left" valign="top">
<p>Consistent with closed files</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Upgrading</p>
</td><td style="text-align: left" valign="top">
<p>Rolling upgrades</p>
</td><td style="text-align: left" valign="top">
<p>Planned</p>
</td><td style="text-align: left" valign="top">
<p>Rolling upgrades</p>
</td><td style="text-align: left" valign="top">
<p>Planned</p>
</td><td style="text-align: left" valign="top">
<p>Planned</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Manageability</strong></span></p>
</td><td style="text-align: left" valign="top">
<p>Volume Support</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Management Tools</p>
</td><td style="text-align: left" valign="top">
<p>Cloudera Manager</p>
</td><td style="text-align: left" valign="top">
<p>Ambari</p>
</td><td style="text-align: left" valign="top">
<p>MapR Control system</p>
</td><td style="text-align: left" valign="top">
<p>Proprietary console</p>
</td><td style="text-align: left" valign="top">
<p>Proprietary console</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Integration with REST API</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Job replacement control</p>
</td><td style="text-align: left" valign="top">
<p>No </p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="bottom">
<p><span class="strong"><strong>Data Access &amp; Processing</strong></span></p>
</td><td style="text-align: left" valign="top">
<p>File System</p>
</td><td style="text-align: left" valign="top">
<p>HDFS, <a id="id489" class="indexterm"/>Read-only NFS</p>
</td><td style="text-align: left" valign="top">
<p>HDFS, <a id="id490" class="indexterm"/>read-only NFS</p>
</td><td style="text-align: left" valign="top">
<p>HDFS, read/write NFS and POSIX</p>
</td><td style="text-align: left" valign="top">
<p>HDFS, read/write NFS</p>
</td><td style="text-align: left" valign="top">
<p>HDFS, read-only NFS</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>File I/O</p>
</td><td style="text-align: left" valign="top">
<p>Append-only</p>
</td><td style="text-align: left" valign="top">
<p>Append-only</p>
</td><td style="text-align: left" valign="top">
<p>Read/write</p>
</td><td style="text-align: left" valign="top">
<p>Append-only</p>
</td><td style="text-align: left" valign="top">
<p>Append-only</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Security ACLs</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>Authentication</p>
</td><td style="text-align: left" valign="top">
<p>Kerberos</p>
</td><td style="text-align: left" valign="top">
<p>Kerberos</p>
</td><td style="text-align: left" valign="top">
<p>Kerberos and Native</p>
</td><td style="text-align: left" valign="top">
<p>Kerberos and Native</p>
</td><td style="text-align: left" valign="top">
<p>Kerberos and Native</p>
</td></tr></tbody></table></div></li></ul></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Summary</h1></div></div></div><p>In this chapter, we covered all about Hadoop, starting from core frameworks to ecosystem components. At the end of this chapter, readers should be able to set up Hadoop and run some MapReduce functions. Users should be able to run and manage a Hadoop environment and understand the command line usage using one or more ecosystem component.</p><p>In the next chapter, our focus is on the key Machine learning frameworks such as Mahout, Python, R, Spark, and Julia; these either have inherent support on the Hadoop platform, or need direct integration with the Hadoop platform for supporting large datasets.</p></div></body></html>