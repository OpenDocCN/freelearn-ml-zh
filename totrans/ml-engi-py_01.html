<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer023">
<h1 class="chapterNumber">1</h1>
<h1 class="chapterTitle" id="_idParaDest-15">Introduction to ML Engineering</h1>
<p class="normal">Welcome to the second edition of <em class="italic">Machine Learning Engineering with Python</em>, a book that aims to introduce you to the exciting world of making <strong class="keyWord">machine learning</strong> (<strong class="keyWord">ML</strong>) systems production-ready.</p>
<p class="normal">In the two years since the first edition of this book was released, the world of ML has moved on substantially. There are now far more powerful modeling techniques available, more complex technology stacks, and a whole host of new frameworks and paradigms to keep up to date with. To help extract the signal from the noise, the second edition of this book covers a far larger variety of topics in more depth than the first edition, while still focusing on the critical tools and techniques you will need to build up your ML engineering expertise. This edition will cover the same core topics such as how to manage your ML projects, how to create your own high-quality Python ML packages, and how to build and deploy reusable training and monitoring pipelines, while adding discussion around more modern tooling. It will also showcase and dissect different deployment architectures in more depth and discuss more ways to scale your applications using AWS and cloud-agnostic tooling. This will all be done using a variety of the most popular and latest open-source packages and frameworks, from classics like <strong class="keyWord">Scikit-Learn</strong> and <strong class="keyWord">Apache Spark </strong>to<strong class="keyWord"> Kubeflow</strong>,<strong class="keyWord"> Ray</strong>,<strong class="keyWord"> </strong>and<strong class="keyWord"> ZenML</strong>. Excitingly, this edition also has new sections dedicated entirely to Transformers and <strong class="keyWord">Large Language Models</strong> (<strong class="keyWord">LLMs</strong>) like ChatGPT and GPT-4, including examples using Hugging Face and OpenAI APIs to fine-tune and build pipelines using these extraordinary new models. As in the first edition, the focus is on equipping you with the solid foundation you need to go far deeper into each of these components of ML engineering. The aim is that by the end of this book, you will be able to confidently build, scale, and deploy production-grade ML systems in Python using these latest tools and concepts.</p>
<p class="normal">You will get a lot from this book even if you do not run the technical examples, or even if you try to apply the main points in other programming languages or with different tools. As already mentioned, the aim is to create a solid conceptual foundation you can build on. In covering the key principles, the aim is that you come away from this book feeling more confident in tackling your own ML engineering challenges, whatever your chosen toolset.</p>
<p class="normal">In this first chapter, you will learn about the different types of data roles relevant to ML engineering and why they are important, how to use this knowledge to build and work within appropriate teams, some of the key points to remember when building working ML products in the real world, how to start to isolate appropriate problems for engineered ML solutions, and how to create your own high-level ML system designs for a variety of typical business problems.</p>
<p class="normal">We will cover these topics in the following sections:</p>
<ul>
<li class="bulletList">Defining a taxonomy of data disciplines</li>
<li class="bulletList">Assembling your team</li>
<li class="bulletList">ML engineering in the real world</li>
<li class="bulletList">What does an ML solution look like?</li>
<li class="bulletList">High-level ML system design</li>
</ul>
<p class="normal">Now that we have explained what we are going after in this first chapter, let’s get started!</p>
<h1 class="heading-1" id="_idParaDest-16">Technical requirements</h1>
<p class="normal">Throughout the book, all code examples will assume the use of Python 3.10.8 unless specified otherwise. Examples in this edition have been run on a 2022 Macbook Pro with an M2 Apple silicon chip, with Rosetta 2 installed to allow backward compatibility with Intel-based applications and packages. Most examples have also been tested on a Linux machine running Ubuntu 22.04 LTS. The required Python packages for each chapter are stored in <code class="inlineCode">conda</code> environment <code class="inlineCode">.yml</code> files in the appropriate chapter folder in the book’s Git repository. We will discuss package and environment management in detail later in the book. But in the meantime, assuming you have a GitHub account and have configured your environment to be able to pull and push from GitHub remote repositories, to get started you can clone the book repository from the command line:</p>
<pre class="programlisting con"><code class="hljs-con">git clone https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition.git
</code></pre>
<p class="normal">Assuming you have Anaconda or Miniconda installed, you can then navigate to the <em class="italic">Chapter01</em> folder of the Git repository for this book and run:</p>
<pre class="programlisting con"><code class="hljs-con">conda env create –f mlewp-chapter01.yml
</code></pre>
<p class="normal">This will set up the environment you can use to run the examples given in this chapter. A similar procedure can be followed for each chapter, but each section will also call out any installation requirements specific to those examples.</p>
<p class="normal">Now we have done some setup, we will start to explore the world of ML engineering and how it fits into a modern data ecosystem. Let’s begin our exploration of the world of ML engineering!</p>
<div class="note">
<p class="normal">Note: Before running the <code class="inlineCode">conda</code> commands given in this section you may have to install a specific library manually. Some versions of the Facebook Prophet library require versions of PyStan that can struggle to build on Macbooks running Apple silicon. If you run into this issue, then you should try to install the package <code class="inlineCode">httpstan</code> manually. First, go to <a href="https://github.com/stan-dev/httpstan/tags"><span class="url">https://github.com/stan-dev/httpstan/tags</span></a> and select a version of the package to install. Download the <code class="inlineCode">tar.gz</code> or <code class="inlineCode">.zip</code> of that version and extract it. Then you can navigate to the extracted folder and run the following commands:</p>
<pre class="programlisting con"><code class="hljs-con">make
python3 -m pip install poetry
python3 -m poetry build
python3 -m pip install dist/*.whl
</code></pre>
<p class="normal">You may also run into an error like the following when you call <code class="inlineCode">model.fit()</code> in the later example:</p>
<pre class="programlisting con"><code class="hljs-con">dyld[29330]: Library not loaded: '@rpath/libtbb.dylib'
</code></pre>
<p class="normal">If this is the case you will have to run the following commands, substituting in the correct path for your Prophet installation location in the Conda environment:</p>
<pre class="programlisting con"><code class="hljs-con">cd /opt/homebrew/Caskroom/miniforge/base/envs/mlewp-chapter01/lib/python3.10/site-packages/prophet/stan_model/
install_name_tool -add_rpath @executable_path/cmdstan-2.26.1/stan/lib/stan_math/lib/tbb prophet_model.bin
</code></pre>
<p class="normal">Oh, the joys of doing ML on Apple silicon!</p>
</div>
<h1 class="heading-1" id="_idParaDest-17">Defining a taxonomy of data disciplines</h1>
<p class="normal">The explosion of data and <a id="_idIndexMarker000"/>the potential applications of it over the past few years have led to a proliferation of job roles and responsibilities. The debate that once raged over how a <em class="italic">data scientist</em> was different from a <em class="italic">statistician</em> has now become extremely complex. I would argue, however, that it does not have to be so complicated. The activities that have to be undertaken to get value from data are pretty consistent, no matter what business vertical you are in, so it should be reasonable to expect that the skills and roles you need to perform these steps will also be relatively consistent. In this chapter, we will explore some of the main data disciplines that I think you will always need in any data project. As you can guess, given the name of this book, I will be particularly keen to explore the notion of <em class="italic">ML engineering</em> and how this fits into the mix.</p>
<p class="normal">Let’s now look at some of the roles involved in using data in the modern landscape.</p>
<h2 class="heading-2" id="_idParaDest-18">Data scientist</h2>
<p class="normal">After the <em class="italic">Harvard Business Review</em> declared that being a data scientist<a id="_idIndexMarker001"/> was <em class="italic">The Sexiest Job of the 21st Century</em> (<a href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century"><span class="url">https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century</span></a>), this job role became one of the most sought after, but also hyped, in the mix. Its popularity remains high, but the challenges of taking advanced analytics and ML into production have meant there has been more and more of a shift toward engineering roles within data-driven organizations. The traditional data scientist role can cover an entire spectrum of duties, skills, and responsibilities depending on the business vertical, the organization, or even just personal preference. No matter how this role is defined, however, there are some key areas of focus that should always be part of the data scientist’s job profile:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Analysis</strong>: A data scientist<a id="_idIndexMarker002"/> should be able to wrangle, munge, manipulate, and consolidate datasets before performing calculations on the data that help us to understand it. <em class="italic">Analysis</em> is a broad term, but it’s clear that the end result is knowledge of your dataset that you didn’t have before you started, no matter how basic or complex.</li>
<li class="bulletList"><strong class="keyWord">Modeling</strong>: The thing that <a id="_idIndexMarker003"/>gets everyone excited (potentially including you, dear reader) is the idea of modeling phenomena found in your data. A data scientist usually has to be able to apply statistical, mathematical, and ML techniques to data, in order to explain processes or relationships contained within it and to perform some sort of prediction.</li>
<li class="bulletList"><strong class="keyWord">Working with the customer or user</strong>: The data scientist role usually has some more <a id="_idIndexMarker004"/>business-directed elements so that the results of the previous two points can support decision-making in an organization. This could be done by presenting the results of the analysis in PowerPoint presentations or Jupyter notebooks, or even sending an email with a summary of the key results. It involves communication and business acumen in a way that goes beyond classic tech roles.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-19">ML engineer</h2>
<p class="normal">The gap between creating ML proof-of-concept and building robust software, what I often refer to in talks as “the chasm,” has led to the rise of what I would now argue is one of the most important <a id="_idIndexMarker005"/>roles in technology. The ML engineer serves an acute need to translate the world of data science modeling and exploration into the world of software products and systems engineering. Since this is no easy feat, the ML engineer has become increasingly sought after and is now a critical piece of the data-driven software value chain. If you cannot get things into production, you are not generating value, and if you are not generating value, well we know that’s not great!</p>
<p class="normal">You can articulate the need for this type of role quite nicely by considering a classic voice assistant. In this case, a data scientist would usually focus on translating the business requirements into a working <em class="italic">speech-to-text</em> model, potentially a very complex neural network, and showing that it can perform the desired voice transcription task <em class="italic">in principle</em>. ML engineering is then all about how you take that speech-to-text model and build it into a product, service, or tool that can be used <em class="italic">in production</em>. Here, it may mean building some software to train, retrain, deploy, and track the performance of the model as more transcription data is accumulated, or as user preferences are understood. It may also involve understanding how to interface with other systems and provide results from the model in the appropriate formats. For example, the results of the model may need to be packaged into a JSON object and sent via a REST API call to an online supermarket, in order to fulfill an order.</p>
<p class="normal">Data scientists and ML engineers have a lot of overlapping skillsets and competencies but have different areas of focus and strengths (more on this later), so they will usually be part of the same project team and may have either title, but it will be clear what hat they wear from what they do in that project.</p>
<p class="normal">Similar to the data scientist, we can define the key areas of focus for the ML engineer:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Translation</strong>: Taking <a id="_idIndexMarker006"/>models and research code in a variety of formats and translating them into slicker, more robust pieces of code. 
    <p class="bulletList">This can be done using OO programming, functional programming, a mix, or something else, but it basically helps to take the <em class="italic">proof-of-concept</em><strong class="keyWord"> </strong>work of the data scientist and turn it into something that is far closer to being trusted in a production environment.</p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Architecture</strong>: Deployments <a id="_idIndexMarker007"/>of any piece of software do not occur in a vacuum and will always involve lots of integrated parts. This is true of ML solutions as well. The ML engineer has to understand how the appropriate tools and processes link together so that the models built with the data scientist can do their job and do it at scale.</li>
<li class="bulletList"><strong class="keyWord">Productionization</strong>: The <a id="_idIndexMarker008"/>ML engineer is focused on delivering a solution and so should understand the customer’s requirements inside out, as well as be able to understand what that means for the project development. The end goal of the ML engineer is not to provide a good model (though that is part of it), nor is it to provide something that <em class="italic">basically works</em>. Their job is to make sure that the hard work on the data science side of things generates the maximum potential value in a real-world setting.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-20">ML operations engineer</h2>
<p class="normal">ML engineering will be the focus of this book, but there is an important role now emerging with the aim of enabling ML engineers to do their work with higher quality, at greater pace, and at a larger scale. These <a id="_idIndexMarker009"/>are the <strong class="keyWord">Machine Learning Operations</strong> (<strong class="keyWord">MLOps</strong>) engineers. This role is all about<a id="_idIndexMarker010"/> building out the tools and capabilities that enable the tasks of the ML engineer and data scientists. This role focuses more on building out the tooling, platforms, and automation used by the other roles, and so connects them nicely. That is not to say MLOps engineers will not be used in specific projects or builds; it is just that their main value-add comes not from this but from enabling the capabilities used during a specific project or build. If we revisit the example of the speech-to-text solution described in the <em class="italic">ML engineer</em> section, we can get a flavor of this. Where the ML engineer will be worried about building out a solution that works seamlessly in production, the MLOps engineer will work hard to build out the platform or toolset that the ML engineer uses to do this. The ML engineer will build pipelines, but the MLOps engineer may build pipeline templates; the ML engineer<a id="_idIndexMarker011"/> may use <strong class="keyWord">continuous integration/continuous deployment</strong> (<strong class="keyWord">CI/CD</strong>) practices (more on this later), but the MLOps engineer will enable that capability and define the best practice to use CI/CD smoothly. Finally, where the ML engineer thinks “How do I solve this specific problem robustly using the proper tools and techniques?”, the MLOps engineer asks “How do I make sure that the ML engineers and data scientists will be able to, in general, solve the types of problems they need to, and how can I continually update and improve that setup?”</p>
<p class="normal">As we did with the data scientist and ML engineer, let us define some of the key areas of focus for the MLOps engineer:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Automation</strong>: Increasing <a id="_idIndexMarker012"/>the level of automation across the data science and ML engineering workflows through the use of techniques <a id="_idIndexMarker013"/>such as CI/CD and <strong class="keyWord">Infrastructure-as-Code</strong> (<strong class="keyWord">IAC</strong>). Pre-package software that can be deployed to allow for smoother deployments of solutions through these capabilities and more, such as automation scripts or standardized templates.</li>
<li class="bulletList"><strong class="keyWord">Platform engineering</strong>: Working<a id="_idIndexMarker014"/> to integrate a series of useful services together in order to build out the ML platform for the different data-driven teams to use. This can include developing integrations across orchestration tools, compute, and more data-driven services until they become a holistic whole for use by ML engineers and data scientists.</li>
<li class="bulletList"><strong class="keyWord">Enabling key MLOps capabilities</strong>: MLOps consists of a set of practices and techniques <a id="_idIndexMarker015"/>that enable the productionization of ML models by the other engineers in the team. Capabilities such as model management and model monitoring should be enabled by the MLOps engineers in a way that can be used at scale across multiple projects.</li>
</ul>
<p class="normal">It should be noted that some <a id="_idIndexMarker016"/>of the topics covered in this book could be carried out by an MLOps engineer and that there is naturally some overlap. This should not concern us too much, as MLOps is based on quite a generic set of practices and capabilities that can be encompassed by multiple roles (see <em class="italic">Figure 1.1</em>).</p>
<h2 class="heading-2" id="_idParaDest-21">Data engineer</h2>
<p class="normal">The data engineers are <a id="_idIndexMarker017"/>the people responsible for getting the commodity that everything else in the preceding sections is based on from A to B with high fidelity, appropriate latency, and as little effort on the part of the other team members as possible. You cannot create any type of software product, never mind an ML product, without data.</p>
<p class="normal">The key areas of focus for a data engineer are as follows:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Quality</strong>: Getting<a id="_idIndexMarker018"/> data from A to B is a pointless exercise if the data is garbled, fields are missing, or IDs are screwed up. The data engineer cares about avoiding this and uses a variety of techniques and tools, generally to ensure that the data that left the source system is what lands in your data storage layer.</li>
<li class="bulletList"><strong class="keyWord">Stability</strong>: Similar<a id="_idIndexMarker019"/> to the previous point on quality, if the data comes from A to B but it only arrives every second Wednesday if it’s not a rainy day, then what’s the point? 
    <p class="bulletList">Data engineers spend a lot of time and effort and use their considerable skills to ensure that data pipelines are robust, reliable, and can be trusted to deliver when promised.</p></li>
</ul>
<ul>
<li class="bulletList"><strong class="keyWord">Access</strong>: Finally, the aim of <a id="_idIndexMarker020"/>getting data from A to B is for it to be used by applications, analyses, and ML models, so the nature of <em class="italic">B</em> is important. The data engineer will have a variety of technologies to hand to surface data and should work with the data consumers (our data scientists and ML engineers, among others) to define and create appropriate data models within these solutions:</li>
</ul>
<figure class="mediaobject"><img alt="Figure 1.1 – A diagram showing the relationships between data science, ML engineering, and data engineering " height="292" src="../Images/B19525_01_01.png" width="435"/></figure>
<p class="packt_figref">Figure 1.1: A diagram showing the relationships between data science, ML engineering, and data engineering.</p>
<p class="normal">As mentioned previously, this book focuses on the work of the ML engineer and how you can learn some of the skills useful for that role, but it is important to remember that you will not be working in a vacuum. Always keep in mind the profiles of the other roles (and many more not covered here that will exist in your project team) so that you work most effectively together. Data is a team sport after all!</p>
<p class="normal">Now that you understand the key roles in a modern data team and how they cover the spectrum of activities required to build successful ML products, let’s look at how you can put them together to work efficiently and effectively.</p>
<h1 class="heading-1" id="_idParaDest-22">Working as an effective team</h1>
<p class="normal">In modern <a id="_idIndexMarker021"/>software organizations, there are many different methodologies to organize teams and get them to work effectively together. We will cover some of the project management methodologies that are relevant in <em class="chapterRef">Chapter 2</em>, <em class="italic">The Machine Learning Development Process</em>, but in the meantime, this section will discuss some important points you should consider if you are ever involved in forming a team, or even if you just work as part of a team, that will help you become an effective teammate or lead.</p>
<p class="normal">First, always bear in mind that <em class="italic">nobody can do everything</em>. You can find some very talented people out there, but do not ever think one person can do everything you will need to the level you require. This is not just unrealistic; it is bad practice and will negatively impact the quality of your products. Even when you are severely resource-constrained, the key is for your team members to have a laser-like focus to succeed.</p>
<p class="normal">Second, <em class="italic">blended is best</em>. We all know the benefits of diversity for organizations and teams in general and this should, of course, apply to your ML team as well. Within a project, you will need mathematics, code, engineering, project management, communication, and a variety of other skills to succeed. So, given the previous point, make sure you cover this in at least some sense across your team.</p>
<p class="normal">Third, <em class="italic">tie your team structure to your projects in a dynamic way</em>. If you work on a project that is mostly about getting the data in the right place and the actual ML models are really simple, focus your team profile on the engineering and data modeling aspects. If the project requires a detailed understanding of the model, and it is quite complex, then reposition your team to make sure this is covered. This is just sensible and frees up team members who would otherwise have been underutilized to work on other projects.</p>
<p class="normal">As an example, suppose that you have been tasked with building a system that classifies customer data as it comes into your shiny new data lake, and the decision has been taken that this should be done at the point of ingestion via a streaming application. The classification has already been built for another project. It is already clear that this solution will heavily involve the skills of the data engineer and the ML engineer, but not so much the <a id="_idIndexMarker022"/>data scientist, since that portion of the work will have been completed in another project.</p>
<p class="normal">In the next section, we will look at some important points to consider when deploying your team on a real-world business problem.</p>
<h1 class="heading-1" id="_idParaDest-23">ML engineering in the real world</h1>
<p class="normal">The majority of us who <a id="_idIndexMarker023"/>work in ML, analytics, and related disciplines do so for organizations with a variety of different structures and motives. These could be for for-profit corporations, not-for-profits, charities, or public sector organizations like government or universities. In pretty much all of these cases, we do not do this work in a vacuum and not with an infinite budget of time or resources. It is important, therefore, that we consider some of the important aspects of doing this type of work in the <em class="italic">real world</em>.</p>
<p class="normal">First of all, the <a id="_idIndexMarker024"/>ultimate goal of your work is to generate <strong class="keyWord">value</strong>. This can be calculated and defined in a variety of ways, but fundamentally your work has to improve something for the company or its customers in a way that justifies the investment put in. This is why most companies will not be happy for you to take a year to play with new tools and then generate nothing concrete to show for it, or to spend your days only reading the latest papers. Yes, these things are part of any job in technology, and they can definitely be super-fun, but you have to be strategic about how you spend your time and always be aware of your value proposition.</p>
<p class="normal">Secondly, to be a successful ML engineer in the real world, you cannot just understand the technology; you must understand<em class="italic"> the business</em>. You will have to understand how the company works day to day, you will have to understand how the different pieces of the company fit together, and you will have to understand the people of the company and their roles. Most importantly, you have to understand <em class="italic">the customer</em>, both of the business and your work. If you do not know the motivations, pains, and needs of the people you build for, then how can you be expected to build the right thing?</p>
<p class="normal">Finally, and this may be controversial, the most important skill for you to become a successful ML engineer in the real world is one that this book will not teach you, and that is the ability to communicate effectively. You will have to work in a team, with a manager, with the wider community and business, and, of course, with your customers, as mentioned above. If you can do this and you know the technology and techniques (many of which are discussed in this book), then what can stop you?</p>
<p class="normal">But what kinds of problems can you solve with ML when you work in the real world? Well, let’s start with another potentially controversial statement: <em class="italic">a lot of the time, ML is not the answer</em>. This may seem strange given the title of this book, but it is just as important to know when <em class="italic">not</em> to apply ML as when to apply it. This will save you tons of expensive development time and resources.</p>
<p class="normal">ML is ideal for cases when you want to do a semi-routine task faster, with more accuracy, or at a far larger scale than is possible with other solutions. </p>
<p class="normal">Some typical examples are given in the following table, along with some discussion as to whether or not ML would be an appropriate tool to solve the problem:</p>
<table class="table-container" id="table001">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Requirement</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Is ML Appropriate?</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Details</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Anomaly detection of energy pricing signals.</p>
</td>
<td class="table-cell">
<p class="normal">Yes</p>
</td>
<td class="table-cell">
<p class="normal">You will want to do this on large numbers of points on potentially varying time signals.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Improving data quality in an ERP system.</p>
</td>
<td class="table-cell">
<p class="normal">No</p>
</td>
<td class="table-cell">
<p class="normal">This sounds more like a process problem. You can try and apply ML to this but often it is better to make the data entry process more automated or the process more robust.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Forecasting item consumption for a warehouse.</p>
</td>
<td class="table-cell">
<p class="normal">Yes</p>
</td>
<td class="table-cell">
<p class="normal">ML will be able to do this more accurately than a human can, so this is a good area of application.</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">Summarizing data for business reviews.</p>
</td>
<td class="table-cell">
<p class="normal">Maybe</p>
</td>
<td class="table-cell">
<p class="normal">This can be required at scale but it is not an ML problem – simple queries against your data will do.</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 1.1: Potential use cases for ML.</p>
<p class="normal">As this table of <a id="_idIndexMarker025"/>simple examples hopefully starts to make clear, the cases where ML <em class="italic">is</em> the answer are ones that can usually be very well framed as a mathematical or statistical problem. After all, this is what ML really is – a series of algorithms rooted in mathematics that can iterate some internal parameters based on data. Where the lines start to blur in the modern world are through advances in areas such as deep learning or reinforcement learning, where problems that we previously thought would be very hard to phrase appropriately for standard ML algorithms can now be tackled.</p>
<p class="normal">The other tendency<a id="_idIndexMarker026"/> to watch out for in the real world (to go along with <em class="italic">let’s use ML for everything</em>) is the worry that people have about ML coming for their job and that it should not be trusted. This is understandable: a report by PwC in 2018 suggested that 30% of UK jobs will be impacted by automation by the 2030s (<em class="italic">Will Robots Really Steal Our Jobs?</em>: <a href="https://www.pwc.co.uk/economic-services/assets/international-impact-of-automation-feb-2018.pdf"><span class="url">https://www.pwc.co.uk/economic-services/assets/international-impact-of-automation-feb-2018.pdf</span></a>). What you have to try and make clear when working with your colleagues and customers is that what you are building is there to supplement and augment their capabilities, not to replace them.</p>
<p class="normal">Let’s conclude this section by revisiting an important point: the fact that you work for a company means, of course, that the aim of the game is to create value appropriate to the investment. In other words, you need to show a<a id="_idIndexMarker027"/> good <strong class="keyWord">Return on Investment</strong> (<strong class="keyWord">ROI</strong>). This means a couple of things for you practically:</p>
<ul>
<li class="bulletList">You have to understand how different designs require different levels of investment. If you can solve your problem by training a deep neural net on a million images with a GPU running 24/7 for a month, or you know you can solve the same problem with some basic clustering and a few statistics on some standard hardware in a few hours, which should you choose?</li>
<li class="bulletList">You have to be clear about the <em class="italic">value</em> you will generate. This means you need to work with experts and try to translate the results of your algorithm into actual dollar values. This is so much more difficult than it sounds, so you should take the time you need to get it right. And never, ever over-promise. <em class="italic">You should always under-promise and over-deliver</em>.</li>
</ul>
<p class="normal">Adoption is <a id="_idIndexMarker028"/>not guaranteed. Even when building products for your colleagues within a company, it is important to understand that your solution will be tested every time someone uses it post-deployment. If you build shoddy solutions, then people will not use them, and the value proposition of what you have done will start to disappear.</p>
<p class="normal">Now that you understand some of the important points when using ML to solve business problems, let’s explore what these solutions can look like.</p>
<h1 class="heading-1" id="_idParaDest-24">What does an ML solution look like?</h1>
<p class="normal">When you<a id="_idIndexMarker029"/> think of ML engineering, you would be forgiven for defaulting to imagining working on voice assistance and visual recognition apps (I fell into this trap in previous pages – did you notice?). The power of ML, however, lies in the fact that wherever there is data and an appropriate problem, it can help and be integral to the solution.</p>
<p class="normal">Some examples might help make this clearer. When you type a text message and your phone suggests the next words, it can very often be using a natural language model under the hood. When you scroll any social media feed or watch a streaming service, recommendation algorithms are working double time. If you take a car journey and an app forecasts when you are likely to arrive at your destination, there is going to be some kind of regression at work. Your loan application often results in your characteristics and application details being passed through a classifier. These applications are not the ones shouted about on the news (perhaps with the exception of when they go horribly wrong), but they are all examples of brilliantly put-together ML engineering.</p>
<p class="normal">In this book, the examples we will work through will be more like these – typical scenarios for ML encountered in products and businesses every day. These are solutions that, if you can build them confidently, will make you an asset to any organization.</p>
<p class="normal">We should start by considering the broad elements that should constitute any ML solution, as indicated in the following diagram:</p>
<figure class="mediaobject"><img alt="" height="153" role="presentation" src="../Images/B19525_01_02.png" width="403"/></figure>
<p class="packt_figref">Figure 1.2: Schematic of the general components or layers of any ML solution and what they are responsible for.</p>
<p class="normal">Your <strong class="keyWord">storage layer</strong> constitutes<a id="_idIndexMarker030"/> the endpoint of the data engineering process and the beginning of the ML one. It includes your data for training, your results from running your models, your artifacts, and important metadata. We can also consider this as the layer including your stored code.</p>
<p class="normal">The <strong class="keyWord">compute layer</strong> is where<a id="_idIndexMarker031"/> the <em class="italic">magic</em> happens and where most of the focus of this book will be. It is where training, testing, prediction, and transformation all (mostly) happen. This book is all about making this layer as well engineered as possible and interfacing with the other layers. </p>
<p class="normal">You can break this layer down to incorporate these pieces as shown in the following workflow:</p>
<figure class="mediaobject"><img alt="Figure 1.4 – The key elements of the compute layer " height="199" src="../Images/B19525_01_03.png" width="460"/></figure>
<p class="packt_figref">Figure 1.3: The key elements of the compute layer.</p>
<div class="note">
<p class="normal">IMPORTANT NOTE</p>
<p class="normal">The details are discussed later in the book, but this highlights the fact that at a fundamental level, your compute processes for any ML solution are really just about taking some data in and pushing some data out.</p>
</div>
<p class="normal">The <strong class="keyWord">application layer</strong> is<a id="_idIndexMarker032"/> where you share your ML solution’s results with other systems. This could be through anything from application database insertion to API endpoints, message queues, or visualization tools. This is the layer through which your customer eventually gets to use the results, so you must engineer your system to provide clean and understandable outputs, something we will discuss later.</p>
<p class="normal">And that is it in a nutshell. We will go into detail about all of these layers and points later, but for now, just remember these broad concepts and you will start to understand how all the detailed technical pieces fit together.</p>
<h2 class="heading-2" id="_idParaDest-25">Why Python?</h2>
<p class="normal">Before moving on to more detailed topics, it is important to discuss why Python has been selected as the programming language for this book. Everything that follows that pertains to higher-level topics, such as architecture and system design, can be applied to solutions using any or multiple languages, but Python has been singled out here for a few reasons.</p>
<p class="normal">Python<a id="_idIndexMarker033"/> is colloquially known as the <em class="italic">lingua franca</em> of data. It is a non-compiled, not strongly typed, and multi-paradigm programming language that has a clear and simple syntax. Its tooling ecosystem is also extensive, especially in the analytics and ML space. </p>
<p class="normal">Packages such as scikit-learn, numpy, scipy, and a host of others form the backbone of a huge amount of technical and scientific development across the world. Almost every major new software library for use in the data world has a Python API. It is the most <a id="_idIndexMarker034"/>popular programming language in the world, according to the <strong class="keyWord">TIOBE index</strong> (<a href="https://www.tiobe.com/tiobe-index/"><span class="url">https://www.tiobe.com/tiobe-index/</span></a>) at the time of writing (August 2023).</p>
<p class="normal">Given this, being able to build your systems using Python means you will be able to leverage all of the excellent ML and data science tools available in this ecosystem, while also ensuring that you build applications that can play nicely with other software.</p>
<h1 class="heading-1" id="_idParaDest-26">High-level ML system design</h1>
<p class="normal">When you get down to the<a id="_idIndexMarker035"/> nuts and bolts of building your solution, there are so many options for tools, tech, and approaches that it can be very easy to be overwhelmed. However, as alluded to in the previous sections, a lot of this complexity can be abstracted to understand the bigger picture via some <em class="italic">back-of-the-envelope</em> architecture and designs. This is always a useful exercise once you know what problem you will try and solve, and it is something I recommend doing before you make any detailed choices about implementation.</p>
<p class="normal">To give you an idea of how this works in practice, what follows are a few worked-through examples where a team has to create a high-level ML systems design for some typical business problems. These problems are similar to ones I have encountered before and will likely be similar to ones you will encounter in your own work.</p>
<h2 class="heading-2" id="_idParaDest-27">Example 1: Batch anomaly detection service</h2>
<p class="normal">You work for a tech-savvy taxi ride<a id="_idIndexMarker036"/> company with a <a id="_idIndexMarker037"/>fleet of thousands of cars. The organization wants to start making ride times more consistent and understand longer journeys in order to improve the customer experience and, thereby, increase retention and return business. Your ML team is employed to create an anomaly detection service to find rides that have unusual ride time or ride length behaviors. You all get to work, and your data scientists find that if you perform clustering on sets of rides using the features of ride distance and time, you can clearly identify outliers worth investigating by the operations team. The data scientists present the findings to the CTO and other stakeholders before getting the go-ahead to develop this into a service that will provide an outlier flag as a new field in one of the main tables of the company’s internal analysis tool.</p>
<p class="normal">In this example, we will simulate some data to show how the taxi company’s data scientists could proceed. In the repository for the book, which can be found at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Python-Second-Edition</span></a>, if you navigate to the folder <em class="italic">Chapter01</em>, you will see a script called <code class="inlineCode">clustering_example.py</code>. If you have activated the <code class="inlineCode">conda</code> environment provided via the <code class="inlineCode">mlewp-chapter01.yml</code> environment file, then you can run this script with:</p>
<pre class="programlisting con"><code class="hljs-con">python3 clustering_example.py
</code></pre>
<p class="normal">After a successful run you should see that three files are created: <code class="inlineCode">taxi-rides.csv</code>, <code class="inlineCode">taxi-labels.json</code>, and <code class="inlineCode">taxi-rides.png</code>. The image in <code class="inlineCode">taxi-rides.png</code> should look something like that shown in <em class="italic">Figure 1.4</em>.</p>
<p class="normal">We will walk through how this script is built up:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">First, let’s define a function that will simulate some ride distances based on the random distribution given in <code class="inlineCode">numpy</code> and return a <code class="inlineCode">numpy</code> array containing the results. The reason for the repeated lines is so that we can create some base behavior and anomalies in the data, and you can clearly compare against the speeds we will generate for each set of taxis in the next step:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> MT19937
<span class="hljs-keyword">from</span> numpy.random <span class="hljs-keyword">import</span> RandomState, SeedSequence
rs = RandomState(MT19937(SeedSequence(<span class="hljs-number">123456789</span>)))

<span class="hljs-comment"># Define simulate ride data function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">simulate_ride_distances</span>():
    ride_dists = np.concatenate(
        (
            <span class="hljs-number">10</span> * np.random.random(size=<span class="hljs-number">370</span>),
            <span class="hljs-number">30</span> * np.random.random(size=<span class="hljs-number">10</span>), <span class="hljs-comment"># long distances</span>
            <span class="hljs-number">10</span> * np.random.random(size=<span class="hljs-number">10</span>), <span class="hljs-comment"># same distance</span>
            <span class="hljs-number">10</span> * np.random.random(size=<span class="hljs-number">10</span>) <span class="hljs-comment"># same distance</span>
        )
    )
    <span class="hljs-keyword">return</span> ride_dists
</code></pre>
</li>
<li class="numberedList">We <a id="_idIndexMarker038"/>can now do the exact same thing for speeds, and again we have split the taxis into sets of <code class="inlineCode">370</code>, <code class="inlineCode">10</code>, <code class="inlineCode">10</code>, and <code class="inlineCode">10</code> so <a id="_idIndexMarker039"/>that we can create some data with “typical” behavior and some sets of anomalies, while allowing for clear matching of the values with the <code class="inlineCode">distances</code> function:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">simulate_ride_speeds</span>():
    ride_speeds = np.concatenate(
        (
            np.random.normal(loc=<span class="hljs-number">30</span>, scale=<span class="hljs-number">5</span>, size=<span class="hljs-number">370</span>),
            np.random.normal(loc=<span class="hljs-number">30</span>, scale=<span class="hljs-number">5</span>, size=<span class="hljs-number">10</span>),
            np.random.normal(loc=<span class="hljs-number">50</span>, scale=<span class="hljs-number">10</span>, size=<span class="hljs-number">10</span>),
            np.random.normal(loc=<span class="hljs-number">15</span>, scale=<span class="hljs-number">4</span>, size=<span class="hljs-number">10</span>) 
        )
    )
    <span class="hljs-keyword">return</span> ride_speeds
</code></pre>
</li>
<li class="numberedList">We can now use both of these helper functions inside a function that will call them and bring them together to create a simulated dataset containing ride IDs, speeds, distances, and times. The result is returned as a <code class="inlineCode">pandas</code> DataFrame for use in modeling:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">simulate_ride_data</span>():
    <span class="hljs-comment"># Simulate some ride data …</span>
    ride_dists = simulate_ride_distances()
    ride_speeds = simulate_ride_speeds()
    ride_times = ride_dists/ride_speeds
    <span class="hljs-comment"># Assemble into Data Frame</span>
    df = pd.DataFrame(
        {
          <span class="hljs-string">'ride_dist'</span>: ride_dists,
          <span class="hljs-string">'ride_time'</span>: ride_times,
          <span class="hljs-string">'ride_speed'</span>: ride_speeds
        }
    )
    ride_ids = datetime.datetime.now().strftime(<span class="hljs-string">"%Y%m%d"</span>) +\
               df.index.astype(<span class="hljs-built_in">str</span>)
    df[<span class="hljs-string">'ride_id'</span>] = ride_ids
    <span class="hljs-keyword">return</span> df
</code></pre>
</li>
<li class="numberedList">Now, we <a id="_idIndexMarker040"/>get to the core of what data scientists produce in their projects, which is a simple function that wraps some <code class="inlineCode">sklearn</code> code to return a dictionary with the clustering run metadata and<a id="_idIndexMarker041"/> results. 
    <p class="numberedList">We include the relevant imports here for ease:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.preprocessing <span class="hljs-keyword">import</span> StandardScaler
<span class="hljs-keyword">from</span> sklearn.cluster <span class="hljs-keyword">import</span> DBSCAN
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics 

<span class="hljs-keyword">def</span> <span class="hljs-title">cluster_and_label</span>(<span class="hljs-params">data, create_and_show_plot=</span><span class="hljs-literal">True</span>):
    data = StandardScaler().fit_transform(data)
    db = DBSCAN(eps=<span class="hljs-number">0.3</span>, min_samples=<span class="hljs-number">10</span>).fit(data)
    <span class="hljs-comment"># Find labels from the clustering</span>
    core_samples_mask = np.zeros_like(db.labels_,dtype=<span class="hljs-built_in">bool</span>)
    core_samples_mask[db.core_sample_indices_] = <span class="hljs-literal">True</span>
    labels = db.labels_
    <span class="hljs-comment"># Number of clusters in labels, ignoring noise if present.</span>
    n_clusters_ = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(labels)) -
                         (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> -<span class="hljs-number">1</span> <span class="hljs-keyword">in</span> labels <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
    n_noise_ = <span class="hljs-built_in">list</span>(labels).count(-<span class="hljs-number">1</span>)
    run_metadata = {
        <span class="hljs-string">'</span><span class="hljs-string">nClusters'</span>: n_clusters_,
        <span class="hljs-string">'nNoise'</span>: n_noise_,
        <span class="hljs-string">'silhouetteCoefficient'</span>:
         metrics.silhouette_score(data, labels),
        <span class="hljs-string">'labels'</span>: labels,
    }
    <span class="hljs-keyword">if</span> create_and_show_plot:
        plot_cluster_results(data, labels, core_samples_mask,
                             n_clusters_)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">pass</span>
    <span class="hljs-keyword">return</span> run_metadata
</code></pre>
<p class="normal">Note<a id="_idIndexMarker042"/> that the function in <em class="italic">step 4</em> leverages<a id="_idIndexMarker043"/> a utility function for plotting that is shown below:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-keyword">def</span> <span class="hljs-title">plot_cluster_results</span>(<span class="hljs-params">data, labels, core_samples_mask,</span>
<span class="hljs-params">                         n_clusters_</span>):
    fig = plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>))
    <span class="hljs-comment"># Black removed and is used for noise instead.</span>
    unique_labels = <span class="hljs-built_in">set</span>(labels)
    colors = [plt.cm.cool(each) <span class="hljs-keyword">for</span> each <span class="hljs-keyword">in</span> np.linspace(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>,
              <span class="hljs-built_in">len</span>(unique_labels))]
    <span class="hljs-keyword">for</span> k, col <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(unique_labels, colors):
        <span class="hljs-keyword">if</span> k == -<span class="hljs-number">1</span>:
            <span class="hljs-comment"># Black used for noise.</span>
            col = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]
        class_member_mask = (labels == k)
        xy = data[class_member_mask &amp; core_samples_mask]
        plt.plot(xy[:, <span class="hljs-number">0</span>], xy[:, <span class="hljs-number">1</span>], <span class="hljs-string">'o'</span>,
                 markerfacecolor=<span class="hljs-built_in">tuple</span>(col),
                 markeredgecolor=<span class="hljs-string">'k'</span>, markersize=<span class="hljs-number">14</span>)
        xy = data[class_member_mask &amp; ~core_samples_mask]
        plt.plot(xy[:, <span class="hljs-number">0</span>], xy[:, <span class="hljs-number">1</span>], <span class="hljs-string">'^'</span>,
                 markerfacecolor=<span class="hljs-built_in">tuple</span>(col),
                 markeredgecolor=<span class="hljs-string">'k'</span>, markersize=<span class="hljs-number">14</span>)
    plt.xlabel(<span class="hljs-string">'Standard Scaled Ride Dist.'</span>)
    plt.ylabel(<span class="hljs-string">'Standard Scaled Ride Time'</span>)
    plt.title(<span class="hljs-string">'Estimated number of clusters: %d'</span> % n_clusters_)
    plt.savefig(<span class="hljs-string">'taxi-rides.png'</span>)
</code></pre>
<p class="normal">Finally, this is all brought together at the entry point of the program, as shown below:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> logging
logging.basicConfig()
logging.getLogger().setLevel(logging.INFO)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> os
    <span class="hljs-comment"># If data present, read it in</span>
    file_path = <span class="hljs-string">'taxi-rides.csv'</span>
    <span class="hljs-keyword">if</span> os.path.exists(file_path):
        df = pd.read_csv(file_path)
    <span class="hljs-keyword">else</span>:
        logging.info(<span class="hljs-string">'Simulating ride data'</span>)
        df = simulate_ride_data()
        df.to_csv(file_path, index=<span class="hljs-literal">False</span>)
    X = df[[<span class="hljs-string">'ride_dist'</span>, <span class="hljs-string">'ride_time'</span>]]
    
    logging.info(<span class="hljs-string">'Clustering and labelling'</span>)
    results = cluster_and_label(X, create_and_show_plot=<span class="hljs-literal">True</span>)
    df[<span class="hljs-string">'label'</span>] = results[<span class="hljs-string">'labels'</span>]
    
    logging.info(<span class="hljs-string">'Outputting to json ...'</span>)
    df.to_json(<span class="hljs-string">'taxi-labels.json'</span>, orient=<span class="hljs-string">'records'</span>)
</code></pre></li>
</ol>
<p class="normal">This script, once run, creates<a id="_idIndexMarker044"/> a dataset showing each simulated taxi<a id="_idIndexMarker045"/> journey with its clustering label in <code class="inlineCode">taxi-labels.json</code>, as well as the simulated dataset in <code class="inlineCode">taxi-rides.csv</code> and the plot showing the results of the clustering in <code class="inlineCode">taxi-rides.png</code>, as shown in <em class="italic">Figure 1.4</em>.</p>
<figure class="mediaobject"><img alt="Figure 1.5 – An example set of results from performing clustering on some taxi ride data " height="520" src="../Images/B19525_01_04.png" width="520"/></figure>
<p class="packt_figref">Figure 1.4: An example set of results from performing clustering on some taxi ride data.</p>
<p class="normal">Now that you<a id="_idIndexMarker046"/> have a basic model that works, you have to start<a id="_idIndexMarker047"/> thinking about how to pull this into an engineered solution – how could you do it?</p>
<p class="normal">Well, since the solution here will support longer-running investigations by another team, there is no need for a very low-latency solution. The stakeholders agree that the insights from clustering can be delivered at the end of each day. Working with the data science part of the team, the ML engineers (led by you) understand that if clustering is run daily, this provides enough data to give appropriate clusters, but doing the runs any more frequently could lead to poorer results due to smaller amounts of data. So, a daily batch process is agreed upon.</p>
<p class="normal">The next question is, how do you schedule that run? Well, you will need an orchestration layer, which is a tool or tools that will enable you to schedule and manage pre-defined jobs. A tool like Apache Airflow would do exactly this.</p>
<p class="normal">What do you do next? Well, you know the frequency of runs is daily, but the volume of data is still very high, so it makes sense to leverage a distributed computing paradigm. Two options immediately come to mind and are skillsets that exist within the team, Apache Spark and Ray. To provide as much decoupling as possible from the underlying infrastructure and minimize the refactoring of your code required, you decide to use Ray. You know that the end consumer of the data is a table in a SQL database, so you need to work with the database team to design an appropriate handover of the results. Due to security and reliability concerns, it is not a good idea to write to the production database directly. You, therefore, agree that another database in the cloud will be used as an intermediate staging area for the data, which the main database can query against on its daily builds.</p>
<p class="normal">It might not seem like we<a id="_idIndexMarker048"/> have done anything technical here, but <a id="_idIndexMarker049"/>actually, you have already performed the high-level system design for your project. The rest of this book tells you how to fill in the gaps in the following diagram!</p>
<figure class="mediaobject"><img alt="Figure 1.6 – Example 1 workflow " height="234" src="../Images/B19525_01_05.png" width="663"/></figure>
<p class="packt_figref">Figure 1.5: Example 1 workflow.</p>
<p class="normal">Let’s now move on to the next example!</p>
<h2 class="heading-2" id="_idParaDest-28">Example 2: Forecasting API</h2>
<p class="normal">In this example, you<a id="_idIndexMarker050"/> work for the logistics arm of a large retail chain. To maximize<a id="_idIndexMarker051"/> the flow of goods, the company would like to help regional logistics planners get ahead of particularly busy periods and avoid product sell-outs. After discussions with stakeholders and subject matter experts across the business, it is agreed that the ability for planners to dynamically request and explore forecasts for particular warehouse items through a web-hosted dashboard is optimal. This allows the planners to understand likely future demand profiles before they make orders.</p>
<p class="normal">The data scientists come good again and find that the data has very predictable behavior at the level of any individual store. They decide to use the Facebook Prophet library for their modeling to help speed up the process of training many different models. In the following example we will show how they could do this, but we will not spend time optimizing the model to create the best predictive performance, as this is just for illustration purposes.</p>
<p class="normal">This example will use the Kaggle API in order to retrieve an exemplar dataset for sales in a series of different retail stores. In the book repository under <em class="italic">Chapter01/forecasting</em> there is a script called <code class="inlineCode">forecasting_example.py</code>. If you have your Python environment configured appropriately you can run this example with the following command at the command line:</p>
<pre class="programlisting con"><code class="hljs-con">python3 forecasting_example.py
</code></pre>
<p class="normal">The script downloads the dataset, transforms it, and uses it to train a Prophet forecasting model, before running a prediction on a test set and saving a plot. As mentioned, this is for illustration purposes only and so does not create a validation set or perform any more complex hyperparameter tuning than the defaults provided by the Prophet library.</p>
<p class="normal">To help you see how this example is pieced together, we will now break down the different components of the script. Any functionality that is purely for plotting or logging is excluded here for brevity:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">If we look at the main block of the script, we can see that the first steps all concern reading in the dataset if it is already in the correct directory, or downloading and then reading it in otherwise:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    <span class="hljs-keyword">import</span> os
    file_path = <span class="hljs-string">train.csv</span>
    <span class="hljs-keyword">if</span> os.path.exists(file_path):
        df = pd.read_csv(file_path)
    <span class="hljs-keyword">else</span>:
        download_kaggle_dataset()
        df = pd.read_csv(file_path)
</code></pre>
</li>
<li class="numberedList">The<a id="_idIndexMarker052"/> function that performed the download used the Kaggle <a id="_idIndexMarker053"/>API and is given below; you can refer to the Kaggle API documentation to ensure this is set up correctly (which requires a Kaggle account):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> kaggle

<span class="hljs-keyword">def</span> <span class="hljs-title">download_kaggle_dataset</span>(<span class="hljs-params"> kaggle_dataset: </span><span class="hljs-built_in">str</span><span class="hljs-params"> =</span><span class="hljs-string">"pratyushakar/</span>
<span class="hljs-string">                             rossmann-store-sales"</span><span class="hljs-params"> </span>) -&gt; <span class="hljs-literal">None</span>:
    api = kaggle.api
    kaggle.api.dataset_download_files(kaggle_dataset, path=<span class="hljs-string">"./"</span>,
                                      unzip=<span class="hljs-literal">True</span>, quiet=<span class="hljs-literal">False</span>)
</code></pre>
</li>
<li class="numberedList">Next, the script calls a function to transform the dataset called <code class="inlineCode">prep_store_data</code>. This is called with two default values, one for a store ID and the other specifying that we only want to see data for when the store was open. The definition of this function is given below:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">prep_store_data</span>(<span class="hljs-params">df: pd.DataFrame, </span>
<span class="hljs-params">                    store_id: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">4</span><span class="hljs-params">, </span>
<span class="hljs-params">                    store_open: </span><span class="hljs-built_in">int</span><span class="hljs-params"> = </span><span class="hljs-number">1</span>) -&gt; pd.DataFrame:
    df[<span class="hljs-string">'</span><span class="hljs-string">Date'</span>] = pd.to_datetime(df[<span class="hljs-string">'Date'</span>])
    df.rename(columns= {<span class="hljs-string">'Date'</span>:<span class="hljs-string">'ds'</span>,<span class="hljs-string">'Sales'</span>:<span class="hljs-string">'y'</span>}, inplace=<span class="hljs-literal">True</span>)
    df_store = df[
        (df[<span class="hljs-string">'Store'</span>] == store_id) &amp; 
        (df[<span class="hljs-string">'Open'</span>] == store_open)
        ].reset_index(drop=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">    return</span> df_store.sort_values(<span class="hljs-string">'ds'</span>, ascending=<span class="hljs-literal">True</span>)
</code></pre>
</li>
<li class="numberedList">The Prophet forecasting model is then trained on the first 80% of the data and makes a prediction on the remaining 20% of the data. Seasonality parameters are provided to the model in order to guide its optimization:
        <pre class="programlisting code"><code class="hljs-code">seasonality = {
    <span class="hljs-string">'yearly'</span>: <span class="hljs-literal">True</span>,
    <span class="hljs-string">'weekly'</span>: <span class="hljs-literal">True</span>,
    <span class="hljs-string">'daily'</span>: <span class="hljs-literal">False</span>
}
predicted, df_train, df_test, train_index = train_predict(
    df = df,
    train_fraction = <span class="hljs-number">0.8</span>,
    seasonality=seasonality
)
</code></pre>
<p class="normal">The <a id="_idIndexMarker054"/>definition of the <code class="inlineCode">train_predict</code> method is given<a id="_idIndexMarker055"/> below, and you can see that it wraps some further data prep and the main calls to the Prophet package:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train_predict</span>(<span class="hljs-params">df: pd.DataFrame, train_fraction: </span><span class="hljs-built_in">float</span><span class="hljs-params">, </span>
<span class="hljs-params">                  seasonality: </span><span class="hljs-built_in">dict</span>) -&gt; <span class="hljs-built_in">tuple</span>[
                      pd.DataFrame,pd.DataFrame,pd.DataFrame, <span class="hljs-built_in">int</span>]:
    train_index = <span class="hljs-built_in">int</span>(train_fraction*df.shape[<span class="hljs-number">0</span>])
    df_train = df.copy().iloc[<span class="hljs-number">0</span>:train_index]
    df_test = df.copy().iloc[train_index:]
    model=Prophet(
        yearly_seasonality=seasonality[<span class="hljs-string">'yearly'</span>],
        weekly_seasonality=seasonality[<span class="hljs-string">'weekly'</span>],
        daily_seasonality=seasonality[<span class="hljs-string">'daily'</span>],
        interval_width = <span class="hljs-number">0.95</span>
    )
    model.fit(df_train)
    predicted = model.predict(df_test)
    <span class="hljs-keyword">return</span> predicted, df_train, df_test, train_index
</code></pre></li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="5">Then, finally, a utility plotting function is called, which when run will create the output shown in <em class="italic">Figure 1.6</em>. This shows a zoomed-in view of the prediction on the test dataset. The details of this function are not given here for brevity, as discussed above:
        <pre class="programlisting code"><code class="hljs-code">plot_forecast(df_train, df_test, predicted)
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="" height="379" role="presentation" src="../Images/B19525_01_06.png" width="759"/></figure>
<p class="packt_figref">Figure 1.6: Forecasting store sales.</p>
<p class="normal">One issue<a id="_idIndexMarker056"/> here is that implementing a forecasting model like the one <a id="_idIndexMarker057"/>above for every store can quickly lead to hundreds or even thousands of models if the chain gathers enough data. Another issue is that not all stores are on the resource planning system used at the company yet, so some planners would like to retrieve forecasts for other stores they know are similar to their own. It is agreed that if users like this can explore regional profiles they believe are similar to their own data, then they can still make the optimal decisions.</p>
<p class="normal">Given this and the customer requirements for dynamic, ad hoc requests, you quickly rule out a full batch process. This wouldn’t cover the use case for regions not on the core system and wouldn’t allow for dynamic retrieval of up-to-date forecasts via the website, which would allow you to deploy models that forecast at a variety of time horizons in the future. It also means you could save on compute as you don’t need to manage the storage and updating of thousands of forecasts every day and your resources can be focused on model training.</p>
<p class="normal">Therefore, you decide that, actually, a web-hosted API with an endpoint that can return forecasts as needed by the user makes the most sense. To give efficient responses, you have to consider what happens in a typical user session. By workshopping with the potential users of the dashboard, you quickly realize that although the requests are dynamic, most planners will focus on particular items of interest in any one session. They will also not look at many regions. You then decide that it makes sense to have a caching strategy, where you take certain requests that you think might be common and cache them for reuse in the application. </p>
<p class="normal">This means that after the user makes their first selections, results can <a id="_idIndexMarker058"/>be returned more quickly for a better user experience. This<a id="_idIndexMarker059"/> leads to the rough system sketch in <em class="italic">Figure 1.7</em>:</p>
<figure class="mediaobject"><img alt="Figure 1.8 – Example 2 workflow " height="251" src="../Images/B19525_01_07.png" width="545"/></figure>
<p class="packt_figref">Figure 1.7: Example 2 workflow.</p>
<p class="normal">Next, let’s look at the final example.</p>
<h2 class="heading-2" id="_idParaDest-29">Example 3: Classification pipeline</h2>
<p class="normal">In this final example, you work for a <a id="_idIndexMarker060"/>web-based company that wants to <a id="_idIndexMarker061"/>classify users based on their usage patterns as targets for different types of advertising, in order to more effectively target marketing spend. For example, if the user uses the site less frequently, we may want to entice them with more aggressive discounts. One of the key requirements from the business is that the end results become part of the data landed in a data store used by other applications.</p>
<p class="normal">Based on these requirements, your team determines that a pipeline running a classification model is the simplest solution that ticks all the boxes. The data engineers focus their efforts on building the ingestion and data store infrastructure, while the ML engineer works to wrap up the classification model the data science team has trained on historical data. The base algorithm that the data scientists settle on is implemented in <code class="inlineCode">sklearn</code>, which we will work through below by applying it to a marketing dataset that would be similar to that produced in this use case.</p>
<p class="normal">This <a id="_idIndexMarker062"/>hypothetical example aligns with a lot of classic datasets, including the <a id="_idIndexMarker063"/>Bank Marketing dataset from the UCI ML repository: <a href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#"><span class="url">https://archive.ics.uci.edu/ml/datasets/Bank+Marketing#</span></a>. As in the previous example, there is a script you can run from the command line, this time in the <em class="italic">Chapter01/classifying</em> folder and called <code class="inlineCode">classify_example.py</code>:</p>
<pre class="programlisting con"><code class="hljs-con">python3 classify_example.py
</code></pre>
<p class="normal">Running this script will read in the downloaded bank data, rebalance the training dataset, and then execute a hyperparameter optimization run on a randomized grid search for a random forest classifier. Similarly to before, we will show how these pieces work to give a flavor of how a data science team might have approached this problem:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">The main block of the script contains all the relevant steps, which are neatly wrapped up into methods we will dissect over the next few steps:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:
    X_train, X_test, y_train, y_test = ingest_and_prep_data()
    X_balanced, y_balanced = rebalance_classes(X_train, y_train)
    rf_random = get_randomised_rf_cv(
                  random_grid=get_hyperparam_grid()
                  )
    rf_random.fit(X_balanced, y_balanced)
</code></pre>
</li>
<li class="numberedList">The <code class="inlineCode">ingest_and_prep_data</code> function is given below, and it does assume that the <code class="inlineCode">bank.csv</code> data is stored in a directory called <code class="inlineCode">bank_data</code> in the current folder. It reads the data into a <code class="inlineCode">pandas</code> DataFrame, before performing a train-test split on the data and one-hot encoding the training features, before returning all the train and test features and targets. As in the other examples, most of these concepts and tools will be explained throughout the book, particularly in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">ingest_and_prep_data</span>(
<span class="hljs-params">        bank_dataset: </span><span class="hljs-built_in">str</span><span class="hljs-params"> = </span><span class="hljs-string">'bank_data/bank.csv'</span>
<span class="hljs-params">        </span>) -&gt; <span class="hljs-built_in">tuple</span>[pd.DataFrame, pd.DataFrame, pd.DataFrame,
                   pd.DataFrame]:
    df = pd.read_csv(<span class="hljs-string">'bank_data/bank.csv'</span>, delimiter=<span class="hljs-string">';'</span>,
                      decimal=<span class="hljs-string">','</span>)
    
    feature_cols = [<span class="hljs-string">'job'</span>, <span class="hljs-string">'</span><span class="hljs-string">marital'</span>, <span class="hljs-string">'education'</span>, <span class="hljs-string">'contact'</span>,
                    <span class="hljs-string">'housing'</span>, <span class="hljs-string">'loan'</span>, <span class="hljs-string">'default'</span>, <span class="hljs-string">'day'</span>]
    X = df[feature_cols].copy()
    y = df[<span class="hljs-string">'y'</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> x == <span class="hljs-string">'yes'</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>).copy()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_
                                          size=<span class="hljs-number">0.2</span>, random_state=<span class="hljs-number">42</span>)
    enc = OneHotEncoder(handle_unknown=<span class="hljs-string">'ignore'</span>)
    X_train = enc.fit_transform(X_train)
    <span class="hljs-keyword">return</span> X_train, X_test, y_train, y_test
</code></pre>
</li>
<li class="numberedList">Because the data is imbalanced, we need to rebalance the training data with an oversampling technique. In this<a id="_idIndexMarker064"/> example, we will use the <strong class="keyWord">Synthetic Minority Over-Sampling Technique</strong> (<strong class="keyWord">SMOTE</strong>) from the <code class="inlineCode">imblearn</code> package:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">rebalance_classes</span>(<span class="hljs-params">X: pd.DataFrame, y: pd.DataFrame</span>
                      ) -&gt; <span class="hljs-built_in">tuple</span>[pd.DataFrame, pd.DataFrame]:
    sm = SMOTE()
    X_balanced, y_balanced = sm.fit_resample(X, y)
    <span class="hljs-keyword">return</span> X_balanced, y_balanced
</code></pre>
</li>
<li class="numberedList">Now we<a id="_idIndexMarker065"/> will move on to the main ML components of the <a id="_idIndexMarker066"/>script. We will perform a hyperparameter search (there’ll be more on this in <em class="chapterRef">Chapter 3</em>, <em class="italic">From Model to Model Factory</em>), so we have to define a grid to search over:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_hyperparam_grid</span>() -&gt; <span class="hljs-built_in">dict</span>:
    n_estimators = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(start=<span class="hljs-number">200</span>,
                    stop=<span class="hljs-number">2000</span>, num=<span class="hljs-number">10</span>)]
    max_features = [<span class="hljs-string">'auto'</span>, <span class="hljs-string">'sqrt'</span>]
    max_depth = [<span class="hljs-built_in">int</span>(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> np.linspace(<span class="hljs-number">10</span>, <span class="hljs-number">110</span>, num=<span class="hljs-number">11</span>)]
    max_depth.append(<span class="hljs-literal">None</span>)
    min_samples_split = [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>]
    min_samples_leaf = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>]
    bootstrap = [<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]  <span class="hljs-comment"># Create the random grid</span>
    random_grid = {
        <span class="hljs-string">'n_estimators'</span>: n_estimators,
        <span class="hljs-string">'max_features'</span>: max_features,
        <span class="hljs-string">'max_depth'</span>: max_depth,
        <span class="hljs-string">'min_samples_split'</span>: min_samples_split,
        <span class="hljs-string">'</span><span class="hljs-string">min_samples_leaf'</span>: min_samples_leaf,
        <span class="hljs-string">'bootstrap'</span>: bootstrap
    }
    <span class="hljs-keyword">return</span> random_grid
</code></pre>
</li>
<li class="numberedList">Then finally, this <a id="_idIndexMarker067"/>grid of hyperparameters will <a id="_idIndexMarker068"/>be used in the definition of a <code class="inlineCode">RandomisedSearchCV</code> object that allows us to optimize an estimator (here, a <code class="inlineCode">RandomForestClassifier</code>) over the hyperparameter values:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">get_randomised_rf_cv</span>(<span class="hljs-params">random_grid: </span><span class="hljs-built_in">dict</span>) -&gt; sklearn.model_
                               selection._search.RandomizedSearchCV:
    rf = RandomForestClassifier()
    rf_random = RandomizedSearchCV(
        estimator=rf,
        param_distributions=random_grid,
        n_iter=<span class="hljs-number">100</span>,
        cv=<span class="hljs-number">3</span>,
        verbose=<span class="hljs-number">2</span>,
        random_state=<span class="hljs-number">42</span>,
        n_jobs=-<span class="hljs-number">1</span>,
        scoring=<span class="hljs-string">'f1'</span>
    )
    <span class="hljs-keyword">return</span> rf_random
</code></pre>
</li>
</ol>
<p class="normal">The example above highlights the basic components of creating a typical classification model, but the question we have to ask ourselves as engineers is, “Then what?” It is clear that we have to actually run predictions with the model that has been produced, so we’ll need to persist it somewhere and read it in later. This is similar to the other use cases discussed in this chapter. Where things are more challenging here is that in this case, the engineers may actually consider not running in a batch or request-response scenario but in a streaming context. This means we will have to consider new technologies like <strong class="keyWord">Apache Kafka</strong> that enable you to both publish and subscribe to “topics” where packets of data called “events” can be shared. Not only that, but we will also have to make decisions about how to interact with data in this way using an ML model, raising questions about the appropriate model hosting mechanism. There will also be some subtleties around how often you want to retrain your algorithm to make sure that the classifier does not go stale. This is before we consider questions of latency or of monitoring the model’s performance in this very different setting. As you can see, this means that the ML engineer’s job here is quite a complex one. <em class="italic">Figure 1.8</em> subsumes all this complexity into a very high-level diagram that would allow you to start considering the sort of system interactions you would need to build if you were the engineer on this project. </p>
<p class="normal">We will not cover streaming in that much detail in this book, but we will cover all of the other key components that would help you build out this example into a real solution in a lot of detail. For more details on streaming ML applications please see the book <em class="italic">Machine Learning for Streaming Data with Python</em> by Joose Korstanje, Packt, 2022.</p>
<figure class="mediaobject"><img alt="Figure 1.9 – Example 3 workflow " height="285" src="../Images/B19525_01_08.png" width="586"/></figure>
<p class="packt_figref">Figure 1.8: Example 3 workflow.</p>
<p class="normal">We have now explored three high-level ML system designs and discussed the rationale behind our workflow choices. We have also explored in detail the sort of code that would often be produced by data scientists working on modeling, but which would act as input to future ML engineering work. This section should, therefore, have given us an appreciation of where our engineering work begins in a typical project and what types of problems we aim to solve. And there you go. You are already on your way to becoming an ML engineer!</p>
<h1 class="heading-1" id="_idParaDest-30">Summary</h1>
<p class="normal">In this chapter, we introduced the idea of ML engineering and how that fits within a modern team building valuable solutions based on data. There was a discussion of how the focus of ML engineering is complementary to the strengths of data science and data engineering and where these disciplines overlap. Some comments were made about how to use this information to assemble an appropriately resourced team for your projects.</p>
<p class="normal">The challenges of building ML products in modern real-world organizations were then discussed, along with pointers to help you overcome some of these challenges. In particular, the notions of reasonably estimating value and effectively communicating with your stakeholders were emphasized.</p>
<p class="normal">This chapter then rounded off with a taster of the technical content to come in later chapters, through a discussion of what typical ML solutions look like and how they should be designed (at a high level) for some common use cases.</p>
<p class="normal">These topics are important to cover before we dive deeper into the rest of the book, as they will help you to understand why ML engineering is such a critical discipline and how it ties into the complex ecosystem of data-focused teams and organizations. It also helps to give a taster of the complex challenges that ML engineering encompasses, while giving you some of the conceptual tools to start reasoning about those challenges. My hope is that this not only motivates you to engage with the material in the rest of this edition, but it also sets you down the path of exploration and self-study that will be required to have a successful career as an ML engineer.</p>
<p class="normal">The next chapter will focus on how to set up and implement your development processes to build the ML solutions you want, providing some insight into how this is different from standard software development processes. Then there will be a discussion of some of the tools you can use to start managing the tasks and artifacts from your projects without creating major headaches. This will set you up for the technical details of how to build the key elements of your ML solutions in later chapters.</p>
<h1 class="heading-1" id="_idParaDest-31">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussion with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/mle"><span class="url">https://packt.link/mle</span></a></p>
<p class="normal"><img alt="" height="177" role="presentation" src="../Images/QR_Code102810325355484.png" width="177"/></p>
</div>
</div></body></html>