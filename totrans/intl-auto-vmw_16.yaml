- en: Cloud Application Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will guide you through how to support next-gen cloud app development
    by providing developers with access to traditional, cloud-native, and modern application
    development frameworks and resources, including production-grade container services
    and open APIs. These will be used on a common vSphere platform and will also support
    legacy or traditional applications side by side with cloud-native and containerized
    apps, across a virtualized environment.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn how to optimize resources to get maximum output by defining parameters
    and what-if scenarios. These will be considered for future scalability, so that
    we can configure and autoscale parameters across different clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Pivotal Container Service** (**PKS**) on vSphere
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download VMware Enterprise PKS from [https://cloud.vmware.com/vmware-enterprise-pks/resources](https://cloud.vmware.com/vmware-enterprise-pks/resources).
  prefs: []
  type: TYPE_NORMAL
- en: Cloud-native applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Digital technologies are always changing, due to today's dynamic business objectives.
    Everything is connected via mobile, social networks, wearable devices, connected
    cars, and so on, and they are all influencing the way that we behave and engage
    with technologies today. Customers are demanding more innovative, flexible, and
    fast ways to experience products and services, due to this innovation in technology.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the systems operating in isolation from each other, responsibility,
    and skills set.We are going through a digital transformation and need all of these
    operations across various segments. Digital transformation redesigns organizational
    structures in many environments, so that they're collaborative. Technology can
    enhance performance and an organization's reach across the globe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud-native applications have four characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cloud-native apps are composed of microservices**: Cloud-native apps adopt
    a microservices architecture, where each application is a collection of small
    services that can be operated independent of one another. Microservices are often
    owned by individual development teams that operate on their own schedules to develop,
    deploy, scale, and upgrade their services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-native apps are packaged in containers**: Containers provide isolation
    contexts for microservices. They are highly accessible, scalable, easily portable
    from one environment to another, and fast to create or tear down, making them
    ideal for building and running applications that are composed of microservices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-native apps are running in a continuous delivery model**: Software
    developers and IT operations teams collaborate under this model to build, test,
    and release software updates as soon as they are ready, without affecting end
    users or developers on other teams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud-native applications are dynamically managed in the cloud**: They are
    often built and run on modern cloud-native platforms, which offer easy scale-out
    and hardware decoupling, helping in terms of the orchestration, management, and
    automation of the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automation with containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customers who have a substantial deployment of the VMware automation tools can
    easily drive agility and streamline the consumption of IT services. VMware will
    help customers to deliver both application and container services. This platform
    will extend the benefits of BOSH (autoscaling, self-healing, load balancing, and
    so on) to **container as a service** (**CaaS**) solutions (PKS). BOSH is an open
    source tool which helps in deployment and life cycle management of distributed
    systems. PKS is the only CaaS solution that can deliver fully managed Kubernetes
    clusters on premise, as well as a public **Infrastructure as a Service** (**IaaS**).
    This platform will also include **functions as a service** (**FaaS**). This will
    allow organizations to secure their abstraction planning, regardless of IaaS,
    by providing application deployment and runtime constructs on one platform. Because
    of this, we have to plan with various teams that are responsible for the app's
    rationalization and subsequent migration related to business and technical requirements,
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The **Pivotal Cloud Foundry** (**PCF**) includes both **Pivotal Application
    Service** (**PAS**) and PKS as critical components. PAS is the cloud-native platform
    for deploying and operating modern applications. PKS enables customers and service
    providers to deliver a production-ready Kubernetes on a VMware SDDC and other
    public cloud environments.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if we have a system of 10 apps running in containers, those 10
    apps will have 10 instances of isolated user spaces. Imagine that two applications
    are installed on the same operating system, but each needs a different version
    of that file. We can manage this condition with containers by using a common shared
    library file. Containers (more specifically, Linux containers) have been around
    for a while, and companies such as Oracle, HP, and IBM have been using containers
    for decades. However, Docker has become more popular among users.
  prefs: []
  type: TYPE_NORMAL
- en: Easy-to-use API and CLI tools for deploying apps that support namespace and
    resource limits reduce the complexity involved in deploying and managing containers.
    A container is a running instance of an image that runs a container. We need to
    download an image to use this. An image is a layered filesystem, where each layer
    has its own filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: When you want to make changes, there's no need to crack open a single, large,
    monolithic application and shove new changes in. If we have to make changes, then
    we can just add them to a new layer.
  prefs: []
  type: TYPE_NORMAL
- en: Containers are doing to operating systems what VMs did to server hardware. Tools
    and organizational processes that are required to run and operate containers are
    generally not defined. VMware and Pivotal are in a unique position to solve these
    new challenges and become the **incumbent**.Containers virtualize the operating
    system by limiting the number of application dependencies that we need to install
    on the OS.
  prefs: []
  type: TYPE_NORMAL
- en: Container use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the use cases for containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The need for a developer sandbox**:Developers often want access to a cluster
    of machines running a particular framework to build or quickly test and validate
    their applications. Provisioning such environments is time-consuming and often
    involves tickets and approvals. As a result, developers either request VMs and
    customize them to their needs, creating snowflake deployments, or they never give
    up these resources, because they are worried about obtaining a new one, which
    could be a tedious process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application repackaging**:Customers can take their existing applications
    and package them as a container. You don''t need to refractor code or make changes
    to the architecture. While this forms the first logical step in a customer''s
    containerization journey, it allows customers to derive certain benefits. Patching
    and maintaining the application is one primary benefit, where updates can be restricted
    to just the individual layers of the image. This ensures that other layers are
    intact, reducing errors and configuration issues that could arise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Portability**:Packaging an application as a container enables portability.
    The container image, by virtue of packaging not just the application code but
    also all of its dependencies, is guaranteed to work anywhere. We are now able
    to move this image from a developer''s laptop to your test/dev or production environment
    without having to invest time and resources in getting the target environments
    to exactly mimic the dev environment (or vice versa) as a result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges with containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We focus on enabling developer code to instantiate all the resources developers
    need, even for legacy systems, to provide high levels of automation during the
    waterfall approach and to enable the customers to self-serve their resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional model uses traditional application architectures, tooling, and processes,
    where developers have to raise tickets for resources in cloud delivery models.
    Resources are provided through self-service. Cloud-native applications initiate
    these requests through code and provide service **infrastructure as code** (**IaC**).
  prefs: []
  type: TYPE_NORMAL
- en: Code replaces the service tickets, and APIs play a critical role. A developer-ready
    infrastructure can be achieved with automated VMware SDDC tool by providing APIs
    that assist in running containers, such as OpenStack, PCF, and so on, as normal
    VM environments. Containers can be managed from existing operating models, since
    developers get all of the benefits. This is because IT has to manage the underlying
    resources in a consistent way.
  prefs: []
  type: TYPE_NORMAL
- en: The globally consistent infrastructure layer has benefits in the microservices
    architecture, as each service defines its relationship to other microservices.
    This can be broken if the underlying network is complex and doesn't have visibility.
    The network should be wide open to avoid this problem. Pivotal value VMware NSX
    and developer-ready infrastructure have the same code, which defines the relationships
    between microservices and instantiates secure micro-segmented network connectivity.
    Even serverless architectures can have an Internal Server Error message.
  prefs: []
  type: TYPE_NORMAL
- en: PKS on vSphere
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**vSphere cluster groups** are sets of ESXi hosts that have a common compute
    entity; there are from 2 to 64 hosts per vSphere cluster when vSphere HA and DRS
    are activated at the cluster level. Resource pools are created under a vSphere
    cluster instance, and vCenter is able to manage multiple vSphere clusters instances,
    as there is no hard limit to the number of vSphere clusters. We can create different
    types of vSphere clusters, such as management clusters, compute clusters, and
    Edge clusters, since PKS fully leverages the vSphere cluster construct.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following vSphere clusters are recommended in a typical PKS deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Management cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosted components**: vCenter, NSX manager, and controller VMs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere HA and DRS enabled
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ESXi hosts need to be NSX prepared, as micro-segmentation is enforced on the
    hosted VMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute cluster(s)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosted components**: Kubernetes (K8s) clusters nodes VMs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere HA and DRS should be enabled, as BOSH will check whether DRS is turned
    on
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ESXi hosts need to be NSX prepared
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hosted components**: NSX Edge Nodes VMs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: vSphere HA and DRS enabled
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ESXi hosts don't need to be NSX prepared
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The PKS Management Plane can reside on the management cluster or compute cluster,
    depending on the selected design scenario. PKS Management Plane VMs are the Ops
    Manager, BOSH, the PKS control plane, and Harbor.
  prefs: []
  type: TYPE_NORMAL
- en: The PKS data plane (or compute plane) will only reside in a compute cluster.
    Up to three K8s master nodes and 50 worker nodes are allowed per K8s cluster,
    and many K8s clusters can be created in the same PKS environment.
  prefs: []
  type: TYPE_NORMAL
- en: The K8s master node also hosts the etcd component. vSphere DRS and HA must be
    enabled on the vSphere compute cluster. vSphere DRS Automation has to be set to
    Partially Automated or Fully Automated. vSphere HA is set with Host failure =
    Restart VMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the compute and storage requirements for the PKS component:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PKS Component** | **CPU** | **RAM (GB)** | **Storage (GB)** |'
  prefs: []
  type: TYPE_TB
- en: '| Ops Manager | 1 | 8 | HD1: 160 |'
  prefs: []
  type: TYPE_TB
- en: '| PKS Control Plane VM | 2 | 8 | HD1: 3HD2: 16HD3: 10 |'
  prefs: []
  type: TYPE_TB
- en: '| BOSH | 2 | 8 | HD1: 3HD2: 50HD3: 50 |'
  prefs: []
  type: TYPE_TB
- en: '| Harbor | 2 | 8 | HD1: 3HD2: 64HD3: 30 |'
  prefs: []
  type: TYPE_TB
- en: '| K8s master node | Configurable per PKS plan | Configurable per PKS plan |
    Ephemeral disk: 8 to 256 GBPersistent disk: 1 GB to 32 TB (Configurable per PKS
    plan) |'
  prefs: []
  type: TYPE_TB
- en: '| K8s worker node | Configurable per PKS plan | Configurable per PKS plan |
    Ephemeral disk: 8 to 256 GBPersistent disk: 1 GB to 32 TB (Configurable per PKS
    plan) |'
  prefs: []
  type: TYPE_TB
- en: PKS availability zone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PKS supports the concept of an **availability zone** (**AZ**), that is, *AZ
    = vSphere cluster + resource pool*. The AZ dictates the placement of a VM that's
    created by BOSH/PKS into the corresponding vSphere cluster/resource pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of AZ:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Management AZ**: Used for BOSH, PKS control plane, and Harbor VMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute AZ**: Used for K8s master and worker node VMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PKS supports multiple compute availability zones, and each PKS plan supports
    up to three distinct ones. Each K8s master node (for a max of three) will land
    in one separate AZ. K8s worker nodes will be dispatched across the three zones.
  prefs: []
  type: TYPE_NORMAL
- en: Three PKS plans are allowed (for a total of nine distinct compute zones). Each
    PKS plan can use the same three zones or a completely different set of three AZs.
    An AZ is commonly used to set the locality of a VM against different locations; or,
    we can say that AZ = physical rack (or room).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the PKS design topologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physical topologies (integration with vSphere)**: Multiple topologies can
    be deployed with PKS/NSX-T integration'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS Management Plane in the management cluster**: Multi-compute clusters:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PKS Management Plane is hosted in a management cluster and connected to
    a DVS virtual switch
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple compute clusters, to support K8s cluster nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between the
    AZ and vSphere cluster).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AZ can represent a physical location**: Each compute cluster can be in a
    dedicated rack or room'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS Management Plane in the management cluster for** **a single compute cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PKS Management Plane is hosted in a management cluster and connected to
    a DVS virtual switch
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Single Compute Clusters to support K8s cluster nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each AZ is mapped to a unique vSphere cluster/different resource pool
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AZs can be used to limit the CPU/memory per PKS plan
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS Management Plane in the compute cluster for** **multi-compute clusters**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PKS Management Plane is hosted in a compute cluster and connected to an
    NSX-T logical switch
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple compute clusters, to support K8s cluster nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between AZs
    and vSphere clusters)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An AZ can represent a physical location**: Each compute cluster can be in
    a dedicated rack or room'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS Management Plane in a compute cluster, or a single compute cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PKS Management Plane is hosted in a compute cluster and connected to an
    NSX-T logical switch
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Single compute clusters, to support K8s cluster nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each AZ is mapped to a unique vSphere cluster/different resource pool:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An AZ can be used to the limit CPU/memory per PKS plan
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS AZ (Single/multiple Compute and Management Clusters) design model**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PKS AZ with single vSphere compute cluster**: By default, there is no guarantee
    that K8s master nodes land on different ESXi hosts. A workaround is to create
    a DRS affinity rule on the vSphere Compute Cluster.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type**: Separate VMs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Members**: All K8s master node VMs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The vSphere cluster must have a minimum of three ESXi hosts (this is a vSAN
    prerequisite). However, to protect against one host failure (and to make sure
    that the DRS affinity rule will operate properly), a recommendation is to start
    with four ESXi hosts in the cluster.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: NSX-T 2.2 supports all types of traffic on N-VDS. This means that an ESXi host
    in the compute cluster can start with two physical NICs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The minimum vSphere cluster configuration for a production environment is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Management cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-vSAN**: Min hosts: Two'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must have two replicas and one witness)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute cluster(s)**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single compute cluster topology**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-vSAN**: Min hosts: Three (To guarantee one K8s master node VM per ESXi
    host by using a DRS affinity rule)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must have two replicas and one witness)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple compute clusters topology**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-vSAN**: Min hosts: Two per AZ, with three AZs in total (The K8s master
    node is instantiated across different compute clusters. Each compute cluster is
    1:1 mapped with one AZ)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSAN**: Min hosts: Three per AZ, with three AZs in total (To guarantee data
    protection for vSAN objects, you must use two replicas and one witness)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge Cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-vSAN**: Min hosts: Two.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must use two replicas and one witness.) Note: An Edge Cluster can be collapsed
    with a compute cluster (or even a management cluster) if you need to lower the
    number of starting ESXi hosts.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table gives information about PKS/NSX-T networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Network** | **Description** | **CIDR** |'
  prefs: []
  type: TYPE_TB
- en: '| **PKS Management Network** |'
  prefs: []
  type: TYPE_TB
- en: This network hosts the Ops Manager, BOSH, the PKS control plane, and Harbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be co-located with vCenter, NSX-T management, and the control plane, if
    desired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PKS Management Network is routable or non-routable, depending on NO-NAT or NAT
    topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| 192.168.1.0/28 (for instance)CIDR with /28 is a good starting point. |'
  prefs: []
  type: TYPE_TB
- en: '| **Nodes IP Block** |'
  prefs: []
  type: TYPE_TB
- en: This block will be carved to create a network that will host K8s cluster node
    VMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each K8s cluster will be allocated a /24 portion of the block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nodes IP block is routable or non-routable, depending on NO-NAT or NAT topology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Depends on the NAT or NO-NAT topology.172.23.0.0/16 (for instance) |'
  prefs: []
  type: TYPE_TB
- en: '| **Pods IP Block** |'
  prefs: []
  type: TYPE_TB
- en: This block will be carved to create a network that will host K8s pods belonging
    to the same K8s namespace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each k8s namespace will be allocated a /24 portion of the block
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pods IP block is always non-routable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| 172.16.0.0/16 (for instance) |'
  prefs: []
  type: TYPE_TB
- en: '| **Floating IP Pool** |'
  prefs: []
  type: TYPE_TB
- en: 'This pool will be used for two purposes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SNAT rules for each K8s namespace on T0 (for pods networking)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: LB virtual servers IP allocation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The floating IP pool is always routable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| 192.168.20.2-192.168.20.254 (for instance) |'
  prefs: []
  type: TYPE_TB
- en: '** CIDR for nodes IP block**:'
  prefs: []
  type: TYPE_NORMAL
- en: Must be unique in the case of a routable scenario (NO-NAT topology)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be duplicated in the case of a non-routable scenario (NAT topology)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `172.17.0.0/16`CIDR must not be used in all cases, as Docker on the K8s
    worker node is using the subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'If PKS is deployed with Harbor, then the following CIDR must not be used, as
    Harbor is leveraging it for its internal Docker bridges:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Each K8s cluster uses the following IP block for Kubernetes services, so avoid
    using it for a nodes IP block: `10.100.200.0/24`.
  prefs: []
  type: TYPE_NORMAL
- en: PKS/NSX-T logical topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PKS supports two types of topologies when it's integrated with NSX-T. NAT and
    NO-NAT topology selection is done in the PKS tile | Networking section. NAT topology
    is the default, but you can uncheck NAT mode to go with the NO-NAT topology. The
    NAT and NO-NAT terminology essentially applies to the PKS Management Network and
    the K8s cluster nodes network (that is, whether to use routable subnets). Irrespective
    of the NAT or NO-NAT topology, the same procedure is used to access the K8s API.
  prefs: []
  type: TYPE_NORMAL
- en: 'A virtual server on the NSX-T LB instance that''s allocated to the K8s cluster
    is created for the following purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: One IP from the PKS Floating IP Pool is extracted (`1x.x0.1x.1xx` here), and
    the port is `8443`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same IP address is shown from the output of the `pks cluster <cluster name>` command
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Following are the objectives with different NAT topologies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NAT topology**: For customers with a limited amount of available routable
    IP addresses in their DC and who want to automate PKS deployment using a concourse
    pipeline (for instance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**:For customers who avoid NAT as NATs break full path visibility
    and having plenty of routable IP address resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use cases with different configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the use cases with different configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to PKS Management Plane components (Ops Manager, BOSH, PKS control plane
    VM, Harbor) from the corporate network:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**: No action is required as those components use routable
    IP addresses'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT topology**: User needs to create DNAT rules on T0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access to K8s API (using a kubectl CLI, for instance):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**: 1 virtual server (on the NSX-T LB instance that''s dedicated
    to the K8s cluster) is automatically created using 1 routable IP from the PKS
    Floating IP block'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT topology**: The user needs to point to this IP to access K8s API'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One virtual server (on the NSX-T LB instance that''s dedicated to the K8s cluster)
    is automatically created using one routable IP from the PKS floating IP block:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**: The user needs to point to this IP to access the K8s API'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT topology**: The user needs access to the K8s nodes VM (like BOSH SSH,
    for instance)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Components using routable IP addresses:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**: The user needs to SSH to the Ops Manager to perform BOSH
    commands against the K8s nodes VM'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT topology**: An alternative is to install a jumpbox server on the same
    subnet, instead of the PKS Management Plane components'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the K8s nodes VM to access the corporate network (or internet):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NO-NAT topology**: No action is required, as those components use routable
    IP addresses'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NAT topology**: PKS automatically creates a SNAT rule on T0 for each K8s
    cluster, using one IP address from the PKS Floating IP Pool'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: PKS and NSX-T Edge Nodes and Edge Cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PKS only supports NSX-T Edge Node VM configurations that are large in size.
    PKS only supports one Edge Cluster instance of T0 (8 vCPU, 16 GB RAM). The T0
    router must be configured in Active/Standby mode, as SNAT rules will be applied
    there by PKS. An NSX-T Edge Cluster can contain up to eight Edge **Transport Nodes** (**TN**).
    You can add new Edge Nodes (up to eight) in the Edge Cluster to increase the overall
    capacity (LB, for instance) and provide scalability to the NSX-T Edge Cluster.
    You can use two different Edge Nodes for the T0 uplinks IP addresses (two IPs
    in total) to provide HA to NSX-T T0 in an Edge Cluster. We should enable HA VIP
    on T0 so that it's always operational, even if one T0 uplink is down. The physical
    router will only interoperate with the T0 HA VIP.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the NSX-T and load balancer scale numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | **LB small** | **LB medium** | **LB large** | **Pool members** |'
  prefs: []
  type: TYPE_TB
- en: '| **NSX-T release** | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Edge VM: Small** | - | - | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| **Edge VM: Medium** | 1 | 1 | - | - | - | - | 30 | 30 |'
  prefs: []
  type: TYPE_TB
- en: '| **Edge VM: Large** | 4 | 40 | 1 | 1 | - | - | 120 | 1,200 |'
  prefs: []
  type: TYPE_TB
- en: '| **Edge: Bare Metal** | 100 | 750 | 10 | 100 | 1 | 1 | 3,000 | 22,500 |'
  prefs: []
  type: TYPE_TB
- en: PKS and NSX-T communications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple PKS components need to communicate with the NSX-T manager. A PKS control
    plane VM using an NSX-T superuser principal identity certificate as an authentication
    mechanism is needed to create a T1/LS for each K8s cluster node network and an
    LB instance for each K8s cluster.
  prefs: []
  type: TYPE_NORMAL
- en: BOSH uses credentials as an authentication mechanism to tag all of a VM's logical
    ports with a special BOSH ID tag and NCP pod. It uses the NSX-T superuser principal
    identity certificate as an authentication mechanism to create T1/LS for each namespace,
    a SNAT rule on T0 for each namespace, and an LB virtual server for each K8s service
    of the type LB.
  prefs: []
  type: TYPE_NORMAL
- en: The following is a list of the NSX-T objects that are created, for each K8s
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a new K8s cluster is created, the following NSX-T objects are created
    by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NSX-T LS**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One LS for K8s master and worker nodes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One LS for each K8s namespace, that is, kube-public, kube-system, and pks-infrastructure
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One LS for the NSX-T LB associated with the K8s cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T T1**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One T1 for K8s master and worker nodes (called cluster-router)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One T1 for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One T1 for the NSX-T LB associated with the K8s cluster
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T LB**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One NSX-T LB small instance, containing the following objects:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One virtual server to access the K8s control plane API (with port 8443)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One server pool containing the three K8s master nodes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One virtual server for the ingress controller (HTTP)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One virtual server for the ingress controller (HTTPS)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each virtual server is allocated an IP address derived from the PKS Floating
    IP Pool
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a new K8s cluster is created, the following NSX-T objects are created, by
    default:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NSX-T DDI/IPAM**: A /24 subnet from the nodes IP block will be extracted
    and allocated for the K8s master and worker nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T DDI/IPAM**: A /24 subnet from the PODs IP Block will be extracted and
    allocated for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T T0 router**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One SNAT rule created for each K8s namespace (default, kube-public, kube-system,
    pks-infrastructure), using one IP from the Floating IP Pool as the translated
    IP address.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: One SNAT rule created for each K8s cluster (in the case that NAT topology is
    used), using 1 IP from the Floating IP Pool as the translated IP address. The
    K8s cluster subnet is derived from the nodes IP block, using a /24 netmask.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NSX-T DFW**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One DFW rule for kubernetes-dashboard: Source=K8s worker node (hosting the
    dashboard POD/Destination= dashboard POD IP/Port: TCP/8443/Action: allow'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One DFW rule for kube-dns: Source=K8s worker node (hosting the DNS POD)/ Destination
    = DNS POD IP/Port: TCP/8081 and TCP/10054/Action: allow'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage for K8s cluster node VMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can provide storage for K8s PODs by using **persistent volumes** (**PV**).
    A PV can be mapped to a **virtual machine disk** (**VMDK**) file on vSphere by
    using the **vCP** (short for **Cloud Provider**) plugin. A VMDK file will then
    be attached to the worker node VM as a disk. We can then POD mount the volume
    from that disk.
  prefs: []
  type: TYPE_NORMAL
- en: Datastores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a table of information regarding datastores:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Deployment topology/storage technology** | **vSAN datastores** | **VMFS
    over NFS/iSCSI/FC datastores** |'
  prefs: []
  type: TYPE_TB
- en: '| Single vSphere compute cluster (single AZ, or multiple AZs if using RPs)
    with a datastore local to a single vSphere compute cluster |'
  prefs: []
  type: TYPE_TB
- en: '**Static PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-vSphere compute clusters (multiple AZs) with datastore(s) local to
    each vSphere compute cluster |'
  prefs: []
  type: TYPE_TB
- en: '**Static PV provisioning**: No*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic PV provisioning**: No*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static PV provisioning**: No*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic PV provisioning**: No*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-vSphere compute clusters (multiple AZs) with datastore(s) shared across
    all vSphere compute clusters |'
  prefs: []
  type: TYPE_TB
- en: N/A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: vSAN does not support shared datastores across vSphere clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '**Static PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic PV provisioning**: Yes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following are the steps to provision Static PV:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually create a VMDK file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a PV referencing the aforementioned VMDK file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a PVC
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy a stateful POD or StatefulSets by using a reference to the PVC
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Following are the steps for Dynamic PV provisioning:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a PVC (vCP K8s storage plugin; a hatchway will automatically create PV
    and VMDK files)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy stateful POD or StatefulSets using a reference to PVC
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are some vSAN considerations in regards to PKS/NSX-T:'
  prefs: []
  type: TYPE_NORMAL
- en: Using vSAN, a vSphere cluster must start with a minimum of three ESXi hosts
    to guarantee data protection (in this case, for RAID1 with failure to tolerate
    set to 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PKS AZ does not map with the vSAN fault domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PKS with a single compute cluster is currently supported with vSAN (all ESXi
    hosts are located in the same site)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Caution**: A PKS with a vSAN stretched cluster is not a supported configuration,
    as of right now (no mapping of AZs with the vSAN fault domain)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PKS with multiple compute clusters is not a supported configuration with a
    vSAN-only datastore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Master and worker nodes can be created across the different ESXi clusters (BOSH
    tile allows you to specify multiple persistent and ephemeral datastores for the
    VMs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PV VMDK disks are created for only one vSAN datastore (and no replication across
    the different vSAN datastores will be performed automatically)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data centers maintain independent PKS instances, NSX deployments, Kubernetes
    (K8s) clusters, and vSphere infrastructures. A **Global Server Load Balancer**
    (**GSLB**), which is available through a third party, monitors the availability
    of the sites' K8s cluster API and PKS controller API. Operations and development
    direct API requests to the GSLB virtual server URL for creating and managing K8s
    clusters and deploying apps. Manually deployed apps (through kubectl, for instance)
    are not automatically replicated between environments and need to be redeployed
    following a failover to site B.
  prefs: []
  type: TYPE_NORMAL
- en: You can configure a CI/CD automation server to execute build pipelines against
    the K8s' URL in each environment or single builds against the GSLB virtual server
    URL. Harbor policy based replication, a built-in feature, manages cloning images
    to the standby location. You can replicate the datastore(s) between environments
    to support PV. Following a site A failure, the pods are redeployed at site B,
    mounting the original persistent volume's VMDK file.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a new IT approach called cloud-native behind this digitalization trend,
    which is one of the driving forces of business digitalization. The cloud-native
    methodology allows enterprises to greatly increase developer productivity, allowing
    them to deliver new apps and services to the market much more quickly than before;
    they can therefore improve customer experience and satisfaction. If adopted successfully,
    the cloud-native methodology can also help to cut operations and infrastructure
    costs, as well as to enhance app security.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 14](557b79e8-1cf1-4c07-be7d-29ad5d965c3e.xhtml),
    *High Performance Computing for Machine Learning*, you will learn about the specific
    aspects of virtualization that can enhance the productivity of a **high-performance
    computing** (**HPC**) environment. We will explore the capabilities which are
    enabled by VMware vSphere to meet the requirements for researching computing,
    academic, scientific, and engineering HPC workloads.
  prefs: []
  type: TYPE_NORMAL
