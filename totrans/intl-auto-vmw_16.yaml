- en: Cloud Application Scaling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云应用扩展
- en: This chapter will guide you through how to support next-gen cloud app development
    by providing developers with access to traditional, cloud-native, and modern application
    development frameworks and resources, including production-grade container services
    and open APIs. These will be used on a common vSphere platform and will also support
    legacy or traditional applications side by side with cloud-native and containerized
    apps, across a virtualized environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导您了解如何通过为开发者提供访问传统、云原生和现代应用开发框架和资源，包括生产级容器服务和开放API，来支持下一代云应用开发。这些将在共同的vSphere平台上使用，并将支持与云原生和容器化应用并行运行的遗留或传统应用，在整个虚拟化环境中。
- en: You will learn how to optimize resources to get maximum output by defining parameters
    and what-if scenarios. These will be considered for future scalability, so that
    we can configure and autoscale parameters across different clouds.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习如何通过定义参数和假设情景来优化资源以获得最大产出。这些将考虑未来的可扩展性，以便我们可以在不同的云环境中配置和自动扩展参数。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Cloud-native applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云原生应用
- en: The **Pivotal Container Service** (**PKS**) on vSphere
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于vSphere的**Pivotal容器服务**（**PKS**）
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can download VMware Enterprise PKS from [https://cloud.vmware.com/vmware-enterprise-pks/resources](https://cloud.vmware.com/vmware-enterprise-pks/resources).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[https://cloud.vmware.com/vmware-enterprise-pks/resources](https://cloud.vmware.com/vmware-enterprise-pks/resources)下载VMware
    Enterprise PKS。
- en: Cloud-native applications
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云原生应用
- en: Digital technologies are always changing, due to today's dynamic business objectives.
    Everything is connected via mobile, social networks, wearable devices, connected
    cars, and so on, and they are all influencing the way that we behave and engage
    with technologies today. Customers are demanding more innovative, flexible, and
    fast ways to experience products and services, due to this innovation in technology.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于今天的动态商业目标，数字技术总是在变化。通过移动、社交网络、可穿戴设备、联网汽车等一切事物都连接在一起，并且它们都在影响着我们今天的行为和与技术互动的方式。由于这种技术创新，客户对体验产品和服务的要求更加创新、灵活和快速。
- en: Let's look at the systems operating in isolation from each other, responsibility,
    and skills set.We are going through a digital transformation and need all of these
    operations across various segments. Digital transformation redesigns organizational
    structures in many environments, so that they're collaborative. Technology can
    enhance performance and an organization's reach across the globe.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看相互独立运作的系统、责任和技能集。我们正处于数字化转型中，需要跨各个段落的这些操作。数字化转型在很多环境中重新设计组织结构，以便它们能够协作。技术可以提升性能，并使组织在全球范围内扩大其影响力。
- en: 'Cloud-native applications have four characteristics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生应用具有四个特征：
- en: '**Cloud-native apps are composed of microservices**: Cloud-native apps adopt
    a microservices architecture, where each application is a collection of small
    services that can be operated independent of one another. Microservices are often
    owned by individual development teams that operate on their own schedules to develop,
    deploy, scale, and upgrade their services.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云原生应用由微服务组成**：云原生应用采用微服务架构，其中每个应用程序都是一组小型服务，可以独立于彼此运行。微服务通常由个人开发团队拥有，这些团队在自己的时间表下开发、部署、扩展和升级他们的服务。'
- en: '**Cloud-native apps are packaged in containers**: Containers provide isolation
    contexts for microservices. They are highly accessible, scalable, easily portable
    from one environment to another, and fast to create or tear down, making them
    ideal for building and running applications that are composed of microservices.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云原生应用打包在容器中**：容器为微服务提供隔离的上下文。它们高度可访问、可扩展，易于从一个环境便携到另一个环境，创建或拆除速度快，这使得它们非常适合构建和运行由微服务组成的应用程序。'
- en: '**Cloud-native apps are running in a continuous delivery model**: Software
    developers and IT operations teams collaborate under this model to build, test,
    and release software updates as soon as they are ready, without affecting end
    users or developers on other teams.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云原生应用运行在持续交付模型中**：软件开发人员和IT运维团队在这个模型下协作，以便在软件准备就绪时立即构建、测试和发布软件更新，而不会影响最终用户或其他团队的开发人员。'
- en: '**Cloud-native applications are dynamically managed in the cloud**: They are
    often built and run on modern cloud-native platforms, which offer easy scale-out
    and hardware decoupling, helping in terms of the orchestration, management, and
    automation of the application.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云原生应用程序在云中动态管理**：它们通常在提供易于扩展和硬件解耦的现代云原生平台上构建和运行，这有助于应用程序的编排、管理和自动化。'
- en: Automation with containers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用容器进行自动化
- en: Customers who have a substantial deployment of the VMware automation tools can
    easily drive agility and streamline the consumption of IT services. VMware will
    help customers to deliver both application and container services. This platform
    will extend the benefits of BOSH (autoscaling, self-healing, load balancing, and
    so on) to **container as a service** (**CaaS**) solutions (PKS). BOSH is an open
    source tool which helps in deployment and life cycle management of distributed
    systems. PKS is the only CaaS solution that can deliver fully managed Kubernetes
    clusters on premise, as well as a public **Infrastructure as a Service** (**IaaS**).
    This platform will also include **functions as a service** (**FaaS**). This will
    allow organizations to secure their abstraction planning, regardless of IaaS,
    by providing application deployment and runtime constructs on one platform. Because
    of this, we have to plan with various teams that are responsible for the app's
    rationalization and subsequent migration related to business and technical requirements,
    in detail.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经大量部署VMware自动化工具的客户来说，可以轻松推动敏捷性和简化IT服务的消费。VMware将帮助客户提供应用程序和容器服务。这个平台将BOSH（自动扩展、自我修复、负载均衡等）的好处扩展到**容器即服务**（**CaaS**）解决方案（PKS）。BOSH是一个开源工具，有助于分布式系统的部署和生命周期管理。PKS是唯一能够提供本地完全管理的Kubernetes集群以及公共**基础设施即服务**（**IaaS**）的CaaS解决方案。这个平台还将包括**函数即服务**（**FaaS**）。这将允许组织通过在一个平台上提供应用程序部署和运行时结构来确保其抽象规划的安全性，无论IaaS如何。因此，我们必须与负责应用程序合理化和与业务和技术需求相关的后续迁移的各个团队进行详细规划。
- en: The **Pivotal Cloud Foundry** (**PCF**) includes both **Pivotal Application
    Service** (**PAS**) and PKS as critical components. PAS is the cloud-native platform
    for deploying and operating modern applications. PKS enables customers and service
    providers to deliver a production-ready Kubernetes on a VMware SDDC and other
    public cloud environments.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pivotal Cloud Foundry**（**PCF**）包括**Pivotal应用程序服务**（**PAS**）和PKS作为关键组件。PAS是用于部署和运行现代应用程序的云原生平台。PKS使客户和服务提供商能够在VMware
    SDDC和其他公共云环境中提供生产就绪的Kubernetes。'
- en: As an example, if we have a system of 10 apps running in containers, those 10
    apps will have 10 instances of isolated user spaces. Imagine that two applications
    are installed on the same operating system, but each needs a different version
    of that file. We can manage this condition with containers by using a common shared
    library file. Containers (more specifically, Linux containers) have been around
    for a while, and companies such as Oracle, HP, and IBM have been using containers
    for decades. However, Docker has become more popular among users.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个在容器中运行的10个应用程序的系统，那么这10个应用程序将会有10个隔离的用户空间实例。想象一下，如果两个应用程序安装在同一操作系统上，但每个都需要该文件的不同版本。我们可以通过使用一个公共共享库文件来管理这种条件。容器（更具体地说，Linux容器）已经存在了一段时间，像Oracle、HP和IBM这样的公司已经使用容器几十年了。然而，Docker在用户中变得更加流行。
- en: Easy-to-use API and CLI tools for deploying apps that support namespace and
    resource limits reduce the complexity involved in deploying and managing containers.
    A container is a running instance of an image that runs a container. We need to
    download an image to use this. An image is a layered filesystem, where each layer
    has its own filesystem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 部署支持命名空间和资源限制的应用程序的易于使用的API和CLI工具简化了部署和管理容器的复杂性。容器是一个运行中的镜像实例，该镜像运行容器。我们需要下载一个镜像来使用它。镜像是一个分层文件系统，其中每一层都有自己的文件系统。
- en: When you want to make changes, there's no need to crack open a single, large,
    monolithic application and shove new changes in. If we have to make changes, then
    we can just add them to a new layer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想进行更改时，没有必要打开一个单一的大型单体应用程序并将新更改塞进去。如果我们必须进行更改，我们只需将它们添加到一个新层即可。
- en: Containers are doing to operating systems what VMs did to server hardware. Tools
    and organizational processes that are required to run and operate containers are
    generally not defined. VMware and Pivotal are in a unique position to solve these
    new challenges and become the **incumbent**.Containers virtualize the operating
    system by limiting the number of application dependencies that we need to install
    on the OS.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 容器正在对操作系统做的是虚拟机对服务器硬件所做的事情。运行和操作容器所需的工具和组织流程通常没有明确定义。VMware和Pivotal处于独特的位置，能够解决这些新的挑战并成为**既得利益者**。容器通过限制我们需要在操作系统上安装的应用程序依赖项的数量来虚拟化操作系统。
- en: Container use cases
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器的用例
- en: 'The following are the use cases for containers:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些容器的用例：
- en: '**The need for a developer sandbox**:Developers often want access to a cluster
    of machines running a particular framework to build or quickly test and validate
    their applications. Provisioning such environments is time-consuming and often
    involves tickets and approvals. As a result, developers either request VMs and
    customize them to their needs, creating snowflake deployments, or they never give
    up these resources, because they are worried about obtaining a new one, which
    could be a tedious process.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发者沙盒的需求**：开发者经常希望访问运行特定框架的一组机器，以构建或快速测试和验证他们的应用程序。配置这样的环境是耗时的，通常涉及工单和审批。因此，开发者要么请求虚拟机并根据他们的需求进行定制，创建雪花部署，要么他们永远不会放弃这些资源，因为他们担心获得新的资源可能是一个繁琐的过程。'
- en: '**Application repackaging**:Customers can take their existing applications
    and package them as a container. You don''t need to refractor code or make changes
    to the architecture. While this forms the first logical step in a customer''s
    containerization journey, it allows customers to derive certain benefits. Patching
    and maintaining the application is one primary benefit, where updates can be restricted
    to just the individual layers of the image. This ensures that other layers are
    intact, reducing errors and configuration issues that could arise.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用程序重新打包**：客户可以将现有的应用程序打包为容器。您不需要重构代码或更改架构。虽然这构成了客户容器化旅程中的第一个逻辑步骤，但它使客户能够获得某些好处。修补和维护应用程序是一个主要的好处，其中更新可以仅限于镜像的各个单独层。这确保了其他层保持完整，减少了可能出现的错误和配置问题。'
- en: '**Portability**:Packaging an application as a container enables portability.
    The container image, by virtue of packaging not just the application code but
    also all of its dependencies, is guaranteed to work anywhere. We are now able
    to move this image from a developer''s laptop to your test/dev or production environment
    without having to invest time and resources in getting the target environments
    to exactly mimic the dev environment (or vice versa) as a result.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：将应用程序打包为容器可以使其具有可移植性。由于容器镜像不仅打包了应用程序代码，还包括了所有依赖项，因此可以保证在任何地方都能运行。现在我们能够将这个镜像从开发者的笔记本电脑移动到您的测试/开发或生产环境，而无需投入时间和资源来确保目标环境与开发环境（或反之）完全一致。'
- en: Challenges with containers
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器面临的挑战
- en: We focus on enabling developer code to instantiate all the resources developers
    need, even for legacy systems, to provide high levels of automation during the
    waterfall approach and to enable the customers to self-serve their resource requirements.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于使开发者的代码能够实例化开发者所需的所有资源，即使是对于遗留系统，以便在瀑布方法中提供高水平的自动化，并使客户能够自助满足其资源需求。
- en: Traditional model uses traditional application architectures, tooling, and processes,
    where developers have to raise tickets for resources in cloud delivery models.
    Resources are provided through self-service. Cloud-native applications initiate
    these requests through code and provide service **infrastructure as code** (**IaC**).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型使用传统的应用程序架构、工具和流程，在云交付模型中，开发者需要提交工单以获取资源。资源通过自助服务提供。云原生应用程序通过代码发起这些请求，并提供**基础设施即代码**（**IaC**）服务。
- en: Code replaces the service tickets, and APIs play a critical role. A developer-ready
    infrastructure can be achieved with automated VMware SDDC tool by providing APIs
    that assist in running containers, such as OpenStack, PCF, and so on, as normal
    VM environments. Containers can be managed from existing operating models, since
    developers get all of the benefits. This is because IT has to manage the underlying
    resources in a consistent way.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 代码取代了服务票证，API 在其中扮演着关键角色。通过提供帮助运行容器的 API，如 OpenStack、PCF 等，可以使用自动化的 VMware SDDC
    工具实现开发者就绪的基础设施。由于开发者获得了所有的好处，因此可以从现有的运营模式中管理容器。这是因为 IT 必须以一致的方式管理底层资源。
- en: The globally consistent infrastructure layer has benefits in the microservices
    architecture, as each service defines its relationship to other microservices.
    This can be broken if the underlying network is complex and doesn't have visibility.
    The network should be wide open to avoid this problem. Pivotal value VMware NSX
    and developer-ready infrastructure have the same code, which defines the relationships
    between microservices and instantiates secure micro-segmented network connectivity.
    Even serverless architectures can have an Internal Server Error message.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务架构中，全局一致的基础设施层具有优势，因为每个服务定义了其与其他微服务的关系。如果底层网络复杂且没有可见性，则可能会破坏这种关系。网络应该完全开放以避免这个问题。Pivotal
    重视 VMware NSX 和开发者就绪基础设施具有相同的代码，该代码定义了微服务之间的关系并实例化了安全的微分段网络连接。即使是无服务器架构也可能出现内部服务器错误消息。
- en: PKS on vSphere
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PKS 在 vSphere 上
- en: '**vSphere cluster groups** are sets of ESXi hosts that have a common compute
    entity; there are from 2 to 64 hosts per vSphere cluster when vSphere HA and DRS
    are activated at the cluster level. Resource pools are created under a vSphere
    cluster instance, and vCenter is able to manage multiple vSphere clusters instances,
    as there is no hard limit to the number of vSphere clusters. We can create different
    types of vSphere clusters, such as management clusters, compute clusters, and
    Edge clusters, since PKS fully leverages the vSphere cluster construct.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**vSphere 集群组**是一组具有共同计算实体的 ESXi 主机；当在集群级别激活 vSphere HA 和 DRS 时，每个 vSphere
    集群有 2 到 64 个主机。在 vSphere 集群实例下创建资源池，vCenter 能够管理多个 vSphere 集群实例，因为没有对 vSphere
    集群数量的硬性限制。我们可以创建不同类型的 vSphere 集群，如管理集群、计算集群和边缘集群，因为 PKS 完全利用了 vSphere 集群结构。'
- en: 'The following vSphere clusters are recommended in a typical PKS deployment:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的 PKS 部署中，以下 vSphere 集群是推荐的：
- en: '**Management cluster**:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理集群**：'
- en: '**Hosted components**: vCenter, NSX manager, and controller VMs'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托管组件**：vCenter、NSX 管理器和控制器虚拟机'
- en: vSphere HA and DRS enabled
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere HA 和 DRS 已启用
- en: ESXi hosts need to be NSX prepared, as micro-segmentation is enforced on the
    hosted VMs
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESXi 主机需要为 NSX 准备，因为托管虚拟机上强制执行微分段
- en: '**Compute cluster(s)**:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算集群**：'
- en: '**Hosted components**: Kubernetes (K8s) clusters nodes VMs'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托管组件**：Kubernetes (K8s) 集群节点虚拟机'
- en: vSphere HA and DRS should be enabled, as BOSH will check whether DRS is turned
    on
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应启用 vSphere HA 和 DRS，因为 BOSH 将检查 DRS 是否已开启
- en: ESXi hosts need to be NSX prepared
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESXi 主机需要为 NSX 准备
- en: '**Edge cluster**:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘集群**：'
- en: '**Hosted components**: NSX Edge Nodes VMs'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**托管组件**：NSX 边缘节点虚拟机'
- en: vSphere HA and DRS enabled
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere HA 和 DRS 已启用
- en: ESXi hosts don't need to be NSX prepared
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESXi 主机不需要为 NSX 准备
- en: The PKS Management Plane can reside on the management cluster or compute cluster,
    depending on the selected design scenario. PKS Management Plane VMs are the Ops
    Manager, BOSH, the PKS control plane, and Harbor.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: PKS 管理平面可以位于管理集群或计算集群，具体取决于所选的设计场景。PKS 管理平面虚拟机包括 Ops Manager、BOSH、PKS 控制平面和
    Harbor。
- en: The PKS data plane (or compute plane) will only reside in a compute cluster.
    Up to three K8s master nodes and 50 worker nodes are allowed per K8s cluster,
    and many K8s clusters can be created in the same PKS environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PKS 数据平面（或计算平面）将仅位于计算集群中。每个 K8s 集群允许最多三个主节点和 50 个工作节点，并且可以在同一个 PKS 环境中创建多个 K8s
    集群。
- en: The K8s master node also hosts the etcd component. vSphere DRS and HA must be
    enabled on the vSphere compute cluster. vSphere DRS Automation has to be set to
    Partially Automated or Fully Automated. vSphere HA is set with Host failure =
    Restart VMs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 主节点也托管 etcd 组件。vSphere 计算集群上必须启用 vSphere DRS 和 HA。vSphere DRS 自动化必须设置为部分自动化或完全自动化。vSphere
    HA 设置为主机故障 = 重启虚拟机。
- en: 'The following are the compute and storage requirements for the PKS component:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 PKS 组件的计算和存储要求的说明：
- en: '| **PKS Component** | **CPU** | **RAM (GB)** | **Storage (GB)** |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **PKS 组件** | **CPU** | **RAM (GB)** | **存储 (GB)** |'
- en: '| Ops Manager | 1 | 8 | HD1: 160 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Ops Manager | 1 | 8 | HD1: 160 |'
- en: '| PKS Control Plane VM | 2 | 8 | HD1: 3HD2: 16HD3: 10 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| PKS 控制平面虚拟机 | 2 | 8 | HD1: 3HD2: 16HD3: 10 |'
- en: '| BOSH | 2 | 8 | HD1: 3HD2: 50HD3: 50 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| BOSH | 2 | 8 | HD1: 3HD2: 50HD3: 50 |'
- en: '| Harbor | 2 | 8 | HD1: 3HD2: 64HD3: 30 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Harbor | 2 | 8 | HD1: 3HD2: 64HD3: 30 |'
- en: '| K8s master node | Configurable per PKS plan | Configurable per PKS plan |
    Ephemeral disk: 8 to 256 GBPersistent disk: 1 GB to 32 TB (Configurable per PKS
    plan) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| K8s 主节点 | 每个 PKS 计划可配置 | 每个 PKS 计划可配置 | 临时磁盘：8 到 256 GB 持久磁盘：1 GB 到 32 TB（每个
    PKS 计划可配置）|'
- en: '| K8s worker node | Configurable per PKS plan | Configurable per PKS plan |
    Ephemeral disk: 8 to 256 GBPersistent disk: 1 GB to 32 TB (Configurable per PKS
    plan) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| K8s 工作节点 | 每个 PKS 计划可配置 | 每个 PKS 计划可配置 | 临时磁盘：8 到 256 GB 持久磁盘：1 GB 到 32 TB（每个
    PKS 计划可配置）|'
- en: PKS availability zone
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PKS 可用区
- en: PKS supports the concept of an **availability zone** (**AZ**), that is, *AZ
    = vSphere cluster + resource pool*. The AZ dictates the placement of a VM that's
    created by BOSH/PKS into the corresponding vSphere cluster/resource pool.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: PKS 支持可用区（**AZ**）的概念，即 **AZ = vSphere 集群 + 资源池**。可用区决定了由 BOSH/PKS 创建的虚拟机放置到相应的
    vSphere 集群/资源池。
- en: 'There are two types of AZ:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的可用区：
- en: '**Management AZ**: Used for BOSH, PKS control plane, and Harbor VMs'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理可用区**：用于 BOSH、PKS 控制平面和 Harbor 虚拟机'
- en: '**Compute AZ**: Used for K8s master and worker node VMs'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算可用区**：用于 K8s 主节点和工作节点虚拟机'
- en: PKS supports multiple compute availability zones, and each PKS plan supports
    up to three distinct ones. Each K8s master node (for a max of three) will land
    in one separate AZ. K8s worker nodes will be dispatched across the three zones.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: PKS 支持多个计算可用区，并且每个 PKS 计划最多支持三个不同的可用区。每个 K8s 主节点（最多三个）将落在单独的一个可用区。K8s 工作节点将在三个区域之间分配。
- en: Three PKS plans are allowed (for a total of nine distinct compute zones). Each
    PKS plan can use the same three zones or a completely different set of three AZs.
    An AZ is commonly used to set the locality of a VM against different locations; or,
    we can say that AZ = physical rack (or room).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 允许三个 PKS 计划（总共九个不同的计算区域）。每个 PKS 计划可以使用相同的三个区域或完全不同的三个可用区。可用区通常用于设置虚拟机相对于不同位置的本地域；或者，我们可以说
    AZ = 物理机架（或房间）。
- en: 'Following are the PKS design topologies:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 PKS 设计拓扑：
- en: '**Physical topologies (integration with vSphere)**: Multiple topologies can
    be deployed with PKS/NSX-T integration'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物理拓扑（与 vSphere 集成）**：可以通过 PKS/NSX-T 集成部署多种拓扑。'
- en: '**PKS Management Plane in the management cluster**: Multi-compute clusters:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理集群中的 PKS 管理平面**：多计算集群：'
- en: The PKS Management Plane is hosted in a management cluster and connected to
    a DVS virtual switch
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 管理平面托管在管理集群中，并连接到 DVS 虚拟交换机
- en: Multiple compute clusters, to support K8s cluster nodes
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个计算集群，以支持 K8s 集群节点
- en: Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between the
    AZ and vSphere cluster).
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可用区映射到不同的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）。
- en: '**AZ can represent a physical location**: Each compute cluster can be in a
    dedicated rack or room'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AZ 可以代表一个物理位置**：每个计算集群可以位于专用机架或房间'
- en: '**PKS Management Plane in the management cluster for** **a single compute cluster**:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单个计算集群的管理集群中的 PKS 管理平面**：'
- en: The PKS Management Plane is hosted in a management cluster and connected to
    a DVS virtual switch
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 管理平面托管在管理集群中，并连接到 DVS 虚拟交换机
- en: Single Compute Clusters to support K8s cluster nodes
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个计算集群以支持 K8s 集群节点
- en: Each AZ is mapped to a unique vSphere cluster/different resource pool
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可用区映射到一个唯一的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）
- en: AZs can be used to limit the CPU/memory per PKS plan
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用区可以用来限制每个 PKS 计划的 CPU/内存
- en: '**PKS Management Plane in the compute cluster for** **multi-compute clusters**:'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多计算集群中的 PKS 管理平面**：'
- en: The PKS Management Plane is hosted in a compute cluster and connected to an
    NSX-T logical switch
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 管理平面托管在计算集群中，并连接到 NSX-T 逻辑交换机
- en: Multiple compute clusters, to support K8s cluster nodes
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个计算集群，以支持 K8s 集群节点
- en: Each AZ is mapped to a different vSphere cluster (with 1:1 mapping between AZs
    and vSphere clusters)
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可用区映射到不同的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）
- en: '**An AZ can represent a physical location**: Each compute cluster can be in
    a dedicated rack or room'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个可用区可以代表一个物理位置**：每个计算集群可以位于专用机架或房间'
- en: '**PKS Management Plane in a compute cluster, or a single compute cluster**:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算集群中的 PKS 管理平面，或单个计算集群**：'
- en: The PKS Management Plane is hosted in a compute cluster and connected to an
    NSX-T logical switch
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 管理平面托管在计算集群中，并连接到 NSX-T 逻辑交换机
- en: Single compute clusters, to support K8s cluster nodes
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个计算集群，以支持 K8s 集群节点
- en: 'Each AZ is mapped to a unique vSphere cluster/different resource pool:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个可用区映射到一个唯一的 vSphere 集群/不同的资源池：
- en: An AZ can be used to the limit CPU/memory per PKS plan
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用区可以用于每个 PKS 计划的 CPU/内存限制
- en: '**PKS AZ (Single/multiple Compute and Management Clusters) design model**:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PKS 可用区（单/多个计算和管理集群）设计模型**：'
- en: '**PKS AZ with single vSphere compute cluster**: By default, there is no guarantee
    that K8s master nodes land on different ESXi hosts. A workaround is to create
    a DRS affinity rule on the vSphere Compute Cluster.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有单个 vSphere 计算集群的 PKS 可用区**：默认情况下，无法保证 K8s 主节点落在不同的 ESXi 主机上。一种解决方案是在 vSphere
    计算集群上创建一个 DRS 亲和规则。'
- en: '**Type**: Separate VMs.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：独立的虚拟机。'
- en: '**Members**: All K8s master node VMs.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成员**：所有 K8s 主节点虚拟机。'
- en: The vSphere cluster must have a minimum of three ESXi hosts (this is a vSAN
    prerequisite). However, to protect against one host failure (and to make sure
    that the DRS affinity rule will operate properly), a recommendation is to start
    with four ESXi hosts in the cluster.
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere 集群必须至少有三个 ESXi 主机（这是 vSAN 的先决条件）。然而，为了防止单个主机故障（并确保 DRS 亲和规则能够正常工作），建议在集群中启动四个
    ESXi 主机。
- en: NSX-T 2.2 supports all types of traffic on N-VDS. This means that an ESXi host
    in the compute cluster can start with two physical NICs.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NSX-T 2.2 支持在 N-VDS 上的所有类型流量。这意味着计算集群中的 ESXi 主机可以从两个物理网卡开始。
- en: 'The minimum vSphere cluster configuration for a production environment is as
    follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境的最低 vSphere 集群配置如下：
- en: '**Management cluster**:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理集群**：'
- en: '**Non-vSAN**: Min hosts: Two'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非-vSAN**: 最小主机数：两个'
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must have two replicas and one witness)'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须有两个副本和一个见证）'
- en: '**Compute cluster(s)**:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算集群**：'
- en: '**Single compute cluster topology**:'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单个计算集群拓扑**：'
- en: '**Non-vSAN**: Min hosts: Three (To guarantee one K8s master node VM per ESXi
    host by using a DRS affinity rule)'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非-vSAN**: 最小主机数：三个（通过使用 DRS 亲和规则保证每个 ESXi 主机有一个 K8s 主节点虚拟机）'
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must have two replicas and one witness)'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须有两个副本和一个见证）'
- en: '**Multiple compute clusters topology**:'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个计算集群拓扑**：'
- en: '**Non-vSAN**: Min hosts: Two per AZ, with three AZs in total (The K8s master
    node is instantiated across different compute clusters. Each compute cluster is
    1:1 mapped with one AZ)'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非-vSAN**: 最小主机数：每个可用区两个，总共三个可用区（K8s 主节点实例化在不同的计算集群中。每个计算集群与一个可用区一一对应）'
- en: '**vSAN**: Min hosts: Three per AZ, with three AZs in total (To guarantee data
    protection for vSAN objects, you must use two replicas and one witness)'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSAN**: 最小主机数：每个可用区三个，总共三个可用区（为保证 vSAN 对象的数据保护，必须使用两个副本和一个见证）'
- en: '**Edge Cluster**:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘集群**：'
- en: '**Non-vSAN**: Min hosts: Two.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非-vSAN**: 最小主机数：两个。'
- en: '**vSAN**: Min hosts: Three (To guarantee data protection for vSAN objects,
    you must use two replicas and one witness.) Note: An Edge Cluster can be collapsed
    with a compute cluster (or even a management cluster) if you need to lower the
    number of starting ESXi hosts.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须使用两个副本和一个见证。）注意：如果需要减少启动 ESXi 主机的数量，可以将边缘集群与计算集群（甚至管理集群）合并。'
- en: 'The following table gives information about PKS/NSX-T networks:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了关于 PKS/NSX-T 网络的信息：
- en: '| **Network** | **Description** | **CIDR** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **网络** | **描述** | **CIDR** |'
- en: '| **PKS Management Network** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **PKS 管理网络** |'
- en: This network hosts the Ops Manager, BOSH, the PKS control plane, and Harbor
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此网络托管 Ops Manager、BOSH、PKS 控制平面和 Harbor
- en: Can be co-located with vCenter, NSX-T management, and the control plane, if
    desired
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，可以与 vCenter、NSX-T 管理和控制平面共置
- en: PKS Management Network is routable or non-routable, depending on NO-NAT or NAT
    topology
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 管理网络是可路由或不可路由的，这取决于 NO-NAT 或 NAT 拓扑
- en: '| 192.168.1.0/28 (for instance)CIDR with /28 is a good starting point. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 192.168.1.0/28（例如）CIDR，/28 是一个良好的起点。 |'
- en: '| **Nodes IP Block** |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **节点 IP 块** |'
- en: This block will be carved to create a network that will host K8s cluster node
    VMs
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此块将被分割以创建一个网络，该网络将托管 K8s 集群节点虚拟机
- en: Each K8s cluster will be allocated a /24 portion of the block
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 K8s 集群将被分配一个 /24 的块
- en: The nodes IP block is routable or non-routable, depending on NO-NAT or NAT topology
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点 IP 块是可路由或不可路由的，这取决于 NO-NAT 或 NAT 拓扑
- en: '| Depends on the NAT or NO-NAT topology.172.23.0.0/16 (for instance) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 取决于 NAT 或 NO-NAT 拓扑。172.23.0.0/16（例如）|'
- en: '| **Pods IP Block** |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **Pods IP 块** |'
- en: This block will be carved to create a network that will host K8s pods belonging
    to the same K8s namespace
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此块将被分割以创建一个网络，该网络将托管属于同一 K8s 命名空间的 K8s Pods
- en: Each k8s namespace will be allocated a /24 portion of the block
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个k8s命名空间将分配到 /24 块的部分
- en: The pods IP block is always non-routable
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods IP 块始终不可路由
- en: '| 172.16.0.0/16 (for instance) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 172.16.0.0/16（例如）|'
- en: '| **Floating IP Pool** |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **浮动 IP 池** |'
- en: 'This pool will be used for two purposes:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此池将用于以下两个目的：
- en: SNAT rules for each K8s namespace on T0 (for pods networking)
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: T0 上每个 K8s 命名空间的 SNAT 规则（用于 Pods 网络）
- en: LB virtual servers IP allocation
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LB 虚拟服务器 IP 分配
- en: The floating IP pool is always routable
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浮动 IP 池始终可路由
- en: '| 192.168.20.2-192.168.20.254 (for instance) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 192.168.20.2-192.168.20.254（例如）|'
- en: '** CIDR for nodes IP block**:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点 IP 块的 CIDR**：'
- en: Must be unique in the case of a routable scenario (NO-NAT topology)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可路由场景中必须是唯一的（NO-NAT 拓扑）
- en: Can be duplicated in the case of a non-routable scenario (NAT topology)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不可路由场景中可以重复（NAT 拓扑）
- en: The `172.17.0.0/16`CIDR must not be used in all cases, as Docker on the K8s
    worker node is using the subnet.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，`172.17.0.0/16`CIDR 都不得使用，因为 K8s 工作节点上的 Docker 正在使用该子网。
- en: 'If PKS is deployed with Harbor, then the following CIDR must not be used, as
    Harbor is leveraging it for its internal Docker bridges:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果与 Harbor 部署 PKS，则必须不使用以下 CIDR，因为 Harbor 正在使用它为其内部 Docker 桥接器：
- en: '[PRE0]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each K8s cluster uses the following IP block for Kubernetes services, so avoid
    using it for a nodes IP block: `10.100.200.0/24`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 每个K8s集群使用以下 IP 块用于 Kubernetes 服务，因此请避免将其用于节点 IP 块：`10.100.200.0/24`。
- en: PKS/NSX-T logical topologies
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PKS/NSX-T 逻辑拓扑
- en: PKS supports two types of topologies when it's integrated with NSX-T. NAT and
    NO-NAT topology selection is done in the PKS tile | Networking section. NAT topology
    is the default, but you can uncheck NAT mode to go with the NO-NAT topology. The
    NAT and NO-NAT terminology essentially applies to the PKS Management Network and
    the K8s cluster nodes network (that is, whether to use routable subnets). Irrespective
    of the NAT or NO-NAT topology, the same procedure is used to access the K8s API.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当与 NSX-T 集成时，PKS 支持两种类型的拓扑。NAT 和 NO-NAT 拓扑选择在 PKS 瓦片 | 网络部分完成。NAT 拓扑是默认的，但您可以取消选中
    NAT 模式以使用 NO-NAT 拓扑。NAT 和 NO-NAT 术语基本上适用于 PKS 管理网络和 K8s 集群节点网络（即是否使用可路由子网）。无论
    NAT 还是 NO-NAT 拓扑，访问 K8s API 都使用相同的程序。
- en: 'A virtual server on the NSX-T LB instance that''s allocated to the K8s cluster
    is created for the following purpose:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在分配给 K8s 集群的 NSX-T LB 实例上创建的虚拟服务器用于以下目的：
- en: One IP from the PKS Floating IP Pool is extracted (`1x.x0.1x.1xx` here), and
    the port is `8443`
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 PKS 浮动 IP 池中提取一个 IP（此处为 `1x.x0.1x.1xx`），端口号为 `8443`
- en: The same IP address is shown from the output of the `pks cluster <cluster name>` command
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pks cluster <cluster name>` 命令的输出显示了相同的 IP 地址'
- en: 'Following are the objectives with different NAT topologies:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同 NAT 拓扑的目标：
- en: '**NAT topology**: For customers with a limited amount of available routable
    IP addresses in their DC and who want to automate PKS deployment using a concourse
    pipeline (for instance)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT 拓扑**：对于在他们的数据中心拥有有限的可路由 IP 地址并且希望使用 concourse 管道（例如）自动化 PKS 部署的客户'
- en: '**NO-NAT topology**:For customers who avoid NAT as NATs break full path visibility
    and having plenty of routable IP address resources'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NO-NAT 拓扑**：对于避免 NAT（NAT 会破坏完整路径可见性并且拥有大量可路由 IP 地址资源）的客户'
- en: Use cases with different configurations
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同配置的使用案例
- en: 'The following are the use cases with different configurations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是与不同配置相关的使用案例：
- en: 'Access to PKS Management Plane components (Ops Manager, BOSH, PKS control plane
    VM, Harbor) from the corporate network:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从企业网络访问 PKS 管理平面组件（Ops Manager、BOSH、PKS 控制平面 VM、Harbor）：
- en: '**NO-NAT topology**: No action is required as those components use routable
    IP addresses'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NO-NAT 拓扑**：无需采取任何行动，因为这些组件使用可路由 IP 地址'
- en: '**NAT topology**: User needs to create DNAT rules on T0'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT 拓扑**：用户需要在 T0 上创建 DNAT 规则'
- en: 'Access to K8s API (using a kubectl CLI, for instance):'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 K8s API（例如使用 kubectl CLI）：
- en: '**NO-NAT topology**: 1 virtual server (on the NSX-T LB instance that''s dedicated
    to the K8s cluster) is automatically created using 1 routable IP from the PKS
    Floating IP block'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NO-NAT 拓扑**：1 个虚拟服务器（在专用于 K8s 集群的 NSX-T LB 实例上）自动创建，使用 1 个可路由 IP 从 PKS 浮动
    IP 块'
- en: '**NAT topology**: The user needs to point to this IP to access K8s API'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT 拓扑**：用户需要指向此 IP 以访问 K8s API'
- en: 'One virtual server (on the NSX-T LB instance that''s dedicated to the K8s cluster)
    is automatically created using one routable IP from the PKS floating IP block:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PKS浮动IP块中的一个可路由IP地址，自动创建一个虚拟服务器（在专门用于K8s集群的NSX-T负载均衡器实例上）：
- en: '**NO-NAT topology**: The user needs to point to this IP to access the K8s API'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无NAT拓扑结构**：用户需要指向此IP以访问K8s API'
- en: '**NAT topology**: The user needs access to the K8s nodes VM (like BOSH SSH,
    for instance)'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT拓扑结构**：用户需要访问K8s节点VM（例如，BOSH SSH）'
- en: 'Components using routable IP addresses:'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可路由IP地址的组件：
- en: '**NO-NAT topology**: The user needs to SSH to the Ops Manager to perform BOSH
    commands against the K8s nodes VM'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无NAT拓扑结构**：用户需要SSH到Ops Manager以执行针对K8s节点VM的BOSH命令'
- en: '**NAT topology**: An alternative is to install a jumpbox server on the same
    subnet, instead of the PKS Management Plane components'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT拓扑结构**：一种替代方案是在同一子网上安装一个跳转盒服务器，而不是PKS管理平面组件'
- en: 'Using the K8s nodes VM to access the corporate network (or internet):'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K8s节点VM访问公司网络（或互联网）：
- en: '**NO-NAT topology**: No action is required, as those components use routable
    IP addresses'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无NAT拓扑结构**：不需要采取任何行动，因为这些组件使用可路由IP地址'
- en: '**NAT topology**: PKS automatically creates a SNAT rule on T0 for each K8s
    cluster, using one IP address from the PKS Floating IP Pool'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NAT拓扑结构**：PKS会自动为每个K8s集群在T0上创建一个SNAT规则，使用PKS浮动IP池中的一个IP地址'
- en: PKS and NSX-T Edge Nodes and Edge Cluster
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PKS和NSX-T边缘节点和边缘集群
- en: PKS only supports NSX-T Edge Node VM configurations that are large in size.
    PKS only supports one Edge Cluster instance of T0 (8 vCPU, 16 GB RAM). The T0
    router must be configured in Active/Standby mode, as SNAT rules will be applied
    there by PKS. An NSX-T Edge Cluster can contain up to eight Edge **Transport Nodes** (**TN**).
    You can add new Edge Nodes (up to eight) in the Edge Cluster to increase the overall
    capacity (LB, for instance) and provide scalability to the NSX-T Edge Cluster.
    You can use two different Edge Nodes for the T0 uplinks IP addresses (two IPs
    in total) to provide HA to NSX-T T0 in an Edge Cluster. We should enable HA VIP
    on T0 so that it's always operational, even if one T0 uplink is down. The physical
    router will only interoperate with the T0 HA VIP.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: PKS仅支持大型尺寸的NSX-T边缘节点VM配置。PKS仅支持一个T0（8 vCPU，16 GB RAM）的Edge Cluster实例。T0路由器必须配置在活动/备用模式，因为PKS将在那里应用SNAT规则。NSX-T边缘集群可以包含多达八个边缘**传输节点**（**TN**）。您可以在边缘集群中添加新的边缘节点（最多八个）以增加整体容量（例如LB）并为NSX-T边缘集群提供可伸缩性。您可以使用两个不同的边缘节点为T0上行链路IP地址（总共两个IP地址）提供NSX-T
    T0在边缘集群中的高可用性。我们应该在T0上启用HA VIP，以确保其始终处于运行状态，即使一个T0上行链路出现故障。物理路由器将仅与T0 HA VIP交互操作。
- en: 'The following are the NSX-T and load balancer scale numbers:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了NSX-T和负载均衡器的缩放数字：
- en: '|   | **LB small** | **LB medium** | **LB large** | **Pool members** |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|   | **小型LB** | **中型LB** | **大型LB** | **池成员** |'
- en: '| **NSX-T release** | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **NSX-T版本** | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 |'
- en: '| **Edge VM: Small** | - | - | - | - | - | - | - | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| **边缘VM：小型** | - | - | - | - | - | - | - | - |'
- en: '| **Edge VM: Medium** | 1 | 1 | - | - | - | - | 30 | 30 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| **边缘VM：中型** | 1 | 1 | - | - | - | - | 30 | 30 |'
- en: '| **Edge VM: Large** | 4 | 40 | 1 | 1 | - | - | 120 | 1,200 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **边缘VM：大型** | 4 | 40 | 1 | 1 | - | - | 120 | 1,200 |'
- en: '| **Edge: Bare Metal** | 100 | 750 | 10 | 100 | 1 | 1 | 3,000 | 22,500 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **裸金属边缘** | 100 | 750 | 10 | 100 | 1 | 1 | 3,000 | 22,500 |'
- en: PKS and NSX-T communications
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PKS和NSX-T通信
- en: Multiple PKS components need to communicate with the NSX-T manager. A PKS control
    plane VM using an NSX-T superuser principal identity certificate as an authentication
    mechanism is needed to create a T1/LS for each K8s cluster node network and an
    LB instance for each K8s cluster.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 多个PKS组件需要与NSX-T管理器通信。需要一个使用NSX-T超级用户主体身份证书作为认证机制的PKS控制平面VM，以创建每个K8s集群节点网络的T1/LS以及每个K8s集群的LB实例。
- en: BOSH uses credentials as an authentication mechanism to tag all of a VM's logical
    ports with a special BOSH ID tag and NCP pod. It uses the NSX-T superuser principal
    identity certificate as an authentication mechanism to create T1/LS for each namespace,
    a SNAT rule on T0 for each namespace, and an LB virtual server for each K8s service
    of the type LB.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: BOSH使用凭证作为认证机制，为VM的所有逻辑端口添加特殊的BOSH ID标签和NCP pod。它使用NSX-T超级用户主体身份证书作为认证机制，为每个命名空间创建T1/LS，为每个命名空间在T0上创建SNAT规则，并为每个类型的LB
    K8s服务创建LB虚拟服务器。
- en: The following is a list of the NSX-T objects that are created, for each K8s
    cluster.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为每个K8s集群创建的NSX-T对象的列表。
- en: 'When a new K8s cluster is created, the following NSX-T objects are created
    by default:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个新的K8s集群时，以下NSX-T对象默认创建：
- en: '**NSX-T LS**:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T LS**：'
- en: One LS for K8s master and worker nodes
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为K8s主节点和工作节点创建一个LS
- en: One LS for each K8s namespace, that is, kube-public, kube-system, and pks-infrastructure
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个K8s命名空间创建一个LS，即kube-public、kube-system和pks-infrastructure
- en: One LS for the NSX-T LB associated with the K8s cluster
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个与K8s集群关联的NSX-T负载均衡器LS
- en: '**NSX-T T1**:'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T T1**：'
- en: One T1 for K8s master and worker nodes (called cluster-router)
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为K8s主节点和工作节点创建一个T1（称为cluster-router）
- en: One T1 for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure)
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个K8s命名空间（默认，kube-public，kube-system和pks-infrastructure）创建一个T1
- en: One T1 for the NSX-T LB associated with the K8s cluster
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为与K8s集群关联的NSX-T负载均衡器创建一个T1
- en: '**NSX-T LB**:'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T负载均衡器**：'
- en: 'One NSX-T LB small instance, containing the following objects:'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含以下对象的NSX-T负载均衡器小实例：
- en: One virtual server to access the K8s control plane API (with port 8443)
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个虚拟服务器用于访问K8s控制平面API（端口8443）
- en: One server pool containing the three K8s master nodes
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含三个K8s主节点的服务器池
- en: One virtual server for the ingress controller (HTTP)
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为入口控制器创建一个虚拟服务器（HTTP）
- en: One virtual server for the ingress controller (HTTPS)
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为入口控制器创建一个虚拟服务器（HTTPS）
- en: Each virtual server is allocated an IP address derived from the PKS Floating
    IP Pool
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个虚拟服务器分配一个从PKS浮动IP池派生的IP地址
- en: 'When a new K8s cluster is created, the following NSX-T objects are created, by
    default:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个新的K8s集群时，以下NSX-T对象默认创建：
- en: '**NSX-T DDI/IPAM**: A /24 subnet from the nodes IP block will be extracted
    and allocated for the K8s master and worker nodes.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T DDI/IPAM**：从节点IP块中提取并分配一个/24子网给K8s主节点和工作节点。'
- en: '**NSX-T DDI/IPAM**: A /24 subnet from the PODs IP Block will be extracted and
    allocated for each K8s namespace (default, kube-public, kube-system, and pks-infrastructure).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T DDI/IPAM**：从PODs IP块中提取并分配一个/24子网给每个K8s命名空间（默认，kube-public，kube-system和pks-infrastructure）。'
- en: '**NSX-T T0 router**:'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T T0路由器**：'
- en: One SNAT rule created for each K8s namespace (default, kube-public, kube-system,
    pks-infrastructure), using one IP from the Floating IP Pool as the translated
    IP address.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个K8s命名空间（默认，kube-public，kube-system，pks-infrastructure）创建一个SNAT规则，使用浮动IP池中的一个IP作为翻译的IP地址。
- en: One SNAT rule created for each K8s cluster (in the case that NAT topology is
    used), using 1 IP from the Floating IP Pool as the translated IP address. The
    K8s cluster subnet is derived from the nodes IP block, using a /24 netmask.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个K8s集群（如果使用NAT拓扑）创建一个SNAT规则，使用浮动IP池中的一个IP作为翻译的IP地址。K8s集群子网是从节点IP块中派生的，使用/24子网掩码。
- en: '**NSX-T DFW**:'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NSX-T DFW**：'
- en: 'One DFW rule for kubernetes-dashboard: Source=K8s worker node (hosting the
    dashboard POD/Destination= dashboard POD IP/Port: TCP/8443/Action: allow'
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为kubernetes-dashboard创建一个DFW规则：源=K8s工作节点（托管仪表板POD）/ 目标=仪表板POD IP/端口：TCP/8443/操作：允许
- en: 'One DFW rule for kube-dns: Source=K8s worker node (hosting the DNS POD)/ Destination
    = DNS POD IP/Port: TCP/8081 and TCP/10054/Action: allow'
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为kube-dns创建一个DFW规则：源=K8s工作节点（托管DNS POD）/ 目标 = DNS POD IP/端口：TCP/8081和TCP/10054/操作：允许
- en: Storage for K8s cluster node VMs
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K8s集群节点VM的存储
- en: You can provide storage for K8s PODs by using **persistent volumes** (**PV**).
    A PV can be mapped to a **virtual machine disk** (**VMDK**) file on vSphere by
    using the **vCP** (short for **Cloud Provider**) plugin. A VMDK file will then
    be attached to the worker node VM as a disk. We can then POD mount the volume
    from that disk.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用**持久卷**（**PV**）为K8s POD提供存储。可以通过使用**vCP**（代表**云提供商**）插件将PV映射到vSphere上的**虚拟机磁盘**（**VMDK**）文件。然后，将VMDK文件作为磁盘附加到工作节点VM。然后我们可以从该磁盘挂载卷。
- en: Datastores
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储
- en: 'The following is a table of information regarding datastores:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个关于数据存储的信息表：
- en: '| **Deployment topology/storage technology** | **vSAN datastores** | **VMFS
    over NFS/iSCSI/FC datastores** |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **部署拓扑/存储技术** | **vSAN数据存储** | **VMFS通过NFS/iSCSI/FC数据存储** |'
- en: '| Single vSphere compute cluster (single AZ, or multiple AZs if using RPs)
    with a datastore local to a single vSphere compute cluster |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 单个vSphere计算集群（单个可用区，或使用RPs时的多个可用区）具有本地数据存储 |'
- en: '**Static PV provisioning**: Yes'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态PV配置**：是'
- en: '**Dynamic PV provisioning**: Yes'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态PV配置**：是'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '**Static PV provisioning**: Yes'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态PV配置**：是'
- en: '**Dynamic PV provisioning**: Yes'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态PV配置**：是'
- en: '|'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multi-vSphere compute clusters (multiple AZs) with datastore(s) local to
    each vSphere compute cluster |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 多个vSphere计算集群（多个可用区）具有本地数据存储 |'
- en: '**Static PV provisioning**: No*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态PV配置**：否*'
- en: '**Dynamic PV provisioning**: No*'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态 PV 提供程序**：否*'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '**Static PV provisioning**: No*'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态 PV 提供程序**：否*'
- en: '**Dynamic PV provisioning**: No*'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态 PV 提供程序**：否*'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multi-vSphere compute clusters (multiple AZs) with datastore(s) shared across
    all vSphere compute clusters |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 多个 vSphere 计算集群（多个 AZs）共享所有 vSphere 计算集群的数据存储（s）|'
- en: N/A
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N/A
- en: vSAN does not support shared datastores across vSphere clusters
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSAN 不支持跨 vSphere 集群的共享数据存储
- en: '|'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '**Static PV provisioning**: Yes'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态 PV 提供程序**：是'
- en: '**Dynamic PV provisioning**: Yes'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态 PV 提供程序**：是'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Following are the steps to provision Static PV:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为静态 PV 提供的步骤：
- en: Manually create a VMDK file
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动创建 VMDK 文件
- en: Create a PV referencing the aforementioned VMDK file
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个引用上述 VMDK 文件的 PV
- en: Create a PVC
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 PVC
- en: Deploy a stateful POD or StatefulSets by using a reference to the PVC
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过引用 PVC 来部署有状态的 POD 或 StatefulSets
- en: 'Following are the steps for Dynamic PV provisioning:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为动态 PV 提供程序提供的步骤：
- en: Create a PVC (vCP K8s storage plugin; a hatchway will automatically create PV
    and VMDK files)
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 PVC（vCP K8s 存储插件；hatchway 将自动创建 PV 和 VMDK 文件）
- en: Deploy stateful POD or StatefulSets using a reference to PVC
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 PVC 引用部署有状态的 POD 或 StatefulSets
- en: 'The following are some vSAN considerations in regards to PKS/NSX-T:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于 PKS/NSX-T 的 vSAN 考虑事项：
- en: Using vSAN, a vSphere cluster must start with a minimum of three ESXi hosts
    to guarantee data protection (in this case, for RAID1 with failure to tolerate
    set to 1)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 vSAN，vSphere 集群必须从至少三个 ESXi 主机开始，以确保数据保护（在这种情况下，对于 RAID1，容错设置为 1）
- en: A PKS AZ does not map with the vSAN fault domain
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PKS 的 AZ 无法与 vSAN 故障域进行映射
- en: A PKS with a single compute cluster is currently supported with vSAN (all ESXi
    hosts are located in the same site)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前支持带有 vSAN 的单个计算集群的 PKS（所有 ESXi 主机都位于同一站点）
- en: '**Caution**: A PKS with a vSAN stretched cluster is not a supported configuration,
    as of right now (no mapping of AZs with the vSAN fault domain)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**: 目前为止，带有 vSAN 拉伸集群的 PKS 不是一个受支持的配置（因为没有将 AZs 与 vSAN 故障域进行映射）'
- en: A PKS with multiple compute clusters is not a supported configuration with a
    vSAN-only datastore
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有多个计算集群的 PKS 在仅使用 vSAN 数据存储的情况下不是一个受支持的配置
- en: Master and worker nodes can be created across the different ESXi clusters (BOSH
    tile allows you to specify multiple persistent and ephemeral datastores for the
    VMs)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主节点和工作节点可以跨不同的 ESXi 集群创建（BOSH 瓦片允许您为虚拟机指定多个持久和临时数据存储）
- en: PV VMDK disks are created for only one vSAN datastore (and no replication across
    the different vSAN datastores will be performed automatically)
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅为单个 vSAN 数据存储创建 PV VMDK 磁盘（并且不会自动在不同 vSAN 数据存储之间执行复制）
- en: Data centers maintain independent PKS instances, NSX deployments, Kubernetes
    (K8s) clusters, and vSphere infrastructures. A **Global Server Load Balancer**
    (**GSLB**), which is available through a third party, monitors the availability
    of the sites' K8s cluster API and PKS controller API. Operations and development
    direct API requests to the GSLB virtual server URL for creating and managing K8s
    clusters and deploying apps. Manually deployed apps (through kubectl, for instance)
    are not automatically replicated between environments and need to be redeployed
    following a failover to site B.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中心维护独立的 PKS 实例、NSX 部署、Kubernetes（K8s）集群和 vSphere 基础设施。一个**全局服务器负载均衡器**（**GSLB**），通过第三方提供，监控站点
    K8s 集群 API 和 PKS 控制器 API 的可用性。操作和开发将 API 请求直接指向 GSLB 虚拟服务器 URL 以创建和管理 K8s 集群以及部署应用程序。手动部署的应用程序（例如通过
    kubectl）不会在环境之间自动复制，并在站点 B 的故障转移后需要重新部署。
- en: You can configure a CI/CD automation server to execute build pipelines against
    the K8s' URL in each environment or single builds against the GSLB virtual server
    URL. Harbor policy based replication, a built-in feature, manages cloning images
    to the standby location. You can replicate the datastore(s) between environments
    to support PV. Following a site A failure, the pods are redeployed at site B,
    mounting the original persistent volume's VMDK file.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以配置一个 CI/CD 自动化服务器，以执行针对每个环境中的 K8s URL 的构建管道，或者针对 GSLB 虚拟服务器 URL 的单个构建。基于
    Harbor 策略的复制，这是一个内置功能，负责将镜像克隆到备用位置。您可以在环境之间复制数据存储以支持 PV。在站点 A 失败后，Pods 将在站点 B
    重新部署，挂载原始持久卷的 VMDK 文件。
- en: Summary
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: There is a new IT approach called cloud-native behind this digitalization trend,
    which is one of the driving forces of business digitalization. The cloud-native
    methodology allows enterprises to greatly increase developer productivity, allowing
    them to deliver new apps and services to the market much more quickly than before;
    they can therefore improve customer experience and satisfaction. If adopted successfully,
    the cloud-native methodology can also help to cut operations and infrastructure
    costs, as well as to enhance app security.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数字化趋势的背后，有一个名为云原生的新IT方法，它是商业数字化的推动力之一。云原生方法允许企业大幅提高开发者的生产力，使他们能够比以前更快地将新应用和服务推向市场；因此，他们可以改善客户体验和满意度。如果成功采用，云原生方法还可以帮助降低运营和基础设施成本，以及增强应用安全性。
- en: In the next chapter, [Chapter 14](557b79e8-1cf1-4c07-be7d-29ad5d965c3e.xhtml),
    *High Performance Computing for Machine Learning*, you will learn about the specific
    aspects of virtualization that can enhance the productivity of a **high-performance
    computing** (**HPC**) environment. We will explore the capabilities which are
    enabled by VMware vSphere to meet the requirements for researching computing,
    academic, scientific, and engineering HPC workloads.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第14章](557b79e8-1cf1-4c07-be7d-29ad5d965c3e.xhtml)，《机器学习的高性能计算》，你将了解可以增强高性能计算（**HPC**）环境生产力的虚拟化具体方面。我们将探讨VMware
    vSphere提供的功能，以满足研究计算、学术、科学和工程高性能计算工作负载的需求。
