<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Recommendation Systems</h1>
                </header>
            
            <article>
                
<p><span>Imagine an online shop with thousands of articles. If you're not a registered user, you'll probably see a homepage with some highlights, but if you've already bought some items, it would be interesting if the website showed products that you would probably buy, instead of a random selection. This is the purpose of a recommender system, and in this chapter, we're going to discuss the most common techniques to create such a system.</span></p>
<p>The basic concepts are users, items, and ratings (or an implicit feedback about the products, like the fact of having bought them). Every model must work with known data (like in a supervised scenario), to be able to suggest the most suitable items or to predict the ratings for all the items not evaluated yet.</p>
<p>We're going to discuss two different kinds of strategies:</p>
<ul>
<li>User or content based</li>
<li>Collaborative filtering</li>
</ul>
<p>The first approach is based on the information we have about users or products and its target is to associate a new user with an existing group of peers to suggest all the items positively rated by the other members, or to cluster the products according to their features and propose a subset of items similar to the one taken into account. The second approach, which is a little bit more sophisticated, works with explicit ratings and its purpose is to predict this value for every item and every user. Even if collaborative filtering needs more computational power as, nowadays, the great availability of cheap resources, allows using this algorithm with millions of users and products to provide the most accurate recommendations in real-time. The model can also be retrained or updated every day.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Naive user-based systems</h1>
                </header>
            
            <article>
                
<p>In this first scenario, we assume that we have a set of users represented by feature vectors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="33" width="229" src="assets/46d9a100-5870-4a68-b406-0ac4cc3a5d91.png"/></div>
<p class="CDPAlignLeft CDPAlign">Typical features are age, gender, interests, and so on. All of them must be encoded using one of the techniques discussed in the previous chapters (for example, they can be binarized). Moreover, we have a set of items:</p>
<div class="CDPAlignCenter CDPAlign"><img height="32" width="119" src="assets/538f4289-3d0a-4275-8d41-371d3413d5f7.png"/></div>
<p class="CDPAlignLeft CDPAlign">Let's assume also that there is a relation which associates each user with a subset of items (bought or positively reviewed), items for which an explicit action or feedback has been performed:</p>
<div class="CDPAlignCenter CDPAlign"><img height="28" width="256" src="assets/eb0e6274-4416-4300-af08-32961368bafc.png"/></div>
<p class="CDPAlignLeft CDPAlign">In a user-based system, the users are periodically clustered (normally using a <strong>k-nearest neighbors</strong> approach), and therefore, considering a generic user <em>u</em> (also new), we can immediately determine the ball containing all the users who are similar (therefore neighbors) to our sample:</p>
<div class="CDPAlignCenter CDPAlign"><img height="31" width="163" src="assets/09add924-472f-4bf6-936c-dfb641de04e4.png"/></div>
<p class="CDPAlignLeft CDPAlign"> At this point, we can create the set of suggested items using the relation previously introduced:</p>
<div class="CDPAlignCenter CDPAlign"><img height="57" width="311" src="assets/3fecf5a0-21aa-464d-bd53-0ddb651235fe.png"/></div>
<p class="CDPAlignLeft CDPAlign">In other words, the set contains all the unique products positively rated or bought by the neighborhood. I've used the adjective naive because there's a similar alternative that we're going to discuss in the section dedicated to collaborative filtering.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">User-based system implementation with scikit-learn</h1>
                </header>
            
            <article>
                
<p>For our purposes, we need to create a dummy dataset of users and products:</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; nb_users = 1000</strong><br/><strong>&gt;&gt;&gt; users = np.zeros(shape=(nb_users, 4))</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(nb_users):</strong><br/><strong>&gt;&gt;&gt;    users[i, 0] = np.random.randint(0, 4)</strong><br/><strong>&gt;&gt;&gt;    users[i, 1] = np.random.randint(0, 2)</strong><br/><strong>&gt;&gt;&gt;    users[i, 2] = np.random.randint(0, 5)</strong><br/><strong>&gt;&gt;&gt;    users[i, 3] = np.random.randint(0, 5)</strong></pre>
<p>We assume that we have 1,000 users with four features represented by integer numbers bounded between 0 and 4 or 5. It doesn't matter what they mean; their role is to characterize a user and allow for clustering of the set.</p>
<p>For the products, we also need to create the association:</p>
<pre><strong>&gt;&gt;&gt; nb_product = 20</strong><br/><strong>&gt;&gt;&gt; user_products = np.random.randint(0, nb_product, size=(nb_users, 5))</strong></pre>
<p>We assume that we have 20 different items (from 1 to 20; 0 means that a user didn't buy anything) and an association matrix where each user is linked to a number of products bounded between 0 and 5 (maximum). For example:</p>
<div class="CDPAlignCenter CDPAlign"> <img height="115" width="237" src="assets/58fec9ca-aca7-45b5-a60a-a1c34802b4ad.png"/></div>
<p class="CDPAlignLeft CDPAlign">At this point, we need to cluster the users using the <kbd>NearestNeighbors</kbd> implementation provided by scikit-learn:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from sklearn.neighbors import NearestNeighbors</strong><br/><br/><strong>&gt;&gt;&gt; nn = NearestNeighbors(n_neighbors=20, radius=2.0)</strong><br/><strong>&gt;&gt;&gt; nn.fit(users)</strong><br/><strong>NearestNeighbors(algorithm='auto', leaf_size=30, metric='minkowski',</strong><br/><strong>         metric_params=None, n_jobs=1, n_neighbors=20, p=2, radius=2.0)</strong></pre>
<p>We have selected to have 20 neighbors and a Euclidean radius equal to 2. This parameter is used <span>when</span> we want to query the model to know which items are contained in the ball whose center is a sample and with a fixed radius. In our case, we are going to query the model to get all the neighbors of a test user:</p>
<pre><strong>&gt;&gt;&gt; test_user = np.array([2, 0, 3, 2])</strong><br/><strong>&gt;&gt;&gt; d, neighbors = nn.kneighbors(test_user.reshape(1, -1))</strong><br/><br/><strong>&gt;&gt;&gt; print(neighbors)</strong><br/><strong>array([[933,  67, 901, 208,  23, 720, 121, 156, 167,  60, 337, 549,  93,</strong><br/><strong>        563, 326, 944, 163, 436, 174,  22]], dtype=int64)</strong></pre>
<p>Now we need to build the recommendation list using the association matrix:</p>
<pre><strong>&gt;&gt;&gt; suggested_products = []</strong><br/><br/><strong>&gt;&gt;&gt; for n in neighbors:</strong><br/><strong>&gt;&gt;&gt;    for products in user_products[n]:</strong><br/><strong>&gt;&gt;&gt;       for product in products:</strong><br/><strong>&gt;&gt;&gt;          if product != 0 and product not in suggested_products:</strong><br/><strong>&gt;&gt;&gt;             suggested_products.append(product)</strong><br/><br/><strong>&gt;&gt;&gt; print(suggested_products)</strong><br/><strong>[14, 5, 13, 4, 8, 9, 16, 18, 10, 7, 1, 19, 12, 11, 6, 17, 15, 3, 2]</strong></pre>
<p>For each neighbor, we retrieve the products he/she bought and perform a union, avoiding the inclusion of items with zero value (meaning no product) and double elements. The result is a list (not sorted) of suggestions that can be obtained almost in real time for many different systems. In some cases, when the number of users or items is too huge, it's possible to limit the list to a fixed number of elements and to reduce the number of neighbors. This approach is also naive because it doesn't consider the actual distance (or similarity) between users to weigh the suggestions. It's possible to consider the distance as a weighing factor, but it's simpler to adopt the collaborative filtering approach which provides a more robust solution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Content-based systems</h1>
                </header>
            
            <article>
                
<p>This is probably the simplest method and it's based only on the products, modeled as feature vectors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="36" width="229" src="assets/92064032-622a-45c1-9ef4-63e267b14286.png"/></div>
<p class="CDPAlignLeft CDPAlign">Just like the users, the features can also be categorical (indeed, for products it's easier), for example, the genre of a book or a movie, and they can be used together with numerical values (like price, length, number of positive reviews, and so on) after encoding them.</p>
<p class="CDPAlignLeft CDPAlign">Then a clustering strategy is adopted, even if the most used is <strong>k-nearest neighbors</strong> as it allows controlling the size of each neighborhood to determine, given a sample product, the quality and the number of suggestions.</p>
<p class="CDPAlignLeft CDPAlign">Using scikit-learn, first of all we create a dummy product dataset:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; nb_items = 1000</strong><br/><strong>&gt;&gt;&gt; items = np.zeros(shape=(nb_items, 4))</strong><br/><br/><strong>&gt;&gt;&gt; for i in range(nb_items):</strong><br/><strong>&gt;&gt;&gt;    items[i, 0] = np.random.randint(0, 100)</strong><br/><strong>&gt;&gt;&gt;    items[i, 1] = np.random.randint(0, 100)</strong><br/><strong>&gt;&gt;&gt;    items[i, 2] = np.random.randint(0, 100)</strong><br/><strong>&gt;&gt;&gt;    items[i, 3] = np.random.randint(0, 100)</strong></pre>
<p>In this case, we have 1000 samples with four integer features bounded between 0 and 100. Then we proceed, as in the previous example, towards clustering them:</p>
<pre><strong>&gt;&gt;&gt; nn = NearestNeighbors(n_neighbors=10, radius=5.0)</strong><br/><strong>&gt;&gt;&gt; nn.fit(items)</strong></pre>
<p>At this point, it's possible to query our model with the method <kbd>radius_neighbors()</kbd>,<strong> </strong>which allows us to restrict our research only to a limited subset. The default radius (set through the parameter <kbd>radius</kbd>) is 5.0, but we can change it dynamically:</p>
<pre><strong>&gt;&gt;&gt; test_product = np.array([15, 60, 28, 73])</strong><br/><strong>&gt;&gt;&gt; d, suggestions = nn.radius_neighbors(test_product.reshape(1, -1), radius=20)</strong><br/><br/><strong>&gt;&gt;&gt; print(suggestions)</strong><br/><strong>[array([657, 784, 839, 342, 446, 196], dtype=int64)]</strong><br/><br/><strong>&gt;&gt;&gt; d, suggestions = nn.radius_neighbors(test_product.reshape(1, -1), radius=30)</strong><br/><br/><strong>&gt;&gt;&gt; print(suggestions)</strong><br/><strong>[ array([844, 340, 657, 943, 461, 799, 715, 863, 979, 784, 54, 148, 806,</strong><br/><strong> 465, 585, 710, 839, 695, 342, 881, 864, 446, 196, 73, 663, 580, 216], dtype=int64)]</strong></pre>
<p>Of course, when trying these examples, the number of suggestions can be different, as we are using random datasets, so I suggest trying different values for the radius (in particular when using different metrics).</p>
<p>When clustering with <strong>k-nearest neighbors</strong>, it's important to consider the metric adopted for determining the distance between the samples. The default for scikit-learn is the Minkowski distance, which is a generalization of Euclidean and Manhattan distance, and is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="73" width="217" src="assets/c39c54b7-284d-4d24-bac8-b7614e489373.png"/></div>
<p class="CDPAlignLeft CDPAlign">The parameter <em>p</em> controls the type of distance and the default value is 2, so that the resulting metric is a classical Euclidean distance. Other distances are offered by SciPy (in the package <span><kbd>scipy.spatial.distance</kbd>) and include, for example, the <strong>Hamming</strong> and <strong>Jaccard</strong> distances. The former is defined as the disagree proportion between two vectors (if they are binary this is the normalized number of different bits). For example:</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>from scipy.spatial.distance import hamming</strong><br/><br/><strong>&gt;&gt;&gt; a = np.array([0, 1, 0, 0, 1, 0, 1, 1, 0, 0])</strong><br/><strong>&gt;&gt;&gt; b = np.array([1, 1, 0, 0, 0, 1, 1, 1, 1, 0])</strong><br/><strong>&gt;&gt;&gt; d = hamming(a, b)</strong><br/><br/><strong>&gt;&gt;&gt; print(d)</strong><br/><strong>0.40000000000000002</strong></pre>
<p>It means there's a disagree proportion of 40 percent, or, considering that both vectors are binary, there 4 different bits (out of 10). This measure can be useful when it's necessary to emphasize the presence/absence of a particular feature.</p>
<p>The Jaccard distance is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="56" width="256" src="assets/f566e77a-f605-4e6d-abb8-3a3467235362.png"/></div>
<p class="CDPAlignLeft CDPAlign">It's particularly useful to measure the dissimilarity between two different sets (<em>A</em> and <em>B</em>) of items. If our feature vectors are binary, it's immediate to apply this distance using Boolean logic. Using the previous test values, we get:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from scipy.spatial.distance import jaccard</strong><br/><br/><strong>&gt;&gt;&gt; d = jaccard(a, b)</strong><br/><strong>&gt;&gt;&gt; print(d)</strong><br/><strong>0.5714285714285714</strong></pre>
<p>This measure is bounded between 0 (equal vectors) and 1 (total dissimilarity).</p>
<p>As for the Hamming distance, it can be very useful when it's necessary to compare items where their representation is made up of binary states (like present/absent, yes/no, and so forth). If you want to adopt a different metric for <strong>k-nearest neighbors</strong>, it's possible to specify it directly using the <kbd>metric</kbd> parameter:</p>
<pre><strong>&gt;&gt;&gt; nn = NearestNeighbors(n_neighbors=10, radius=5.0, metric='hamming')</strong><br/><strong>&gt;&gt;&gt; nn.fit(items)<br/><br/></strong><strong>&gt;&gt;&gt; nn = NearestNeighbors(n_neighbors=10, radius=5.0, metric='jaccard')</strong><br/><strong>&gt;&gt;&gt; nn.fit(items)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-free (or memory-based) collaborative filtering</h1>
                </header>
            
            <article>
                
<p>As with the user-based approach, let's consider having two sets of elements: users and items. However, in this case, we don't assume that they have explicit features. Instead, we try to model a user-item matrix based on the preferences of each user (rows) for each item (columns). For example:</p>
<div class="CDPAlignCenter CDPAlign"><img height="189" width="303" src="assets/7a0283ec-a54b-4cb6-b25c-9c08e1974a30.png"/></div>
<p class="CDPAlignLeft CDPAlign">In this case, the ratings are bounded between 1 and 5 (0 means no rating), and our goal is to cluster the users according to their rating vector (which is, indeed, an internal representation based on a particular kind of feature). This allows producing recommendations even when there are no explicit pieces of information about the user. However, it has a drawback, called <strong>cold-startup</strong>, which means that when a new user has no ratings, it's impossible to find the right neighborhood, because he/she can belong to <span>virtually</span><span> </span><span>any cluster.</span></p>
<p class="CDPAlignLeft CDPAlign">Once the clustering is done, it's easy to check which products (not rated yet) have the higher rating for a given user and therefore are more likely to be bought. It's possible to implement a solution in scikit-learn as we've done before, but I'd like to introduce a small framework called <strong>Crab</strong> (see the box at the end of this section) that simplifies this process.</p>
<p class="CDPAlignLeft CDPAlign">In order to build the model, we <span>first</span><span> </span><span>need to define the user-item matrix as a Python dictionary with the structure:</span></p>
<pre class="CDPAlignLeft CDPAlign"><strong>{ user_1: { item1: rating, item2: rating, ... }, ..., user_n: ... }</strong></pre>
<p>A missing value in a user internal dictionary means no rating. In our example, we consider 5 users with 5 items:</p>
<pre><strong>from scikits.crab.models import MatrixPreferenceDataModel</strong><br/><br/><strong>&gt;&gt;&gt; user_item_matrix = {</strong><br/><strong>       1: {1: 2, 2: 5, 3: 3},</strong><br/><strong>       2: {1: 5, 4: 2},</strong><br/><strong>       3: {2: 3, 4: 5, 3: 2},</strong><br/><strong>       4: {3: 5, 5: 1},</strong><br/><strong>       5: {1: 3, 2: 3, 4: 1, 5: 3}</strong><br/><strong>   }</strong><br/><br/><strong>&gt;&gt;&gt; model = MatrixPreferenceDataModel(user_item_matrix)</strong></pre>
<p>Once the user-item matrix has been defined, we need to pick a metric and therefore, a distance function <em>d(u<sub>i</sub>, u<sub>j</sub>)</em>, to build a similarity matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img height="79" width="234" src="assets/29ff8a1a-9a66-41c3-bf82-f12ad7a731c5.png"/></div>
<p class="CDPAlignLeft CDPAlign">Using Crab, we do this in the following way (using a Euclidean metric):</p>
<pre class="CDPAlignLeft CDPAlign"><strong>from scikits.crab.similarities import UserSimilarity</strong><br/><strong>from scikits.crab.metrics import euclidean_distances</strong><br/><br/><strong>&gt;&gt;&gt; similarity_matrix = UserSimilarity(model, euclidean_distances)</strong></pre>
<p>There are many metrics, like Pearson or Jaccard, so I suggest visiting the website (<a href="http://muricoca.github.io/crab">http://muricoca.github.io/crab</a>) to retrieve further information. At this point, it's possible to build the recommendation system (based on <span>the</span> k-nearest neighbors clustering method) and test it:</p>
<pre><strong>from scikits.crab.recommenders.knn import UserBasedRecommender</strong><br/><br/><strong>&gt;&gt;&gt; recommender = UserBasedRecommender(model, similarity_matrix, with_preference=True)</strong><br/><br/><strong>&gt;&gt;&gt; print(recommender.recommend(2))</strong><br/><strong>[(2, 3.6180339887498949), (5, 3.0), (3, 2.5527864045000417)]</strong></pre>
<p>So the recommender suggests the following predicted rating for user 2:</p>
<ul>
<li><strong>Item 2</strong>: 3.6 (which can be rounded to 4.0)</li>
<li><strong>Item 5</strong>: 3</li>
<li><strong>Item 3</strong>: 2.5 (which can be rounded to 3.0)</li>
</ul>
<p>When running the code, it's possible to see some warnings (Crab is still under development); however, they don't condition the functionality. If you want to avoid them, you can use the <kbd>catch_warnings()</kbd> context manager:</p>
<pre><strong>import warnings</strong><br/><br/><strong>&gt;&gt;&gt; with warnings.catch_warnings():</strong><br/><strong>&gt;&gt;&gt;    warnings.simplefilter("ignore")</strong><br/><strong>&gt;&gt;&gt;    print(recommender.recommend(2))</strong></pre>
<p>It's possible to suggest all the items, or limit the list to the higher ratings (so, for example, avoiding the item 3). This approach is quite similar to the user-based model. However, it's faster (very big matrices can be processed in parallel) and it doesn't take care of details that can produce misleading results. Only the ratings are considered as useful features to define a user. Like model-based collaborative filtering, the cold-startup problem can be addressed in two ways:</p>
<ul>
<li>Asking the user to rate some items (this approach is often adopted because it's easy to show some movie/book covers, asking the user to select what they like and what they don't).</li>
<li>Placing the user in an average neighborhood by randomly assigning some mean ratings. In this approach, it's possible to start using the recommendation system immediately. However, it's necessary to accept a certain degree of error at the beginning and to correct the dummy ratings when the real ones are produced.</li>
</ul>
<div class="packt_infobox">Crab is an open-source framework for building collaborative filtering systems. It's still under development and therefore, doesn't implement all possible features. However, it's very easy to use and is quite powerful for many tasks. The home page with installation instructions and documentation is: <a href="http://muricoca.github.io/crab/index.html" target="_blank">http://muricoca.github.io/crab/index.html</a>. Crab depends on scikits.learn, which still has some issues with Python 3. Therefore, I recommend using Python 2.7 for this example. It's possible to install both packages using pip: <kbd>pip install -U scikits.learn</kbd> and <kbd>pip install -U crab</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-based collaborative filtering</h1>
                </header>
            
            <article>
                
<p>This is currently one of the most advanced approaches and is an extension of what was already seen in the previous section. The starting point is always a rating-based user-item matrix:</p>
<div class="CDPAlignCenter CDPAlign"><img height="65" width="174" src="assets/e4a3a7f1-4ccd-4132-9a8c-3ff4fd0ca8a1.png"/></div>
<p class="CDPAlignLeft CDPAlign">However, in this case, we assume the presence of <strong>latent factors</strong> for both the users and the items. In other words, we define a generic user as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="24" width="225" src="assets/74322669-b70e-4833-9bbd-ae36981b2a83.png"/></div>
<p class="CDPAlignLeft CDPAlign">A generic item is defined as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="29" width="240" src="assets/92ac9795-5486-4ff0-980f-bba392ddcb95.png"/></div>
<p class="CDPAlignLeft CDPAlign">We don't know the value of each vector component (for this reason they are called latent), but we assume that a ranking is obtained as:</p>
<div class="CDPAlignCenter CDPAlign"><img height="28" width="87" src="assets/e38e388a-30a6-4f06-8a31-b82a19cc5da0.png"/></div>
<p class="CDPAlignLeft CDPAlign">So we can say that a ranking is obtained from a latent space of rank <em>k</em>, where <em>k</em> is the number of latent variables we want to consider in our model. In general, there are rules to determine the right value for <em>k</em>, so the best approach is to check different values and test the model with a subset of known ratings. However, there's still a big problem to solve: finding the latent variables. There are several strategies, but before discussing them, it's important to understand the dimensionality of our problem. If we have 1000 users and 500 products, <em>M</em> has 500,000 elements. If we decide to have rank equal to 10, it means that we need to find 5000000 variables constrained by the known ratings. As you can imagine, this problem can easily become impossible to solve with standard approaches and parallel solutions must be employed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Singular Value Decomposition strategy</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">The first approach is based on the <strong>Singular Value Decomposition</strong> (<strong>SVD</strong>) of the user-item matrix. This technique allows transforming a matrix through a low-rank factorization and can also be used in an incremental way as described in Sarwar B., Karypis G., Konstan J., Riedl J., <em>Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems</em>, 2002. In particular, if the user-item matrix has <em>m</em> rows and <em>n</em> columns:</p>
<div class="CDPAlignCenter CDPAlign"><img height="38" width="377" src="assets/1797809a-47b0-41ba-911f-9759b959e05b.png"/></div>
<p class="CDPAlignLeft CDPAlign">We have assumed that we have real matrices (which is often true in our case), but, in general, they are complex. <em>U</em> and <em>V</em> are unitary, while sigma is diagonal. The columns of <em>U</em> contain the left singular vectors, the rows of transposed <em>V</em> contain the right singular vectors, while the diagonal matrix Sigma contains the singular values. Selecting <em>k</em> latent factors means taking the first <em>k</em> singular values and, therefore, the corresponding <em>k</em> left and right singular vectors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="37" width="110" src="assets/db5354fb-d50d-40b2-ae40-bae0ff85b189.png"/></div>
<p class="CDPAlignLeft CDPAlign">This technique has the advantage of minimizing the Frobenius norm of the difference between <em>M</em> and <em>M<sub>k</sub></em> for any value of <em>k</em>, and therefore, it's an optimal choice to approximate the full decomposition. Before moving to the prediction stage, let's create an example using SciPy. The first thing to do is to create a dummy user-item matrix:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; M = np.random.randint(0, 6, size=(20, 10))</strong><br/><br/><strong>&gt;&gt;&gt; print(M)</strong><br/><strong>array([[0, 4, 5, 0, 1, 4, 3, 3, 1, 3],</strong><br/><strong>       [1, 4, 2, 5, 3, 3, 3, 4, 3, 1],</strong><br/><strong>       [1, 1, 2, 2, 1, 5, 1, 4, 2, 5],</strong><br/><strong>       [0, 4, 1, 2, 2, 5, 1, 1, 5, 5],</strong><br/><strong>       [2, 5, 3, 1, 1, 2, 2, 4, 1, 1],</strong><br/><strong>       [1, 4, 3, 3, 0, 0, 2, 3, 3, 5],</strong><br/><strong>       [3, 5, 2, 1, 5, 3, 4, 1, 0, 2],</strong><br/><strong>       [5, 2, 2, 0, 1, 0, 4, 4, 1, 0],</strong><br/><strong>       [0, 2, 4, 1, 3, 1, 3, 0, 5, 4],</strong><br/><strong>       [2, 5, 1, 5, 3, 0, 1, 4, 5, 2],</strong><br/><strong>       [1, 0, 0, 5, 1, 3, 2, 0, 3, 5],</strong><br/><strong>       [5, 3, 1, 5, 0, 0, 4, 2, 2, 2],</strong><br/><strong>       [5, 3, 2, 4, 2, 0, 4, 4, 0, 3],</strong><br/><strong>       [3, 2, 5, 1, 1, 2, 1, 1, 3, 0],</strong><br/><strong>       [1, 5, 5, 2, 5, 2, 4, 5, 1, 4],</strong><br/><strong>       [4, 0, 2, 2, 1, 0, 4, 4, 3, 3],</strong><br/><strong>       [4, 2, 2, 3, 3, 4, 5, 3, 5, 1],</strong><br/><strong>       [5, 0, 5, 3, 0, 0, 3, 5, 2, 2],</strong><br/><strong>       [1, 3, 2, 2, 3, 0, 5, 4, 1, 0],</strong><br/><strong>       [1, 3, 1, 4, 1, 5, 4, 4, 2, 1]])</strong></pre>
<p>We're assuming that we have 20 users and 10 products. The ratings are bounded between 1 and 5, and 0 means no rating. Now we can decompose <em>M</em>:</p>
<pre><strong>from scipy.linalg import svd</strong><br/><br/><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; U, s, V = svd(M, full_matrices=True)</strong><br/><strong>&gt;&gt;&gt; S = np.diag(s)</strong><br/><br/><strong>&gt;&gt;&gt; print(U.shape)</strong><br/><strong>(20L, 20L)</strong><br/><br/><strong>&gt;&gt;&gt; print(S.shape)</strong><br/><strong>(10L, 10L)</strong><br/><br/><strong>&gt;&gt;&gt; print(V.shape)</strong><br/><strong>(10L, 10L)</strong></pre>
<p>Now let's consider only the first eight singular values, which will have eight latent factors for both the users and items:</p>
<pre><strong>&gt;&gt;&gt; Uk = U[:, 0:8]</strong><br/><strong>&gt;&gt;&gt; Sk = S[0:8, 0:8]</strong><br/><strong>&gt;&gt;&gt; Vk = V[0:8, :]</strong></pre>
<p>Bear in mind that in SciPy SVD implementation, <em>V</em> is already transposed. According to Sarwar B., Karypis G., Konstan J., Riedl J., <em>Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems</em>, 2002, we can easily get a prediction considering the cosine similarity (which is proportional to the dot product) between customers and products. The two latent factor matrices are:</p>
<div class="CDPAlignCenter CDPAlign"><img height="63" width="123" src="assets/58bf2712-c778-4fa1-8581-acf00903214f.png"/></div>
<p class="CDPAlignLeft CDPAlign">In order to take into account the loss of precision, it's useful also <span>to</span><span> </span><span>consider the average rating per user (which corresponds to the mean row value of the user-item matrix), so that the result rating prediction for the user</span> <em>i</em> <span>and the item</span> <em>j</em> <span>becomes:</span></p>
<div class="CDPAlignCenter CDPAlign"><img height="34" width="179" src="assets/958c5037-4bea-41be-9b57-1eb258b1b4b8.png"/></div>
<p class="CDPAlignLeft CDPAlign">Here <em>S<sub>U</sub>(i)</em> and <em>S<sub>I</sub>(j)</em> are the user and product vectors respectively. Continuing with our example, let's determine the rating prediction for user 5 and item 2:</p>
<pre class="CDPAlignLeft CDPAlign"><strong>&gt;&gt;&gt; Su = Uk.dot(np.sqrt(Sk).T)</strong><br/><strong>&gt;&gt;&gt; Si = np.sqrt(Sk).dot(Vk).T</strong><br/><strong>&gt;&gt;&gt; Er = np.mean(M, axis=1)</strong><br/><br/><strong>&gt;&gt;&gt; r5_2 = Er[5] + Su[5].dot(Si[2])<br/>&gt;&gt;&gt; print(r5_2)</strong><br/><strong>2.38848720112</strong></pre>
<p>This approach has medium complexity. In particular, the SVD is <em>O(m<sup>3</sup>)</em> and an incremental strategy (as described in Sarwar B., Karypis G., Konstan J., Riedl J., <em>Incremental Singular Value Decomposition Algorithms for Highly </em><em>Scalable Recommender Systems</em>, 2002) must be employed when new users or items are added; however, it can be effective when the number of elements is not too big. In all the other cases, the next strategy (together with a parallel architecture) can be adopted.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alternating least squares strategy</h1>
                </header>
            
            <article>
                
<p>The problem of finding the latent factors can be easily expressed as a least square optimization <span>problem</span> by defining the following loss function:</p>
<div class="CDPAlignCenter CDPAlign"><img height="58" width="290" src="assets/91166122-a1b9-4316-9d42-a74a00575199.png"/></div>
<p class="CDPAlignLeft CDPAlign">L is limited only to known samples (user, item). The second term works as a regularization factor and the whole problem can easily <span>be</span><span> </span><span>solved with any optimization method. However, there's an additional issue:</span> we have two different sets of variables to determine (user and item factors). <span>We can solve this problem with an approach called</span> <strong>alternating least squares</strong><span>, described in Koren Y., Bell R., Volinsky C., </span><em>Matrix Factorization Techniques for Recommender Systems</em><span>, IEEE Computer Magazine, 08/2009. The algorithm is very easy to describe and can be summarized in two main iterating steps:</span></p>
<ul>
<li><em>p<sub>i</sub></em> is fixed and <em>q<sub>j</sub></em> is optimized</li>
<li><em>q</em><sub><em>j</em> </sub>is fixed and <em>p<sub>i</sub></em> is optimized</li>
</ul>
<p>The algorithm stops when a predefined precision has been achieved. It can be easily implemented with parallel strategies to be able to process huge matrices in a short time. Moreover, considering the price of virtual clusters, it's also possible to retrain the model periodically, to immediately (with an acceptable delay) include new products and users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Alternating least squares with Apache Spark MLlib</h1>
                </header>
            
            <article>
                
<p>Apache Spark is beyond the scope of this book, so if you want to know more about this powerful framework, I suggest you read the online documentation or one the many books available. In Pentreath N., <em>Machine Learning with Spark</em>, Packt, there's an interesting introduction on the library MLlib and how to implement most of the algorithms discussed in this book.</p>
<p>Spark is a parallel computational engine that is now part of the Hadoop project (even if it doesn't use its code), that can run in local mode or on very large clusters (with thousands of nodes), to execute complex tasks using huge amounts of data. It's mainly based on Scala, though there are interfaces for Java, Python, and R. In this example, we're going to use PySpark, which is the built-in shell for running Spark with Python code.</p>
<p>After launching PySpark in local mode, we get a standard Python prompt and we can start working, just like with any other standard Python environment:</p>
<pre><strong># Linux<br/>&gt;&gt;&gt; ./pyspark<br/><br/># Mac OS X<br/>&gt;&gt;&gt; pyspark<br/><br/># Windows<br/>&gt;&gt;&gt; pyspark</strong><br/><br/><strong>Python 2.7.12 |Anaconda 4.0.0 (64-bit)| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)] on win32</strong><br/><strong>Type "help", "copyright", "credits" or "license" for more information.</strong><br/><strong>Anaconda is brought to you by Continuum Analytics.</strong><br/><strong>Please check out: http://continuum.io/thanks and https://anaconda.org</strong><br/><strong>Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</strong><br/><strong>Setting default log level to "WARN".</strong><br/><strong>To adjust logging level use sc.setLogLevewl(newLevel).</strong><br/><strong>Welcome to</strong><br/><strong> ____ __</strong><br/><strong> / __/__ ___ _____/ /__</strong><br/><strong> _\ \/ _ \/ _ `/ __/ '_/</strong><br/><strong> /__ / .__/\_,_/_/ /_/\_\ version 2.0.2</strong><br/><strong> /_/</strong><br/><br/><strong>Using Python version 2.7.12 (default, Jun 29 2016 11:07:13)</strong><br/><strong>SparkSession available as 'spark'.</strong><br/><strong>&gt;&gt;&gt;</strong></pre>
<p>Spark MLlib implements the ALS algorithm through a very simple mechanism. The class <kbd>Rating</kbd> is a wrapper for the tuple (user, product, rating), so we can easily define a dummy dataset (which must be considered only as an example, because it's very limited):</p>
<pre><strong>from pyspark.mllib.recommendation import Rating</strong><br/><br/><strong>import numpy as np</strong><br/><br/><strong>&gt;&gt;&gt; nb_users = 200</strong><br/><strong>&gt;&gt;&gt; nb_products = 100</strong><br/><br/><strong>&gt;&gt;&gt; ratings = []</strong><br/><br/><strong>&gt;&gt;&gt; for _ in range(10):</strong><br/><strong>&gt;&gt;&gt;    for i in range(nb_users):</strong><br/><strong>&gt;&gt;&gt;        rating = Rating(user=i, </strong><br/><strong>&gt;&gt;&gt;                        product=np.random.randint(1, nb_products), </strong><br/><strong>&gt;&gt;&gt;                        rating=np.random.randint(0, 5))</strong><br/><strong>&gt;&gt;&gt;        ratings.append(rating)</strong><br/><br/><strong>&gt;&gt;&gt; ratings = sc.parallelize(ratings)</strong></pre>
<p>We assumed that we have 200 users and 100 products and we have populated a list of ratings by iterating 10 times the main loop which assigns a rating to a random product. We're not controlling repetitions or other uncommon situations. The last command <kbd>sc.parallelize()</kbd> is a way to ask Spark to transform our list into a structure called <strong>resilient distributed dataset</strong> (<strong>RDD</strong>), which will be used for the remaining operations. There are no actual limits to the size of these structures, because they are distributed across different executors (if in clustered mode) and can work with petabytes datasets just like we work with kilobytes ones.</p>
<p>At this point, we can train an <kbd>ALS</kbd> model (which is formally <kbd>MatrixFactorizationModel</kbd>) and use it to make some predictions:</p>
<pre><strong>from pyspark.mllib.recommendation import ALS<br/></strong><br/><strong>&gt;&gt;&gt; model = ALS.train(ratings, rank=5, iterations=10)</strong></pre>
<p>We want 5 latent factors and 10 optimization iterations. As discussed before, it's not very easy to determine the right rank for each model, so, after a training phase, there should always be a validation phase with known data. The mean squared error is a good measure to understand how the model is working. We can do it using the same training data set. The first thing to do is to remove the ratings (because we need only the tuple made up of user and product):</p>
<pre><strong>&gt;&gt;&gt; test = ratings.map(lambda rating: (rating.user, rating.product))</strong></pre>
<p>If you're not familiar with the MapReduce paradigm, you only need to know that <kbd>map()</kbd> applies the same function (in this case, a lambda) to all the elements. Now we can massively predict the ratings:</p>
<pre><strong>&gt;&gt;&gt; predictions = model.predictAll(test)</strong></pre>
<p>However, in order to compute the error, we also need to add the user and product, to have tuples that can be compared:</p>
<pre><strong>&gt;&gt;&gt; full_predictions = predictions.map(lambda pred: ((pred.user, pred.product), pred.rating))</strong></pre>
<p>The result is a sequence of rows with a structure <kbd>((user, item), rating)</kbd>, just like a standard dictionary entry <kbd>(key, value)</kbd>. This is useful because, using Spark, we can join two RDDs by using their keys. We do the same thing for the original dataset also, and then we proceed by joining the training values with the predictions:</p>
<pre><strong>&gt;&gt;&gt; split_ratings = ratings.map(lambda rating: ((rating.user, rating.product), rating.rating))</strong><br/><strong>&gt;&gt;&gt; joined_predictions = split_ratings.join(full_predictions)</strong></pre>
<p>Now for each key <kbd>(user, product)</kbd>, we have two values: target and prediction. Therefore, we can compute the mean squared error:</p>
<pre><strong>&gt;&gt;&gt; mse = joined_predictions.map(lambda x: (x[1][0] - x[1][1]) ** 2).mean()</strong></pre>
<p>The first map transforms each row into the squared difference between the target and prediction, while the <kbd>mean()</kbd> function computes the average value. At this point, let's check our error and produce a prediction:</p>
<pre><strong>&gt;&gt;&gt; print('MSE: %.3f' % mse)</strong><br/><strong>MSE: 0.580</strong><br/><br/><strong>&gt;&gt;&gt; prediction = model.predict(10, 20)</strong><br/><strong>&gt;&gt;&gt; print('Prediction: %3.f' % prediction)</strong><br/><strong>Prediction: 2.810</strong></pre>
<p>So, our error is quite low but<span> </span>it can be improved by changing the rank or the number of iterations. The prediction for the rating of the product 20 by the user 10 is about 2.8 (that can be rounded to 3). If you run the code, these values can be different as we're using a random user-item matrix. Moreover, if you don't want to use the shell and run the code directly, you need to declare a <kbd>SparkContext</kbd> explicitly at the beginning of your file:</p>
<pre><strong>from pyspark import SparkContext, SparkConf</strong><br/><br/><strong>&gt;&gt;&gt; conf = SparkConf().setAppName('ALS').setMaster('local[*]')</strong><br/><strong>&gt;&gt;&gt; sc = SparkContext(conf=conf)</strong></pre>
<p>We have created a configuration through the <kbd>SparkConf</kbd> class and specified both an application name and a master (in local mode with all cores available). This is enough to run our code. However, if you need further information, visit the page mentioned in the information box at the end of the chapter. To run the application (since Spark 2.0), you must execute the following command:</p>
<pre><strong># Linux, Mac OSx</strong><br/><strong>./spark-submit als_spark.py</strong><br/><br/><strong># Windows</strong><br/><strong>spark-submit als_spark.py</strong></pre>
<div class="packt_infobox">When running a script using <kbd>spark-submit</kbd>, you will see hundreds of log lines that inform you about all the operations that are being performed. Among them, at the end of the computation, you'll also see the print function messages (<kbd>stdout</kbd>).</div>
<p>Of course, this is only an introduction to Spark ALS, but I hope it was useful to understand how easy this process can be and, at the same time, how the dimensional limitations can be effectively addressed.</p>
<div class="packt_tip packt_infobox">If you don't know how to set up the environment and launch PySpark, I suggest reading the online quick-start guide (<a href="https://spark.apache.org/docs/2.1.0/quick-start.html">https://spark.apache.org/docs/2.1.0/quick-start.html</a>) that can be useful even if you don't know all the details and configuration parameters.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li>Sarwar B., Karypis G., Konstan J., Riedl J., <em>Incremental Singular Value Decomposition Algorithms for Highly </em><em>Scalable Recommender Systems</em>, 2002</li>
<li>Koren Y., Bell R., Volinsky C., <em>Matrix Factorization Techniques For Recommender Systems</em>, IEEE Computer Magazine, 08/2009</li>
<li>Pentreath N., <em>Machine Learning with Spark</em>, Packt</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the main techniques for building a recommender system. In a user-based scenario, we assume that we have enough pieces of information about the users to be able to cluster them, and moreover, we implicitly assume that similar users would like the same products. In this way, it's immediate to determine the neighborhood of every new user and to suggest the products positively rated by his/her peers. In a similar way, a content-based scenario is based on the clustering of products according to their peculiar features. In this case, the assumption is weaker, because it's more probable that a user who bought an item or rated it positively will do the same with similar products.</p>
<p>Then we introduced collaborative filtering, which is a technique based on explicit ratings, used to predict all missing values for all users and products. In the memory-based variant, we don't train a model but we try to work directly with a user-product matrix, looking for the k-nearest neighbors of a test user, and computing the ranking through an average. This approach is very similar to the user-based scenario and has the same limitations; in particular, it's very difficult to manage large matrices. On the other hand, the model-based approach is more complex, but, after training the model, it can predict the ratings in real time. Moreover, there are parallel frameworks like Spark, which can be employed to process a huge amount of data using a cluster of cheap servers. </p>
<p>In the next chapter, we're going to introduce some natural language processing techniques, which are very important when automatically classifying texts or working with machine translation systems.</p>


            </article>

            
        </section>
    </body></html>