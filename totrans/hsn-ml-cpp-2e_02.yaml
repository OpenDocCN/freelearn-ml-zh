- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the essential things in **machine learning** (**ML**) is the data that
    we use for training. We can gather training data from the processes we work with,
    or we can take already prepared training data from third-party sources. In any
    case, we have to store training data in a file format that should satisfy our
    development requirements. These requirements depend on the task we solve, as well
    as the data-gathering process. Sometimes, we need to transform data stored in
    one format to another to satisfy our needs. Examples of such needs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing human readability to ease communication with engineers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The existence of compression possibility to allow data to occupy less space
    on secondary storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of data in the binary form to speed up the parsing process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting complex relations between different parts of data to make precise
    mirroring of a specific domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platform independence to be able to use the dataset in different development
    and production environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today, there exists a variety of file formats that are used for storing different
    kinds of information. Some of these are very specific, and some of them are general-purpose.
    There are software libraries that allow us to manipulate these file formats. There
    is rarely a need to develop a new format and parser from scratch. Using existing
    software for reading a format can significantly reduce development and testing
    time, which allows us to focus on particular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter discusses how to process popular file formats that we use for storing
    data. It shows what libraries exist for working with `OpenCV` and `Dlib` libraries
    and how to convert the data format used in these libraries to data types used
    in linear algebra libraries. It also describes data normalization techniques such
    as feature scaling and standardization procedures to deal with heterogeneous data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing data formats to C++ data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing matrix and tensor objects from C++ data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating images with the `OpenCV` and `Dlib` libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming images into matrix and tensor objects of various libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The required technologies and installations for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Modern C++ compiler with C++17/C++20 support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMake build system version >= 3.22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dlib` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlpack` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flashlight` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Eigen` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hdf5lib` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HighFive` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nlohmann-json` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Fast-CPP-CSV-Parser` library installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub repo: [https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition](https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition)'
  prefs: []
  type: TYPE_NORMAL
- en: Parsing data formats to C++ data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most popular format for representing structured data is called **CSV**.
    This format is just a text file with a two-dimensional table in it whereby values
    in a row are separated with commas, and rows are placed on every new line. It
    looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The advantages of this file format are that it has a straightforward structure,
    many software tools can process it, it is human-readable, and it is supported
    on a variety of computer platforms. Disadvantages are a lack of support for multidimensional
    data and data with complex structuring, as well as slow parsing speed in comparison
    with binary formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another widely used format is **JSON**. Although the format contains JavaScript
    in its abbreviation, we can use it with almost all programming languages. This
    is a file format with name-value pairs and arrays of such pairs. It has rules
    on how to group such pairs into distinct objects and array declarations, and there
    are rules on how to define values of different types. The following code sample
    shows a file in JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The advantages of this format are human readability, software support on many
    computer platforms, and the possibility to store hierarchical and nested data
    structures. Disadvantages are its slow parsing speed in comparison with binary
    formats and the fact it is not very useful for representing numerical matrices.
    In terms of character reading, binary formats offer more direct access to the
    underlying data structure, allowing for faster and more precise character extraction.
    With text formats, additional steps may be required to convert the characters
    into their numerical representations, potentially introducing additional processing
    overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Often, we use a combination of file formats to represent a complex dataset.
    For example, we can describe object relations with JSON, and data/numerical data
    in the binary form can be stored in a folder structure on the filesystem with
    references to it in JSON files.
  prefs: []
  type: TYPE_NORMAL
- en: '**HDF5** is a specialized file format for storing scientific data. This file
    format was developed to store heterogeneous multidimensional data with a complex
    structure. It provides fast access to single elements because it has optimized
    data structures for using secondary storage. Furthermore, HDF5 supports data compression.
    In general, this file format consists of named groups that contain multidimensional
    arrays of multitype data. Each element of this file format can contain metadata,
    as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – HDF5 format structure](img/B19849_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – HDF5 format structure
  prefs: []
  type: TYPE_NORMAL
- en: The advantages of this format are its high read-and-write speed, fast access
    to distinct elements, and its ability to support data with a complex structure
    and various types of data. Disadvantages are the requirement of specialized tools
    for editing and viewing by users, the limited support of type conversions among
    different platforms, and using a single file for the whole dataset. The last issue
    makes data restoration almost impossible in the event of file corruption. So,
    it makes sense to regularly back up your data to prevent data loss in case of
    hardware failure or accidental deletion.
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of other formats for representing datasets for ML, but we found
    the ones mentioned to be the most useful.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV files with the Fast-CPP-CSV-Parser library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider how to deal with CSV format in C++. There are many different libraries
    for parsing CSV format with C++. They have different sets of functions and different
    ways to integrate them into applications. The easiest way to use C++ libraries
    is to use header-only libraries because this eliminates the need to build and
    link them. We propose to use the `Fast-CPP-CSV-Parser` library because it is a
    small single-file header-only library with the minimal required functionality,
    which can be easily integrated into a development code base. It also provides
    a fast and efficient way to read and write CSV data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example of a CSV file format, we use the `Iris` dataset, which describes
    three different types of iris plants (*Iris setosa*, *Iris versicolor*, and *Iris
    virginica*) and was conceived by R.A. Fisher. Each row in the file contains the
    following fields: sepal length, sepal width, petal length, petal width, and a
    string with a class name. This dataset is used for examples of how to classify
    an unknown iris flower based on these four features.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference to the Iris dataset is the following: *Dua, D.* and *Graff, C.*
    (*2019*). *UCI Machine Learning Repository* [[https://archive.ics.uci.edu/static/public/53/iris.zip](https://archive.ics.uci.edu/static/public/53/iris.zip)].
    *Irvine, CA: University of California, School of Information and* *Computer Science*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To read this dataset with the `Fast-CPP-CSV-Parser` library, we need to include
    a single header file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define an object of the type `io::CSVReader`. We must define the number
    of columns as a template parameter. This parameter is one of the library limitations
    because we need to be aware of the CSV file structure. The code for this is illustrated
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define containers for storing the values we read, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to make our code more generic and gather all information about column
    types in one place, we introduce the following helper types and functions. We
    define a tuple object that describes values for a row, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason for using a tuple is that we can easily iterate it with metaprogramming
    techniques. Then, we define two helper functions. One is for reading a row from
    a file, and it uses the `read_row()` method of the `io::CSVReader` class. The
    `read_row()` method takes a variable number of parameters of different types.
    Our `RowType` type describes these values. We do automatic parameter filling by
    using the `std::index_sequence` type with the `std::get` function, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The second helper function uses a similar technique for transforming a row
    tuple object to our value vectors, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can put all the parts together. We define a loop where we continuously
    read row values and move them to our containers. After we read a row, we check
    the return value of the `read_row()` method, which tells us if the read was successful
    or not. A `false` return value means that we have reached the end of the file.
    In the case of a parsing error, we catch an exception from the `io::error` namespace.
    There are exception types for different parsing failures. In the following example,
    we handle number parsing errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Also, notice that we moved only four values to our vector of doubles because
    the last column contains string objects that we put into another vector of categorical
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code sample, we saw how to parse the particular dataset with string
    and numerical values into two containers: `std::vector<std::string> categorical_column`
    and `std::vector<double> values`.'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing CSV files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, the data we have comes in a format that’s incompatible with the libraries
    we want to use. For example, the Iris dataset file contains a column that contains
    strings. Many ML libraries cannot read such values because they assume that CSV
    files contain only numerical values that can be directly loaded into an internal
    matrix representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, before using such datasets, we need to preprocess them. In the case of
    the Iris dataset, we need to replace the `categorical` column containing string
    labels with numeric encoding. In the following code sample, we replace strings
    with distinct numbers, but in general, such an approach is a bad idea, especially
    for classification tasks. ML algorithms usually learn only numerical relations,
    so a more suitable approach would be to use specialized encoding—for example,
    one-hot encoding. One-hot encoding is a method used in ML to represent categorical
    data as numerical values. It involves creating a binary vector for each unique
    value in the categorical feature, where only one element in the vector is set
    to `1` and all others are set to `0`. The code can be seen in the following block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We read the CSV file content to the `std::string` object with the `std::ifstream`
    object. Also, we use `std::regex` routines to replace string class names with
    numbers. Using `regex` functions allows us to reduce code size and make it more
    expressive in comparison with the loop approach, which typically uses the `std::string::find()`
    and `std::string::replace()` methods. After replacing all categorical class names
    in the file, we create a new file with the `std::ofstream` object.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV files with the mlpack library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many ML frameworks already have routines for reading the CSV file format to
    their internal representations. In the following code sample, we show how to load
    a CSV file with the `mlpack` library into the `matrix` object. The CSV parser
    in this library can automatically create numerical mappings for non-numeric values,
    so we easily load the Iris dataset without additional preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read a CSV file with the `mlpack` library, we have to include the corresponding
    headers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the `data::Load` function to load the CSV data from a file, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the `data::Load` function takes the `dataset` matrix object to
    load data and the `info` object of the `DatasetInfo` type that can be used to
    get additional information about the loaded file. Also, the last Boolean `true`
    parameter was used to make the function throw an exception in the loading error
    case. For example, we can get the number of columns and an available mapping for
    non-numeric values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to the fact that data is loaded as is, there are no automatic assumptions
    about dataset structure. So, to extract labels, we need to manually divide the
    loaded matrix object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We used the `arma::conv_to` function to create the standalone `arma::Row` object
    from the dataset row. Then, we deleted the last row from the dataset with the
    `shed_row` method.
  prefs: []
  type: TYPE_NORMAL
- en: Reading CSV files with the Dlib library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Dlib` library can load CSV files directly to its matrix type, as the `mlpack`
    library does. For this operation, we can use a simple C++ streaming operator and
    a standard `std::ifstream` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we make the necessary `include` statements, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a `matrix` object and load data from the file, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the `Dlib` library, `matrix` objects are used for training ML algorithms
    directly without the need to transform them into intermediate dataset types.
  prefs: []
  type: TYPE_NORMAL
- en: Reading JSON files with the nlohmann-json library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some datasets come with structured annotations and can contain multiple files
    and folders. An example of such a complex dataset is the **Common Objects in Context**
    (**COCO**) dataset. This dataset contains a text file with annotations for describing
    relations between objects and their structural parts. This widely known dataset
    is used to train models for segmentation, object detection, and classification
    tasks. Annotations in this dataset are defined in the JSON file format. JSON is
    a widely used file format for objects’ (entities’) representations.
  prefs: []
  type: TYPE_NORMAL
- en: It is just a text file with special notations for describing relations between
    objects and their parts. In the following code samples, we show how to work with
    this file format using the `nlohmann-json` library. This library provides a simple
    and intuitive interface for working with JSON, making it easy to convert between
    JSON strings and C++ data structures such as maps, vectors, and custom classes.
    It also supports various features such as automatic type conversion, pretty printing,
    and error handling. However, we are going to use a more straightforward dataset
    that defines paper reviews. The authors of this dataset are Keith, B., Fuentes,
    E., and Meneses, C., and they made this dataset for their work titled *A Hybrid
    Approach for Sentiment Analysis Applied to Paper* *Reviews* (2017).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following sample shows a reduced part of this JSON-based dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two main approaches to parsing and processing JSON files, which are
    listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first approach assumes the parsing of whole files at once and creating a
    **Document Object Model** (**DOM**). The DOM is a hierarchical structure of objects
    that represents entities stored in files. It is usually stored in computer memory,
    and, in the case of large files, it can occupy a significant amount of memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another approach is to parse the file continuously and provide an **application
    program interface** (**API**) for a user to handle and process each event related
    to the file-parsing process. This second approach is usually called **Simple API
    for XML** (**SAX**). Despite its name, it’s a general approach that is used with
    non-XML data too. SAX is faster than DOM for parsing large XML files because it
    doesn’t build a complete tree representation of the entire document in memory.
    However, it can be more difficult to use for complex operations that require accessing
    specific parts of the document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a DOM for working with training datasets usually requires a lot of memory
    for structures that are useless for ML algorithms. So, in many cases, it is preferable
    to use the SAX interface. It allows us to filter irrelevant data and initialize
    structures that we can use directly in our algorithms. In the following code sample,
    we use this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a preliminary step, we define types for `paper`/`review` entities, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we declare a type for the object, which will be used by the parser to
    handle parsing events. This type should be inherited from the `nlohmann::json::json_sax_t`
    base class, and we need to override virtual handler functions that the parser
    will call when a particular parsing event occurs, as illustrated in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We have to override all methods, but we can provide real handler implementations
    only for objects, arrays parsing events, and events for parsing unsigned `int`/`string`
    values. Other methods can have trivial implementations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `nlohmann::json::sax_parse` method to load a JSON file;
    this method takes the `std::istream` object and a `handler` object as the second
    argument. The following code block shows how to use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When there are no parsing errors, we will have an initialized array of `Paper`
    type objects. Consider, more precisely, the event handler’s implementation details.
    Our event handler works as a state machine. In one state, we populate it with
    the `Review` objects, and in another one, with the `Paper` objects, and there
    are states for other events, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We parse the unsigned `unit` values only for the `Id` attributes of the `Paper`
    and the `Review` objects, and we update these values according to the current
    state and the previously parsed key, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'String values also exist in both types of objects, so we do the same checks
    to update corresponding values, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The event handler for the JSON `key` attribute stores the `key` value to the
    appropriate variable, which we use to identify a current object in the parsing
    process, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `start_object` event handler switches states according to the current `key`
    value and the previous `state` value. We base the current implementation on the
    knowledge of the structure of the current JSON file: there is no array of `Paper`
    objects, and each `Paper` object includes an array of reviews. It is one of the
    limitations of the SAX interface—we need to know the structure of the document
    to implement all event handlers correctly. The code can be seen in the following
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `end_object` event handler, we populate arrays of `Paper` and `Review`
    objects according to the current state. Also, we switch the current state back
    to the previous one by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `start_array` event handler, we switch the current state to a new one
    according to the current `state` value by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `end_array` event handler, we switch the current state to the previous
    one based on our knowledge of the document structure by running the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The vital thing in this approach is to clear the current `key` value after object
    processing. This helps us to debug parsing errors, and we always have actual information
    about the currently processed entity.
  prefs: []
  type: TYPE_NORMAL
- en: For small files, using the DOM approach can be preferable because it leads to
    less code and cleaner algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Writing and reading HDF5 files with the HighFive library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HDF5 is a highly efficient file format for storing datasets and scientific values.
    The `HighFive` library provides a higher-level C++ interface for the C library
    provided by the HDF Group. In this example, we propose to look at its interface
    by transforming the dataset used in the previous section to HDF5 format.
  prefs: []
  type: TYPE_NORMAL
- en: The main concepts of the HDF5 format are groups and datasets. Each group can
    contain other groups and have attributes of different types. Also, each group
    can contain a set of dataset entries. Each dataset is a multidimensional array
    of values of the same type, which also can have attributes of different types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with including the required headers, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to create a `file` object where we will write our dataset, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have a `file` object, we can start creating groups. We define a group
    of papers that should hold all `paper` objects, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iterate through an array of papers (as shown in the previous section)
    and create a group for each `paper` object with two attributes: the numerical
    `id` attribute and the `preliminary_decision` attribute of the `string` type,
    as illustrated in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have created an attribute, we have to put in its value with the `write()`
    method. Notice that the `HighFive::DataSpace::From` function automatically detects
    the size of the attribute value. The size is the amount of memory required to
    hold the attribute’s value. Then, for each `paper_group` object, we create a corresponding
    group of reviews, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We insert into each `reviews_group` object a dataset of numerical values of
    `confidence`, `evaluation`, and `orientation` fields. For the dataset, we define
    `DataSpace` (the number of elements in the dataset) of size `3` and define a storage
    type as a 32-bit integer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: After we have created and initialized all objects, the Papers/Reviews dataset
    in HDF5 format is ready. When the `file` object leaves the scope, its destructor
    saves everything to the secondary storage.
  prefs: []
  type: TYPE_NORMAL
- en: Having the file in the HDF5 format, we can consider the `HighFive` library interface
    for file reading.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the first step, we again create a `HighFive::File` object, but with attributes
    for reading, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use the `getGroup()` method to get the top-level `papers_group` object,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `getGroup` method allows us to get a specific group by its name, so it’s
    a type of navigation through the HDF5 file structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to get a list of all nested objects in this group because we can access
    objects only by their names. We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a loop, we iterate over all `papers_group` objects in the `papers_group`
    container, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'For each `paper` object, we read its attributes and the memory space required
    for the attribute value. Also, because each attribute can be multidimensional,
    we should take care of it and allocate an appropriate container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'For reading datasets, we can use the same approach: get the `reviews` group,
    then get a list of dataset names, and, finally, read each dataset in a loop, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we use the `select()` method for the dataset, which allows us to
    read only a part of the dataset. We define this part with ranges given as arguments.
    There is the `read()` method in the `dataset` type to read a whole dataset at
    once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using these techniques, we can read and transform any HDF5 dataset. This file
    format allows us to work only with part of the required data and not to load the
    whole file to the memory. Also, because this is a binary format, its reading is
    more efficient than reading large text files. Other useful features of HDF5 are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Compression options for datasets and attributes, reducing storage space and
    transfer time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization of I/O operations, enabling multiple threads or processes to
    access the file simultaneously. This can greatly increase throughput and reduce
    processing time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we saw how to load different file formats with data into C++
    data structures provided by various C++ libraries. Especially we learned how to
    fill matrix and tensor objects that will be used in different ML algorithms. In
    the following section, we will see how to initialize the same data structures
    with values from regular C++ containers, which can be important when you implement
    your own data loader.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing matrix and tensor objects from C++ data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a variety of file formats used for datasets, and not all of them might
    be supported by libraries. For using data from unsupported formats, we might need
    to write custom parsers. After we read values to regular C++ containers, we usually
    need to convert them into object types used in the ML framework we use. As an
    example, let’s consider the case of reading matrix data from files into C++ objects.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the Eigen library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the `Eigen` library, we can wrap a C++ array into an `Eigen::Matrix`
    object with the `Eigen::Map` type. The wrapped object will behave as a standard
    `Eigen` matrix. We have to parametrize the `Eigen::Map` type with the type of
    matrix that has the required behavior. Also, when we create the `Eigen::Map` object,
    it takes as arguments a pointer to the C++ array and matrix dimensions, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Working with the Blaze library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Blaze` library has special classes that can be used to create wrappers
    for C++ arrays. To wrap a C++ container with objects of these classes, we have
    to pass a pointer to the data and corresponding dimensions as arguments, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Notice that additional template parameters were used to specify memory layout,
    alignment, and padding.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the Dlib library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library has the `Dlib::mat()` function for wrapping C++ containers
    into the `Dlib` matrix object. It also takes a pointer to the data and matrix
    dimensions as arguments, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The `Dlib::mat` function has other overloads that can take other types of containers
    to create a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the ArrayFire library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ArrayFire` library has a single technique to initialize the array object
    with an external memory pointer. It can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we initialize the `2x3` matrix with the data from the `C` array
    object. We used the `array` type constructor. The first two arguments are matrix
    row and column numbers, and the last one is the pointer to the data. We can initialize
    the `array` type object with the CUDA pointer in the same, but the fourth argument
    should be the `afDevice` specification.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the mlpack library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `mlpack` framework uses the `Armadillo` library for linear algebra objects.
    So, to wrap a C++ container into the `arma::mat` object, we can use the corresponding
    constructor that takes a pointer to the data and matrix dimensions, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: If the fourth parameter named `copy_aux_mem` is set to `false`, the data will
    be not copied into the matrix’s internal buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that all of these functions only make a wrapper for the original C++
    array where the data is stored and don’t copy the values into a new location.
    If we want to copy values from a C++ array to a `matrix` object, we usually need
    to call a `clone()` method or an analog of it for the wrapper object.
  prefs: []
  type: TYPE_NORMAL
- en: After we have a matrix object for an ML framework we use, we can initialize
    other specialized objects for training ML algorithms. Examples of such abstractions
    are the `fl::TensorDataset` class in the `Flashlight` library or the `torch::data::Dataset`
    class in the `libtorch` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we learned how to initialize matrix and tensor objects with
    regular C++ containers and pointers. The following section will move to another
    important topic: manipulation images.'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating images with the OpenCV and Dlib libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many ML algorithms are related to **computer vision** (**CV**) problems. Examples
    of such tasks are object detection in images, segmentation, image classification,
    and others. To be able to deal with such tasks, we need instruments for working
    with images. We usually need routines to load images to computer memory, as well
    as routines for image processing. For example, the standard operation is image
    scaling, because many ML algorithms are trained only on images of a specific size.
    This limitation follows from the algorithm structure or is a hardware requirement.
    For example, we cannot load large images to the **graphics processing unit** (**GPU**)
    memory because of its limited size.
  prefs: []
  type: TYPE_NORMAL
- en: Also, hardware requirements can lead to a limited range of numeric types that
    our hardware supports, so we will need to change the initial image representation
    to one that our hardware can efficiently process. Also, ML algorithms usually
    assume a predefined layout of image channels, which can be different from the
    layout in the original image file.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of image-processing task is the creation of training datasets.
    In many cases, we have a limited number of available images for a specific task.
    However, to make a machine algorithm train well, we usually need more training
    images. So, the typical approach is to augment existing images. Augmentation can
    be done with operations such as random scaling, cropping parts of images, rotations,
    and other operations that can be used to make different images from the existing
    set.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we show how to use two of the most popular libraries for image
    processing for C++. `OpenCV` is a framework for solving CV problems that includes
    many ready-to-use implementations of CV algorithms. Also, it has many functions
    for image processing. `Dlib` is a CV and ML framework with a large number of implemented
    algorithms, as well as a rich set of image-processing routines.
  prefs: []
  type: TYPE_NORMAL
- en: Using OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the `OpenCV` library, an image is treated as a multidimensional matrix of
    values. There is a special `cv::Mat` type for this purpose. There are two base
    functions: the `cv::imread()` function loads the image, and the `cv::imwrite()`
    function writes the image to a file, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Also, there are functions to manage images located in a memory buffer. The `cv::imdecode()`
    function loads an image from the memory buffer, and the `cv::imencode()` function
    writes an image to the memory buffer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling operations in the `OpenCV` library can be done with the `cv::resize()`
    function. This function takes an input image, an output image, the output image
    size or scale factors, and an interpolation type as arguments. The interpolation
    type governs how the output image will look after the scaling. General recommendations
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `cv::INTER_AREA` for shrinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `cv::INTER_CUBIC` (slow) or `cv::INTER_LINEAR` for zooming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `cv::INTER_LINEAR` for all resizing purposes because it is fast
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main difference between linear and cubic scaling lies in their approach
    to scaling pixels. Linear scaling preserves the aspect ratio and is simpler, while
    cubic scaling attempts to maintain details and transitions. The choice between
    the two depends on the specific requirements of your project and the desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code sample shows how to scale an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'There is no special function for image cropping in the `OpenCV` library, but
    the `cv::Mat` type overrides the `operator()` method, which takes a cropping rectangle
    as an argument and returns a new `cv::Mat` object with part of the image surrounded
    by the specified rectangle. Also, note that this object will share the same memory
    with the original image, so its modification will change the original image too.
    To make a deep copy of the `cv::Mat` object, we need to use the `clone()` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, we need to move or rotate an image. The `OpenCV` library supports
    translation and rotation operations for images through affine transformations.
    We have to manually—or with helper functions—create a matrix of 2D affine transformations
    and then apply it to our image. For the move (the translation), we can create
    such a matrix manually and then apply it to an image with the `cv::wrapAffine()`
    function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We can create a rotation matrix with the `cv::getRotationMatrix2D()` function.
    This takes a point of origin and the rotation angle in degrees, as illustrated
    in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful operation is extending an image size without scaling but with
    added borders. There is the `cv::copyMakeBorder()` function in the `OpenCV` library
    for this purpose. This function has different options on how to create borders.
    It takes an input image, an output image, border sizes for the top, the bottom,
    the left, and the right sides, the type of the border, and the border color. Border
    types can be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BORDER_CONSTANT`:Make function fill borders with a single color'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BORDER_REPLICATE`: Make function fill borders with copies of the last pixel
    values on each side (for example, *aaaaaa|abcdefgh|hhhhhhh*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BORDER_REFLECT`: Make function fill borders with copies of opposite pixel
    values on each side (for example, *fedcba|abcdefgh|hgfedcb*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BORDER_WRAP`: Make function fill borders by simulating the image duplication
    (for example, *cdefgh|abcdefgh|abcdefg*)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following example shows how to use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'When we are using this function, we should take care of the origin of the source
    image. The `OpenCV` documentation says: “*If the source image is a part of a bigger
    image, the function will try to use the pixels outside of the ROI (short for region
    of interest) to form a border. To disable this feature and always do extrapolation,
    as if the source image was not a part of another image, use border* *type* `BORDER_ISOLATED`.”'
  prefs: []
  type: TYPE_NORMAL
- en: The function described previously is very helpful when we need to adapt training
    images of different sizes to the one standard image size used in some ML algorithms
    because, with this function, we do not distort target image content.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is the `cv::cvtColor()` function to convert different color spaces in
    the `OpenCV` library. The function takes an input image, an output image, and
    a conversion scheme type. For example, in the following code sample, we convert
    the **red, green, and blue** (**RGB**) color space to a grayscale one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This can be very handy in certain scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Using Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Dlib` is another popular library for image processing. This library has different
    functions and classes for math routines and image processing. The library documentation
    recommends using the `Dlib::array2d` type for images. The `Dlib::array2d` type
    is a template type that has to be parametrized with a pixel type. Pixel types
    in the `Dlib` library are defined with pixel-type traits. There are the following
    predefined pixel types: `rgb_pixel`, `bgr_pixel`, `rgb_alpha_pixel`, `hsi_pixel`,
    `lab_pixel`, and any scalar type can be used for grayscaled pixels’ representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `load_image()` function to load an image from disk, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'For a scaling operation, there is the `Dlib::resize_image()` function. This
    function has two different overloads. One takes a single scale factor and a reference
    to an image object. The second one takes an input image, an output image, the
    desired size, and an interpolation type. To specify the interpolation type in
    the `Dlib` library, we should call special functions: the `interpolate_nearest_neighbor()`,
    the `interpolate_quadratic()`, and the `interpolate_bilinear()` functions. The
    criteria for choosing one of them are the same as for the ones that we discussed
    in the *Using OpenCV* section. Notice that the output image for the `resize_image()`
    function should be already preallocated, as illustrated in the following code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To crop an image with `Dlib`, we can use the `Dlib::extract_image_chips()`
    function. This function takes an original image, rectangle-defined bounds, and
    an output image. Also, there are overloads of this function that take an array
    of rectangle bounds and an array of output images, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Dlib` library supports image transformation operations through affine
    transformations. There is the `Dlib::transform_image()` function, which takes
    an input image, an output image, and an affine transformation object. An example
    of the transformation object could be an instance of the `Dlib::point_transform_affine`
    class, which defines the affine transformation with a rotation matrix and a translation
    vector. Also, the `Dlib::transform_image()` function can take an interpolation
    type as the last parameter, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'In case we only need to do a rotation, `Dlib` has the `Dlib::rotate_image()`
    function. The `Dlib::rotate_image()` function takes an input image, an output
    image, a rotation angle in degrees, and an interpolation type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'There is no complete analog of a function for adding borders to images in the
    `Dlib` library. There are two functions: `Dlib::assign_border_pixels()` and `Dlib::zero_border_pixels()`
    for filling image borders with specified values. Before using these routines,
    we should resize the image and place the content in the right position. The new
    image size should include the borders’ widths. We can use the `Dlib::transform_image()`
    function to move the image content into the right place. The following code sample
    shows how to add borders to an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'For color-space conversions, there exists the `Dlib::assign_image()` function
    in the `Dlib` library. This function uses color-type information from pixel-type
    traits we used for the image definition. So, to convert an image to another color
    space, we should define a new image with the desired type of pixels and pass it
    to this function. The following example shows how to convert the RGB image to
    a **blue, green, red** (**BGR**) one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To make a grayscale image, we can define an image with the `unsigned char`
    pixel type, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we learned how to load and preprocess images with the `OpenCV`
    and `Dlib` libraries. The next important step is to convert images into matrix
    or tensor structures to be able to use them in ML algorithms; it will be described
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming images into matrix or tensor objects of various libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In most cases, images are represented in computer memory in an interleaved
    format, which means that pixel values are placed one by one in linear order. Each
    pixel value consists of several numbers representing a color. For example, for
    the RGB format, there will be three values placed together. So, in the memory,
    we will see the following layout for a 4x4 image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'For image-processing libraries, such a value layout is not a problem, but many
    ML algorithms require different ordering. For example, it’s a common approach
    for **neural networks** (**NNs**) to take image channels separately ordered, one
    by one. The following example shows how such a layout is usually placed in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: So, often, we need to deinterleave image representation before passing it to
    some ML algorithm. It means that we need to extract color channels into separate
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we usually need to convert a color’s value data type too. For example,
    `OpenCV` library users often use floating-point formats, which allows them to
    preserve more color information in image transformations and processing routines.
    The opposite case is when we use a 256-bit type for color-channel information,
    but then we need to convert it to a floating-point type. So, in many cases, we
    need to convert the underlying data type to another one more suitable for our
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: Deinterleaving in OpenCV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, when we load an image with the `OpenCV` library, it loads the image
    in the BGR format and with `char` as the underlying data type. So, we need to
    convert it to the RGB format, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can convert the underlying data type to the `float` type, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, to deinterleave channels, we need to split them with the `cv::split()`
    function, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can place channels back to the `cv::Mat` object in the order we need
    with the `cv::vconcat()` function, which concatenates matrices vertically, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: There is a useful method in the `cv::Mat` type named `isContinuous` that allows
    us to check if the matrix’s data is placed in memory with a single contiguous
    block. If that is `true`, we can copy this block of memory or pass it to routines
    that work with plain C arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Deinterleaving in Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library uses the `unsigned char` type for pixel color representation,
    and we can use floating-point types only for grayscaled images. The `Dlib` library
    stores pixels in row-major order with interleaved channels, and data is placed
    in memory continuously with a single block. There are no special functions in
    the `Dlib` library to manage image channels, so we cannot deinterleave them or
    mix them. However, we can use raw pixel data to manage color values manually.
    Two functions in the `Dlib` library can help us: the `image_data()` function to
    access raw pixel data, and the `width_step()` function to get the padding value.'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach to deinterleave the `Dlib` image object is
    using a loop over all pixels. In such a loop, we can split each pixel value into
    separate colors.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, we define containers for each of the channels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we read color values for each pixel with two nested loops over image
    rows and columns, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The result is three containers with color-channel values, which we can use separately.
    They are suitable to initialize grayscaled images for use in image-processing
    routines. Alternatively, we can use them to initialize a matrix-type object that
    we can process with linear algebra routines.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to load and prepare images for use in linear algebra abstractions
    and ML algorithms. In the next section, we will learn general methods to prepare
    data for use in ML algorithms. Such methods will help us make learning procedures
    more stable and converge faster.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data normalization is a crucial preprocessing step in ML. In general, data normalization
    is a process that transforms multiscale data to the same scale. Feature values
    in a dataset can have very different scales—for example, the height can be given
    in centimeters with small values, but the income can have large-value amounts.
    This fact has a significant impact on many ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if some feature values differ from values of other features several
    times, then this feature will dominate over others in classification algorithms
    based on the Euclidean distance. Some algorithms have a strong requirement for
    normalization of input data; an example of such an algorithm is the **Support
    Vector Machine** (**SVM**) algorithm. NNs also usually require normalized input
    data. Also, data normalization has an impact on optimization algorithms. For example,
    optimizers based on the **gradient descent** (**GD**) approach can converge much
    quicker if data has the same scale.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods of normalization, but from our point of view, the
    most popular are the standardization, the min-max, and the mean normalization
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Standardization** is a process of making data have a zero mean and a standard
    deviation equal to 1\. The formula for standardized vector is ![](img/B19849_Formula_01.png),
    where ![](img/B19849_Formula_02.png) is an original vector, ![](img/B19849_Formula_03.png)
    is an average value of ![](img/B19849_Formula_04.png) calculated with the formula
    ![](img/B19849_Formula_05.png), and ![](img/B19849_Formula_06.png) is the standard
    deviation of ![](img/B19849_Formula_07.png) calculated with the formula ![](img/B19849_Formula_08.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '`[0, 1]`. We can do rescaling with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_091.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Min-max scaling is useful when there are significant differences in the scale
    of different features in your dataset. It helps to make the features comparable,
    which is important for many ML models.
  prefs: []
  type: TYPE_NORMAL
- en: '`[-1, 1]` so that its mean becomes zero. We can use the following formula to
    do mean normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B19849_Formula_101.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This transformation helps to make the data more easily interpretable and improves
    the performance of some ML algorithms by reducing the impact of outliers and ensuring
    that all features are on a similar scale. Consider how we can implement these
    normalization techniques and which ML framework functions can be used to calculate
    them.
  prefs: []
  type: TYPE_NORMAL
- en: We assume that each row of this matrix ![](img/B19849_Formula_11.png) is one
    training sample, and the value in each column is the value of one feature of the
    current sample.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing with Eigen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are no functions for data normalization in the `Eigen` library. However,
    we can implement them according to the provided formulas.
  prefs: []
  type: TYPE_NORMAL
- en: 'For standardization, we first have to calculate the standard deviation, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Notice that some reduction functions in the `Eigen` library work only with array
    representation; examples are the `sum()` and the `sqrt()` functions. We have also
    calculated the mean for each feature—we used the `x.colwise().mean()` function
    combination, which returns a vector of `mean`. We can use the same approach for
    other feature statistics’ calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having the standard deviation value, the rest of the formula for standardization
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Implementation of `min-max` normalization is very straightforward and does
    not require intermediate values, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'We implement the mean normalization in the same way, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we implement formulas in a vectorized way without loops; this approach
    is more computationally efficient because it can be compiled for execution on
    a GPU or the **central processing unit’s** (**CPU’s**) **Single Instruction Multiple
    Data** (**SIMD**) instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing with mlpack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different classes for feature scaling in the `mlpack` library. The
    most interesting for us are `data::data::MinMaxScaler`, which implements min-max
    normalization (or rescaling), and `mlpack::data::StandardScaler`, which implements
    data standardization. We can reuse objects of those classes for scaling different
    data with the same learned statistics. It can be useful in cases when we train
    an ML algorithm on one data format with applied rescaling, and then we use the
    algorithm for predictions on new data. To make this algorithm work as we want,
    we have to rescale new data in the same way as we did in the training process,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: To learn statistics values, we use the `Fit()` method, and for feature modification,
    we use the `Transform()` method of the `MinMaxScaler` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `StandardScaler` class can be used in the same manner, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'To print the matrix object in the `mlpack` library, the standard streaming
    operators can be used as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Also, to revert the applied scaling, these classes have the `InverseTransform`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing with Dlib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Dlib` library provides functionality for feature standardization with
    the `Dlib::vector_normalizer` class. There is one limitation to using this class—we
    cannot use it with one big matrix containing all training samples. Alternatively,
    we should represent each sample with a separate vector object and put them into
    the C++ `std::vector` container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the object of this class can be reused, but it should be trained
    first. The train method implementation can look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `Dlib::mat()` function has different overloads for matrix creation
    from different sources. Also, we use the `reciprocal()` function that makes the
    ![](img/B19849_Formula_121.png) matrix if *m* is the input matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Printing matrices for debugging purposes in the `Dlib` library can be done
    with the simple streaming operator, as illustrated in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the `Dlib` library provides a rich interface for data preprocessing
    that can be easily used.
  prefs: []
  type: TYPE_NORMAL
- en: Normalizing with Flashlight
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `Flashlight` library doesn’t have particular classes to perform feature
    scaling. But it has functions to calculate basic statistics easily, so we can
    implement feature scaling algorithms as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: The `fl::amin` and `fl::amax` functions find the minimum and maximum values.
    The `fl::mean` and `fl::std` functions calculate the mean and standard deviation
    correspondingly. All these functions do their calculation along a specified dimension
    that comes as the second parameter. It means that we scale each `x` feature in
    thedataset separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can print a `fl::Tensor` object with the standard C++ streaming operator,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We saw that despite the `FlashLight` library not providing special classes for
    data preprocessing, we can build them with linear algebra routines.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we considered how to load data from CSV, JSON, and HDF5 formats.
    CSV is easy to read and write, making it suitable for small to medium-sized datasets.
    CSV files are often used for tabular data, such as customer information, sales
    records, or financial transactions. JSON is a lightweight data interchange format
    that is human-readable and easy to parse. It is commonly used for representing
    structured data, including objects, arrays, and key-value pairs. In ML, JSON can
    be used to store data for training models, such as feature vectors, labels, and
    metadata. HDF5 is a high-performance file format designed for scientific data
    storage and analysis. It supports large datasets with complex structures, allowing
    for efficient storage of multidimensional arrays and tables. HDF5 files are commonly
    used in applications where large amounts of data need to be stored and accessed
    efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how to convert the loaded data into objects suitable for use in different
    ML frameworks. We used the libraries’ APIs to convert raw C++ arrays into matrices
    and higher-level dataset objects for ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at how to load and process images with the `OpenCV` and `Dlib` libraries.
    These libraries offer a wide range of functions and algorithms that can be used
    in various applications for CV. The libraries can be used for basic image preprocessing,
    as well as for more complicated systems that use ML for solving industry-important
    tasks such as face detection and recognition, which can be used to build security
    systems, and for access control or facial authentication. Object detection can
    be used for tasks such as counting objects in an image, detecting defects in products,
    identifying specific objects, or tracking their movement. This is useful in industrial
    automation, surveillance systems to identify suspicious activities, and autonomous
    vehicles. Image segmentation allows users to extract specific parts of an image
    for further analysis. This is essential for diagnosing diseases in medical imaging
    analysis. Motion tracking of objects over time is also used for sports analytics,
    traffic monitoring, and surveillance.
  prefs: []
  type: TYPE_NORMAL
- en: We became familiar with the data normalization process, which is very important
    for many ML algorithms. Also, we saw which normalization techniques are available
    in ML libraries, and we implemented some normalization approaches with linear
    algebra functions from the `Eigen` library.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will see how to measure a model’s performance on
    different types of data. We will look at special techniques that help us to understand
    how the model describes the training dataset well and how it performs on new data.
    Also, we will learn the different types of parameters ML models depend on and
    see how to select the best combination of them to improve the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The HDF5® library and file format: [https://www.hdfgroup.org/solutions/hdf5/](https://www.hdfgroup.org/solutions/hdf5/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GitHub link for Fast-CPPCSV Parser: [https://github.com/ben-strasser/Fast-CPP-CSV-Parser](https://github.com/ben-strasser/Fast-CPP-CSV-Parser)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OpenCV`: [https://opencv.org/](https://opencv.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dlib` C++ library: [http://Dlib.net/](http://Dlib.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Flashlight` documentation: [https://fl.readthedocs.io/en/latest/index.html](https://fl.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nlohmann-json` documentation: [https://json.nlohmann.me/](https://json.nlohmann.me/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlpack` documentation: [https://mlpack.org/doc/index.html](https://mlpack.org/doc/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Hybrid Approach for Sentiment Analysis Applied to Paper Reviews* dataset:
    [https://archive.ics.uci.edu/static/public/410/paper+reviews.zip](https://archive.ics.uci.edu/static/public/410/paper+reviews.zip)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
