<html><head></head><body>
<div id="page" style="height:0pt"/><div class="book" title="Chapter&#xA0;6.&#xA0;Where's Wally? Object Detection"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Where's Wally? Object Detection</h1></div></div></div><p class="calibre7">This chapter explains how to use the different options included in the OpenCV object detection module. With the sample code included, it is possible to use Cascade and Latent SVM detectors as well as create custom cascade detectors for a specific object detection application. Additionally, the new Scene Text Detector included in OpenCV 3 is explained in the chapter.</p></div>

<div class="book" title="Chapter&#xA0;6.&#xA0;Where's Wally? Object Detection">
<div class="book" title="Object detection"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec42" class="calibre1"/>Object detection</h1></div></div></div><p class="calibre7">Object detection<a id="id388" class="calibre1"/> deals with the process of locating instances of a certain class of real-world objects, such as faces, cars, pedestrians, and buildings in images or videos. Detection algorithms<a id="id389" class="calibre1"/> typically start by extracting features from two sets of images. One of these sets contains images from the desired object and the other one contains background images where the searched object is not present. Then, the detector is trained based on these features to recognize future instances of the object class.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note26" class="calibre1"/>Note</h3><p class="calibre7">Fingerprint recognition, now <a id="id390" class="calibre1"/>included in some laptops and smartphones, or face detection, seen in most digital cameras, are everyday examples of object detection applications.</p></div></div></div>
<div class="book" title="Detecting objects with OpenCV"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec43" class="calibre1"/>Detecting objects with OpenCV</h1></div></div></div><p class="calibre7">OpenCV<a id="id391" class="calibre1"/> has a<a id="id392" class="calibre1"/> number of object detection algorithms implemented in<a id="id393" class="calibre1"/> its <code class="email">objdetect</code> module. In this module, Cascade and Latent SVM detectors are implemented together with the new Scene Text Detector added in OpenCV 3. All of these algorithms are relatively efficient and obtain accurate results.</p></div>

<div id="page" style="height:0pt"/><div class="book" title="Cascades are beautiful"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec44" class="calibre1"/>Cascades are beautiful</h1></div></div></div><p class="calibre7">Most objects' detection problems, such as face/person detection or lesion detection in medicine, require searching for the object in many image patches. However, examining all image zones and computing the feature set for each zone are time-consuming tasks. Cascade detectors are widely used because of their high efficiency in doing this.</p><p class="calibre7">Cascade detectors<a id="id394" class="calibre1"/> consist of various boosting stages. The boosting algorithm selects the best feature set to create and combine a number of weak tree classifiers. Thus, boosting is not only a detector but also a feature selection method. Each stage is usually trained to detect nearly 100 percent of objects correctly and discard at least 50 percent of the background images. Therefore, background images, which represent a larger number of images, need less processing time as they are discarded at the early stages of the cascade. Moreover, the concluding cascade stages use more features than earlier stages, and even then only objects and difficult background images require more time to be evaluated.</p><p class="calibre7">Discrete AdaBoost<a id="id395" class="calibre1"/> (Adaptive Boosting), <a id="id396" class="calibre1"/>Real AdaBoost, <a id="id397" class="calibre1"/>Gentle AdaBoost, and LogitBoost<a id="id398" class="calibre1"/> are all implemented in OpenCV as boosting stages. On the other hand, it is possible to use Haar-like, <span class="strong"><strong class="calibre8">Local Binary Patterns</strong></span> (<span class="strong"><strong class="calibre8">LBP</strong></span>)<a id="id399" class="calibre1"/> and <span class="strong"><strong class="calibre8">Histograms of Oriented Gradients</strong></span> (<span class="strong"><strong class="calibre8">HOG</strong></span>)<a id="id400" class="calibre1"/> features together with the different boosting algorithms.</p><p class="calibre7">All these advantages and available techniques make cascades very useful for building practical detection applications.</p></div>

<div class="book" title="Cascades are beautiful">
<div class="book" title="Object detection using cascades"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec23" class="calibre1"/>Object detection using cascades</h2></div></div></div><p class="calibre7">OpenCV comes with <a id="id401" class="calibre1"/>several pretrained cascade detectors for the most <a id="id402" class="calibre1"/>common detection problems. They are located under the <code class="email">OPENCV_SOURCE\data</code> directory. The following is a list of some of them and their corresponding subdirectories:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Subdirectory <code class="email">haarcascades</code>:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">haarcascade_frontalface_default.xml</code></li><li class="listitem"><code class="email">haarcascade_eye.xml</code></li><li class="listitem"><code class="email">haarcascade_mcs_nose.xml</code></li><li class="listitem"><code class="email">haarcascade_mcs_mouth.xml</code></li><li class="listitem"><code class="email">haarcascade_upperbody.xml</code></li><li class="listitem"><code class="email">haarcascade_lowerbody.xml</code></li><li class="listitem"><code class="email">haarcascade_fullbody.xml</code></li></ul></div></li><li class="listitem">Subdirectory <code class="email">lbpcascades</code>:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">lbpcascade_frontalface.xml</code></li><li class="listitem"><code class="email">lbpcascade_profileface.xml</code></li><li class="listitem"><code class="email">lbpcascade_silverware.xml</code></li></ul></div></li><li class="listitem">Subdirectory <code class="email">hogcascades</code>:<div class="book"><ul class="itemizedlist1"><li class="listitem"><code class="email">hogcascade_pedestrians.xml</code></li></ul></div></li></ul></div><p class="calibre7">The <a id="id403" class="calibre1"/>following <code class="email">pedestrianDetection</code> example serves to illustrate<a id="id404" class="calibre1"/> how to use a cascade detector and localize pedestrians in a video file with OpenCV:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/core/core.hpp"
<span class="strong"><strong class="calibre8">#include "opencv2/objdetect/objdetect.hpp"</strong></span>
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

int main(int argc, char *argv[]){
    <span class="strong"><strong class="calibre8">CascadeClassifier cascade(argv[1]);</strong></span>
    if (<span class="strong"><strong class="calibre8">cascade.empty()</strong></span>)
        return -1;

    VideoCapture vid(argv[2]);
    if (!vid.isOpened()){
        cout&lt;&lt;"Error. The video cannot be opened."&lt;&lt;endl;
        return -1;
    }

    namedWindow("Pedestrian Detection");
    Mat frame;
    while(1) {
        if (!vid.read(frame))
            break;

        Mat frame_gray;
        if(frame.channels()&gt;1){
            cvtColor( frame, frame_gray, CV_BGR2GRAY );
            equalizeHist( frame_gray, frame_gray );
        }else{
            frame_gray = frame;
        }

        <span class="strong"><strong class="calibre8">vector&lt;Rect&gt; pedestrians;</strong></span>
        <span class="strong"><strong class="calibre8">cascade.detectMultiScale( frame_gray, pedestrians,</strong></span> <span class="strong"><strong class="calibre8">1.1, 2, 0, Size(30, 30),</strong></span> <span class="strong"><strong class="calibre8">Size(150, 150) );</strong></span>

        for( size_t i = 0; i &lt; pedestrians.size(); i++ ) {
            Point center( pedestrians[i].x + 
                          pedestrians[i].width*0.5, 
                          pedestrians[i].y + 
                          pedestrians[i].height*0.5 );
            ellipse( frame, center, Size( pedestrians[i].width*0.5,
                     pedestrians[i].height*0.5), 0, 0, 360, 
                     Scalar( 255, 0, 255 ), 4, 8, 0 );
        }

        imshow("Pedestrian Detection", frame);
        if(waitKey(100) &gt;= 0)
            break;
    }
    return 0;
}</pre></div><p class="calibre7">The code <a id="id405" class="calibre1"/>explanation<a id="id406" class="calibre1"/> is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">CascadeClassifier</code>: This class <a id="id407" class="calibre1"/>provides all the methods needed when working with cascades. An object from this class represents a trained cascade detector.</li><li class="listitem"><code class="email">constructor CascadeClassifier:: CascadeClassifier(const string&amp; filename)</code>: This class initializes the object instance and loads the information of the cascade detector stored in the system file indicated by the variable <code class="email">filename</code>.<div class="note" title="Note"><h3 class="title2"><a id="note27" class="calibre1"/>Note</h3><p class="calibre7">Note that the method <code class="email">bool CascadeClassifier::load(const string&amp; filename)</code> is actually called implicitly after the constructor.</p></div></li><li class="listitem"><code class="email">bool CascadeClassifier:: empty()</code>: This method checks if a cascade detector has been loaded.</li><li class="listitem"><code class="email">cvtColor</code> and <code class="email">equalizeHist</code>: These methods are required for image grayscale <a id="id408" class="calibre1"/>conversion and equalization. Since<a id="id409" class="calibre1"/> the cascade detector is trained with grayscale images and input images can be in different formats, it is necessary to convert them to the correct color space and equalize their histograms in order to obtain better results. This is done by the following code that uses the <code class="email">cvtColor</code> and <code class="email">equalizeHist</code> functions:<div class="informalexample"><pre class="programlisting">Mat frame_gray;
if(frame.channels()&gt;1){
    cvtColor( frame, frame_gray, CV_BGR2GRAY );
    equalizeHist( frame_gray, frame_gray );
}else{
    frame_gray = frame;
}</pre></div></li><li class="listitem"><code class="email">void CascadeClassifier::detectMultiScale(const Mat&amp; image, vector&lt;Rect&gt;&amp; objects, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())</code>: This method examines the image in the <code class="email">image</code> variable applying the loaded cascade and insert all detected objects in <code class="email">objects</code>. Detections are stored in a vector of rectangles of type <code class="email">Rect</code>. The parameters <code class="email">scaleFactor</code> and <code class="email">minNeighbors</code> indicates how much the image size is reduced at each image scale considered and the minimum number of neighbors that indicate a positive detection. Detections are bound by the minimum and maximum sizes, indicated by <code class="email">minSize</code> and <code class="email">maxSize</code>. Finally, the parameter <code class="email">flags</code> is not used when using cascades created with <code class="email">opencv_traincascade</code>.<div class="note" title="Note"><h3 class="title2"><a id="tip08" class="calibre1"/>Tip</h3><p class="calibre7">After obtaining the vector that stores the detected objects, it is easy to show them over the original images by reading the coordinates of each rectangle, represented by objects of the class <code class="email">Rect</code>, and drawing a polygon in the indicated zones.</p></div></li></ul></div><p class="calibre7">The following<a id="id410" class="calibre1"/> screenshot shows the result of applying the <code class="email">hogcascade_pedestrians.xml</code> pretrained HOG-based pedestrian detector over the frames of the <code class="email">768x576.avi</code> video, which is <a id="id411" class="calibre1"/>stored in the <code class="email">OPENCV_SCR/samples</code> folder.</p><div class="mediaobject"><img src="../images/00038.jpeg" alt="Object detection using cascades" class="calibre9"/><div class="caption"><p class="calibre13">Pedestrian detection using the OpenCV-trained HOG cascade detector</p></div></div><p class="calibre10"> </p><p class="calibre7">There are several projects and contributions to the OpenCV community that solve other detection-related problems that involve not only detecting the object but also distinguishing its state. One example of this type of detectors is the smile detector included in OpenCV since Version 2.4.4. The code can be found in the file <code class="email">OPENCV_SCR/samples/c/smiledetect.cpp</code>, and the XML that stores the cascade detector, <code class="email">haarcascade_smile.xml</code>, can be found in <code class="email">OPENCV_SCR/data/haarcascades</code>. This code first detects the frontal face using the pretrained <a id="id412" class="calibre1"/>cascade stored in <code class="email">haarcascade_frontalface_alt.xml</code> and then detects the smiling mouth pattern at the bottom part of the<a id="id413" class="calibre1"/> image. Finally, the intensity of the smile is calculated based on the number of neighbors detected.</p></div></div>

<div class="book" title="Cascades are beautiful">
<div class="book" title="Training your own cascade"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec24" class="calibre1"/>Training your own cascade</h2></div></div></div><p class="calibre7">Although OpenCV <a id="id414" class="calibre1"/>provides pretrained cascades, in some cases it is necessary to train a cascade detector to look for a specific object. For these cases, OpenCV comes with tools to help train a cascade, generating all the data needed during the training process and the final files with the detector information. These are usually stored in the <code class="email">OPENCV_BUILD\install\x64\mingw\bin</code> directory. Some of the applications are listed as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">opencv_haartraining</code>: This application<a id="id415" class="calibre1"/> is historically the first version of the application for creating cascades.</li><li class="listitem"><code class="email">opencv_traincascade</code>: This<a id="id416" class="calibre1"/> application is the latest version of the application for creating cascades.</li><li class="listitem"><code class="email">opencv_createsamples</code>: This <a id="id417" class="calibre1"/>application is used to create the <code class="email">.vec</code> file with the images that contain instances of the object. The file generated is accepted by both the preceding training executables.</li><li class="listitem"><code class="email">opencv_performance</code>: This <a id="id418" class="calibre1"/>application may be used to evaluate a cascade trained with the <code class="email">opencv_haartraining</code> tool. It uses a set of marked images to obtain information about the evaluation, for example, the false alarm or the detection rates.</li></ul></div><p class="calibre7">Since <code class="email">opencv_haartraining</code> is the older version of the program and it comes with fewer features than <code class="email">opencv_traincascade</code>, only the latter will be described here.</p><p class="calibre7">Here, the cascade training process is explained using the MIT CBCL face database. This database contains face and background images of 19 x 19 pixels arranged as shown in the following screenshot:</p><div class="mediaobject"><img src="../images/00039.jpeg" alt="Training your own cascade" class="calibre9"/><div class="caption"><p class="calibre13">Image file organization</p></div></div><p class="calibre10"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note28" class="calibre1"/>Note</h3><p class="calibre7">This section explains the training process on Windows. For Linux and Mac OS X, the process is similar but takes into account the specific aspects of the operating system. More information on training cascade detectors in Linux and Mac OS X can be found at <a class="calibre1" href="http://opencvuser.blogspot.co.uk/2011/08/creating-haar-cascade-classifier-aka.html">http://opencvuser.blogspot.co.uk/2011/08/creating-haar-cascade-classifier-aka.html</a> and <a class="calibre1" href="http://kaflurbaleen.blogspot.co.uk/2012/11/how-to-train-your-classifier-on-mac.html">http://kaflurbaleen.blogspot.co.uk/2012/11/how-to-train-your-classifier-on-mac.html</a> respectively.</p></div><p class="calibre7">The training process <a id="id419" class="calibre1"/>involves the following steps:</p><div class="book"><ol class="orderedlist"><li class="listitem" value="1"><span class="strong"><strong class="calibre8">Setting the current directory</strong></span>: In the <span class="strong"><strong class="calibre8">Command Prompt</strong></span> window, set the current directory to the directory in which training images are stored. For example, if the directory is <code class="email">C:\chapter6\images</code>, use the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;cd C:\chapter6\images</strong></span>
</pre></div></li><li class="listitem" value="2"><span class="strong"><strong class="calibre8">Creating the background images information text file</strong></span>: If background images are stored in <code class="email">C:\chapter6\images\train\non-face</code> and their format is <code class="email">.pgm</code>, it is possible to create the text file required by OpenCV using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;for %i in (C:\chapter6\images\train\non-face\*.pgm) do @echo %i &gt;&gt; train_non-face.txt</strong></span>
</pre></div><p class="calibre15">The following screenshot shows the contents of the background image information file. This file contains the path of the background images:</p><div class="mediaobject"><img src="../images/00040.jpeg" alt="Training your own cascade" class="calibre9"/><div class="caption"><p class="calibre13">Background images information file</p></div></div><p class="calibre14"> </p></li><li class="listitem" value="3"><span class="strong"><strong class="calibre8">Creating the object images file:</strong></span> This <a id="id420" class="calibre1"/>involves the following two steps:<div class="book"><ol class="orderedlist1"><li class="listitem" value="1">Creating the <code class="email">.dat</code> file with the object coordinates. In this particular database, object images only contain one instance of the object and it is located in the center of the image and scaled to occupy the entire image. Therefore, the number of objects per image is 1 and the object coordinates are <code class="email">0 0 19 19</code>, which are the initial point and the width and height of the rectangle that contains the object.<p class="calibre7">If object images are stored in <code class="email">C:\chapter6\images\train\face</code>, it is possible to use the following command to generate the file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;for %i in (C:\chapter6\images\train\face\*.pgm) do @echo %i 1 0 0 19 19 &gt;&gt; train_face.dat</strong></span>
</pre></div><p class="calibre7">The content of the <code class="email">.dat</code> file can be seen in the following screenshot:</p><div class="mediaobject"><img src="../images/00041.jpeg" alt="Training your own cascade" class="calibre9"/><div class="caption"><p class="calibre13">Object images file</p></div></div><p class="calibre10"> </p></li><li class="listitem" value="2">After <a id="id421" class="calibre1"/>creating the <code class="email">.dat</code> file with the object coordinates, it is necessary to create the <code class="email">.vec</code> file that is needed by OpenCV. This step can be performed using the <code class="email">opencv_createsamples</code> program with the arguments <code class="email">–info</code> (<code class="email">.dat</code> file); <code class="email">-vec</code> (<code class="email">.vec</code> output file name); <code class="email">-num</code> (number of images); <code class="email">-w</code> and <code class="email">–h</code> (output image width and height); and <code class="email">–maxxangle</code>, <code class="email">-maxyangle</code>, and <code class="email">-maxzangle</code> (image rotation angles). To see more options, execute <code class="email">opencv_createsamples</code> without arguments. In this case, the command used is:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;opencv_createsamples -info train_face.dat -vec train_face.vec -num 2429 -w 19 -h 19 -maxxangle 0 -maxyangle 0 -maxzangle 0</strong></span>
</pre></div><div class="note" title="Note"><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre7">OpenCV includes a sample <code class="email">.vec</code> file with facial images of size 24 x 24 pixels.</p></div></li></ol><div class="calibre16"/></div></li><li class="listitem" value="4"><span class="strong"><strong class="calibre8">Training the cascade</strong></span>: Finally, use the <code class="email">opencv_traincascade</code> executable and train the cascade detector. The command used in this case is:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;opencv_traincascade -data C:\chapter6\trainedCascade -vec train_face.vec -bg train_non-face.txt -numPos 242 -numNeg 454 -numStages 10 -w 19 -h 19</strong></span>
</pre></div><p class="calibre15">The arguments indicate the output directory (<code class="email">-data</code>), the <code class="email">.vec</code> file (<code class="email">-vec</code>), the background information file (<code class="email">-bg</code>), the number of positive and negative images to train each stage (<code class="email">-numPos</code> and <code class="email">–numNeg</code>), the maximum number of stages (<code class="email">-numStages</code>), and the width and height of the images (<code class="email">-w</code> and <code class="email">–h</code>).</p><p class="calibre15">The output of<a id="id422" class="calibre1"/> the training process is:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">PARAMETERS:</strong></span>
<span class="strong"><strong class="calibre8">cascadeDirName: C:\chapter6\trainedCascade</strong></span>
<span class="strong"><strong class="calibre8">vecFileName: train_face.vec</strong></span>
<span class="strong"><strong class="calibre8">bgFileName: train_non-face.txt</strong></span>
<span class="strong"><strong class="calibre8">numPos: 242</strong></span>
<span class="strong"><strong class="calibre8">numNeg: 454</strong></span>
<span class="strong"><strong class="calibre8">numStages: 10</strong></span>
<span class="strong"><strong class="calibre8">precalcValBufSize[Mb] : 256</strong></span>
<span class="strong"><strong class="calibre8">precalcIdxBufSize[Mb] : 256</strong></span>
<span class="strong"><strong class="calibre8">stageType: BOOST</strong></span>
<span class="strong"><strong class="calibre8">featureType: HAAR</strong></span>
<span class="strong"><strong class="calibre8">sampleWidth: 19</strong></span>
<span class="strong"><strong class="calibre8">sampleHeight: 19</strong></span>
<span class="strong"><strong class="calibre8">boostType: GAB</strong></span>
<span class="strong"><strong class="calibre8">minHitRate: 0.995</strong></span>
<span class="strong"><strong class="calibre8">maxFalseAlarmRate: 0.5</strong></span>
<span class="strong"><strong class="calibre8">weightTrimRate: 0.95</strong></span>
<span class="strong"><strong class="calibre8">maxDepth: 1</strong></span>
<span class="strong"><strong class="calibre8">maxWeakCount: 100</strong></span>
<span class="strong"><strong class="calibre8">mode: BASIC</strong></span>
<span class="strong"><strong class="calibre8">===== TRAINING 0-stage =====</strong></span>
<span class="strong"><strong class="calibre8">&lt;BEGIN</strong></span>
<span class="strong"><strong class="calibre8">POS count : consumed   242 : 242</strong></span>
<span class="strong"><strong class="calibre8">NEG count : acceptanceRatio    454 : 1</strong></span>
<span class="strong"><strong class="calibre8">Precalculation time: 4.524</strong></span>
<span class="strong"><strong class="calibre8">+----+---------+---------+</strong></span>
<span class="strong"><strong class="calibre8">|  N |    HR   |    FA   |</strong></span>
<span class="strong"><strong class="calibre8">+----+---------+---------+</strong></span>
<span class="strong"><strong class="calibre8">|   1|        1|        1|</strong></span>
<span class="strong"><strong class="calibre8">+----+---------+---------+</strong></span>
<span class="strong"><strong class="calibre8">|   2|        1|        1|</strong></span>
<span class="strong"><strong class="calibre8">+----+---------+---------+</strong></span>
<span class="strong"><strong class="calibre8">|   3| 0.995868| 0.314978|</strong></span>
<span class="strong"><strong class="calibre8">+----+---------+---------+</strong></span>
<span class="strong"><strong class="calibre8">END&gt;</strong></span>
<span class="strong"><strong class="calibre8">Training until now has taken 0 days 0 hours 0 minutes 9 seconds.</strong></span>
<span class="strong"><strong class="calibre8">. . . Stages 1, 2, 3, and 4 . . .</strong></span>
<span class="strong"><strong class="calibre8">===== TRAINING 5-stage =====</strong></span>
<span class="strong"><strong class="calibre8">&lt;BEGIN</strong></span>
<span class="strong"><strong class="calibre8">POS count : consumed   242 : 247</strong></span>
<span class="strong"><strong class="calibre8">NEG count : acceptanceRatio    454 : 0.000220059</strong></span>
<span class="strong"><strong class="calibre8">Required leaf false alarm rate achieved. Branch training terminated.</strong></span>
</pre></div></li></ol><div class="calibre16"/></div><p class="calibre7">Finally, the XML files<a id="id423" class="calibre1"/> of the cascade are stored in the output directory. These files are <code class="email">cascade.xml</code>, <code class="email">params.xml</code>, and a set of <code class="email">stageX.xml</code> files where <code class="email">X</code> is the stage number.</p></div></div>
<div class="book" title="Latent SVM"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec45" class="calibre1"/>Latent SVM</h1></div></div></div><p class="calibre7">Latent SVM<a id="id424" class="calibre1"/> is a detector that uses HOG features and a star-structured, part-based model consisting of a root filter and a set of part filters to represent an object category. HOGs are feature descriptors that are obtained by counting the occurrences of gradient orientations in localized portions of an image. On the other hand, a variant of <span class="strong"><strong class="calibre8">support vector machines</strong></span> (<span class="strong"><strong class="calibre8">SVM</strong></span>) classifiers are used in this detector to train models using partially labeled data. The basic idea of an <a id="id425" class="calibre1"/>SVM is constructing a hyperplane or set of hyperplanes in high-dimensional space. These hyperplanes are obtained to have the largest distance to the nearest training data point (functional margin in order to achieve low generalization errors). Like cascade detectors, Latent SVM uses a sliding window with different initial positions and scales where the algorithm is applied in order to detect if there is an object inside.</p><p class="calibre7">One of the advantages of the OpenCV Latent SVM<a id="id426" class="calibre1"/> implementation is that it allows the detection of multiple object categories by combining several simple pretrained detectors within the same multiobject detector instance.</p><p class="calibre7">The following <code class="email">latentDetection</code> example illustrates how to use a Latent SVM detector for localizing <a id="id427" class="calibre1"/>objects from a category in an image:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/core/core.hpp"
#include "opencv2/objdetect/objdetect.hpp"
#include "opencv2/highgui/highgui.hpp"
#include &lt;iostream&gt;

using namespace std;
using namespace cv;

int main(int argc, char* argv[]){
    String model = argv[1];
    <span class="strong"><strong class="calibre8">vector&lt;String&gt; models;</strong></span>
    <span class="strong"><strong class="calibre8">models.push_back( model );</strong></span>
    <span class="strong"><strong class="calibre8">vector&lt;String&gt; names;</strong></span>
    <span class="strong"><strong class="calibre8">names.push_back( "category" );</strong></span>
    <span class="strong"><strong class="calibre8">LatentSvmDetector detector( models , names);</strong></span>
    if( detector.empty() ) {
        cout &lt;&lt; "Model cannot be loaded" &lt;&lt; endl;
        return -1;
    }

    String img = argv[2];
    Mat image = imread( img );
    if( image.empty() ){
        cout &lt;&lt; "Image cannot be loaded" &lt;&lt; endl;
        return -1;
    }

    <span class="strong"><strong class="calibre8">vector&lt;LatentSvmDetector::ObjectDetection&gt; detections;</strong></span>
    <span class="strong"><strong class="calibre8">detector.detect( image, detections, 0.1, 1);</strong></span>
    for( size_t i = 0; i &lt; detections.size(); i++ ) {
        Point center( detections[i].rect.x + 
                      detections[i].rect.width*0.5, 
                      detections[i].rect.y + 
                      detections[i].rect.height*0.5 );
        ellipse( image, center, Size( detections[i].rect.width*0.5, 
                 detections[i].rect.height*0.5), 0, 0, 360, 
                 Scalar( 255, 0, 255 ), 4, 8, 0 );
    }
    imshow( "result", image );
    waitKey(0);
    return 0;
}</pre></div><p class="calibre7">The code<a id="id428" class="calibre1"/> explanation is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">LatentSvmDetector</code>: This class has an object that represents a Latent SVM detector composed of one or more pretrained detectors.</li><li class="listitem"><code class="email">constructor LatentSvmDetector::LatentSvmDetector(const vector&lt;String&gt;&amp; filenames, const vector&lt;string&gt;&amp; classNames=vector&lt;String&gt;())</code>: This class initializes the object instance and loads the information of the detectors stored in the system paths indicated by the vector <code class="email">filenames</code>. The second parameter, the vector <code class="email">classNames</code>, contains the category names. The method <code class="email">bool LatentSvmDetector::load(const vector&lt;string&gt;&amp; filenames, const vector&lt;string&gt;&amp; classNames=vector&lt;string&gt;())</code> is called implicitly after the constructor.</li><li class="listitem"><code class="email">void LatentSvmDetector::detect(const Mat&amp; image, vector&lt;ObjectDetection&gt;&amp; objectDetections, float overlapThreshold = 0.5f, int numThreads = -1)</code>: This method examines the image in the variable <code class="email">image</code> by applying the simple or combined detector on it and puts all detected objects in <code class="email">objectDetections</code>. All detections are stored in a vector of the <code class="email">ObjectDetection</code> struct. This structure has the following three variables:<div class="book"><ul class="itemizedlist1"><li class="listitem">The bounding box of the detection (<code class="email">rect</code>)</li><li class="listitem">The confidence level (<code class="email">score</code>)</li><li class="listitem">The category ID (<code class="email">classID</code>)</li></ul></div><p class="calibre15">The parameter <code class="email">overlapThreshold</code> is the threshold for the non-maximum suppression algorithm for eliminating overlapped detections. Finally, <code class="email">numThreads</code> is the number of threads used in the parallel version of the algorithm.</p></li></ul></div><p class="calibre7">The following screenshot shows a cat detected using the previous code and the files <code class="email">cat.xml</code> and <code class="email">cat.png</code>, and cars detected using <code class="email">car.xml</code> and <code class="email">cars.png</code>. These files are included in the OpenCV extra data that can be found in the official repository. Thus, it is possible to run the program using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong class="calibre8">&gt;latentDetection.exe xmlfile imagefile</strong></span>
</pre></div><p class="calibre7">In the previous <a id="id429" class="calibre1"/>command, <code class="email">xmlfile</code> is the Latent SVM detector and <code class="email">imagefile</code> is the image that has to be examined.</p><div class="informalexample" title="Note"><h3 class="title2"><a id="note29" class="calibre1"/>Note</h3><p class="calibre7">OpenCV extra data provides more samples and test files that can be used by users to create and test their own projects while saving time. It can be found at <a class="calibre1" href="https://github.com/Itseez/opencv_extra">https://github.com/Itseez/opencv_extra</a>.</p></div><p class="calibre7">In addition to the car and cat detectors, OpenCV provides pretrained detectors for the rest of the classes defined in <span class="strong"><em class="calibre12">The PASCAL Visual Object Classes Challenge 2007</em></span> (<a class="calibre1" href="http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007">http://pascallin.ecs.soton.ac.uk/challenges/VOC/voc2007</a>). These detectors are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">aeroplane.xml</code></li><li class="listitem"><code class="email">bicycle.xml</code></li><li class="listitem"><code class="email">bird.xml</code></li><li class="listitem"><code class="email">boat.xml</code></li><li class="listitem"><code class="email">bottle.xml</code></li><li class="listitem"><code class="email">bus.xml</code></li><li class="listitem"><code class="email">car.xml</code></li><li class="listitem"><code class="email">cat.xml</code></li><li class="listitem"><code class="email">chair.xml</code></li><li class="listitem"><code class="email">cow.xml</code></li><li class="listitem"><code class="email">diningtable.xml</code></li><li class="listitem"><code class="email">dog.xml</code></li><li class="listitem"><code class="email">horse.xml</code></li><li class="listitem"><code class="email">motorbike.xml</code></li><li class="listitem"><code class="email">person.xml</code></li><li class="listitem"><code class="email">pottedplant.xml</code></li><li class="listitem"><code class="email">sheep.xml</code></li><li class="listitem"><code class="email">sofa.xml</code></li><li class="listitem"><code class="email">train.xml</code></li><li class="listitem"><code class="email">tvmonitor.xml</code></li></ul></div><div class="mediaobject"><img src="../images/00042.jpeg" alt="Latent SVM" class="calibre9"/><div class="caption"><p class="calibre13">The detection of a cat and some cars using Latent SVM</p></div></div><p class="calibre10"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre7">The false positive rate can be adjusted by changing the value of the <code class="email">overlapThreshold</code> parameter.</p></div></div>
<div class="book" title="Scene text detection"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec46" class="calibre1"/>Scene text detection</h1></div></div></div><p class="calibre7">The scene text detection algorithm<a id="id430" class="calibre1"/> builds a component tree of an image by thresholding it step-by-step from 0 to 255. To enhance the results, this process is done for each color channel, intensity, and gradient magnitude images. After that, the connected components obtained from successive levels are hierarchically organized depending on their inclusion relationship as shown in the following diagram. This tree organization may contain a huge number of regions:</p><div class="mediaobject"><img src="../images/00043.jpeg" alt="Scene text detection" class="calibre9"/><div class="caption"><p class="calibre13">Tree organization example</p></div></div><p class="calibre10"> </p><p class="calibre7">Thus, the<a id="id431" class="calibre1"/> algorithm selects some regions following two stages. Firstly, area, perimeter, bounding box, and Euler number descriptors are computed for each region and used in order to estimate the class-condition probability. External regions with local maximum probabilities are selected if their values are above a global limit and the difference between their local maximum and minimum is also above a specified limit.</p><p class="calibre7">The second stage consists of classifying the external regions selected in the first stage into character and non-character classes using whole area ratio, convex hull ratio, and the number of outer boundary inflexion points as features.</p><p class="calibre7">Finally, the external regions selected are grouped to obtain words, lines, or paragraphs. This part of the algorithm uses a perceptual-organization-based clustering analysis.</p><p class="calibre7">The following <code class="email">textDetection</code> example illustrates how to use the Scene Text Detection algorithm and localize text in an image:</p><div class="informalexample"><pre class="programlisting">#include "opencv2/opencv.hpp"
#include "opencv2/objdetect.hpp"
#include "opencv2/highgui.hpp"
#include "opencv2/imgproc.hpp"

#include &lt;vector&gt;
#include &lt;iostream&gt;
#include &lt;iomanip&gt;

using namespace std;
using namespace cv;

int main(int argc, const char * argv[]){

    Mat src = imread(argv[1]);

    <span class="strong"><strong class="calibre8">vector&lt;Mat&gt; channels;</strong></span>
    <span class="strong"><strong class="calibre8">computeNMChannels(src, channels);</strong></span>

    //Negative images from RGB channels
    channels.push_back(255-channels[0]);
    channels.push_back(255-channels[1]); 
    channels.push_back(255-channels[2]);
    channels.push_back(255-channels[3]);
    for (int c = 0; c &lt; channels.size(); c++){
        stringstream ss;
        ss &lt;&lt; "Channel: " &lt;&lt; c;
        imshow(ss.str(),channels.at(c));
    }

    <span class="strong"><strong class="calibre8">Ptr&lt;ERFilter&gt; er_filter1 = createERFilterNM1(</strong></span>
                                   <span class="strong"><strong class="calibre8">loadClassifierNM1(argv[2]),</strong></span>
                                   <span class="strong"><strong class="calibre8">16, 0.00015f, 0.13f, 0.2f,</strong></span>
<span class="strong"><strong class="calibre8">true, 0.1f );</strong></span>
    <span class="strong"><strong class="calibre8">Ptr&lt;ERFilter&gt; er_filter2 = createERFilterNM2(</strong></span>
                                   <span class="strong"><strong class="calibre8">loadClassifierNM2(argv[3]),</strong></span>  <span class="strong"><strong class="calibre8">0.5 );</strong></span>

    <span class="strong"><strong class="calibre8">vector&lt;vector&lt;ERStat&gt; &gt; regions(channels.size());</strong></span>
    // Apply filters to each channel
    for (int c=0; c&lt;(int)channels.size(); c++){
        <span class="strong"><strong class="calibre8">er_filter1-&gt;run(channels[c], regions[c]);</strong></span>
        <span class="strong"><strong class="calibre8">er_filter2-&gt;run(channels[c], regions[c]);</strong></span>
    }
    for (int c=0; c&lt;(int)channels.size(); c++){
        Mat dst = Mat::zeros( channels[0].rows + 
                              2, channels[0].cols + 2, CV_8UC1 );
        // Show ERs
        for (int r=0; r&lt;(int)regions[c].size(); r++)
        {
            ERStat er = regions[c][r];
            if (er.parent != NULL){
                int newMaskVal = 255;
                int flags = 4 + (newMaskVal &lt;&lt; 8) + 
                                 FLOODFILL_FIXED_RANGE + 
                                 FLOODFILL_MASK_ONLY;
                floodFill( channels[c], dst, Point(er.pixel % 
                           channels[c].cols,er.pixel / 
                           channels[c].cols), Scalar(255), 0, 
                           Scalar(er.level), Scalar(0), flags);
            }
        }
        stringstream ss;
        ss &lt;&lt; "Regions/Channel: " &lt;&lt; c;
        imshow(ss.str(), dst);
    }

    <span class="strong"><strong class="calibre8">vector&lt;Rect&gt; groups;</strong></span>
    <span class="strong"><strong class="calibre8">erGrouping( channels, regions, argv[4], 0.5, groups );</strong></span>
    for (int i=(int)groups.size()-1; i&gt;=0; i--)
    {
        if (src.type() == CV_8UC3)
            rectangle( src,groups.at(i).tl(), groups.at(i).br(), 
                       Scalar( 0, 255, 255 ), 3, 8 );
        else
            rectangle( src,groups.at(i).tl(), groups.at(i).br(), 
                       Scalar( 255 ), 3, 8 );
    }
    imshow("grouping",src);

    waitKey(-1);
    er_filter1.release();
    er_filter2.release();
    regions.clear();
    groups.clear();
}</pre></div><p class="calibre7">The code <a id="id432" class="calibre1"/>explanation is as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="email">void computeNMChannels(InputArray _src, OutputArrayOfArrays _channels, int _mode=ERFILTER_NM_RGBLGrad)</code>: This function computes different channels from the image in <code class="email">_src</code> to be processed independently in order to obtain high localization recall. These channels are red (<code class="email">R</code>), green (<code class="email">G</code>), blue (<code class="email">B</code>), lightness (<code class="email">L</code>), and gradient magnitude (∇) by default (<code class="email">_mode=ERFILTER_NM_RGBLGrad</code>), it is intensity (I), hue (H), saturation (S), and gradient magnitude (∇) if <code class="email">_mode=ERFILTER_NM_IHSGrad</code>. Finally, the computed channels are saved in the <code class="email">_channels</code> parameter.</li><li class="listitem"><code class="email">Ptr&lt;ERFilter&gt; createERFilterNM1(const Ptr&lt;ERFilter::Callback&gt;&amp; cb, int thresholdDelta = 1, float minArea = 0.00025, float maxArea = 0.13, float minProbability = 0.4, bool nonMaxSuppression = true, float minProbabilityDiff = 0.1)</code>: This function creates an Extremal Region Filter for the classifier of the first stage defined by the algorithm. The first parameter loads the classifier by means of the function <code class="email">loadClassifierNM1(const std::string&amp; filename)</code>. The <code class="email">thresholdDelta</code> variable indicates the threshold step during the component tree obtaining process. The parameters <code class="email">minArea</code> and <code class="email">maxArea</code> establish the percentages of the image size between which external regions are retrieved. The value of the <code class="email">bool</code> parameter <code class="email">nonMaxSuppression</code> is <code class="email">true</code> when non-maximum suppression is applied over the branch probabilities, and <code class="email">false</code> otherwise. Finally, the <code class="email">minProbability</code> and <code class="email">minProbabilityDiff</code> parameters control the minimum probability value and the minimum probability difference between local maxima and minima values allowed for retrieving an external region.</li><li class="listitem"><code class="email">Ptr&lt;ERFilter&gt; createERFilterNM2(const Ptr&lt;ERFilter::Callback&gt;&amp; cb, float minProbability = 0.3)</code>: This function creates an External Region Filter for the classifier of the second stage defined by the algorithm. The first parameter loads the classifier by means of the function <code class="email">loadClassifierNM2(const std::string&amp; filename)</code>. The other parameter, <code class="email">minProbability</code>, is the minimum probability allowed for retrieved external regions.</li><li class="listitem"><code class="email">void ERFilter::run( InputArray image, std::vector&lt;ERStat&gt;&amp; regions)</code>: This <a id="id433" class="calibre1"/>method applies the cascade classifier loaded by the filter to obtain the external regions either in the first or the second level. The <code class="email">image</code> parameter is the channel that has to be examined and <code class="email">regions</code> is a vector with the output of the first stage and also the input/output of the second one.</li><li class="listitem"><code class="email">void erGrouping(InputArrayOfArrays src, std::vector&lt;std::vector&lt;ERStat&gt;&gt;&amp; regions, const std::string&amp; filename, float minProbability, std::vector&lt;Rect&gt;&amp; groups)</code>: This function groups the external regions obtained. It uses the extracted channels (<code class="email">src</code>), the obtained external regions by each channel (<code class="email">regions</code>), the path to the grouping classifier, and the minimum probability for accepting a group (<code class="email">minProbability</code>). Final groups, which are rectangles from <code class="email">Rect</code>, are stored in the vector <code class="email">groups</code>.</li></ul></div><p class="calibre7">The following group of<a id="id434" class="calibre1"/> screenshots shows the obtained image channels. These are red (<code class="email">R</code>), green (<code class="email">G</code>), blue (<code class="email">B</code>), intensity (<code class="email">I</code>), gradient magnitude (∇), inverted red (<code class="email">iR</code>), inverted green (<code class="email">iG</code>), inverted blue (<code class="email">iB</code>), and inverted intensity (<code class="email">iI</code>). In the first row, the <code class="email">R</code>, <code class="email">G</code>, and <code class="email">B</code> channels are shown. The second row shows the I, ∇, and iR channels. Finally, in the third row, the iG, iB, and iI channels are shown:</p><div class="mediaobject"><img src="../images/00044.jpeg" alt="Scene text detection" class="calibre9"/><div class="caption"><p class="calibre13">Extracted image channels</p></div></div><p class="calibre10"> </p><p class="calibre7">The following group of screenshots shows it is possible to see the external regions extracted from each channel. Channels <code class="email">R</code>, <code class="email">G</code>, <code class="email">B</code>, <code class="email">L</code>, and ∇ produce more accurate results. In the first row, external regions from the <code class="email">R</code>, <code class="email">G</code>, and <code class="email">B</code> channels are shown. The second row shows the external regions extracted from the I, ∇, and iR channels. Finally, in the third row, the <code class="email">iG</code>, <code class="email">iB</code>, and <code class="email">iI</code> <a id="id435" class="calibre1"/>channels are shown:</p><div class="mediaobject"><img src="../images/00045.jpeg" alt="Scene text detection" class="calibre9"/><div class="caption"><p class="calibre13">External regions obtained from each channel</p></div></div><p class="calibre10"> </p><p class="calibre7">Finally, the following screenshot shows the input image with the text areas grouped into lines and paragraphs:</p><div class="mediaobject"><img src="../images/00046.jpeg" alt="Scene text detection" class="calibre9"/><div class="caption"><p class="calibre13">Groups obtained</p></div></div><p class="calibre10"> </p><div class="informalexample" title="Note"><h3 class="title2"><a id="note30" class="calibre1"/>Note</h3><p class="calibre7">To reproduce these results or use the OpenCV Scene Text Detector, it is possible to use this code with the sample files provided by the library. The input image and classifier can be found in the <code class="email">OPENCV_SCR/samples/cpp</code> directory. The image used here is <code class="email">cenetext01.jpg</code>. The first and second level classifiers are <code class="email">trained_classifierNM1.xml</code> and <code class="email">trained_classifierNM2.xml</code>. Finally, the grouping classifier provided by OpenCV is <code class="email">trained_classifier_erGrouping.xml</code>.</p></div></div>
<div class="book" title="Summary"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec47" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">This chapter covers the OpenCV <code class="email">objdetect</code> module. It explains how to use and train the Cascade detectors as well as how to use Latent SVM detectors. Moreover, the new Scene Text Detector included in OpenCV 3 has been explained in this chapter.</p><p class="calibre7">Methods for detecting and tracking objects in motion are explained in the next chapter.</p></div>
<div class="book" title="What else?"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec48" class="calibre1"/>What else?</h1></div></div></div><p class="calibre7">Cascade detectors have been widely used in several applications such as face recognition and pedestrian detection because they are fast and provide good results. Soft cascades are a variant of the classic cascade detectors. This new type of cascades is implemented in OpenCV 3 in the <code class="email">softcascade</code> module. Soft cascade is trained with AdaBoost but the resulting detector is composed of only one stage. This stage has several weak classifiers that are evaluated in sequence. After evaluating each weak classifier, the result is compared with the corresponding threshold. Similar to the evaluation process carried out in multistage cascades, negative non-object instances are discarded as soon as possible.</p></div></body></html>