<html><head></head><body>
<div id="_idContainer069">
<h1 class="chapter-number" id="_idParaDest-151"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-152"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.2.1">Handling Imbalanced Data</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This chapter delves into the intriguing world of imbalanced data and how conformal prediction can be a game-changer in handling </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">such scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Imbalanced datasets are a common challenge in machine learning, often leading to biased predictions and underperforming models. </span><span class="koboSpan" id="kobo.5.2">This chapter will equip you with the knowledge and skills to tackle these </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">issues head-on.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will be introduced to imbalanced data and learn why it poses a significant challenge in machine learning applications. </span><span class="koboSpan" id="kobo.7.2">We will then explore various methods traditionally used to address imbalanced </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">data problems.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">The highlight of the chapter is the application of conformal prediction to imbalanced </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">data problems.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">This chapter will illustrate how conformal prediction can solve imbalanced data problems by covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Introducing </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">imbalanced data</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Why imbalanced data problems are complex </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">to solve</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Methods for solving </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">imbalanced data</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">How conformal prediction can be applied to help solve imbalanced </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">data problems</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.21.1">Join us on this enlightening journey as we unravel the complexities of imbalanced data and discover innovative solutions through </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">conformal prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">By the end of this chapter, you will have a solid understanding of how conformal prediction can be effectively applied to handle imbalanced data, thereby improving the performance and reliability of your machine </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">learning models.</span></span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.25.1">Introducing imbalanced data</span></h1>
<p><span class="koboSpan" id="kobo.26.1">In machine learning, we often come across datasets that need to be more balanced. </span><span class="koboSpan" id="kobo.26.2">But what does it mean for a dataset to </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">be imbalanced?</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">An imbalanced dataset is</span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.29.1"> one where the distribution of samples across the different classes is not uniform. </span><span class="koboSpan" id="kobo.29.2">In other words, one type has significantly more samples than the other(s). </span><span class="koboSpan" id="kobo.29.3">This is a common scenario in many real-world applications. </span><span class="koboSpan" id="kobo.29.4">For instance, in a dataset for fraud detection, the number of non-fraudulent transactions (majority class) is typically much higher than the number of fraudulent ones (</span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">minority class).</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">Imagine a medical dataset recording instances of a rare disease. </span><span class="koboSpan" id="kobo.31.2">Most patients will be disease-free, resulting in a large class of healthy records, while only a tiny fraction will be affected by the disease. </span><span class="koboSpan" id="kobo.31.3">This disproportion in the distribution of categories is what we call </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">imbalanced data.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">Imbalanced data can lead to a significant challenge in predictive modeling. </span><span class="koboSpan" id="kobo.33.2">By their very nature, machine learning algorithms are designed to minimize errors and maximize accuracy. </span><span class="koboSpan" id="kobo.33.3">When trained on imbalanced data, they tend to be biased toward the majority class, often at the expense of the minority class </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">prediction accuracy.</span></span></p>
<p><span class="koboSpan" id="kobo.35.1">In our medical example, a naive model might predict that no one has the disease, achieving a high accuracy due to the sheer number of healthy records but failing to identify the few crucial cases that do. </span><span class="koboSpan" id="kobo.35.2">Such models, misled by the imbalance, could have dire </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">real-world implications.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">The nature of imbalanced data is pervasive across industries. </span><span class="koboSpan" id="kobo.37.2">From fraud detection in finance, where fraudulent transactions are rare but crucial to detect, to natural disaster predictions in meteorology, where the event of interest (e.g., a tornado or earthquake) is infrequent but significant, imbalances pose challenges that professionals must be equipped </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">to handle.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Recognizing and understanding imbalanced data is the first step in effectively addressing their challenges. </span><span class="koboSpan" id="kobo.39.2">As we proceed, we’ll deep dive into why these problems are particularly tough to crack and explore methodologies to handle them, focusing on the potential of </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">conformal prediction.</span></span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.41.1">Why imbalanced data problems are complex to solve</span></h1>
<p><span class="koboSpan" id="kobo.42.1">Addressing imbalanced data is</span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.43.1"> no walk in the park, and here’s why. </span><span class="koboSpan" id="kobo.43.2">At the core of the challenge is the nature of conventional machine learning algorithms. </span><span class="koboSpan" id="kobo.43.3">These algorithms minimize overall error and are designed with the assumption of balanced class distributions. </span><span class="koboSpan" id="kobo.43.4">This becomes problematic when faced with imbalanced datasets, leading to a pronounced bias toward the </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">majority class.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">The gravity of this problem becomes evident when we realize that in many scenarios, it’s the minority class that carries more significance. </span><span class="koboSpan" id="kobo.45.2">Take fraud detection or medical diagnoses as cases in point. </span><span class="koboSpan" id="kobo.45.3">While fraudulent transactions or disease instances might be sparse, their correct identification is paramount. </span><span class="koboSpan" id="kobo.45.4">Yet, a model trained on skewed data might often lean toward predicting the majority class, achieving superficially high accuracy but failing its </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">core objective.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">To add to the challenge, conventional metrics, such as accuracy, are only sometimes our friends here. </span><span class="koboSpan" id="kobo.47.2">A dataset with just 2% fraudulent transactions can trick us into complacency: a naive model predicting every transaction as legitimate will boast a 98% accuracy, masking its utter failure in </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">detecting fraud.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">The maze of academic literature on this topic makes things even more difficult. </span><span class="koboSpan" id="kobo.49.2">With many methods and theories, determining which ones genuinely work is akin to finding a needle in a haystack. </span><span class="koboSpan" id="kobo.49.3">Methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">Synthetic Minority Oversampling Technique</span></strong><span class="koboSpan" id="kobo.51.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.52.1">SMOTE</span></strong><span class="koboSpan" id="kobo.53.1">), which is frequently discussed, require discerning analysis to gauge their </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">actual effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.55.1">A word of advice for those just starting in data science: approach the realm of imbalanced classification with a discerning eye. </span><span class="koboSpan" id="kobo.55.2">Not all that glitters is gold. </span><span class="koboSpan" id="kobo.55.3">While searching for a magic solution is tempting, sometimes it’s about reframing the problem. </span><span class="koboSpan" id="kobo.55.4">By shifting our perspective and focusing on more relevant metrics, we can find a way through the maze, making informed and </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">effective decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.57.1">We will now look into some common methods for dealing with </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">imbalanced data.</span></span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.59.1">Methods for solving imbalanced data</span></h1>
<p><span class="koboSpan" id="kobo.60.1">Where should we turn when confronted with the challenge of imbalanced class distribution? </span><span class="koboSpan" id="kobo.60.2">While a significant portion of resources in the field suggest using resampling methods, including </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.61.1">undersampling, oversampling, and techniques such as SMOTE, it’s crucial to note that these recommendations often sidestep foundational theory and </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">practical application.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">Before diving into solutions for imbalanced classes, it’s essential first to understand their underlying nature. </span><span class="koboSpan" id="kobo.63.2">The issue might be better approached in specific scenarios such as anomaly detection rather than in a traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">classification problem.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">In specific scenarios, the class imbalance isn’t static. </span><span class="koboSpan" id="kobo.65.2">It can evolve or may be influenced by the need for adequate labels. </span><span class="koboSpan" id="kobo.65.3">For instance, consider a system monitoring network traffic for potential security threats. </span><span class="koboSpan" id="kobo.65.4">Initially, threats might be rare, leading to a class imbalance. </span><span class="koboSpan" id="kobo.65.5">However, as the system matures and more potential hazards are identified and labeled, the imbalance might shift, reducing or reversing </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">the skew.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">Addressing such dynamic imbalances requires adaptive methods that can recalibrate as data characteristics change, ensuring the model remains effective throughout its </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">life cycle.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">When these challenges are absent, it’s prudent to shift focus to the evaluation metrics. </span><span class="koboSpan" id="kobo.69.2">We’ve previously examined metrics such as log loss and Brier loss, which are instrumental in assessing model calibration. </span><span class="koboSpan" id="kobo.69.3">Notably, employing resampling techniques with these metrics might adversely impact the </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">model’s calibration.</span></span></p>
<p><span class="koboSpan" id="kobo.71.1">One frequently proposed remedy for imbalanced data is to modify the dataset through various </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">resampling techniques.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">Resampling methods are techniques used to balance the distribution of classes in an imbalanced dataset. </span><span class="koboSpan" id="kobo.73.2">These methods can be broadly categorized into two </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">main types:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.75.1">Oversampling</span></strong><span class="koboSpan" id="kobo.76.1">: This involves increasing</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.77.1"> the number of </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.78.1">instances in the minority class. </span><span class="koboSpan" id="kobo.78.2">Methods include </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Random oversampling</span></strong><span class="koboSpan" id="kobo.81.1">: This involves </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.82.1">duplicating random records from the </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">minority class.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.84.1">SMOTE</span></strong><span class="koboSpan" id="kobo.85.1">: SMOTE creates synthetic samples for the</span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.86.1"> minority class in a feature space by following a specific algorithm. </span><span class="koboSpan" id="kobo.86.2">It starts by randomly selecting a minority class instance and finding its k-nearest minority </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.87.1">class neighbors. </span><span class="koboSpan" id="kobo.87.2">SMOTE randomly picks one from these neighbors and calculates the difference between its features and the selected instance’s features. </span><span class="koboSpan" id="kobo.87.3">It then multiplies this difference by a random number between 0 and 1, adding the result to the original instance’s features. </span><span class="koboSpan" id="kobo.87.4">This procedure generates a new, synthetic data point that lies somewhere on the line segment, connecting the actual instance with its chosen neighbor, effectively creating plausible new instances that contribute to</span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.88.1"> a more balanced dataset for the classifier to </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">learn from.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Adaptive synthetic (ADASYN) sampling</span></strong><span class="koboSpan" id="kobo.91.1">: Creating synthetic instances for the minority class by following</span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.92.1"> their density </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.93.1">distributions. </span><span class="koboSpan" id="kobo.93.2">Extra synthetic data is produced for minority samples that pose more significant learning challenges than those that are easier </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">to learn.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.95.1">Undersampling</span></strong><span class="koboSpan" id="kobo.96.1">: This involves reducing the </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.97.1">number of instances</span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.98.1"> in the majority class. </span><span class="koboSpan" id="kobo.98.2">Methods include </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.100.1">Random undersampling</span></strong><span class="koboSpan" id="kobo.101.1">: This</span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.102.1"> involves randomly eliminating majority </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">class instances.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.104.1">Tomek links</span></strong><span class="koboSpan" id="kobo.105.1">: This identifies pairs </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.106.1">of instances from nearest neighbor classes and removes the majority instance from </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">the pair.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Cluster centroids</span></strong><span class="koboSpan" id="kobo.109.1">: This method </span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.110.1">replaces a cluster of majority samples with the cluster centroid of a </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">k-means algorithm.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Neighborhood cleaning rule</span></strong><span class="koboSpan" id="kobo.113.1">: This combines undersampling and the </span><strong class="bold"><span class="koboSpan" id="kobo.114.1">edited nearest neighbor</span></strong><span class="koboSpan" id="kobo.115.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.116.1">ENN</span></strong><span class="koboSpan" id="kobo.117.1">) method to</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.118.1"> remove majority </span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.119.1">class instances that are misclassified by the</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.120.1"> KNN classifier and the instances from the minority class that </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">are misclassified.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">Combining oversampling and undersampling</span></strong><span class="koboSpan" id="kobo.123.1">: Techniques can be used to both oversample the </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.124.1">minority class and undersample the majority class to achieve </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">a balance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.126.1">Ensemble resampling</span></strong><span class="koboSpan" id="kobo.127.1">: This involves</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.128.1"> creating multiple</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.129.1"> balanced subsets through resampling and building an ensemble </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">of models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.131.1">While resampling methods can help balance the class distribution, they may not always improve model performance, especially in terms of calibration. </span><span class="koboSpan" id="kobo.131.2">Evaluating models on a separate, untouched validation set and considering other strategies such as choosing appropriate evaluation metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">is crucial.</span></span></p>
<p><span class="koboSpan" id="kobo.133.1">While resampling methods</span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.134.1"> such as SMOTE have been accepted for many years as potential solutions, there is no evidence that such methods work across a wide range of datasets. </span><span class="koboSpan" id="kobo.134.2">For example, in Kaggle competitions, SMOTE was never successfully used as part of </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">winning solutions.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Over the years, resampling methods, notably SMOTE, have been championed as potential solutions to the challenge of imbalanced datasets. </span><span class="koboSpan" id="kobo.136.2">However, a deeper dive into their effectiveness paints a more nuanced picture. </span><span class="koboSpan" id="kobo.136.3">Despite their widespread mention in literature and tutorials, there’s a conspicuous absence of empirical evidence supporting their efficacy across diverse datasets. </span><span class="koboSpan" id="kobo.136.4">A testament to this is the world of Kaggle competitions, where precision, innovation, and effectiveness are paramount. </span><span class="koboSpan" id="kobo.136.5">Notably, SMOTE and similar strategies have rarely, if ever, been components of winning solutions. </span><span class="koboSpan" id="kobo.136.6">This isn’t just a statistical anomaly or coincidence. </span><span class="koboSpan" id="kobo.136.7">It underscores a profound observation: while these methods might offer superficial relief in some contexts, they aren’t universally applicable or reliably effective. </span><span class="koboSpan" id="kobo.136.8">Any practitioner aiming for cutting-edge performance would do well to approach resampling methods with a healthy dose of skepticism and </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">thorough validation.</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">The study </span><em class="italic"><span class="koboSpan" id="kobo.139.1">The harm of class imbalance corrections for risk prediction models: illustration and simulation using logistic </span></em><em class="italic"><span class="koboSpan" id="kobo.140.1">regression </span></em><span class="koboSpan" id="kobo.141.1">by Ruben Van Den Goorbergh, Maarten van Smeden, Dirk Timmerman, Ben Van Calster, investigates the impact of class imbalance adjustments on the performance of logistic regression models. </span><span class="koboSpan" id="kobo.141.2">The research scrutinizes conventional and ridge-penalized versions of the model, assessing how these corrections influence their discrimination ability, calibration accuracy, and </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">classification effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.143.1">The paper analyzed techniques such as random undersampling and SMOTE, leveraging both Monte Carlo simulations and a real-world case study on ovarian </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">cancer diagnosis.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">Interestingly, while these corrective methods consistently resulted in miscalibrated models (with a pronounced overestimation of the likelihood of falling into the minority class), they didn’t necessarily enhance discrimination as measured by the area under the receiver operating characteristic curve. </span><span class="koboSpan" id="kobo.145.2">However, they did improve classification metrics such as sensitivity and specificity. </span><span class="koboSpan" id="kobo.145.3">Similar classification outcomes could be achieved simply by adjusting the </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">probability threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">The paper argues that class imbalance correction techniques can harm the performance of prediction models, particularly in terms of calibration. </span><span class="koboSpan" id="kobo.147.2">The research determined that an imbalance in outcomes does not necessarily pose an issue and that attempts to correct this imbalance </span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.148.1">could degrade the </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">model’s performance.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">The paper’s findings underscore that class imbalance, in isolation, isn’t inherently problematic and that efforts to rectify it might inadvertently degrade </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">model performance.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">In data science, distinguishing between prediction and classification is pivotal. </span><span class="koboSpan" id="kobo.152.2">Classification often mandates a premature decision, merging prediction with the decision-making process, potentially sidelining the actual decision-makers’ considerations. </span><span class="koboSpan" id="kobo.152.3">This is especially true when the cost of incorrect decisions shifts or data sampling criteria change. </span><span class="koboSpan" id="kobo.152.4">On the other hand, predictions remain neutral, serving as tools for </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">any decision-maker.</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">In his article </span><em class="italic"><span class="koboSpan" id="kobo.155.1">Classification vs. </span><span class="koboSpan" id="kobo.155.2">Prediction</span></em><span class="koboSpan" id="kobo.156.1"> (</span><a href="https://www.fharrell.com/post/classification/"><span class="koboSpan" id="kobo.157.1">https://www.fharrell.com/post/classification/</span></a><span class="koboSpan" id="kobo.158.1">), Frank Harell argues that classification can lead to hasty decisions, and its application in machine learning is sometimes misguided. </span><span class="koboSpan" id="kobo.158.2">On the other hand, probability modeling quantifies underlying patterns, typically aligning more closely with the core objectives of </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">a project.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">Classification is most apt when outcomes are clear-cut, and predictors offer near-certain outcomes. </span><span class="koboSpan" id="kobo.160.2">However, many machine learning enthusiasts lean toward classifiers, neglecting the richness of probabilistic thinking, which is deeply rooted in statistics. </span><span class="koboSpan" id="kobo.160.3">An example of this is the frequent misclassification of logistic regression as a mere classification tool when, in essence, it offers rich </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">probability estimates.</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">It’s a misconception that binary decisions necessitate binary classifications. </span><span class="koboSpan" id="kobo.162.2">Often, the decision might be to gather more data or to take a phased approach. </span><span class="koboSpan" id="kobo.162.3">For instance, a physician might opt for progressive treatment based on evolving symptoms rather than making a binary </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">decision upfront.</span></span></p>
<p><span class="koboSpan" id="kobo.164.1">Consider a high-clarity scenario such as optical character recognition. </span><span class="koboSpan" id="kobo.164.2">Here, the outcome is primarily deterministic, and machine learning classifiers excel. </span><span class="koboSpan" id="kobo.164.3">However, probability estimates become crucial when there’s inherent variability, such as in predicting disease outcomes. </span><span class="koboSpan" id="kobo.164.4">They inherently provide error margins, aiding decision-makers in understanding the </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">associated risks.</span></span></p>
<p><span class="koboSpan" id="kobo.166.1">There’s also a challenge with classifiers in imbalanced scenarios. </span><span class="koboSpan" id="kobo.166.2">For instance, in a dataset with an overwhelming majority of non-diseased patients, a naive classifier might label everyone as non-diseased, achieving high accuracy but failing in actual detection. </span><span class="koboSpan" id="kobo.166.3">Addressing this imbalance often involves practices such as subsampling, which can lead to more issues. </span><span class="koboSpan" id="kobo.166.4">Logistic regression, in contrast, can gracefully handle such situations by recalibrating for different datasets </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">or prevalences.</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">The choice of accuracy metrics is also fundamental. </span><span class="koboSpan" id="kobo.168.2">Opting for simplistic accuracy measures can lead to misleading models. </span><span class="koboSpan" id="kobo.168.3">The focus should instead be on more nuanced and statistically sound accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">scoring rules.</span></span></p>
<p><span class="koboSpan" id="kobo.170.1">In conclusion, while</span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.171.1"> classifiers might be suitable for deterministic scenarios with high-clarity outcomes, for most real-world situations with inherent variability and nuances, probability-based models, such as logistic regression, are more apt, versatile, </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">and insightful.</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">The issue with resampling methods is that they destroy calibration, which is critical for decision-making; the resampling techniques do not add any new information. </span><span class="koboSpan" id="kobo.173.2">The general acceptance of the SMOTE paper that has received over 25K citations is very unfortunate, especially considering that the paper is 20 years old, used only a few datasets, and performed experiments using weak classifiers such as C4.5 (decision tree classifier), Ripper (rule-based algorithm), and a naïve </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">Bayes classifier.</span></span></p>
<p><span class="koboSpan" id="kobo.175.1">The paper also concentrated on inappropriate metrics, focusing solely on the </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">area under the curve</span></strong><span class="koboSpan" id="kobo.177.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.178.1">AUC</span></strong><span class="koboSpan" id="kobo.179.1">) and the </span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.180.1">ROC convex hull without considering metrics that measure classifier calibration. </span><span class="koboSpan" id="kobo.180.2">Consequently, the paper failed to report the adverse effects on calibration caused </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">by SMOTE.</span></span></p>
<p><span class="koboSpan" id="kobo.182.1">In the following section, we’ll examine effective strategies to address the challenges of imbalanced datasets in </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">machine learning.</span></span></p>
<h1 id="_idParaDest-156"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.184.1">The methods for solving imbalanced data</span></h1>
<p><span class="koboSpan" id="kobo.185.1">Addressing the challenge of </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.186.1">imbalanced data isn’t just about achieving a balanced class distribution; it’s about understanding the nuances of the problem and adopting a holistic approach that encompasses all facets of model performance. </span><span class="koboSpan" id="kobo.186.2">Let us go through the methods </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">for it:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Understanding the problem</span></strong><span class="koboSpan" id="kobo.189.1">: The first step is a deep understanding of the problem. </span><span class="koboSpan" id="kobo.189.2">It’s essential to discern why the data is imbalanced. </span><span class="koboSpan" id="kobo.189.3">Is it because of the nature of the data or perhaps due to some external factors or biases in data collection? </span><span class="koboSpan" id="kobo.189.4">Recognizing the root cause can offer insights into the most </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">effective strategies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.191.1">Prioritizing calibration</span></strong><span class="koboSpan" id="kobo.192.1">: One critical aspect that’s often overlooked is calibration. </span><span class="koboSpan" id="kobo.192.2">A model’s ability to provide probability estimates that reflect true likelihoods is paramount, especially when decisions are based on these probabilities. </span><span class="koboSpan" id="kobo.192.3">Ensuring the model is well calibrated is often more crucial than mere </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">class separation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.194.1">Metrics beyond ROC AUC</span></strong><span class="koboSpan" id="kobo.195.1">: While the </span><strong class="bold"><span class="koboSpan" id="kobo.196.1">receiver operating characteristic area under the curve</span></strong><span class="koboSpan" id="kobo.197.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.198.1">ROC AUC</span></strong><span class="koboSpan" id="kobo.199.1">) is a popular metric, relying solely on it can be misleading, especially in imbalanced </span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.200.1">datasets. </span><span class="koboSpan" id="kobo.200.2">It’s pivotal to incorporate metrics that capture the essence of calibration. </span><span class="koboSpan" id="kobo.200.3">Metrics such as </span><strong class="bold"><span class="koboSpan" id="kobo.201.1">expected calibration error</span></strong><span class="koboSpan" id="kobo.202.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.203.1">ECE</span></strong><span class="koboSpan" id="kobo.204.1">), log loss, and Brier score, which </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.205.1">we’ve looked into in previous chapters, provide a more comprehensive understanding of a </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">model’s performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.207.1">Resampling techniques</span></strong><span class="koboSpan" id="kobo.208.1">: While techniques such as oversampling, undersampling, and SMOTE have been propagated as potential solutions, it’s crucial to understand their implications. </span><span class="koboSpan" id="kobo.208.2">While they might balance class distributions, they may not always improve or even maintain a model’s calibration. </span><span class="koboSpan" id="kobo.208.3">Therefore, any resampling should be performed cautiously, and the resulting models should be rigorously evaluated on untouched </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">validation sets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.210.1">Cost-sensitive learning</span></strong><span class="koboSpan" id="kobo.211.1">: Another approach is to assign different costs to misclassifications of the minority and majority classes. </span><span class="koboSpan" id="kobo.211.2">By doing so, the algorithm inherently gives more weight to the minority class during training, aiming to reduce the more </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">costly errors.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Threshold tuning</span></strong><span class="koboSpan" id="kobo.214.1">: By adjusting the decision threshold away from the default (usually 0.5 for binary classification), one can perform better in the minority class. </span><span class="koboSpan" id="kobo.214.2">It’s about finding a balance between precision and recall, and this technique can be particularly effective when the real-world costs of false positives and false negatives </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">are different.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.216.1">Ultimately, the goal is to build effective models differentiating classes and offering calibrated reliable probability estimates. </span><span class="koboSpan" id="kobo.216.2">A multifaceted approach emphasizing understanding, calibration, and the right metrics is the way to tackle the imbalanced </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">data problem.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">Next, we’ll explore how</span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.219.1"> conformal prediction can be applied to help solve imbalanced data problem and offer insights into its potential to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">data analysis.</span></span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.221.1">Solving imbalanced data problems by applying conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.222.1">Conformal prediction is a technique</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.223.1"> that can be applied to handle imbalanced data problems. </span><span class="koboSpan" id="kobo.223.2">Here are a few ways it can </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">be used:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">Graceful handling of imbalanced datasets</span></strong><span class="koboSpan" id="kobo.226.1">: conformal prediction can gracefully handle large imbalanced datasets. </span><span class="koboSpan" id="kobo.226.2">It strictly defines the level of similarity needed, removing any ambiguity. </span><span class="koboSpan" id="kobo.226.3">It can handle severely imbalanced datasets with ratios of 1:100 to 1:1000 without oversampling </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">or undersampling.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.228.1">Local clustering conformal prediction </span></strong><span class="koboSpan" id="kobo.229.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.230.1">LCCP</span></strong><span class="koboSpan" id="kobo.231.1">): LCCP incorporates a dual-layer partitioning approach </span><a id="_idIndexMarker592"/><span class="koboSpan" id="kobo.232.1">within the conformal prediction framework. </span><span class="koboSpan" id="kobo.232.2">Initially, it segments the imbalanced training dataset into subsets based on class taxonomy. </span><span class="koboSpan" id="kobo.232.3">Then, it further divides the examples from the majority class into subsets using clustering techniques. </span><span class="koboSpan" id="kobo.232.4">The goal of LCCP is to offer reliable confidence levels for its predictions while also enhancing the efficiency of the </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">prediction process.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.234.1">Mondrian conformal prediction</span></strong><span class="koboSpan" id="kobo.235.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.236.1">MCP</span></strong><span class="koboSpan" id="kobo.237.1">): This can deal with imbalanced datasets. </span><span class="koboSpan" id="kobo.237.2">It categorizes data based </span><a id="_idIndexMarker593"/><span class="koboSpan" id="kobo.238.1">on their respective labels and assigns a distinct significance level to each class, ensuring that predictive validity is maintained across </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">different classes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.240.1">Non-conformity scoring</span></strong><span class="koboSpan" id="kobo.241.1">: The core of conformal prediction is the non-conformity measure, which ranks new observations based on how “strange” they appear compared to the training data. </span><span class="koboSpan" id="kobo.241.2">This measure can be adapted for imbalanced datasets to give more weight to the minority class, ensuring that the model is more sensitive to the patterns associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">this class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.243.1">Calibration with validity</span></strong><span class="koboSpan" id="kobo.244.1">: conformal prediction guarantees that if we claim a prediction interval with a 95% confidence level, it </span><a id="_idIndexMarker594"/><span class="koboSpan" id="kobo.245.1">will contain the actual outcome 95% of the time in the long run. </span><span class="koboSpan" id="kobo.245.2">This built-in calibration, maintained even for imbalanced datasets, ensures that the prediction intervals or sets genuinely reflect the </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">model’s uncertainty.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.247.1">Flexibility with underlying models</span></strong><span class="koboSpan" id="kobo.248.1">: conformal prediction is not tied to a specific machine learning algorithm. </span><span class="koboSpan" id="kobo.248.2">This means that, even in the context of imbalanced data, practitioners can choose the best-performing base model (a tree-based method, neural network, or linear model) and then apply the conformal framework to obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">reliable predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.250.1">Transparency and interpretability</span></strong><span class="koboSpan" id="kobo.251.1">: The conformal prediction framework’s transparent nature allows straightforward interpretation. </span><span class="koboSpan" id="kobo.251.2">This transparency can be invaluable for imbalanced datasets, enabling stakeholders to understand why specific predictions are made and how certain the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">about them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.253.1">Adaptive to changing distributions</span></strong><span class="koboSpan" id="kobo.254.1">: One of the challenges with imbalanced data is that the distribution of the minority class can change over time. </span><span class="koboSpan" id="kobo.254.2">With its emphasis on ranking new observations based on their non-conformity, conformal prediction can adapt to these changes, ensuring that predictions remain calibrated even as the underlying data </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">distribution evolves.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.256.1">conformal prediction provides a framework that can be adapted to handle imbalanced datasets in various ways, offering potential </span><a id="_idIndexMarker595"/><span class="koboSpan" id="kobo.257.1">solutions to this common problem in machine learning. </span><span class="koboSpan" id="kobo.257.2">While classification is now commonplace, the ultimate goal is enabling informed decisions, which requires reliable probability estimates even with skewed </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">class data.</span></span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.259.1">Addressing imbalanced data with Venn-Abers predictors</span></h2>
<p><span class="koboSpan" id="kobo.260.1">In the ever-evolving world of machine</span><a id="_idIndexMarker596"/><span class="koboSpan" id="kobo.261.1"> learning, addressing classification problems has become commonplace. </span><span class="koboSpan" id="kobo.261.2">From distinguishing between cats and dogs to more intricate challenges, the real aim of classification isn’t merely labeling; it’s also facilitating informed decision-making. </span><span class="koboSpan" id="kobo.261.3">For this purpose, more than just class labels are required. </span><span class="koboSpan" id="kobo.261.4">We need well-calibrated </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">class probabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">Most data scientists, especially those in the</span><a id="_idIndexMarker597"/><span class="koboSpan" id="kobo.264.1"> early stages of their journey, tend to evaluate classification models using standard metrics such as accuracy, precision, and recall. </span><span class="koboSpan" id="kobo.264.2">While these metrics are insightful for more straightforward tasks, they can be misleading for more intricate, real-world problems. </span><span class="koboSpan" id="kobo.264.3">The true essence of classification lies in calibration, an aspect often overlooked in </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">introductory courses.</span></span></p>
<p><span class="koboSpan" id="kobo.266.1">For professionals working on critical applications, from finance to healthcare, the calibration of classifiers is paramount. </span><span class="koboSpan" id="kobo.266.2">The heart of a classification problem is to make informed decisions. </span><span class="koboSpan" id="kobo.266.3">These decisions revolve around the probabilities of various scenarios, each with potential costs </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">and benefits.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">Take the banking sector, for instance. </span><span class="koboSpan" id="kobo.268.2">If a model merely predicts that a potential customer won’t default on a loan, it needs to provide more depth for decision-making, especially when substantial amounts of money are at stake. </span><span class="koboSpan" id="kobo.268.3">What’s needed is a model that offers well-calibrated probabilities of various outcomes, allowing for a nuanced evaluation of risks </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">and rewards.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">However, a significant challenge arises: many machine learning models don’t inherently produce class probabilities. </span><span class="koboSpan" id="kobo.270.2">Even if they do, these probabilities can often be miscalibrated, leading to erroneous decision-making. </span><span class="koboSpan" id="kobo.270.3">This is particularly concerning in critical sectors. </span><span class="koboSpan" id="kobo.270.4">For example, a self-driving car that misinterprets an obstacle due to miscalibrated probabilities could result </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">in accidents.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">So, what can be done to achieve better calibration? </span><span class="koboSpan" id="kobo.272.2">Classic methods, such as Platt’s scaling (</span><a href="https://en.wikipedia.org/wiki/Platt_scaling"><span class="koboSpan" id="kobo.273.1">https://en.wikipedia.org/wiki/Platt_scaling</span></a><span class="koboSpan" id="kobo.274.1">) and isotonic regression (</span><a href="https://en.wikipedia.org/wiki/Isotonic_regression"><span class="koboSpan" id="kobo.275.1">https://en.wikipedia.org/wiki/Isotonic_regression</span></a><span class="koboSpan" id="kobo.276.1">), were early solutions. </span><span class="koboSpan" id="kobo.276.2">However, these methods have limitations, often rooted in restrictive assumptions that hamper their efficacy across </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">diverse datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.278.1">Enter </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">Venn-Abers predictors</span></strong><span class="koboSpan" id="kobo.280.1">, a beacon of hope in classifier calibration. </span><span class="koboSpan" id="kobo.280.2">Venn-Abers predictors, a subset of the conformal prediction framework, promise a more robust approach to calibration. </span><span class="koboSpan" id="kobo.280.3">Unlike traditional </span><a id="_idIndexMarker598"/><span class="koboSpan" id="kobo.281.1">methods, they don’t hinge on overly simplistic assumptions and offer a more versatile calibration tool apt for today’s </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">complex datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">In essence, if you aim to harness the true potential of machine learning classifiers in 2022 and beyond, Venn-Abers </span><a id="_idIndexMarker599"/><span class="koboSpan" id="kobo.284.1">and the broader conformal prediction framework are worth exploring. </span><span class="koboSpan" id="kobo.284.2">They might be the key to unlocking well-calibrated, reliable machine </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.286.1">Venn-Abers predictors stand out in machine learning, offering probability-driven predictions for test data labels. </span><span class="koboSpan" id="kobo.286.2">What sets them apart is their built-in assurance of calibration. </span><span class="koboSpan" id="kobo.286.3">This assurance is grounded in the typical premise that data observations are independently sourced from a </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">consistent distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">At its core, the Venn-Abers approach is inspired by isotonic regression. </span><span class="koboSpan" id="kobo.288.2">It refines the probabilistic prediction calibration method pioneered by Zadrozny and Elkan. </span><span class="koboSpan" id="kobo.288.3">In contrast to techniques such as Platt’s scaler and isotonic regression, Venn-Abers predictors come equipped with inherent mathematical proofs, ensuring their </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">unbiased validity.</span></span></p>
<p><span class="koboSpan" id="kobo.290.1">An intriguing feature of Venn-Abers predictors is their ability to produce dual probability predictions for the </span><em class="italic"><span class="koboSpan" id="kobo.291.1">class 1</span></em><span class="koboSpan" id="kobo.292.1"> label. </span><span class="koboSpan" id="kobo.292.2">This dual output captures the range of prediction uncertainty. </span><span class="koboSpan" id="kobo.292.3">As a result, these predictors offer calibrated predictions and shed light on the inherent confidence associated with each prediction. </span><span class="koboSpan" id="kobo.292.4">This makes them invaluable tools for enhancing the calibration of probability-based predictions. </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">Here’s how:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">True-to-life probability intervals</span></strong><span class="koboSpan" id="kobo.295.1">: Venn-Abers predictors shine in delivering calibrated probability intervals. </span><span class="koboSpan" id="kobo.295.2">This ensures that the probabilities they produce genuinely represent the actual chances of an event, eliminating the pitfalls of overconfidence </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">or underestimation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Versatility across models</span></strong><span class="koboSpan" id="kobo.298.1">: The beauty of Venn-Abers calibration is its adaptability. </span><span class="koboSpan" id="kobo.298.2">Whether you’re working with decision trees, random forests, or even XGBoost models, Venn-Abers can recalibrate them, fine-tuning overambitious and cautious models to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">their accuracy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Enhanced decision support with valid intervals</span></strong><span class="koboSpan" id="kobo.301.1">: The predictors don’t just stop at labels. </span><span class="koboSpan" id="kobo.301.2">For every prediction, especially from typically complex models such as</span><a id="_idIndexMarker600"/><span class="koboSpan" id="kobo.302.1"> random forests and XGBoost, Venn-Abers offers a probability interval. </span><span class="koboSpan" id="kobo.302.2">The span of this interval serves as a barometer of the </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">prediction’s reliability</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.304.1">Venn-Abers predictors are a beacon for those navigating the choppy waters of imbalanced data issues. </span><span class="koboSpan" id="kobo.304.2">They refine the predictive accuracy of various machine learning models and arm users with credible probability intervals, making decision-making more informed </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">and confident.</span></span></p>
<p><span class="koboSpan" id="kobo.306.1">To illustrate the various issues in imbalanced data problems, we will use the following </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">notebook: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.308.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_11.ipynb</span></strong></span></p>
<p><span class="koboSpan" id="kobo.309.1">This notebook will look at various methods for handling an imbalanced class problem and apply conformal prediction to calibrate </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">class probabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">We will use the Credit Card Fraud Detection dataset from </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">Kaggle: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud</span></span></p>
<p><span class="koboSpan" id="kobo.314.1">The dataset contains data on credit card transactions in September 2013 by cardholders in Europe. </span><span class="koboSpan" id="kobo.314.2">The transactions occurred over two days, with 492 fraudulent transactions out of 284,807 transactions. </span><span class="koboSpan" id="kobo.314.3">The dataset is highly imbalanced, with the positive class (fraudulent transactions) accounting for 0.17% of </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">all transactions.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">The dataset contains numerical features that are the result of PCA transformation; the original features have been withheld due to confidentiality and </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">privacy issues.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">Features </span><strong class="source-inline"><span class="koboSpan" id="kobo.319.1">V1</span></strong><span class="koboSpan" id="kobo.320.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.321.1">V2</span></strong><span class="koboSpan" id="kobo.322.1">, ... </span><strong class="source-inline"><span class="koboSpan" id="kobo.323.1">V28</span></strong><span class="koboSpan" id="kobo.324.1"> are the principal components obtained </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">using PCA:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.326.1">The only original features are </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">Time</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.328.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">Amount</span></strong></span></li>
<li><span class="koboSpan" id="kobo.330.1">The feature </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">Time</span></strong><span class="koboSpan" id="kobo.332.1"> contains the time (in seconds) for each transaction relative to the first transaction in </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">the dataset</span></span></li>
<li><span class="koboSpan" id="kobo.334.1">The feature </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">Amount</span></strong><span class="koboSpan" id="kobo.336.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">transaction amount</span></span></li>
<li><span class="koboSpan" id="kobo.338.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.339.1">label</span></strong><span class="koboSpan" id="kobo.340.1"> Class is the</span><a id="_idIndexMarker601"/><span class="koboSpan" id="kobo.341.1"> dependent variable that needs to be predicted (fraudulent transactions labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">with 1)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.343.1">We will use various </span><a id="_idIndexMarker602"/><span class="koboSpan" id="kobo.344.1">classifiers, including popular classifiers such as XGBoost, LightGBM, CatBoost, Random Forest, and </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">logistic regression.</span></span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.346.1">Key insights from the Credit Card Fraud Detection notebook</span></h2>
<p><span class="koboSpan" id="kobo.347.1">In our exploration of the Credit Card Fraud Detection dataset, we unearthed several pivotal insights that can reshape our approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">imbalanced data:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.349.1">Embracing simplicity</span></strong><span class="koboSpan" id="kobo.350.1">: The most</span><a id="_idIndexMarker603"/><span class="koboSpan" id="kobo.351.1"> effective strategy is often to leave the data untouched. </span><span class="koboSpan" id="kobo.351.2">Contrary to the push for intricate resampling techniques, a minimalist approach can sometimes yield </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">superior results.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.353.1">Reframing imbalance</span></strong><span class="koboSpan" id="kobo.354.1">: Rather than viewing imbalanced data as a dilemma needing a direct fix, it’s crucial to understand that the imbalance isn’t always the root issue. </span><span class="koboSpan" id="kobo.354.2">The quest shouldn’t be to balance the scales but to derive meaningful insights from the data, irrespective </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">of distribution.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.356.1">The power of robust metrics</span></strong><span class="koboSpan" id="kobo.357.1">: The choice of metrics can make or break your analysis. </span><span class="koboSpan" id="kobo.357.2">By employing a comprehensive set of metrics, you can accurately define the problem and pave the way for </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">practical solutions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.359.1">Calibration’s central role</span></strong><span class="koboSpan" id="kobo.360.1">: Calibration is non-negotiable in real-world decision-making scenarios, especially in critical applications. </span><span class="koboSpan" id="kobo.360.2">Accurate probability estimations are vital, ensuring decisions are based on </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">reliable data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.362.1">The double-edged sword of resampling</span></strong><span class="koboSpan" id="kobo.363.1">: While resampling methods might seem promising, they often compromise the model’s calibration. </span><span class="koboSpan" id="kobo.363.2">Our analysis demonstrated that such techniques could deteriorate calibration metrics such as ECE, log loss, and </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">Brier score.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.365.1">conformal prediction</span></strong><strong class="bold"><span class="koboSpan" id="kobo.366.1"> as a beacon</span></strong><span class="koboSpan" id="kobo.367.1">: Amid the challenges posed by imbalanced data and the potential pitfalls of resampling, conformal prediction emerges as a silver lining. </span><span class="koboSpan" id="kobo.367.2">It offers a reliable method to recalibrate</span><a id="_idIndexMarker604"/><span class="koboSpan" id="kobo.368.1"> probabilities, ensuring that even post-resampling, the data remains conducive for </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">sound decision-making.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.370.1">By internalizing these insights, we can approach imbalanced datasets with a refined perspective, prioritizing meaningful analysis over </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">superficial fixes.</span></span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.372.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.373.1">The challenge of imbalanced datasets in machine learning often results in biased predictions and compromised model outcomes. </span><span class="koboSpan" id="kobo.373.2">This chapter delves deep into the complexities of such datasets and illuminates the path through conformal prediction, a groundbreaking approach to handling </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">these scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">Traditional methods, such as resampling techniques, and metrics, such as ROC AUC, often fail to address the imbalances effectively. </span><span class="koboSpan" id="kobo.375.2">Furthermore, they can sometimes lead to even more skewed results. </span><span class="koboSpan" id="kobo.375.3">On the other hand, conformal prediction emerges as a robust solution, offering calibrated and reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">probability estimates.</span></span></p>
<p><span class="koboSpan" id="kobo.377.1">The practical implications of these methods are illustrated using the Credit Card Fraud Detection dataset from Kaggle, an inherently imbalanced dataset. </span><span class="koboSpan" id="kobo.377.2">The exploration underscores the significance of understanding the data, using robust metrics, and the transformative potential of </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">conformal prediction</span></span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">In essence, while imbalanced data presents challenges, practitioners can navigate toward calibrated and insightful predictions with the right tools such as </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">conformal prediction</span></span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.383.1">In the next chapter of this book, we will dive deep into the fascinating world of multi-class conformal prediction. </span><span class="koboSpan" id="kobo.383.2">This chapter will introduce you to various conformal prediction methods that can be effectively applied to multi-class </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">classification problems.</span></span></p>
</div>
</body></html>