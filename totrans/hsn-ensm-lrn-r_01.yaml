- en: Chapter 1. Introduction to Ensemble Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。集成技术简介
- en: Ensemble techniques are model output aggregating techniques that have evolved
    over the past decade and a half in the area of statistical and machine learning.
    This forms the central theme of this book. Any user of statistical models and
    machine learning tools will be familiar with the problem of building a model and
    the vital decision of choosing among potential candidate models. A model's accuracy
    is certainly not the only relevant criterion; we are also concerned with its complexity,
    as well as whether or not the overall model makes practical sense.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 集成技术是模型输出聚合技术，在过去十多年中在统计和机器学习领域得到了发展。这构成了本书的中心主题。任何统计模型和机器学习工具的用户都会熟悉构建模型的问题以及从潜在候选模型中选择的关键决策。模型的准确性当然不是唯一的相关标准；我们还关心其复杂性，以及整体模型是否具有实际意义。
- en: Common modeling problems include the decision to choose a model, and various
    methodologies exist to aid this task. In statistics, we resort to measures such
    as **Akaike Information Criteria** (**AIC**) and **Bayesian Information Criteria**
    (**BIC**), and on other fronts, the p-value associated with the variable in the
    fitted model helps with the decision. This is a process generally known as **model
    selection**. Ridge penalty, Lasso, and other statistics also help with this task.
    For machine learning models such as neural networks, decision trees, and so on,
    a k-fold cross-validation is useful when the model is built using a part of the
    data referred to as training data, and then accuracy is looked for in the untrained
    area or validation data. If the model is sensitive to its complexity, the exercise
    could be futile.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的建模问题包括选择模型的决定，并且存在各种方法来帮助这项任务。在统计学中，我们求助于诸如**赤池信息准则**（**AIC**）和**贝叶斯信息准则**（**BIC**）这样的度量，而在其他方面，与拟合模型中的变量相关的p值有助于做出决策。这个过程通常被称为**模型选择**。岭回归惩罚、Lasso和其他统计方法也有助于这项任务。对于机器学习模型，如神经网络、决策树等，当模型使用称为训练数据的数据的一部分构建时，k折交叉验证是有用的，然后我们会在未训练区域或验证数据中寻找准确性。如果模型对其复杂性敏感，这种练习可能是徒劳的。
- en: The process of obtaining the *best* model means that we create a host of other
    models, which are themselves nearly as efficient as the best model. Moreover,
    the best model accurately covers the majority of samples, and other models might
    accurately assess the variable space region where it is inaccurate. Consequently,
    we can see that the final shortlisted model has few advantages over the runner
    up. The next models in line are not so poor as to merit outright rejection. This
    makes it necessary to find a way of taking most of the results already obtained
    from the models and combining them in a meaningful way. The search for a method
    for putting together various models is the main objective of ensemble learning.
    Alternatively, one can say that ensemble learning transforms competing models
    into collaborating models. In fact, ensemble techniques are not the end of the
    modeling exercise, as they will also be extended to the unsupervised learning
    problems. We will demonstrate an example that justifies the need for this.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 获得最佳模型的过程意味着我们创建了一大批其他模型，这些模型本身几乎与最佳模型一样高效。此外，最佳模型准确地覆盖了大多数样本，而其他模型可能准确地评估了它不准确变量的空间区域。因此，我们可以看到，最终入围的模型与第二名相比优势很少。接下来的模型并不那么糟糕，以至于可以完全拒绝。这使得有必要找到一种方法，将已从模型中获得的大部分结果以有意义的方式结合起来。寻找将各种模型组合起来的方法是集成学习的主要目标。或者可以说，集成学习将竞争模型转化为协作模型。实际上，集成技术并不是建模练习的终点，因为它们还将扩展到无监督学习问题。我们将演示一个例子，以证明这种需要的合理性。
- en: The implementation of ensemble methods would have been impossible without the
    invention of modern computational power. Statistical methods foresaw techniques
    that required immense computations. Methods such as permutation tests and jackknife
    are evidence of the effectiveness of computational power. We will undertake an
    exercise to learn these later in the chapter, and we will revisit them later on
    in the book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 没有现代计算能力的发明，集成方法的实现将是不可能的。统计方法预见到了需要巨大计算的技术。如排列检验和Jackknife等方法是计算能力有效性的证据。我们将在本章后面进行一项练习来学习这些方法，并在本书后面的部分重新审视它们。
- en: From a machine learning perspective, *supervised* and *unsupervised* are the
    two main types of learning technique. **Supervised learning** is the arm of machine
    learning, the process in which a certain variable is known, and the purpose is
    to understand this variable through various other variables. Here, we have a target
    variable. Since learning takes place with respect to the output variable, supervised
    learning is sometimes referred to as learning with a teacher. All target variables
    are not alike, and they often fall under one of the following four types. If the
    goal is to classify observations into one of *k* types of class (for example,
    Yes/No, Satisfied/Dissatisfied), then we have a classification problem. Such a
    variable is referred to as a *categorical variable* in statistics. It is possible
    that the variable of interest might be a continuous variable, which is numeric
    from a software perspective. This may include car mileage per liter, a person's
    income, or a person's age. For such scenarios, the purpose of the machine learning
    problem is to learn the variables in terms of other associated variables, and
    then predict it for unknown cases in which only the values of associated variables
    are available. We will broadly refer to this class of problem as a **regression
    problem**.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，*监督学习*和*无监督学习*是两种主要的学习方法。**监督学习**是机器学习的一个分支，其过程是已知某个变量，目的是通过其他各种变量来理解这个变量。在这里，我们有一个目标变量。由于学习是在输出变量的基础上进行的，因此监督学习有时被称为有教师的学习。并非所有目标变量都是相同的，它们通常属于以下四种类型之一。如果目标是把观察结果分类到*k*种类型之一（例如，是/否，满意/不满意），那么我们有一个分类问题。这样的变量在统计学中被称为*分类变量*。感兴趣的变量可能是一个连续变量，从软件的角度来看是数值的。这可能包括每升油的汽车里程数，一个人的收入，或一个人的年龄。对于这样的场景，机器学习问题的目的是通过其他相关变量来学习变量，然后预测未知案例，在这些案例中，只有相关变量的值是可用的。我们将广泛地将这类问题称为**回归问题**。
- en: In clinical trials, the time to event is often of interest. When an illness
    is diagnosed, we would ask whether the proposed drug is an improvement on the
    existing one. While the variable in question here is the length of time between
    diagnosis and death, clinical trial data poses several other problems. The analysis
    cannot wait until all the patients have died, and/or some of the patients may
    have moved away from the study, making it no longer possible to know their status.
    Consequently, we have censored data. As part of the study observations, complete
    information is not available. Survival analysis largely deals with such problems,
    and we will undertake the problem of creating ensemble models here.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在临床试验中，事件发生的时间通常是一个关注的焦点。当疾病被诊断出来时，我们会询问所提出的药物是否比现有的药物有所改进。在这里，所讨论的变量是诊断和死亡之间的时间长度，而临床试验数据会带来几个其他问题。分析不能等到所有患者都去世，或者有些患者可能已经离开了研究，使得我们无法再了解他们的状态。因此，我们得到了截尾数据。作为研究观察的一部分，完整的信息并不总是可用。生存分析主要处理这类问题，我们将在这里着手解决创建集成模型的问题。
- en: With classification, regression, and survival data, it may be assumed that that
    the instances/observations are independent of each other. This is a very reasonable
    assumption in that there is a valid reason to believe that patients will respond
    to a drug independently of other patients, a customer will churn or pay the loan
    independently of other customers, and so forth. In yet another important class
    of problems, this assumption is not met, and we are left with observations depending
    on each other via time series data. An example of time series data is the closure
    stock exchange points of a company. Clearly, the performance of a company's stock
    can't be independent each day, and thus we need to factor in dependency.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类、回归和生存数据，可以假设实例/观察是相互独立的。这是一个非常合理的假设，因为有一个合理的理由相信患者会对药物的反应是独立的，客户会流失或支付贷款也是独立的，等等。在另一个重要的问题类别中，这个假设并不成立，我们面临的是通过时间序列数据相互依赖的观察。时间序列数据的一个例子是某公司股票交易所的收盘点。显然，一家公司股票的表现不可能每天都是独立的，因此我们需要考虑依赖性。
- en: In many practical problems, the goal is to understand patterns or find groups
    of observations, and we don't have a specific variable of interest with regard
    to which algorithm needs to be trained. Finding groups or clusters is referred
    to as unsupervised learning or learning without a teacher. Two main practical
    problems that arise in finding clusters is that (i) it is generally not known
    in advance how many clusters are in the population, and (ii) different choices
    of initial cluster centers lead to different solutions. Thus, we need a solution
    that is free from, or at least indifferent to, initialization and takes the positives
    of each useful solution into consideration. This will lead us toward unsupervised
    ensemble techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际问题中，目标是理解模式或找到观察值的组，我们没有关于需要训练哪个算法的具体变量。找到组或聚类被称为无监督学习或无导师学习。在寻找聚类时出现的两个主要实际问题包括：(i)
    通常事先不知道总体中有多少个聚类，以及(ii) 不同初始聚类中心的选择会导致不同的解决方案。因此，我们需要一个不受初始化影响，或者至少对初始化不敏感的解决方案，并考虑每个有用解决方案的优点。这将引导我们走向无监督集成技术。
- en: The search for the best models, supervised or unsupervised, is often hindered
    by the presence of outliers. The presence of a single outlier is known to heavily
    influence the overall fit of linear models, and it is also known to significantly
    impact even nonlinear models. Outlier detection is a challenge in itself, and
    a huge body of statistical methods help in identifying outliers. A host of machine
    learning methods also help in identifying outliers. Of course, ensembles will
    help here, and we will develop R programs that will help solve the problem of
    identifying outliers. This method will be referred to as outlier ensembles.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳模型，无论是监督学习还是无监督学习，常常受到异常值的存在而受阻。已知单个异常值会严重影响线性模型的整体拟合度，而且对非线性模型也有显著影响。异常值检测本身就是一项挑战，大量的统计方法有助于识别异常值。众多机器学习方法也有助于识别异常值。当然，集成方法在这里也会有所帮助，我们将开发R程序来帮助解决识别异常值的问题。这种方法将被称作异常值集成。
- en: At the outset, it is important that the reader becomes familiar with the datasets
    used in this book. All major datasets will be introduced in the first section.
    We begin the chapter with a brief introduction to the core statistical/machine
    learning models and put them into action immediately afterward. It will quickly
    become apparent that there is not a single class of model that would perform better
    than any other model. If any such solution existed, we wouldn't need the ensemble
    technique.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在一开始，读者熟悉本书中使用的数据集非常重要。所有主要数据集将在第一部分介绍。我们首先简要介绍核心的统计/机器学习模型，并在之后立即将其付诸实践。很快就会明显看出，没有一种模型会比其他模型表现得更好。如果存在这样的解决方案，我们就不需要集成技术了。
- en: 'In this chapter, we will cover:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: '**Datasets**: The core datasets that will be used throughout the book'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：本书中将使用的核心数据集'
- en: '**Statistical/machine learning models**: Important classification models will
    be explained here'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计/机器学习模型**：这里将解释重要的分类模型'
- en: '**The right model dilemma**: The absence of a *dominating* model'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确模型困境**：没有占主导地位的模型'
- en: '**An ensemble purview**: The need for ensembles'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成概述**：集成方法的需求'
- en: '**Complementary statistical tests**: Important statistical tests that will
    be useful for model comparisons will be discussed here'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互补统计测试**：这里将讨论对模型比较有用的重要统计测试'
- en: 'The following R packages will be required for this chapter:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将需要以下R包：
- en: '`ACSWR`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ACSWR`'
- en: '`caret`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret`'
- en: '`e1071`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`e1071`'
- en: '`factoextra`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factoextra`'
- en: '`mlbench`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlbench`'
- en: '`NeuralNetTools`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NeuralNetTools`'
- en: '`perm`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`perm`'
- en: '`pROC`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pROC`'
- en: '`RSADBE`'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RSADBE`'
- en: '`Rpart`'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Rpart`'
- en: '`survival`'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`survival`'
- en: '`nnet`'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nnet`'
- en: Datasets
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Data is undoubtedly the most important component of machine learning. If there
    was no data, we wouldn't have a common purpose. In most cases, the purpose for
    which the data is collected defines the problem itself. As we know that the variable
    might be of several types, the way it is stored and organized is also very important.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无疑是机器学习最重要的组成部分。如果没有数据，我们就不会有共同的目标。在大多数情况下，收集数据的目的本身就定义了问题本身。正如我们所知，变量可能属于几种类型，其存储和组织方式也非常重要。
- en: Lee and Elder (1997) considered a series of datasets and introduced the need
    for ensemble models. We will begin by looking at the details of the datasets considered
    in their paper, and we will then refer to other important datasets later on in
    the book.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 李和艾尔德（1997）考虑了一系列数据集，并引入了集成模型的需求。我们将首先查看他们论文中考虑的数据集的细节，然后在本书的后续部分介绍其他重要数据集。
- en: Hypothyroid
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 甲状腺功能减退
- en: 'The hypothyroid dataset `Hypothyroid.csv` is available in the book''s code
    bundle packet, located at `/…/Chapter01/Data`. While we have 26 variables in the
    dataset, we will only be using seven of these variables. Here, the number of observations
    is *n* = 3163\. The dataset is downloaded from [http://archive.ics.uci.edu/ml/datasets/thyroid+disease](http://archive.ics.uci.edu/ml/datasets/thyroid+disease)
    and the filename is `hypothyroid.data` ([http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data)).
    After some tweaks to the order of relabeling certain values, the CSV file is made
    available in the book''s code bundle. The purpose of the study is to classify
    a patient with a thyroid problem based on the information provided by other variables.
    There are multiple variants of the dataset and the reader can delve into details
    at the following web page: [http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO).
    Here, the column representing the variable of interest is named `Hypothyroid`,
    which shows that we have 151 patients with thyroid problems. The remaining 3012
    tested negative for it. Clearly, this dataset is an example of *unbalanced data*,
    which means that one of the two cases is outnumbered by a huge number; for each
    thyroid case, we have about 20 negative cases. Such problems need to be handled
    differently, and we need to get into the subtleties of the algorithms to build
    meaningful models. The additional variables or covariates that we will use while
    building the predictive models include `Age`, `Gender`, `TSH`, `T3`, `TT4`, `T4U`,
    and `FTI`. The data is first imported into an R session and is subset according
    to the variables of interest as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 甲状腺功能减退数据集 `Hypothyroid.csv` 可在本书的代码包中找到，位于 `/…/Chapter01/Data`。虽然数据集中有 26 个变量，但我们只会使用其中的
    7 个变量。在这里，观测值的数量是 *n* = 3163。数据集是从 [http://archive.ics.uci.edu/ml/datasets/thyroid+disease](http://archive.ics.uci.edu/ml/datasets/thyroid+disease)
    下载的，文件名为 `hypothyroid.data` ([http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data))。经过对某些值重新标记顺序的调整后，CSV
    文件被包含在本书的代码包中。研究的目的是根据其他变量提供的信息对患有甲状腺问题的患者进行分类。数据集有多种变体，读者可以在以下网页深入了解细节：[http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO)。在这里，代表感兴趣变量的列被命名为
    `Hypothyroid`，这表明我们有 151 名患有甲状腺问题的患者。其余 3012 名测试结果为阴性。显然，这个数据集是 *不平衡数据* 的一个例子，这意味着两种情况中的一种数量远远超过另一种；对于每个甲状腺病例，我们大约有
    20 个阴性病例。这类问题需要不同的处理方式，我们需要深入算法的细微差别来构建有意义的模型。在构建预测模型时，我们将使用的额外变量或协变量包括 `Age`（年龄）、`Gender`（性别）、`TSH`（促甲状腺激素）、`T3`（三碘甲状腺原氨酸）、`TT4`（总甲状腺素）、`T4U`（甲状腺素自由指数）和
    `FTI`（游离三碘甲状腺原氨酸）。数据首先被导入 R 会话，并根据感兴趣的变量进行子集化，如下所示：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first line of code imports the data from the `Hypothyroid.csv` file using
    the `read.csv` function. The dataset now has a lot of missing data in the variables,
    as seen here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码使用 `read.csv` 函数从 `Hypothyroid.csv` 文件中导入数据。现在数据集中变量有很多缺失数据，如下所示：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Consequently, we remove all the rows that have a missing value, and then split
    the data into training and testing datasets. We will also create a formula for
    the classification problem:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们删除了所有包含缺失值的行，然后将数据分为训练集和测试集。我们还将为分类问题创建一个公式：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `set.seed` function ensures that the results are reproducible each time
    we run the program. After removing the missing observations with the `na.omit`
    function, we split the hypothyroid data into training and testing parts. The former
    is used to build the model and the latter is used to validate it, using data that
    has not been used to build the model. Quinlan – the inventor of the popular tree
    algorithm C4.5 – used this dataset extensively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`set.seed` 函数确保每次运行程序时结果都是可重复的。在用 `na.omit` 函数删除缺失观测值后，我们将甲状腺数据分为训练集和测试集。前者用于构建模型，后者用于验证，使用的是未用于构建模型的数据。Quinlan
    – C4.5 算法的发明者 – 广泛使用了这个数据集。'
- en: Waveform
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 波形
- en: 'This dataset is an example of a simulation study. Here, we have twenty-one
    variables as input or independent variables, and a class variable referred to
    as `classes`. The data is generated using the `mlbench.waveform` function from
    the `mlbench` R package. For more details, refer to the following link: [ftp://ftp.ics.uci.edu/pub/machine-learning-databases](ftp://ftp.ics.uci.edu/pub/machine-learning-databases).
    We will simulate 5,000 observations for this dataset. As mentioned earlier, the
    `set.seed` function guarantees reproducibility. Since we are solving binary classification
    problems, we will reduce the three classes generated by the waveform function
    to two, and then partition the data into training and testing parts for model
    building and testing purposes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是一个模拟研究的例子。在这里，我们有二十一个变量作为输入或独立变量，以及一个被称为`classes`的类别变量。数据是通过`mlbench`
    R包中的`mlbench.waveform`函数生成的。更多详情，请参考以下链接：[ftp://ftp.ics.uci.edu/pub/machine-learning-databases](ftp://ftp.ics.uci.edu/pub/machine-learning-databases)。我们将为这个数据集模拟5000个观测值。如前所述，`set.seed`函数保证了可重复性。由于我们正在解决二元分类问题，我们将波形函数生成的三个类别减少到两个，然后为了模型构建和测试目的，将数据分为训练集和测试集：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The R function `mlbench.waveform` creates a new object of the `mlbench` class.
    Since it consists of two sub-parts in `x` and classes, we will convert it into
    `data.frame` following some further manipulations. The `cbind` function binds
    the two objects `x` (a matrix) and classes (a numeric vector) into a single matrix.
    The `data.frame` function converts the matrix object into a data frame, which
    is the class desired for the rest of the program.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: R函数`mlbench.waveform`创建了一个新的`mlbench`类对象。由于它由`x`和`classes`两个子部分组成，我们将通过一些进一步的操作将其转换为`data.frame`。`cbind`函数将两个对象`x`（一个矩阵）和`classes`（一个数值向量）绑定成一个单一的矩阵。`data.frame`函数将矩阵对象转换成数据框，这是程序其余部分所需的数据类型。
- en: 'After partitioning the data, we will create the required `formula` for the
    waveform dataset:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分区后，我们将为波形数据集创建所需的`formula`：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: German Credit
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 德国信贷
- en: 'Loans are not always repaid in full, and there are defaulters. In this case,
    it becomes important for the bank to identify potential defaulters based on the
    available information. Here, we adapt the `GC` dataset from the `RSADBE` package
    to properly reflect the labels of the factor variable. The transformed dataset
    is available as `GC2.RData` in the data folder. The `GC` dataset itself is mainly
    an adaptation of the version available at [https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)).
    Here, we have 1,000 observations, and 20 covariate/independent variables such
    as the status of existing checking account, duration, and so forth. The final
    status of whether the loan was completely paid or not is available in the `good_bad`
    column. We will partition the data into training and testing parts, and create
    the formula too:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款并不总是全额偿还，存在违约者。在这种情况下，银行根据可用的信息识别潜在的违约者变得很重要。在这里，我们改编了`RSADBE`包中的`GC`数据集，以正确反映因子变量的标签。转换后的数据集作为`GC2.RData`在数据文件夹中可用。`GC`数据集本身主要是对[https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))上可用的版本的一种改编。在这里，我们有1000个观测值，以及20个协变量/独立变量，如现有支票账户的状态、期限等。贷款是否完全偿还的最终状态在`good_bad`列中可用。我们将数据分为训练集和测试集，并创建公式：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Iris
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Iris
- en: 'Iris is probably the most famous classification dataset. The great statistician
    Sir R. A. Fisher popularized the dataset, which he used for classifying the three
    types of `iris` plants based on length and width measurements of their petals
    and sepals. Fisher used this dataset to pioneer the invention of the statistical
    classifier linear discriminant analysis. Since there are three species of `iris`,
    we converted this into a binary classification problem, separated the dataset,
    and created a formula as seen here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Iris可能是最著名的分类数据集。伟大的统计学家R.A.费舍尔使这个数据集流行起来，他使用这个数据集根据花瓣和萼片的长度和宽度测量来对三种类型的`iris`植物进行分类。费舍尔使用这个数据集开创了统计分类器线性判别分析的发端。由于有三种`iris`物种，我们将这个问题转化为二元分类问题，分离了数据集，并创建了如这里所见到的公式：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pima Indians Diabetes
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 皮马印第安人糖尿病
- en: 'Diabetes is a health hazard, which is mostly incurable, and patients who are
    diagnosed with it have to adjust their lifestyles in order to cater to this condition.
    Based on variables such as `pregnant`, `glucose`, `pressure`, `triceps`, `insulin`,
    `mass`, `pedigree`, and `age`, the problem here is to classify the person as diabetic
    or not. Here, we have 768 observations. This dataset is drawn from the `mlbench`
    package:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 糖尿病是一种健康危害，通常是不可治愈的，被诊断出患有这种病的患者必须调整他们的生活方式以适应这种状况。基于`pregnant`、`glucose`、`pressure`、`triceps`、`insulin`、`mass`、`pedigree`和`age`等变量，这里的问题是判断一个人是否患有糖尿病。在这里，我们有768个观测值。这个数据集来自`mlbench`包：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The five datasets described up to this point are classification problems. We
    look at one example each for regression, time series, survival, clustering, and
    outlier detection problems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的五个数据集都是分类问题。我们分别看一个回归、时间序列、生存、聚类和异常值检测问题的例子。
- en: US Crime
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 美国犯罪
- en: 'A study of the crime rate per million of the population among the 47 different
    states of the US is undertaken here, and an attempt is made to find its dependency
    on 13 variables. These include age distribution, indicator of southern states,
    average number of schooling years, and so on. As with the earlier datasets, we
    will also partition this one into the following chunks of R program:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对美国47个不同州的每百万人口犯罪率的研究被进行，并试图找出它与13个变量的依赖关系。这些包括年龄分布、南方州的指标、平均在校年数等。与早期数据集一样，我们也将这个数据集分成以下R程序块：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In each example discussed in this section thus far, we had a reason to believe
    that the observations are independent of each other. This assumption simply means
    that the regressands and regressors of one observation have no relationship with
    other observations' regressands and regressors. This is a simple and reasonable
    assumption. We have another class of observations/datasets where such assumptions
    are not practical. For example, the maximum temperature of a day is not completely
    independent of the previous day's temperature. If that were to be the case, we
    could have a scorchingly hot day, followed by winter, followed by another hot
    day, which in turn is followed by a very heavy rainy day. However, weather does
    not happen in this way as on successive days, the weather is dependent on previous
    days. In the next example, we consider the number of overseas visitors to New
    Zealand.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节迄今为止讨论的每个示例中，我们都有理由相信观测值之间是独立的。这个假设简单来说就是，一个观测值的回归因变量和回归自变量与其他观测值的回归因变量和回归自变量之间没有关系。这是一个简单且合理的假设。我们还有另一类观测值/数据集，其中这样的假设并不实用。例如，一天的最高气温并不完全独立于前一天的温度。如果是这样的话，我们可能会经历一个酷热无比的日子，接着是冬天，然后又是另一个炎热的夏天，接着是一个非常大雨的天气。然而，天气并不以这种方式发生，因为连续几天，天气依赖于前一天。在下一个例子中，我们考虑新西兰的海外游客数量。
- en: Overseas visitors
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 海外游客
- en: 'The New Zealand overseas dataset is dealt with in detail in Chapter 10 of Tattar,
    et al. (2017). Here, the number of overseas visitors is captured on a monthly
    basis from January 1977 to December 1995\. We have visitors'' data available for
    over 228 months. The `osvisit.dat` file is available at multiple web links, including
    [https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat](https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat)
    and [https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat](https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat).
    It is also available in the book''s code bundle. We will import the data in R,
    convert it into a time series object, and visualize it:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 新西兰海外数据集在Tattar等人（2017）的第10章中进行了详细处理。在这里，海外游客的数量从1977年1月到1995年12月按月记录。我们有超过228个月的游客数据。`osvisit.dat`文件可在多个网页链接中找到，包括[https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat](https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat)和[https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat](https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat)。它也包含在书籍的代码包中。我们将使用R导入数据，将其转换为时间序列对象，并可视化：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Overseas visitors](img/00002.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![海外游客](img/00002.jpeg)'
- en: 'Figure 1: New Zealand overseas visitors'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：新西兰海外游客
- en: 'Here, the dataset is not partitioned! Time series data can''t be arbitrarily
    partitioned into training and testing parts. The reason is quite simple: if we
    have five observations in a time sequential order *y1*, *y2*, *y3*, *y4*, *y5*,
    and we believe that the order of impact is *y1→y2→y3→y4→y5*, an arbitrary partition
    of *y1*, *y2*, *y5*, will have different behavior. It won''t have the same information
    as three consecutive observations. Consequently, the time series partitioning
    has to preserve the dependency structure; we keep the most recent part of the
    time as the test data. For the five observations example, we choose a sample of
    *y1*, *y2*, *y3*, as the test data. The partitioning is simple, and we will cover
    this in [Chapter 11](part0076_split_000.html#28FAO1-2006c10fab20488594398dc4871637ee
    "Chapter 11. Ensembling Time Series Models"), *Ensembling Time Series Models*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据集没有被分割！时间序列数据不能任意分割成训练和测试部分。原因很简单：如果我们有五个按时间顺序排列的观察结果 *y1*，*y2*，*y3*，*y4*，*y5*，并且我们相信影响顺序是
    *y1→y2→y3→y4→y5*，任意分割 *y1*，*y2*，*y5* 将会有不同的行为。它不会与三个连续观察结果具有相同的信息。因此，时间序列分割必须保留依赖结构；我们保留最近的部分时间作为测试数据。对于五个观察结果的例子，我们选择
    *y1*，*y2*，*y3* 作为测试数据。分割很简单，我们将在[第11章](part0076_split_000.html#28FAO1-2006c10fab20488594398dc4871637ee
    "第11章。集成时间序列模型")，*集成时间序列模型*中介绍这一点。
- en: Live testing experiments rarely yield complete observations. In reliability
    analysis, as well as survival analysis/clinical trials, the units/patients are
    observed up to a predefined time and a note is made regarding whether a specific
    event occurs, which is usually failure or death. A considerable fraction of observations
    would not have failed by the pre-decided time, and the analysis cannot wait for
    all units to fail. A reason to curtail the study might be that the time by which
    all units would have failed would be very large, and it would be expensive to
    continue the study until such a time. Consequently, we are left with incomplete
    observations; we only know that the lifetime of the units lasts for at least the
    predefined time before the study was called off, and the event of interest may
    occur sometime in the future. Consequently, some observations are censored and
    the data is referred to as censored data. Special statistical methods are required
    for the analysis of such datasets. We will give an example of these types of datasets
    next, and analyze them later, in [Chapter 10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "Chapter 10. Ensembling Survival Models"), *Ensembling Survival Models*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 实时测试实验很少能得出完整的观察结果。在可靠性分析以及生存分析/临床试验中，单位/患者会被观察到预定义的时间，并记录是否发生了特定事件，这通常是指故障或死亡。相当一部分观察结果在预定的截止时间之前并未发生故障，分析不能等待所有单位都发生故障。缩短研究的原因可能是因为所有单位都会发生故障的时间点可能非常长，并且继续研究到那时会非常昂贵。因此，我们只能得到不完整的观察结果；我们只知道在研究被取消之前，单位的寿命至少持续了预定义的时间，而感兴趣的事件可能在未来的某个时间发生。因此，一些观察结果是截断的，数据被称为截断数据。分析此类数据集需要特殊的统计方法。我们将在下一节给出这些类型数据集的例子，并在[第10章](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "第10章。集成生存模型")，*集成生存模型*中进行分析。
- en: Primary Biliary Cirrhosis
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原发性胆汁性肝硬化
- en: The `pbc` dataset from the survival package is a benchmark dataset in the domain
    of clinical trials. Mayo Clinic collected the data, which is concerned with the
    primary biliary cirrhosis (PBC) of the liver. The study was conducted between
    1974 and 1984\. More details can be found by running `pbc`, followed by `library(survival)`
    on the R terminal. Here, the main time to the event of interest is the number
    of days between registration and either death, transplantation, or study analysis
    in July 1986, and this is captured in the time variable. Similarly to a survival
    study, the events might be censored and the indicator is in the column status.
    The time to event needs to be understood, factoring in variables such as `trt`,
    `age`, `sex`, `ascites`, `hepato`, `spiders`, `edema`, `bili`, `chol`, `albumin`,
    `copper`, `alk.phos`, `ast`, `trig`, `platelet`, `protime`, and `stage`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 来自生存包的`pbc`数据集是临床试验领域的一个基准数据集。梅奥诊所收集了这些数据，它涉及肝脏的原发性胆汁性肝硬化（PBC）。该研究是在1974年至1984年之间进行的。更多详细信息可以通过在R终端上运行`pbc`，然后是`library(survival)`来找到。在这里，感兴趣事件的主要时间是注册和死亡、移植或1986年7月的研究分析之间的天数，这被捕获在时间变量中。与生存研究类似，事件可能被截尾，指示器在状态列中。需要理解事件时间，考虑到变量如`trt`、`age`、`sex`、`ascites`、`hepato`、`spiders`、`edema`、`bili`、`chol`、`albumin`、`copper`、`alk.phos`、`ast`、`trig`、`platelet`、`protime`和`stage`。
- en: The eight datasets discussed up until this point have a target variable, or
    a regressand/dependent variable, and are examples of the supervised learning problem.
    On the other hand, there are practical cases in which we simply attempt to understand
    the data and find useful patterns and groups/clusters in it. Of course, it is
    important to note that the purpose of clustering is to find an identical group
    and give it a sensible label. For instance, if we are trying to group cars based
    on their characteristics such as length, width, horsepower, engine cubic capacity,
    and so on, we may find groups that might be labeled as hatch, sedan, and saloon
    classes, while another clustering solutions might result in labels of basic, premium,
    and sports variant groups. The two main problems posed in clustering are the choice
    of the number of groups and the formation of robust clusters. We consider a simple
    dataset from the `factoextra` R package.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止讨论的八个数据集都有一个目标变量，或回归因变量，是监督学习问题的例子。另一方面，还有一些实际案例，我们只是试图理解数据，并从中找到有用的模式和组/簇。当然，重要的是要注意，聚类的目的是找到一个相同的组并给它一个合理的标签。例如，如果我们试图根据长度、宽度、马力、发动机排量等特征对汽车进行分组，我们可能会找到被标记为敞篷车、轿车和沙龙车类的组，而另一个聚类解决方案可能会导致基本、高端和运动变体组的标签。聚类中提出的主要问题是组数的选择和形成稳健的簇。我们考虑来自`factoextra`R包的一个简单数据集。
- en: Multishapes
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多形状
- en: 'The `multishapes` dataset from the `factoextra` package consists of three variables:
    `x`, `y`, and `shape`. It consists of different shapes, with each shape forming
    a cluster. Here, we have two concurrent circle shapes, two parallel rectangles/beds,
    and one cluster of points at the bottom-right. Outliers are also added across
    scatterplots. Some brief R code gives a useful display:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`factoextra`包的`multishapes`数据集包含三个变量：`x`、`y`和`shape`。它由不同的形状组成，每个形状形成一个簇。在这里，我们有两个并行的圆形形状，两个平行的矩形/床，以及右下角的一个点簇。散点图中也添加了异常值。一些简短的R代码提供了有用的显示：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Multishapes](img/00003.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![多形状](img/00003.jpeg)'
- en: 'Figure 2: Finding shapes or groups'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：寻找形状或组
- en: This dataset includes a column named shape, as it is a hypothetical dataset.
    In true clustering problems, we will have neither a cluster group indicator nor
    the visualization luxury of only two variables. Later in this book, we will see
    how ensemble clustering techniques help overcome the problems of deciding the
    number of clusters and the consistency of cluster membership.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含一个名为`shape`的列，因为它是一个假设数据集。在真正的聚类问题中，我们将没有簇组指示器，也没有仅两个变量的可视化奢侈。在这本书的后面，我们将看到集成聚类技术如何帮助克服确定簇数量和簇成员一致性的问题。
- en: Although it doesn't happen that often, frustrations can arise when fine-tuning
    different parameters, fitting different models, and other tricks all fail to find
    a useful working model. The culprit of this is often the outlier. A single outlier
    is known to wreak havoc on an otherwise potentially useful model, and their detection
    is of paramount importance. Hitherto this, the parametric and nonparametric outlier
    detections would be a matter of deep expertise. In complex scenarios, the identification
    would be an insurmountable task. A consensus on an observation being an outlier
    can be achieved using the ensemble outlier framework. To consider this, the board
    stiffness dataset will be considered. We will see how an outlier is pinned down
    in the conclusion of this book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种情况并不常见，但在微调不同参数、拟合不同模型和其他技巧都未能找到有用的工作模型时，可能会产生挫败感。这种情况的罪魁祸首通常是异常值。已知单个异常值会破坏一个本可能有用的模型，因此它们的检测至关重要。到目前为止，参数和非参数异常值检测一直是专业知识深厚的问题。在复杂场景中，识别将是一项无法逾越的任务。可以使用集成异常值框架达成对观测值是否为异常值的共识。为了考虑这一点，我们将考虑板刚度数据集。我们将在本书的结论部分看到如何确定异常值。
- en: Board Stiffness
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 板刚度
- en: 'The board stiffness dataset is available in the `ACSWR` package through the
    stiff `data.frame` stiff. The dataset consists of four measures of stiffness for
    30 boards. The first measure of stiffness is obtained by sending a shock wave
    down the board, the second measure is obtained by vibrating the board, and the
    remaining two are obtained from static tests. A quick method of identifying the
    outliers in a multivariate dataset is by using the Mahalanobis distance function.
    The further the distance an observation is from the center, the more likely it
    is that the observation will be an outlier:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 板刚度数据集可通过`ACSWR`包中的`stiff`数据框获得。该数据集包含30块板的四个刚度度量。第一个刚度度量是通过在板上发送冲击波获得的，第二个是通过振动板获得的，其余两个是通过静态测试获得的。在多元数据集中识别异常值的一种快速方法是使用马氏距离函数。观测值离中心的距离越远，该观测值成为异常值的可能性就越大：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Statistical/machine learning models
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计/机器学习模型
- en: The previous section introduced a host of problems through real datasets, and
    we will now discuss some standard model variants that are useful for dealing with
    such problems. First, we set up the required mathematical framework.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节通过真实数据集介绍了一系列问题，现在我们将讨论一些标准模型变体，这些变体有助于处理此类问题。首先，我们建立所需的数学框架。
- en: 'Suppose that we have *n* independent pairs of observations, ![Statistical/machine
    learning models](img/00004.jpeg), where ![Statistical/machine learning models](img/00005.jpeg)
    denotes the random variable of interest, also known as the *dependent variable*,
    regress and, endogenous variable, and so on. ![Statistical/machine learning models](img/00006.jpeg)
    is the associated vector of explanatory variables, or independent/exogenous variables.
    The explanatory vector will consist of *k* elements, that is, ![Statistical/machine
    learning models](img/00007.jpeg). The data realized is of the form ![Statistical/machine
    learning models](img/00008.jpeg), where ![Statistical/machine learning models](img/00009.jpeg)
    is the realized value (data) of random variable ![Statistical/machine learning
    models](img/00010.jpeg). A convention will be adapted throughout the book that
    ![Statistical/machine learning models](img/00011.jpeg), and this will take care
    of the intercept term. We assume that the observations are from the true distribution
    *F*, which is not completely known. The general regression model, including the
    classification model as well as the regression model, is specified by:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有 *n* 对独立的观测数据，![统计/机器学习模型](img/00004.jpeg)，其中![统计/机器学习模型](img/00005.jpeg)表示感兴趣的随机变量，也称为*因变量*、内生变量等。![统计/机器学习模型](img/00006.jpeg)是与解释变量相关的向量，或独立/外生变量。解释向量将包含
    *k* 个元素，即![统计/机器学习模型](img/00007.jpeg)。实现的数据形式为![统计/机器学习模型](img/00008.jpeg)，其中![统计/机器学习模型](img/00009.jpeg)是随机变量![统计/机器学习模型](img/00010.jpeg)的实现值（数据）。本书将采用一种惯例，即![统计/机器学习模型](img/00011.jpeg)，这将处理截距项。我们假设观测数据来自真实的分布
    *F*，该分布并不完全已知。一般回归模型，包括分类模型以及回归模型，由以下指定：
- en: '![Statistical/machine learning models](img/00012.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![统计/机器学习模型](img/00012.jpeg)'
- en: Here, the function *f* is an unknown function and ![Statistical/machine learning
    models](img/00013.jpeg) is the regression parameter, which captures the influence
    of ![Statistical/machine learning models](img/00014.jpeg) on ![Statistical/machine
    learning models](img/00015.jpeg). The error ![Statistical/machine learning models](img/00016.jpeg)
    is the associated unobservable error term. Diverse methods can be applied to model
    the relationship between the Ys and the `xes`. The statistical regression model
    focused on the complete specification of the error distribution ![Statistical/machine
    learning models](img/00017.jpeg), and in general the functional form would be
    linear as in ![Statistical/machine learning models](img/00018.jpeg). The function
    ![Statistical/machine learning models](img/00019.jpeg) is the link function in
    the class of generalized linear models. Nonparametric and semiparametric regression
    models are more flexible, as we don't place a restriction on the error's probability
    distribution. Flexibility would come with a price though, and here we need a much
    higher number of observations to make a valid inference, although that number
    is unspecified and is often subjective.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，函数 *f* 是一个未知函数，![统计/机器学习模型](img/00013.jpeg) 是回归参数，它捕捉了 ![统计/机器学习模型](img/00014.jpeg)
    对 ![统计/机器学习模型](img/00015.jpeg) 的影响。误差 ![统计/机器学习模型](img/00016.jpeg) 是相关的不可观测误差项。可以应用多种方法来建模
    Ys 和 `xes` 之间的关系。统计回归模型专注于误差分布 ![统计/机器学习模型](img/00017.jpeg) 的完整指定，通常函数形式会是线性的，如
    ![统计/机器学习模型](img/00018.jpeg) 所示。函数 ![统计/机器学习模型](img/00019.jpeg) 是广义线性模型类中的连接函数。非参数和半参数回归模型更加灵活，因为我们不对误差的概率分布施加限制。然而，灵活性是有代价的，在这里我们需要更多的观测数据来做出有效的推断，尽管这个数量是不确定的，并且通常是主观的。
- en: The machine learning paradigm includes some *black box* methods, and we have
    a healthy overlap between this paradigm and non- and semi-parametric models. The
    reader is also cautioned that black box does not mean unscientific in any sense.
    The methods have a firm mathematical foundation and are reproducible every time.
    Next, we quickly review some of the most important statistical and machine learning
    models, and illustrate them through the datasets discussed earlier.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习范式包括一些 *黑盒* 方法，并且这种方法与非参数和半参数模型之间存在健康的重叠。读者也应注意，黑盒并不意味着在任何意义上都是非科学的。这些方法有坚实的数学基础，并且每次都是可复制的。接下来，我们将快速回顾一些最重要的统计和机器学习模型，并通过前面讨论的数据集来展示它们。
- en: Logistic regression model
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: 'The logistic regression model is a binary classification model, and it is a
    member of the exponential family which belongs to the class of generalized linear
    models. Now, let ![Logistic regression model](img/00020.jpeg)denote the binary
    label:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型是一种二元分类模型，它是广义线性模型类中的指数族成员。现在，用 ![逻辑回归模型](img/00020.jpeg) 表示二元标签：
- en: '![Logistic regression model](img/00021.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归模型](img/00021.jpeg)'
- en: 'Using the information contained in the explanatory vector ![Logistic regression
    model](img/00022.jpeg) we are trying to build a model that will help in this task.
    The logistic regression model is the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含在解释向量 ![逻辑回归模型](img/00022.jpeg) 中的信息，我们试图构建一个有助于这项任务的模型。逻辑回归模型如下：
- en: '![Logistic regression model](img/00023.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归模型](img/00023.jpeg)'
- en: 'Here, ![Logistic regression model](img/00024.jpeg) is the vector of regression
    coefficients. Note that the logit function ![Logistic regression model](img/00025.jpeg)
    is linear in the regression coefficients and hence the name for the model is a
    logistic regression model. A logistic regression model can be equivalently written
    as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![逻辑回归模型](img/00024.jpeg) 是回归系数的向量。请注意，logit 函数 ![逻辑回归模型](img/00025.jpeg)
    在回归系数上是线性的，因此模型的名称是逻辑回归模型。逻辑回归模型可以等价地写成以下形式：
- en: '![Logistic regression model](img/00026.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归模型](img/00026.jpeg)'
- en: Here, ![Logistic regression model](img/00027.jpeg) is the binary error term
    that follows a Bernoulli distribution. For more information, refer to Chapter
    17 of Tattar, et al. (2016). The estimation of the parameters of the logistic
    regression requires the **iterative reweighted least squares** (**IRLS**) algorithm,
    and we would use the `glm` R function to get this task done. We will use the Hypothyroid
    dataset in this section. In the previous section, the training and test datasets
    and formulas were already created, and we will carry on from that point.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![逻辑回归模型](img/00027.jpeg)是遵循伯努利分布的二进制误差项。更多信息，请参阅Tattar等人（2016年）的第17章。逻辑回归参数的估计需要**迭代加权最小二乘法**（**IRLS**），我们将使用R函数`glm`来完成这项任务。在本节中，我们将使用甲状腺功能减退症数据集。在前一节中，已经创建了训练和测试数据集以及公式，我们将从那个点继续进行。
- en: Logistic regression for hypothyroid classification
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 甲状腺功能减退症分类的逻辑回归
- en: 'For the `hypothyroid` dataset, we had `HT2_Train` as the training dataset.
    The test dataset is split as the covariate matrix in `HT2_TestX` and the outputs
    of the test dataset in `HT2_TestY`, while the formula for the logistic regression
    model is available in `HT2_Formula`. First, the logistic regression model is fitted
    to the training dataset using the `glm` function and the fitted model is christened
    `LR_fit`, and then we inspect it for model fit summaries using `summary(LR_fit)`.
    The fitted model is then applied to the covariate data in the test part using
    the `predict` function to create `LR_Predict`. The predicted probabilities are
    then labeled in `LR_Predict_Bin`, and these labels are compared with the actual
    `testY_numeric` and overall accuracy is obtained:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`hypothyroid`数据集，我们使用`HT2_Train`作为训练数据集。测试数据集被分为`HT2_TestX`中的协变量矩阵和测试数据集的输出`HT2_TestY`，而逻辑回归模型的公式可在`HT2_Formula`中找到。首先，使用`glm`函数将逻辑回归模型拟合到训练数据集，并将拟合的模型命名为`LR_fit`，然后使用`summary(LR_fit)`检查模型拟合摘要。然后，使用`predict`函数将拟合的模型应用于测试部分的协变量数据以创建`LR_Predict`。然后，在`LR_Predict_Bin`中标记预测概率，并将这些标签与实际的`testY_numeric`进行比较，从而获得整体准确率：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It can be seen from the summary of the fitted GLM (the output following the
    line `summary(LR_fit)`) that we are having four significant variables `in Age`,
    `TT4`, `T4U`, and `FTI`. Using the `predict` function, we apply the fitted model
    on unknown test cases in `HT2_TestX`, compare it with the actuals, and find the
    accuracy to be 97.33%. Consequently, logistic regression is easily deployed in
    the R software.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从拟合的GLM（在`summary(LR_fit)`行之后的输出）的摘要中可以看出，我们有四个显著的变量`in Age`、`TT4`、`T4U`和`FTI`。使用`predict`函数，我们将拟合的模型应用于`HT2_TestX`中的未知测试案例，并与实际值进行比较，发现准确率为97.33%。因此，逻辑回归在R软件中很容易部署。
- en: Neural networks
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'Logistic regression might appear restricted as it allows only a linear impact
    of the covariates through the link function. The linearity assumption might not
    hold, and in most practical cases, we don''t have enough information to specify
    the functional form of the nonlinear relationship. Thus, all we know is that there
    is most likely an unknown nonlinear relationship. Neural networks are the nonlinear
    generalization of logistic regression, and this involves two important components:
    hidden neurons and learning rate. We will revise the structure of neural networks
    first.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可能看起来受限，因为它只允许通过链接函数对协变量的线性影响。线性假设可能不成立，而且在大多数实际情况下，我们没有足够的信息来指定非线性关系的函数形式。因此，我们只知道很可能存在一个未知的非线性关系。神经网络是逻辑回归的非线性推广，这涉及到两个重要组成部分：隐藏神经元和学习率。我们首先将修订神经网络的结构。
- en: 'In a neural network, the input variables are considered the first layer of
    neurons and the output the final and concluding layer of neurons. The structure
    of a neural network model can be visualized using the R package `NeuralNetTools`.
    Suppose that we have three input variables and two hidden layers, and each contains
    two hidden neurons. Here, we have a neural network with four layers. The next
    code segment gives a visualization of a neural network''s structure with three
    input variables, two hidden neurons in two hidden layers, and one output variable:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，输入变量被认为是神经元的第一个层次，输出是最终和结论性的神经元层次。神经网络模型的架构可以使用R包`NeuralNetTools`进行可视化。假设我们有三个输入变量和两个隐藏层，每个隐藏层包含两个隐藏神经元。在这里，我们有一个具有四个层次的神经网络。下面的代码段给出了一个具有三个输入变量、两个隐藏层中的两个隐藏神经元和一个输出变量的神经网络结构的可视化：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We find the R package `NeuralNetTools` very useful in visualizing the structure
    of a neural network. Neural networks built using the core R package `nnet` can
    also be visualized using the `NeuralNetTools::plotnet` function. The `plotnet`
    function sets up a neural network whose structure consists of three neurons in
    the first layer, two neurons in each of the second and third layers, and one in
    the final output layer, through the `struct` option. The weights along the arcs
    are set at zero in `rep(0,17)`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现R包`NeuralNetTools`在可视化神经网络结构方面非常有用。使用核心R包`nnet`构建的神经网络也可以使用`NeuralNetTools::plotnet`函数进行可视化。`plotnet`函数通过`struct`选项设置一个神经网络，其结构由第一层三个神经元、第二和第三层每个层两个神经元以及最终输出层的一个神经元组成。弧上的权重在`rep(0,17)`中设置为零：
- en: '![Neural networks](img/00028.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/00028.jpeg)'
- en: 'Figure 3: Structure of a neural network'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：神经网络的架构
- en: In the previous diagram, we have four layers of the neural network. The first
    layer consists of **B1** (the bias), **I1 (X1)**, **I2 (X2)**, and **I3 (X3)**.
    The second layer consists of three neurons in **B2** (the bias of the first hidden
    layer), **H1**, and **H2**. Note that the bias **B2** does not receive any input
    from the first hidden layer. Next, each neuron receives an overall input from
    each of the neurons of the previous layer, which are **B1**, **X1**, **X2**, and
    **X3** here. However, **H1** and **H2** of the first hidden layer will receive
    different aggregated input from **B1**, **X1**, **X2**, and **X3**. Appropriate
    weights are in action on each of the arcs of the network and it is the weights
    that form the parameters of the neural networks; that is, the arrival of **H1**
    (of the first layer) would be like
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个图中，我们看到了神经网络的四层。第一层由**B1**（偏差）、**I1 (X1**)、**I2 (X2**)和**I3 (X3**)组成。第二层由**B2**（第一隐藏层的偏差）、**H1**和**H2**三个神经元组成。请注意，偏差**B2**不接收来自第一隐藏层的任何输入。接下来，每个神经元从前一层的每个神经元接收整体输入，在这里是**B1**、**X1**、**X2**和**X3**。然而，第一隐藏层的**H1**和**H2**将接收来自**B1**、**X1**、**X2**和**X3**的不同聚合输入。网络中的每条弧上都作用着适当的权重，这些权重构成了神经网络的参数；也就是说，第一层**H1**的到达将类似于
- en: '![Neural networks](img/00029.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/00029.jpeg)'
- en: and the effective arrival is through a *transfer function*. A transfer function
    might be an identity function, sigmoidal function, and so on. Similarly, the arrival
    at the second neuron of the first layer is
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有效到达是通过一个*传递函数*实现的。传递函数可能是一个恒等函数、S形函数等等。同样，第一层第二个神经元的到达是
- en: '![Neural networks](img/00030.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![神经网络](img/00030.jpeg)'
- en: . By extension, **B2**, **H1**, and **H2** (of the first layer) will be the
    input for the second hidden layer, and **B3**, **H1**, and **H2** will be the
    input for the final output. At each stage of the neural network, we have weights.
    The weights need to be determined in such a manner that the difference between
    predicted output **O1** and the true **Y1** is as small as possible. Note that
    the logistic regression is a particular case of the neural network as can be seen
    by directly removing all hidden layers and input layer leads in the output one.
    The neural network will be fitted for the hypothyroid problem.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 。通过扩展，**B2**、**H1**和**H2**（第一层）将成为第二隐藏层的输入，**B3**、**H1**和**H2**将成为最终输出的输入。在神经网络的每个阶段，我们都有权重。这些权重需要以这种方式确定，即预测输出**O1**与真实**Y1**之间的差异尽可能小。请注意，逻辑回归是神经网络的一个特例，可以通过直接移除所有隐藏层和输入层来看到，这会导致输出层。神经网络将被拟合到甲状腺功能减退问题。
- en: Neural network for hypothyroid classification
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 甲状腺功能减退分类的神经网络
- en: 'We use the `nnet` function from the package of the same name to set up the
    neural network for the hypothyroid classification problem. The formula, training,
    and test datasets continue as before. The accuracy calculation follows along similar
    lines to the segment in logistic regression. The fitted neural network is visualized
    using the `plotnet` graphical function from the `NeuralNetTools` package:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用同名的`nnet`函数来设置甲状腺功能减退分类问题的神经网络。公式、训练集和测试集继续与之前相同。准确度计算遵循与逻辑回归中类似的步骤。拟合的神经网络使用`NeuralNetTools`包中的`plotnet`图形函数进行可视化：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, the accuracy is 98.27%, which is an improvement on the logistic regression
    model. The visual display of the fitted model is given in the following diagram.
    We have fixed the seed for the random initialization of the neural network parameters
    at `12345`, using `set.seed(12345)`, so that the results are reproducible at the
    reader''s end. This is an interesting case for ensemble modeling. Different initial
    seeds – which the reader can toy around with – will lead to different accuracies.
    Sometimes, you will get an accuracy lower than any of the models considered in
    this section, and at other times you will get the highest accuracy. The choice
    of seed as arbitrary leads to the important question of which solution is useful.
    Since the seeds are arbitrary, the question of a good seed or a bad seed does
    not arise. In this case, if a model is giving you a higher accuracy, it does not
    necessarily mean anything:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，准确度为98.27%，这比逻辑回归模型有所提高。拟合模型的视觉展示如下图所示。我们使用`set.seed(12345)`将神经网络参数的随机初始化固定在`12345`，以确保结果在读者端可重复。这是一个有趣的集成建模案例。不同的初始种子（读者可以尝试）会导致不同的准确度。有时，你可能会得到低于本节中考虑的任何模型的准确度，而在其他时候，你可能会得到最高的准确度。种子选择的任意性导致了一个重要的问题：哪种解决方案是有用的。由于种子是任意的，因此不会出现好种子或坏种子的问题。在这种情况下，如果一个模型给你更高的准确度，这并不一定意味着什么：
- en: '![Neural network for hypothyroid classification](img/00031.jpeg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![甲状腺功能减退症分类的神经网络](img/00031.jpeg)'
- en: 'Figure 4: Neural network for the hypothyroid classification'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：甲状腺功能减退症分类的神经网络
- en: Naïve Bayes classifier
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: The naïve Bayes classifier is a simplistic implementation based on the Bayes
    formula. It is based on simple empirical and conditional probabilities, as evidenced
    in the actual data. Beyond the simplest assumption of observation independence,
    we don't have any restrictions in using this model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯公式的朴素贝叶斯分类器是一种简单的实现。它基于简单的经验概率和条件概率，这在实际数据中得到了体现。除了观察独立性的最简单假设之外，我们在使用此模型时没有任何限制。
- en: Naïve Bayes for hypothyroid classification
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于甲状腺功能减退症分类的朴素贝叶斯
- en: 'A naïve Bayes classifier is fit using the `naiveBayes` function from the `e1071`
    R package. The prediction and accuracy assessment is carried out using two functions,
    `predict` and `sum`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器使用来自`e1071` R包的`naiveBayes`函数进行拟合。预测和准确度评估是通过两个函数`predict`和`sum`完成的：
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The accuracy of the naïve Bayes classifier is 97.33%, which is the same as the
    logistic regression model and less than the one provided by the neural network.
    We remark here that it is only a coincidence that the accuracy of this method
    and logistic regression is the same.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器的准确度为97.33%，与逻辑回归模型相同，但低于神经网络提供的准确度。我们在此指出，这种方法和逻辑回归的准确度相同只是一个巧合。
- en: Decision tree
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Breiman and Quinlan mainly developed decision trees, which have evolved a lot
    since the 1980s. If the dependent variable is continuous, the decision tree will
    be a regression tree and if it is categorical variable, it will be a classification
    tree. Of course, we can have a survival tree as well. Decision trees will be the
    main model that will be the beneficiary of the ensemble technique, as will be
    seen throughout the book.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 布莱曼和奎南主要开发了决策树，自20世纪80年代以来，决策树已经发展了很多。如果因变量是连续的，决策树将是一个回归树；如果是分类变量，它将是一个分类树。当然，我们也可以有生存树。决策树将是受益于集成技术的模型，这一点将在整本书中看到。
- en: 'Consider the regression tree given in the following diagram. We can see that
    there are three input variables, which are ![Decision tree](img/00032.jpeg), and
    the output variable is *Y*. Strictly speaking, a decision tree will not display
    all the variables used to build the tree. In this tree structure, a decision tree
    is conventionally displayed upside down. We have four terminal nodes. If the condition
    ![Decision tree](img/00033.jpeg) is satisfied, we move to the right side of the
    tree and conclude that the average *Y* value is 40\. If the condition is not satisfied,
    we move to the left, and check whether ![Decision tree](img/00034.jpeg). If this
    condition is not satisfied, we move to the left side of the tree and conclude
    that the average *Y* value is 100\. Upon the satisfactory meeting of this condition,
    we move to the right side and then if the categorical variable ![Decision tree](img/00035.jpeg),
    the average *Y* value would be 250, or 10 otherwise. This decision tree can be
    captured in the form of an equation too, as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图表中给出的回归树。我们可以看到有三个输入变量，即 ![决策树](img/00032.jpeg)，输出变量是 *Y*。严格来说，决策树不会显示构建树时使用的所有变量。在这个树结构中，决策树传统上是倒置显示的。我们有四个终端节点。如果满足条件
    ![决策树](img/00033.jpeg)，我们移动到树的右侧，并得出结论，平均 *Y* 值为 40。如果不满足条件，我们移动到左侧，并检查是否满足 ![决策树](img/00034.jpeg)。如果不满足这个条件，我们移动到树的左侧，并得出结论，平均
    *Y* 值为 100。在满足这个条件后，我们移动到右侧，然后如果满足分类变量 ![决策树](img/00035.jpeg)，平均 *Y* 值将是 250，否则是
    10。这个决策树也可以用以下方程表示：
- en: '![Decision tree](img/00036.jpeg)![Decision tree](img/00037.jpeg)![Decision
    tree](img/00038.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/00036.jpeg)![决策树](img/00037.jpeg)![决策树](img/00038.jpeg)'
- en: 'Figure 5: Regression tree'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：回归树
- en: The statistician Terry Therneau developed the `rpart` R package.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学家 Terry Therneau 开发了 `rpart` R 包。
- en: Decision tree for hypothyroid classification
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 甲状腺功能减退分类的决策树
- en: 'Using the `rpart` function from the `rpart` package, we build a classification
    tree for the same formula as the earlier partitioned data. The constructed tree
    can be visualized using the plot function, and the variable name is embossed on
    the tree with the text function. The equation of the fitted classification tree
    (see Figure *Classification Tree for Hypothyroid*) is the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `rpart` 包中的 `rpart` 函数，我们为之前分区的数据构建了相同的分类树。该树可以使用绘图函数进行可视化，变量名可以用文本函数刻在树上。拟合的分类树（见图
    *甲状腺功能减退分类的决策树*）的方程如下：
- en: '![Decision tree for hypothyroid classification](img/00039.jpeg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![甲状腺功能减退分类的决策树](img/00039.jpeg)'
- en: 'Prediction and accuracy is carried out in a similar way as mentioned earlier:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 预测和准确度执行的方式与之前提到的类似：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Decision tree for hypothyroid classification](img/00040.jpeg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![甲状腺功能减退分类的决策树](img/00040.jpeg)'
- en: 'Figure 6: Classification tree for Hypothyroid'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：甲状腺功能减退分类树
- en: Consequently, the classification tree gives an accuracy of 98.74%, which is
    the best of the four models considered thus far. Next, we will consider the final
    model, support vector machines.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分类树给出了 98.74% 的准确度，是目前考虑的四个模型中最好的。接下来，我们将考虑最终的模型，即支持向量机。
- en: Support vector machines
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持向量机
- en: '**Support vector machines**, abbreviated popularly as **SVM**, are an important
    class of machine learning techniques. Theoretically, SVM can take an infinite
    number of features/covariates and build the appropriate classification or regression
    SVMs.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**，通常简称为 **SVM**，是机器学习的一个重要类别。理论上，SVM 可以处理无限数量的特征/协变量，并构建适当的分类或回归 SVM。'
- en: SVM for hypothyroid classification
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于甲状腺功能减退分类的支持向量机（SVM）
- en: 'The `svm` function from the `e1071` package will be useful for building an
    `SVM` classifier on the Hypothyroid dataset. Following the usual practice, we
    have the following output in the R session:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `e1071` 包的 `svm` 函数将有助于在甲状腺功能减退数据集上构建 `SVM` 分类器。按照惯例，我们在 R 会话中得到了以下输出：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The SVM technique gives us an accuracy of 98.43%, which is the second best of
    the models set up thus far.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 技术为我们提供了 98.43% 的准确度，是目前设置模型中的第二好。
- en: In the next section, we will run each of the five classification models for
    the Waveform, German Credit, Iris, and Pima Indians Diabetes problem datasets.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将为波形、德国信贷、鸢尾花和皮马印第安人糖尿病问题数据集的五个分类模型分别运行。
- en: The right model dilemma!
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正确模型困境！
- en: 'In the previous section, we ran five classification models for the `Hypothyroid`
    dataset. Here, the task is to repeat the exercise for four other datasets. It
    would be a very laborious task to change the code in the appropriate places and
    repeat the exercise four times over. Thus, to circumvent this problem, we will
    create a new function referred to as `Multiple_Model_Fit`. This function will
    take four arguments: `formula`, `train`, `testX`, and `testY`. The four arguments
    have already been set up for each of the five datasets. The function is then set
    up in a way that generalizes the steps of the previous section for each of the
    five models.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们对`Hypothyroid`数据集运行了五个分类模型。这里，任务是重复对其他四个数据集进行同样的练习。改变适当位置的代码并重复四次将是一项非常繁琐的任务。因此，为了避免这个问题，我们将创建一个新的函数，称为`Multiple_Model_Fit`。这个函数将接受四个参数：`formula`、`train`、`testX`和`testY`。这四个参数已经为五个数据集中的每一个设置好了。然后，该函数被设置成以概括上一节中每个模型的步骤。
- en: 'The function proceeds to create a matrix whose first column consists of the
    model name, while the second column consists of the accuracy. This matrix is returned
    as the output of this function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 函数继续创建一个矩阵，其第一列包含模型名称，第二列包含准确率。这个矩阵作为该函数的输出返回：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`Multiple_Model_Fit` is now applied to the `Hypothyroid` dataset, and the results
    can be seen to be in agreement with the previous section:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`Multiple_Model_Fit`现在应用于`Hypothyroid`数据集，结果可以看到与上一节一致：'
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `Multiple_Model_Fit` function is then applied to the other four classification
    datasets:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`Multiple_Model_Fit`函数应用于其他四个分类数据集：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The results for each of the datasets are summarized in the following table:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集的结果总结在下表中：
- en: '![The right model dilemma!](img/00041.jpeg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![正确的模型困境！](img/00041.jpeg)'
- en: 'Table 1: Accuracy of five models for five datasets'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：五个数据集五个模型的准确率
- en: The `iris` dataset is a straightforward and simplistic problem, and therefore
    each of the five models gives us 100% accuracy on the test data. This dataset
    will not be pursued any further.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`iris`数据集是一个简单直接的问题，因此五个模型在测试数据上都能给出100%的准确率。这个数据集将不再进一步研究。'
- en: For each dataset, we highlight the highest accuracy cell in grey, and highlight
    the next highest in yellow.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据集，我们用灰色突出显示最高准确率的单元格，并用黄色突出显示次高准确率的单元格。
- en: Here is the modeling dilemma. The naïve Bayes method turns out the best for
    the `German` and `Pima Indian Diabetes` datasets. The decision tree gives the
    highest accuracy for the `Hypothyroid` dataset, while SVM gives the best results
    for `Waveform`. The runner-up place is secured twice by logistic regression and
    twice by SVM. However, we also know that, depending on the initial seeds and maybe
    the number of hidden neurons, the neural networks are also expected to perform
    the best for some datasets. We then also have to consider whether the results
    will turn out differently for different partitions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是建模困境。朴素贝叶斯方法对于`German`和`Pima Indian Diabetes`数据集表现最佳。决策树对`Hypothyroid`数据集给出了最高的准确率，而SVM对`Waveform`给出了最佳结果。逻辑回归和SVM各两次获得了第二名。然而，我们也知道，根据初始种子和可能隐藏神经元的数量，神经网络也预期在某些数据集上表现最佳。我们还得考虑结果是否会因不同的分区而有所不同。
- en: It is in such practical scenarios we would prefer to have a single approach
    that ensures reasonable properties. With the `Hypothyroid` dataset, the accuracy
    for each of the models is 97% or higher, and one might not go wrong with any of
    the models. However, in the `German` and `Pima Indian Diabetes` problems, the
    maximum accuracy is 80% and 78%, respectively. It would then be better if we can
    make good use of all the models and build a single unified one with increased
    accuracy.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的实际场景中，我们更希望有一个单一的方法来确保合理的属性。对于`Hypothyroid`数据集，每个模型的准确率都是97%或更高，因此使用任何模型都不会出错。然而，在`German`和`Pima
    Indian Diabetes`问题上，最大准确率分别是80%和78%。因此，如果我们能充分利用所有模型并构建一个准确率更高的单一统一模型会更好。
- en: An ensemble purview
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成概述
- en: 'The `caret` R package is core to ensemble machine learning methods. It provides
    a large framework and we can also put different statistical and machine learning
    models together to create an ensemble. For the recent version of the package on
    the author''s laptop, the `caret` package provides access to the following models:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`caret` R包是集成机器学习方法的基石。它提供了一个大型的框架，我们还可以将不同的统计和机器学习模型组合起来创建集成。对于作者笔记本电脑上该包的最新版本，`caret`包提供了以下模型的访问权限：'
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You need to key in the number `1` and continue. The package will be installed
    and loaded, and the program will continue. It is good to know the host of options
    for ensemble methods. A brief method for stack ensembling analytical models is
    provided here, and the details will unfold later in the book.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要输入数字`1`并继续。包将被安装和加载，程序将继续。了解集成方法的众多选项是很好的。这里提供了一个堆叠集成分析模型的简要方法，详细内容将在本书的后续部分展开。
- en: 'For the `Hypothyroid` dataset, we had a high accuracy of an average of 98%
    between the five models. The `Waveform` dataset saw an average accuracy of approximately
    88%, while the average for `German` Credit data is 75%. We will try to increase
    the accuracy for this dataset. The accuracy improvement will be attempted using
    three models: naïve Bayes, logistic regression, and classification tree. First,
    we need to partition the data into three parts: train, test, and stack:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`Hypothyroid`数据集，五个模型之间的平均准确率高达98%。`Waveform`数据集的平均准确率约为88%，而`German` Credit数据集的平均准确率为75%。我们将尝试提高这个数据集的准确率。将尝试使用三个模型：朴素贝叶斯、逻辑回归和分类树来提高准确率。首先，我们需要将数据分为三部分：训练、测试和堆叠：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The model will be built on the training data first and accuracy will be assessed
    using the metric of Area Under Curve, the curve being the ROC. The control parameters
    will be set up first and the three models, naïve Bayes, classification tree, and
    logistic regression, will be created using the training dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将首先建立在训练数据上，并使用曲线下的面积（ROC曲线）来评估准确性。首先设置控制参数，然后使用训练数据集创建三个模型：朴素贝叶斯、分类树和逻辑回归：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Predictions for the test and stack blocks are carried out next. We store the
    predicted probabilities along the test and stack data frames:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来进行测试和堆叠块的预测。我们将预测概率存储在测试和堆叠数据框中：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The ROC is an important measure for model assessments. The higher the area
    under the ROC, the better the model would be. Note that these measures, or any
    other measure, will not be the same as the models fitted earlier since the data
    has changed:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ROC（受试者工作特征曲线）是模型评估的重要指标。ROC曲线下的面积越大，模型越好。请注意，这些指标或任何其他指标都不会与之前拟合的模型相同，因为数据已经发生了变化：
- en: '[PRE26]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the `test` dataset, we can see that the area under curve for the naïve
    Bayes, classification tree, and logistic regression are respectively `0.7543`,
    `0.6777`, and `0.7446`. If we put the predicted values together in some format,
    and that leads to an increase in the accuracy, the purpose of the ensemble technique
    has been accomplished. As such, we consider the new predicted probabilities under
    the three models and append them to the stacked data frame. These three columns
    will now be treated as new input vectors. We then build a naïve Bayes model, an
    arbitrary choice, and you can try any other model (not necessarily restricted
    to one of these) for the stacked data frame. The AUC can then be predicted and
    calculated:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`test`数据集，我们可以看到朴素贝叶斯、分类树和逻辑回归的曲线下面积分别为`0.7543`、`0.6777`和`0.7446`。如果我们以某种格式将预测值组合在一起，并且这导致准确性的提高，那么集成技术的目的就达到了。因此，我们考虑三个模型下的新预测概率，并将它们附加到堆叠数据框中。这三个列现在将被视为新的输入向量。然后我们构建一个朴素贝叶斯模型，这是一个任意的选择，您可以为堆叠数据框尝试任何其他模型（不一定是这些中的任何一个）。然后可以预测并计算AUC：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The AUC for the stacked data observations is higher than any of the earlier
    models, which is an improvement.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠数据观察的AUC高于之前的任何模型，这是一个改进。
- en: A host of questions should arise for the critical thinker. Why should this technique
    work? Will it lead to improvisations under all possible cases? If yes, will simply
    adding new model predictions lead to further improvements? If no, how does one
    pick the base models so that we can be reasonably assured of improvisations? What
    are the restrictions on the choice of models? We will provide solutions to most
    of these questions throughout this book. In the next section, we will quickly
    look at some useful statistical tests that will aid the assessment of model performance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批判性思考者来说，应该会涌现出许多问题。这项技术为什么能起作用？它是否会在所有可能的情况下导致即兴发挥？如果是的话，仅仅添加新的模型预测是否会带来进一步的改进？如果不是，我们如何选择基础模型以确保我们可以合理地确信即兴发挥？模型选择有哪些限制？我们将在本书中提供对这些问题的多数解决方案。在下一节中，我们将快速查看一些有用的统计检验，这些检验将有助于评估模型性能。
- en: Complementary statistical tests
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 补充统计检验
- en: Here, a model is selected over another plausible one. The accuracy of one model
    seems higher than the other. The **area under curve** (**AUC**) of the ROC of
    a model is greater than that of another. However, it is not appropriate to base
    the conclusion on pure numbers only. It is important to conclude whether the numbers
    hold significance from the point of view of statistical inference. In the analytical
    world, it is pivotal that we make use of statistical tests whenever they are available
    to validate claims/hypotheses. A reason for using statistical tests is that probability
    can be highly counterintuitive, and what appears on the surface might not be the
    case upon closer inspection, after incorporating the chance variation. For instance,
    if a fair coin is tossed 100 times, it is imprudent to think that the number of
    heads must be exactly 50\. Hence, if a fair coin shows up 45 heads, we need to
    incorporate the chance variation that the number of heads can be less than 50
    too. Caution must be exerted all the while when we are dealing with uncertain
    data. A few examples are in order here. Two variables might appear to be independent
    of each other, and the correlation might also be nearly equal to zero. However,
    applying the correlation test might result in the conclusion that the correlation
    is not significantly zero. Since we will be sampling and resampling a lot in this
    text, we will look at related tests.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，一个模型被选为另一个可能的模型。一个模型的准确性似乎高于另一个。该模型的ROC曲线下的面积（**AUC**）大于另一个。然而，仅基于纯数字得出结论是不合适的。从统计推断的角度来看，重要的是要得出这些数字是否具有统计学意义的结论。在分析世界中，当我们有机会时，使用统计测试来验证主张/假设是至关重要的。使用统计测试的一个原因是概率可能非常反直觉，表面上看起来可能的情况，在仔细检查并考虑到机会变化后可能并不成立。例如，如果一枚公平的硬币抛掷100次，认为正面数必须是正好50是不明智的。因此，如果一枚公平的硬币出现45次正面，我们需要考虑到正面数可能少于50的机会变化。在处理不确定数据时，我们必须始终保持谨慎。以下是一些例子。两个变量可能看起来彼此独立，相关系数也可能几乎等于零。然而，应用相关测试可能会得出结论，相关系数并不显著为零。由于我们将在本文中大量进行抽样和重抽样，我们将查看相关的测试。
- en: Permutation test
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排列测试
- en: Suppose that we have two processes, A and B, and the variances of these two
    processes are known to be equal, though unknown. Three independent observations
    from process A result in yields of 18, 20, and 22, while three independent observations
    from process B gives yields of 24, 26, and 28\. Under the assumption that the
    yield follows a normal distribution, we would like to test whether the means of
    processes A and B are the same. This is a suitable case for applying the t-test,
    since the number of observations is smaller. An application of the `t.test` function
    shows that the two means are different to each other, and this intuitively appears
    to be the case.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有两个过程，A和B，这两个过程的方差已知是相等的，尽管是未知的。从过程A的三个独立观察结果产生了18、20和22的产量，而从过程B的三个独立观察结果产生了24、26和28的产量。在假设产量遵循正态分布的情况下，我们想测试过程A和B的均值是否相同。这是一个适合应用t检验的情况，因为观察的数量较少。`t.test`函数的应用表明，两个均值彼此不同，这直观上似乎是正确的。
- en: Now, the assumption under the null hypothesis is that the means are equal, and
    that the variance is unknown and assumed to be equal under the two processes.
    Consequently, we have a genuine reason to believe that the observations from process
    A might well have occurred in process B too, and vice versa. We can therefore
    swap one observation in process B with process A, and recompute the t-test. The
    process can be repeated for all possible permutations of the two samples. In general,
    if we have m samples from population 1 and n samples from population 2, we can
    have
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在零假设下的假设是均值相等，方差在两个过程中未知且假设相等。因此，我们有充分的理由相信过程A的观察结果也可能发生在过程B中，反之亦然。因此，我们可以将过程B中的一个观察结果与过程A交换，并重新计算t检验。这个过程可以针对两个样本的所有可能的排列重复进行。一般来说，如果我们有来自总体1的m个样本和来自总体2的n个样本，我们可以有
- en: '![Permutation test](img/00043.jpeg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![排列测试](img/00043.jpeg)'
- en: different samples and as many tests. An overall test can be based on such permutation
    samples and such tests are called **permutation tests**.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的样本和尽可能多的测试。一个综合测试可以基于这样的排列样本和这样的测试被称为**排列测试**。
- en: 'For process A and B observations, we will first apply the t-test and then the
    permutation test. The `t.test` is available in the core `stats` package and the
    permutation t-test is taken from the `perm` package:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于过程A和B的观察，我们首先将应用t检验，然后是排列检验。`t.test`可在核心`stats`包中找到，而排列t检验则来自`perm`包：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The smaller p-value suggests that the means of processes A and B are not equal.
    Consequently, we now apply the permutation test `permTS` from the `perm` package:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的p值表明过程A和B的均值不相等。因此，我们现在应用来自`perm`包的排列检验`permTS`：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The p-value is now at 0.1, which means that the permutation test concludes
    that the means of the processes are equal. Does this mean that the permutation
    test will always lead to this conclusion, contradicting the t-test? The answer
    is given in the next code segment:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的p值为0.1，这意味着排列检验得出结论，过程的均值是相等的。这难道意味着排列检验将始终得出这个结论，与t检验相矛盾？答案是下一段代码给出的：
- en: '[PRE30]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Chi-square and McNemar test
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卡方检验和McNemar检验
- en: 'We had five models for the hypothyroid test. We then calculated the accuracy
    and were satisfied with the numbers. Let''s first look at the number of errors
    that the fitted model makes. We have 636 observations in the test partition and
    42 of them test positive for the hypothyroid problem. Note that if we mark all
    the patients as negative, we would be getting an accuracy of *1-42/636 = 0.934*,
    or about 93.4%. Using the table function, we pit the actuals against the predicted
    values and see how often the fitted model goes wrong. We remark here that identifying
    the hypothyroid cases as the same and the negative cases as negative is the correct
    prediction, while marking the hypothyroid case as negative and vice versa leads
    to errors. For each model, we look at the misclassification errors:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对甲状腺功能减退测试有五个模型。然后我们计算了准确度，并对这些数字感到满意。让我们首先看看拟合模型犯的错误数量。测试分区中有636个观察值，其中42个测试呈甲状腺功能减退阳性。请注意，如果我们将所有患者标记为阴性，我们将得到准确度为*1-42/636
    = 0.934*，即约93.4%。使用表格函数，我们将实际值与预测值进行比较，看看拟合模型出错有多频繁。我们在此指出，将甲状腺功能减退病例识别为相同，将阴性病例识别为阴性是正确的预测，而将甲状腺功能减退病例标记为阴性，反之亦然会导致错误。对于每个模型，我们查看误分类错误：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From the misclassification table, we can see that the neural network identifies
    41 out of the 42 cases of hypothyroid correctly, but it identifies way more cases
    of hypothyroid incorrectly too. The question that arises is whether the correct
    predictions of the fitted models only occur by chance, or whether they depend
    on truth and can be explained. To test this, in the hypotheses framework we would
    like to test whether the actuals and predicted values of the actuals are independent
    of or dependent on each other. Technically, the null hypothesis is that the prediction
    is independent of the actual, and if a model explains the truth, the null hypothesis
    must be rejected. We should conclude that the fitted model predictions depend
    on the truth. We deploy two solutions here, the chi-square test and the McNemar
    test:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从错误分类表中，我们可以看到神经网络正确地识别了42个甲状腺功能减退病例中的41个，但它也错误地识别了更多的甲状腺功能减退病例。由此产生的问题是，拟合模型的正确预测是否只是偶然发生，或者它们是否依赖于真实情况并可解释。为了测试这一点，在假设框架中，我们想测试实际值和预测值是否相互独立或相互依赖。技术上，零假设是预测与实际值无关，如果一个模型解释了真实情况，则必须拒绝零假设。我们应该得出结论，拟合模型的预测依赖于真实情况。在这里，我们部署了两种解决方案，即卡方检验和McNemar检验：
- en: '[PRE32]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The answer provided by the chi-square tests clearly shows that the predictions
    of each fitted model is not down to chance. It also shows that the prediction
    of hypothyroid cases, as well as the negative cases, is expected of the fitted
    models. The interpretation of and conclusions from the McNemar's test is left
    to the reader. The final important measure in classification problems is the ROC
    curve, which is considered next.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 卡方检验提供的答案清楚地表明，每个拟合模型的预测并非偶然。它还表明，拟合模型对甲状腺功能减退病例以及阴性病例的预测是预期的。McNemar检验的解释和结论留给读者。在分类问题中，最重要的度量标准是ROC曲线，接下来我们将讨论它。
- en: ROC test
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ROC检验
- en: The ROC curve is an important improvement on the false positive and true negative
    measures of model performance. For a detailed explanation, refer to Chapter 9
    of Tattar et al. (2017). The ROC curve basically plots the true positive rate
    against the false positive rate, and we measure the AUC for the fitted model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线是对模型性能的假阳性率和真阴性率的重要改进。有关详细解释，请参阅Tattar等人（2017年）的第9章。ROC曲线基本上是绘制真阳性率与假阳性率的关系图，我们测量拟合模型的AUC。
- en: 'The main goal that the ROC test attempts to achieve is the following. Suppose
    that Model 1 gives an AUC of 0.89 and Model 2 gives 0.91\. Using the simple AUC
    criteria, we outright conclude that Model 2 is better than Model 1\. However,
    an important question that arises is whether 0.91 is significantly higher than
    0.89\. The `roc.test`, from the `pROC` R package, provides the answer here. For
    the neural network and classification tree, the following R segment gives the
    required answer:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ROC测试试图实现的主要目标是以下内容。假设模型1给出AUC为0.89，模型2给出0.91。使用简单的AUC标准，我们直接得出结论，模型2优于模型1。然而，一个重要的问题是，0.91是否显著高于0.89。来自`pROC`R包的`roc.test`提供了答案。对于神经网络和分类树，以下R段提供了所需的答案：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Since the p-value is very large, we conclude that the AUC for the two models
    is not significantly different.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于p值非常大，我们得出结论，两个模型的AUC没有显著差异。
- en: Statistical tests are vital and we recommend that they be used whenever suitable.
    The concepts highlighted in this chapter will be drawn on in more detail in the
    rest of the book.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 统计测试至关重要，我们建议在合适的情况下使用它们。本章中强调的概念将在本书的其余部分进行更详细的阐述。
- en: Summary
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The chapter began with an introduction to some of the most important datasets
    that will be used in the rest of the book. The datasets covered a range of analytical
    problems including classification, regression, time series, survival, clustering,
    and a dataset in which identifying an outlier is important. Important families
    of classification models were then introduced in the statistical/machine learning
    models section. Following the introduction of a variety of models, we immediately
    saw the shortcoming, in that we don't have a model for all seasons. Model performance
    varies from dataset to dataset. Depending on the initialization, the performance
    of certain models (such as neural networks) is affected. Consequently, there is
    a need to find a way to ensure that the models can be improved upon in most scenarios.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以介绍本书其余部分将使用的一些最重要的数据集开始。这些数据集涵盖了包括分类、回归、时间序列、生存、聚类以及识别异常值很重要的数据集在内的各种分析问题。然后在统计/机器学习模型部分介绍了重要的分类模型家族。在介绍了各种模型后，我们立即看到了不足之处，即我们没有适用于所有季节的模型。模型性能因数据集而异。根据初始化的不同，某些模型（如神经网络）的性能受到影响。因此，有必要找到一种方法来确保模型可以在大多数情况下得到改进。
- en: This paves the way for the ensemble method, which forms the title of this book.
    We will elaborate on this method in the rest of the book. This chapter closed
    with quick statistical tests that will help in carrying out model comparisons.
    Resampling forms the core of ensemble methods, and we will look at the important
    jackknife and bootstrap methods in the next chapter.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这为集成方法铺平了道路，这也是本书的标题。我们将在本书的其余部分详细阐述这种方法。本章以快速统计测试结束，这些测试有助于进行模型比较。重采样是集成方法的核心，我们将在下一章探讨重要的Jackknife和Bootstrap方法。
