- en: Chapter 1. Introduction to Ensemble Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble techniques are model output aggregating techniques that have evolved
    over the past decade and a half in the area of statistical and machine learning.
    This forms the central theme of this book. Any user of statistical models and
    machine learning tools will be familiar with the problem of building a model and
    the vital decision of choosing among potential candidate models. A model's accuracy
    is certainly not the only relevant criterion; we are also concerned with its complexity,
    as well as whether or not the overall model makes practical sense.
  prefs: []
  type: TYPE_NORMAL
- en: Common modeling problems include the decision to choose a model, and various
    methodologies exist to aid this task. In statistics, we resort to measures such
    as **Akaike Information Criteria** (**AIC**) and **Bayesian Information Criteria**
    (**BIC**), and on other fronts, the p-value associated with the variable in the
    fitted model helps with the decision. This is a process generally known as **model
    selection**. Ridge penalty, Lasso, and other statistics also help with this task.
    For machine learning models such as neural networks, decision trees, and so on,
    a k-fold cross-validation is useful when the model is built using a part of the
    data referred to as training data, and then accuracy is looked for in the untrained
    area or validation data. If the model is sensitive to its complexity, the exercise
    could be futile.
  prefs: []
  type: TYPE_NORMAL
- en: The process of obtaining the *best* model means that we create a host of other
    models, which are themselves nearly as efficient as the best model. Moreover,
    the best model accurately covers the majority of samples, and other models might
    accurately assess the variable space region where it is inaccurate. Consequently,
    we can see that the final shortlisted model has few advantages over the runner
    up. The next models in line are not so poor as to merit outright rejection. This
    makes it necessary to find a way of taking most of the results already obtained
    from the models and combining them in a meaningful way. The search for a method
    for putting together various models is the main objective of ensemble learning.
    Alternatively, one can say that ensemble learning transforms competing models
    into collaborating models. In fact, ensemble techniques are not the end of the
    modeling exercise, as they will also be extended to the unsupervised learning
    problems. We will demonstrate an example that justifies the need for this.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of ensemble methods would have been impossible without the
    invention of modern computational power. Statistical methods foresaw techniques
    that required immense computations. Methods such as permutation tests and jackknife
    are evidence of the effectiveness of computational power. We will undertake an
    exercise to learn these later in the chapter, and we will revisit them later on
    in the book.
  prefs: []
  type: TYPE_NORMAL
- en: From a machine learning perspective, *supervised* and *unsupervised* are the
    two main types of learning technique. **Supervised learning** is the arm of machine
    learning, the process in which a certain variable is known, and the purpose is
    to understand this variable through various other variables. Here, we have a target
    variable. Since learning takes place with respect to the output variable, supervised
    learning is sometimes referred to as learning with a teacher. All target variables
    are not alike, and they often fall under one of the following four types. If the
    goal is to classify observations into one of *k* types of class (for example,
    Yes/No, Satisfied/Dissatisfied), then we have a classification problem. Such a
    variable is referred to as a *categorical variable* in statistics. It is possible
    that the variable of interest might be a continuous variable, which is numeric
    from a software perspective. This may include car mileage per liter, a person's
    income, or a person's age. For such scenarios, the purpose of the machine learning
    problem is to learn the variables in terms of other associated variables, and
    then predict it for unknown cases in which only the values of associated variables
    are available. We will broadly refer to this class of problem as a **regression
    problem**.
  prefs: []
  type: TYPE_NORMAL
- en: In clinical trials, the time to event is often of interest. When an illness
    is diagnosed, we would ask whether the proposed drug is an improvement on the
    existing one. While the variable in question here is the length of time between
    diagnosis and death, clinical trial data poses several other problems. The analysis
    cannot wait until all the patients have died, and/or some of the patients may
    have moved away from the study, making it no longer possible to know their status.
    Consequently, we have censored data. As part of the study observations, complete
    information is not available. Survival analysis largely deals with such problems,
    and we will undertake the problem of creating ensemble models here.
  prefs: []
  type: TYPE_NORMAL
- en: With classification, regression, and survival data, it may be assumed that that
    the instances/observations are independent of each other. This is a very reasonable
    assumption in that there is a valid reason to believe that patients will respond
    to a drug independently of other patients, a customer will churn or pay the loan
    independently of other customers, and so forth. In yet another important class
    of problems, this assumption is not met, and we are left with observations depending
    on each other via time series data. An example of time series data is the closure
    stock exchange points of a company. Clearly, the performance of a company's stock
    can't be independent each day, and thus we need to factor in dependency.
  prefs: []
  type: TYPE_NORMAL
- en: In many practical problems, the goal is to understand patterns or find groups
    of observations, and we don't have a specific variable of interest with regard
    to which algorithm needs to be trained. Finding groups or clusters is referred
    to as unsupervised learning or learning without a teacher. Two main practical
    problems that arise in finding clusters is that (i) it is generally not known
    in advance how many clusters are in the population, and (ii) different choices
    of initial cluster centers lead to different solutions. Thus, we need a solution
    that is free from, or at least indifferent to, initialization and takes the positives
    of each useful solution into consideration. This will lead us toward unsupervised
    ensemble techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The search for the best models, supervised or unsupervised, is often hindered
    by the presence of outliers. The presence of a single outlier is known to heavily
    influence the overall fit of linear models, and it is also known to significantly
    impact even nonlinear models. Outlier detection is a challenge in itself, and
    a huge body of statistical methods help in identifying outliers. A host of machine
    learning methods also help in identifying outliers. Of course, ensembles will
    help here, and we will develop R programs that will help solve the problem of
    identifying outliers. This method will be referred to as outlier ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: At the outset, it is important that the reader becomes familiar with the datasets
    used in this book. All major datasets will be introduced in the first section.
    We begin the chapter with a brief introduction to the core statistical/machine
    learning models and put them into action immediately afterward. It will quickly
    become apparent that there is not a single class of model that would perform better
    than any other model. If any such solution existed, we wouldn't need the ensemble
    technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Datasets**: The core datasets that will be used throughout the book'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical/machine learning models**: Important classification models will
    be explained here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The right model dilemma**: The absence of a *dominating* model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An ensemble purview**: The need for ensembles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complementary statistical tests**: Important statistical tests that will
    be useful for model comparisons will be discussed here'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following R packages will be required for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ACSWR`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`caret`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`e1071`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`factoextra`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlbench`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NeuralNetTools`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pROC`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RSADBE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`survival`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nnet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data is undoubtedly the most important component of machine learning. If there
    was no data, we wouldn't have a common purpose. In most cases, the purpose for
    which the data is collected defines the problem itself. As we know that the variable
    might be of several types, the way it is stored and organized is also very important.
  prefs: []
  type: TYPE_NORMAL
- en: Lee and Elder (1997) considered a series of datasets and introduced the need
    for ensemble models. We will begin by looking at the details of the datasets considered
    in their paper, and we will then refer to other important datasets later on in
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Hypothyroid
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The hypothyroid dataset `Hypothyroid.csv` is available in the book''s code
    bundle packet, located at `/…/Chapter01/Data`. While we have 26 variables in the
    dataset, we will only be using seven of these variables. Here, the number of observations
    is *n* = 3163\. The dataset is downloaded from [http://archive.ics.uci.edu/ml/datasets/thyroid+disease](http://archive.ics.uci.edu/ml/datasets/thyroid+disease)
    and the filename is `hypothyroid.data` ([http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/hypothyroid.data)).
    After some tweaks to the order of relabeling certain values, the CSV file is made
    available in the book''s code bundle. The purpose of the study is to classify
    a patient with a thyroid problem based on the information provided by other variables.
    There are multiple variants of the dataset and the reader can delve into details
    at the following web page: [http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO](http://archive.ics.uci.edu/ml/machine-learning-databases/thyroid-disease/HELLO).
    Here, the column representing the variable of interest is named `Hypothyroid`,
    which shows that we have 151 patients with thyroid problems. The remaining 3012
    tested negative for it. Clearly, this dataset is an example of *unbalanced data*,
    which means that one of the two cases is outnumbered by a huge number; for each
    thyroid case, we have about 20 negative cases. Such problems need to be handled
    differently, and we need to get into the subtleties of the algorithms to build
    meaningful models. The additional variables or covariates that we will use while
    building the predictive models include `Age`, `Gender`, `TSH`, `T3`, `TT4`, `T4U`,
    and `FTI`. The data is first imported into an R session and is subset according
    to the variables of interest as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line of code imports the data from the `Hypothyroid.csv` file using
    the `read.csv` function. The dataset now has a lot of missing data in the variables,
    as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Consequently, we remove all the rows that have a missing value, and then split
    the data into training and testing datasets. We will also create a formula for
    the classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `set.seed` function ensures that the results are reproducible each time
    we run the program. After removing the missing observations with the `na.omit`
    function, we split the hypothyroid data into training and testing parts. The former
    is used to build the model and the latter is used to validate it, using data that
    has not been used to build the model. Quinlan – the inventor of the popular tree
    algorithm C4.5 – used this dataset extensively.
  prefs: []
  type: TYPE_NORMAL
- en: Waveform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset is an example of a simulation study. Here, we have twenty-one
    variables as input or independent variables, and a class variable referred to
    as `classes`. The data is generated using the `mlbench.waveform` function from
    the `mlbench` R package. For more details, refer to the following link: [ftp://ftp.ics.uci.edu/pub/machine-learning-databases](ftp://ftp.ics.uci.edu/pub/machine-learning-databases).
    We will simulate 5,000 observations for this dataset. As mentioned earlier, the
    `set.seed` function guarantees reproducibility. Since we are solving binary classification
    problems, we will reduce the three classes generated by the waveform function
    to two, and then partition the data into training and testing parts for model
    building and testing purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The R function `mlbench.waveform` creates a new object of the `mlbench` class.
    Since it consists of two sub-parts in `x` and classes, we will convert it into
    `data.frame` following some further manipulations. The `cbind` function binds
    the two objects `x` (a matrix) and classes (a numeric vector) into a single matrix.
    The `data.frame` function converts the matrix object into a data frame, which
    is the class desired for the rest of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'After partitioning the data, we will create the required `formula` for the
    waveform dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: German Credit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Loans are not always repaid in full, and there are defaulters. In this case,
    it becomes important for the bank to identify potential defaulters based on the
    available information. Here, we adapt the `GC` dataset from the `RSADBE` package
    to properly reflect the labels of the factor variable. The transformed dataset
    is available as `GC2.RData` in the data folder. The `GC` dataset itself is mainly
    an adaptation of the version available at [https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)).
    Here, we have 1,000 observations, and 20 covariate/independent variables such
    as the status of existing checking account, duration, and so forth. The final
    status of whether the loan was completely paid or not is available in the `good_bad`
    column. We will partition the data into training and testing parts, and create
    the formula too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Iris
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Iris is probably the most famous classification dataset. The great statistician
    Sir R. A. Fisher popularized the dataset, which he used for classifying the three
    types of `iris` plants based on length and width measurements of their petals
    and sepals. Fisher used this dataset to pioneer the invention of the statistical
    classifier linear discriminant analysis. Since there are three species of `iris`,
    we converted this into a binary classification problem, separated the dataset,
    and created a formula as seen here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Pima Indians Diabetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Diabetes is a health hazard, which is mostly incurable, and patients who are
    diagnosed with it have to adjust their lifestyles in order to cater to this condition.
    Based on variables such as `pregnant`, `glucose`, `pressure`, `triceps`, `insulin`,
    `mass`, `pedigree`, and `age`, the problem here is to classify the person as diabetic
    or not. Here, we have 768 observations. This dataset is drawn from the `mlbench`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The five datasets described up to this point are classification problems. We
    look at one example each for regression, time series, survival, clustering, and
    outlier detection problems.
  prefs: []
  type: TYPE_NORMAL
- en: US Crime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A study of the crime rate per million of the population among the 47 different
    states of the US is undertaken here, and an attempt is made to find its dependency
    on 13 variables. These include age distribution, indicator of southern states,
    average number of schooling years, and so on. As with the earlier datasets, we
    will also partition this one into the following chunks of R program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In each example discussed in this section thus far, we had a reason to believe
    that the observations are independent of each other. This assumption simply means
    that the regressands and regressors of one observation have no relationship with
    other observations' regressands and regressors. This is a simple and reasonable
    assumption. We have another class of observations/datasets where such assumptions
    are not practical. For example, the maximum temperature of a day is not completely
    independent of the previous day's temperature. If that were to be the case, we
    could have a scorchingly hot day, followed by winter, followed by another hot
    day, which in turn is followed by a very heavy rainy day. However, weather does
    not happen in this way as on successive days, the weather is dependent on previous
    days. In the next example, we consider the number of overseas visitors to New
    Zealand.
  prefs: []
  type: TYPE_NORMAL
- en: Overseas visitors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The New Zealand overseas dataset is dealt with in detail in Chapter 10 of Tattar,
    et al. (2017). Here, the number of overseas visitors is captured on a monthly
    basis from January 1977 to December 1995\. We have visitors'' data available for
    over 228 months. The `osvisit.dat` file is available at multiple web links, including
    [https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat](https://www.stat.auckland.ac.nz/~ihaka/courses/726-/osvisit.dat)
    and [https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat](https://github.com/AtefOuni/ts/blob/master/Data/osvisit.dat).
    It is also available in the book''s code bundle. We will import the data in R,
    convert it into a time series object, and visualize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Overseas visitors](img/00002.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: New Zealand overseas visitors'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the dataset is not partitioned! Time series data can''t be arbitrarily
    partitioned into training and testing parts. The reason is quite simple: if we
    have five observations in a time sequential order *y1*, *y2*, *y3*, *y4*, *y5*,
    and we believe that the order of impact is *y1→y2→y3→y4→y5*, an arbitrary partition
    of *y1*, *y2*, *y5*, will have different behavior. It won''t have the same information
    as three consecutive observations. Consequently, the time series partitioning
    has to preserve the dependency structure; we keep the most recent part of the
    time as the test data. For the five observations example, we choose a sample of
    *y1*, *y2*, *y3*, as the test data. The partitioning is simple, and we will cover
    this in [Chapter 11](part0076_split_000.html#28FAO1-2006c10fab20488594398dc4871637ee
    "Chapter 11. Ensembling Time Series Models"), *Ensembling Time Series Models*.'
  prefs: []
  type: TYPE_NORMAL
- en: Live testing experiments rarely yield complete observations. In reliability
    analysis, as well as survival analysis/clinical trials, the units/patients are
    observed up to a predefined time and a note is made regarding whether a specific
    event occurs, which is usually failure or death. A considerable fraction of observations
    would not have failed by the pre-decided time, and the analysis cannot wait for
    all units to fail. A reason to curtail the study might be that the time by which
    all units would have failed would be very large, and it would be expensive to
    continue the study until such a time. Consequently, we are left with incomplete
    observations; we only know that the lifetime of the units lasts for at least the
    predefined time before the study was called off, and the event of interest may
    occur sometime in the future. Consequently, some observations are censored and
    the data is referred to as censored data. Special statistical methods are required
    for the analysis of such datasets. We will give an example of these types of datasets
    next, and analyze them later, in [Chapter 10](part0070_split_000.html#22O7C2-2006c10fab20488594398dc4871637ee
    "Chapter 10. Ensembling Survival Models"), *Ensembling Survival Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Primary Biliary Cirrhosis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pbc` dataset from the survival package is a benchmark dataset in the domain
    of clinical trials. Mayo Clinic collected the data, which is concerned with the
    primary biliary cirrhosis (PBC) of the liver. The study was conducted between
    1974 and 1984\. More details can be found by running `pbc`, followed by `library(survival)`
    on the R terminal. Here, the main time to the event of interest is the number
    of days between registration and either death, transplantation, or study analysis
    in July 1986, and this is captured in the time variable. Similarly to a survival
    study, the events might be censored and the indicator is in the column status.
    The time to event needs to be understood, factoring in variables such as `trt`,
    `age`, `sex`, `ascites`, `hepato`, `spiders`, `edema`, `bili`, `chol`, `albumin`,
    `copper`, `alk.phos`, `ast`, `trig`, `platelet`, `protime`, and `stage`.
  prefs: []
  type: TYPE_NORMAL
- en: The eight datasets discussed up until this point have a target variable, or
    a regressand/dependent variable, and are examples of the supervised learning problem.
    On the other hand, there are practical cases in which we simply attempt to understand
    the data and find useful patterns and groups/clusters in it. Of course, it is
    important to note that the purpose of clustering is to find an identical group
    and give it a sensible label. For instance, if we are trying to group cars based
    on their characteristics such as length, width, horsepower, engine cubic capacity,
    and so on, we may find groups that might be labeled as hatch, sedan, and saloon
    classes, while another clustering solutions might result in labels of basic, premium,
    and sports variant groups. The two main problems posed in clustering are the choice
    of the number of groups and the formation of robust clusters. We consider a simple
    dataset from the `factoextra` R package.
  prefs: []
  type: TYPE_NORMAL
- en: Multishapes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `multishapes` dataset from the `factoextra` package consists of three variables:
    `x`, `y`, and `shape`. It consists of different shapes, with each shape forming
    a cluster. Here, we have two concurrent circle shapes, two parallel rectangles/beds,
    and one cluster of points at the bottom-right. Outliers are also added across
    scatterplots. Some brief R code gives a useful display:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Multishapes](img/00003.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Finding shapes or groups'
  prefs: []
  type: TYPE_NORMAL
- en: This dataset includes a column named shape, as it is a hypothetical dataset.
    In true clustering problems, we will have neither a cluster group indicator nor
    the visualization luxury of only two variables. Later in this book, we will see
    how ensemble clustering techniques help overcome the problems of deciding the
    number of clusters and the consistency of cluster membership.
  prefs: []
  type: TYPE_NORMAL
- en: Although it doesn't happen that often, frustrations can arise when fine-tuning
    different parameters, fitting different models, and other tricks all fail to find
    a useful working model. The culprit of this is often the outlier. A single outlier
    is known to wreak havoc on an otherwise potentially useful model, and their detection
    is of paramount importance. Hitherto this, the parametric and nonparametric outlier
    detections would be a matter of deep expertise. In complex scenarios, the identification
    would be an insurmountable task. A consensus on an observation being an outlier
    can be achieved using the ensemble outlier framework. To consider this, the board
    stiffness dataset will be considered. We will see how an outlier is pinned down
    in the conclusion of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Board Stiffness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The board stiffness dataset is available in the `ACSWR` package through the
    stiff `data.frame` stiff. The dataset consists of four measures of stiffness for
    30 boards. The first measure of stiffness is obtained by sending a shock wave
    down the board, the second measure is obtained by vibrating the board, and the
    remaining two are obtained from static tests. A quick method of identifying the
    outliers in a multivariate dataset is by using the Mahalanobis distance function.
    The further the distance an observation is from the center, the more likely it
    is that the observation will be an outlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Statistical/machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous section introduced a host of problems through real datasets, and
    we will now discuss some standard model variants that are useful for dealing with
    such problems. First, we set up the required mathematical framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have *n* independent pairs of observations, ![Statistical/machine
    learning models](img/00004.jpeg), where ![Statistical/machine learning models](img/00005.jpeg)
    denotes the random variable of interest, also known as the *dependent variable*,
    regress and, endogenous variable, and so on. ![Statistical/machine learning models](img/00006.jpeg)
    is the associated vector of explanatory variables, or independent/exogenous variables.
    The explanatory vector will consist of *k* elements, that is, ![Statistical/machine
    learning models](img/00007.jpeg). The data realized is of the form ![Statistical/machine
    learning models](img/00008.jpeg), where ![Statistical/machine learning models](img/00009.jpeg)
    is the realized value (data) of random variable ![Statistical/machine learning
    models](img/00010.jpeg). A convention will be adapted throughout the book that
    ![Statistical/machine learning models](img/00011.jpeg), and this will take care
    of the intercept term. We assume that the observations are from the true distribution
    *F*, which is not completely known. The general regression model, including the
    classification model as well as the regression model, is specified by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical/machine learning models](img/00012.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, the function *f* is an unknown function and ![Statistical/machine learning
    models](img/00013.jpeg) is the regression parameter, which captures the influence
    of ![Statistical/machine learning models](img/00014.jpeg) on ![Statistical/machine
    learning models](img/00015.jpeg). The error ![Statistical/machine learning models](img/00016.jpeg)
    is the associated unobservable error term. Diverse methods can be applied to model
    the relationship between the Ys and the `xes`. The statistical regression model
    focused on the complete specification of the error distribution ![Statistical/machine
    learning models](img/00017.jpeg), and in general the functional form would be
    linear as in ![Statistical/machine learning models](img/00018.jpeg). The function
    ![Statistical/machine learning models](img/00019.jpeg) is the link function in
    the class of generalized linear models. Nonparametric and semiparametric regression
    models are more flexible, as we don't place a restriction on the error's probability
    distribution. Flexibility would come with a price though, and here we need a much
    higher number of observations to make a valid inference, although that number
    is unspecified and is often subjective.
  prefs: []
  type: TYPE_NORMAL
- en: The machine learning paradigm includes some *black box* methods, and we have
    a healthy overlap between this paradigm and non- and semi-parametric models. The
    reader is also cautioned that black box does not mean unscientific in any sense.
    The methods have a firm mathematical foundation and are reproducible every time.
    Next, we quickly review some of the most important statistical and machine learning
    models, and illustrate them through the datasets discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The logistic regression model is a binary classification model, and it is a
    member of the exponential family which belongs to the class of generalized linear
    models. Now, let ![Logistic regression model](img/00020.jpeg)denote the binary
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression model](img/00021.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the information contained in the explanatory vector ![Logistic regression
    model](img/00022.jpeg) we are trying to build a model that will help in this task.
    The logistic regression model is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression model](img/00023.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Logistic regression model](img/00024.jpeg) is the vector of regression
    coefficients. Note that the logit function ![Logistic regression model](img/00025.jpeg)
    is linear in the regression coefficients and hence the name for the model is a
    logistic regression model. A logistic regression model can be equivalently written
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Logistic regression model](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![Logistic regression model](img/00027.jpeg) is the binary error term
    that follows a Bernoulli distribution. For more information, refer to Chapter
    17 of Tattar, et al. (2016). The estimation of the parameters of the logistic
    regression requires the **iterative reweighted least squares** (**IRLS**) algorithm,
    and we would use the `glm` R function to get this task done. We will use the Hypothyroid
    dataset in this section. In the previous section, the training and test datasets
    and formulas were already created, and we will carry on from that point.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression for hypothyroid classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the `hypothyroid` dataset, we had `HT2_Train` as the training dataset.
    The test dataset is split as the covariate matrix in `HT2_TestX` and the outputs
    of the test dataset in `HT2_TestY`, while the formula for the logistic regression
    model is available in `HT2_Formula`. First, the logistic regression model is fitted
    to the training dataset using the `glm` function and the fitted model is christened
    `LR_fit`, and then we inspect it for model fit summaries using `summary(LR_fit)`.
    The fitted model is then applied to the covariate data in the test part using
    the `predict` function to create `LR_Predict`. The predicted probabilities are
    then labeled in `LR_Predict_Bin`, and these labels are compared with the actual
    `testY_numeric` and overall accuracy is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It can be seen from the summary of the fitted GLM (the output following the
    line `summary(LR_fit)`) that we are having four significant variables `in Age`,
    `TT4`, `T4U`, and `FTI`. Using the `predict` function, we apply the fitted model
    on unknown test cases in `HT2_TestX`, compare it with the actuals, and find the
    accuracy to be 97.33%. Consequently, logistic regression is easily deployed in
    the R software.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression might appear restricted as it allows only a linear impact
    of the covariates through the link function. The linearity assumption might not
    hold, and in most practical cases, we don''t have enough information to specify
    the functional form of the nonlinear relationship. Thus, all we know is that there
    is most likely an unknown nonlinear relationship. Neural networks are the nonlinear
    generalization of logistic regression, and this involves two important components:
    hidden neurons and learning rate. We will revise the structure of neural networks
    first.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a neural network, the input variables are considered the first layer of
    neurons and the output the final and concluding layer of neurons. The structure
    of a neural network model can be visualized using the R package `NeuralNetTools`.
    Suppose that we have three input variables and two hidden layers, and each contains
    two hidden neurons. Here, we have a neural network with four layers. The next
    code segment gives a visualization of a neural network''s structure with three
    input variables, two hidden neurons in two hidden layers, and one output variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We find the R package `NeuralNetTools` very useful in visualizing the structure
    of a neural network. Neural networks built using the core R package `nnet` can
    also be visualized using the `NeuralNetTools::plotnet` function. The `plotnet`
    function sets up a neural network whose structure consists of three neurons in
    the first layer, two neurons in each of the second and third layers, and one in
    the final output layer, through the `struct` option. The weights along the arcs
    are set at zero in `rep(0,17)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural networks](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Structure of a neural network'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous diagram, we have four layers of the neural network. The first
    layer consists of **B1** (the bias), **I1 (X1)**, **I2 (X2)**, and **I3 (X3)**.
    The second layer consists of three neurons in **B2** (the bias of the first hidden
    layer), **H1**, and **H2**. Note that the bias **B2** does not receive any input
    from the first hidden layer. Next, each neuron receives an overall input from
    each of the neurons of the previous layer, which are **B1**, **X1**, **X2**, and
    **X3** here. However, **H1** and **H2** of the first hidden layer will receive
    different aggregated input from **B1**, **X1**, **X2**, and **X3**. Appropriate
    weights are in action on each of the arcs of the network and it is the weights
    that form the parameters of the neural networks; that is, the arrival of **H1**
    (of the first layer) would be like
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural networks](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: and the effective arrival is through a *transfer function*. A transfer function
    might be an identity function, sigmoidal function, and so on. Similarly, the arrival
    at the second neuron of the first layer is
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural networks](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: . By extension, **B2**, **H1**, and **H2** (of the first layer) will be the
    input for the second hidden layer, and **B3**, **H1**, and **H2** will be the
    input for the final output. At each stage of the neural network, we have weights.
    The weights need to be determined in such a manner that the difference between
    predicted output **O1** and the true **Y1** is as small as possible. Note that
    the logistic regression is a particular case of the neural network as can be seen
    by directly removing all hidden layers and input layer leads in the output one.
    The neural network will be fitted for the hypothyroid problem.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network for hypothyroid classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the `nnet` function from the package of the same name to set up the
    neural network for the hypothyroid classification problem. The formula, training,
    and test datasets continue as before. The accuracy calculation follows along similar
    lines to the segment in logistic regression. The fitted neural network is visualized
    using the `plotnet` graphical function from the `NeuralNetTools` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the accuracy is 98.27%, which is an improvement on the logistic regression
    model. The visual display of the fitted model is given in the following diagram.
    We have fixed the seed for the random initialization of the neural network parameters
    at `12345`, using `set.seed(12345)`, so that the results are reproducible at the
    reader''s end. This is an interesting case for ensemble modeling. Different initial
    seeds – which the reader can toy around with – will lead to different accuracies.
    Sometimes, you will get an accuracy lower than any of the models considered in
    this section, and at other times you will get the highest accuracy. The choice
    of seed as arbitrary leads to the important question of which solution is useful.
    Since the seeds are arbitrary, the question of a good seed or a bad seed does
    not arise. In this case, if a model is giving you a higher accuracy, it does not
    necessarily mean anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neural network for hypothyroid classification](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Neural network for the hypothyroid classification'
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The naïve Bayes classifier is a simplistic implementation based on the Bayes
    formula. It is based on simple empirical and conditional probabilities, as evidenced
    in the actual data. Beyond the simplest assumption of observation independence,
    we don't have any restrictions in using this model.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes for hypothyroid classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A naïve Bayes classifier is fit using the `naiveBayes` function from the `e1071`
    R package. The prediction and accuracy assessment is carried out using two functions,
    `predict` and `sum`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of the naïve Bayes classifier is 97.33%, which is the same as the
    logistic regression model and less than the one provided by the neural network.
    We remark here that it is only a coincidence that the accuracy of this method
    and logistic regression is the same.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Breiman and Quinlan mainly developed decision trees, which have evolved a lot
    since the 1980s. If the dependent variable is continuous, the decision tree will
    be a regression tree and if it is categorical variable, it will be a classification
    tree. Of course, we can have a survival tree as well. Decision trees will be the
    main model that will be the beneficiary of the ensemble technique, as will be
    seen throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the regression tree given in the following diagram. We can see that
    there are three input variables, which are ![Decision tree](img/00032.jpeg), and
    the output variable is *Y*. Strictly speaking, a decision tree will not display
    all the variables used to build the tree. In this tree structure, a decision tree
    is conventionally displayed upside down. We have four terminal nodes. If the condition
    ![Decision tree](img/00033.jpeg) is satisfied, we move to the right side of the
    tree and conclude that the average *Y* value is 40\. If the condition is not satisfied,
    we move to the left, and check whether ![Decision tree](img/00034.jpeg). If this
    condition is not satisfied, we move to the left side of the tree and conclude
    that the average *Y* value is 100\. Upon the satisfactory meeting of this condition,
    we move to the right side and then if the categorical variable ![Decision tree](img/00035.jpeg),
    the average *Y* value would be 250, or 10 otherwise. This decision tree can be
    captured in the form of an equation too, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree](img/00036.jpeg)![Decision tree](img/00037.jpeg)![Decision
    tree](img/00038.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Regression tree'
  prefs: []
  type: TYPE_NORMAL
- en: The statistician Terry Therneau developed the `rpart` R package.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree for hypothyroid classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the `rpart` function from the `rpart` package, we build a classification
    tree for the same formula as the earlier partitioned data. The constructed tree
    can be visualized using the plot function, and the variable name is embossed on
    the tree with the text function. The equation of the fitted classification tree
    (see Figure *Classification Tree for Hypothyroid*) is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision tree for hypothyroid classification](img/00039.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Prediction and accuracy is carried out in a similar way as mentioned earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Decision tree for hypothyroid classification](img/00040.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Classification tree for Hypothyroid'
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the classification tree gives an accuracy of 98.74%, which is
    the best of the four models considered thus far. Next, we will consider the final
    model, support vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Support vector machines**, abbreviated popularly as **SVM**, are an important
    class of machine learning techniques. Theoretically, SVM can take an infinite
    number of features/covariates and build the appropriate classification or regression
    SVMs.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM for hypothyroid classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `svm` function from the `e1071` package will be useful for building an
    `SVM` classifier on the Hypothyroid dataset. Following the usual practice, we
    have the following output in the R session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The SVM technique gives us an accuracy of 98.43%, which is the second best of
    the models set up thus far.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will run each of the five classification models for
    the Waveform, German Credit, Iris, and Pima Indians Diabetes problem datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The right model dilemma!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we ran five classification models for the `Hypothyroid`
    dataset. Here, the task is to repeat the exercise for four other datasets. It
    would be a very laborious task to change the code in the appropriate places and
    repeat the exercise four times over. Thus, to circumvent this problem, we will
    create a new function referred to as `Multiple_Model_Fit`. This function will
    take four arguments: `formula`, `train`, `testX`, and `testY`. The four arguments
    have already been set up for each of the five datasets. The function is then set
    up in a way that generalizes the steps of the previous section for each of the
    five models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function proceeds to create a matrix whose first column consists of the
    model name, while the second column consists of the accuracy. This matrix is returned
    as the output of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '`Multiple_Model_Fit` is now applied to the `Hypothyroid` dataset, and the results
    can be seen to be in agreement with the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Multiple_Model_Fit` function is then applied to the other four classification
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The results for each of the datasets are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The right model dilemma!](img/00041.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: Accuracy of five models for five datasets'
  prefs: []
  type: TYPE_NORMAL
- en: The `iris` dataset is a straightforward and simplistic problem, and therefore
    each of the five models gives us 100% accuracy on the test data. This dataset
    will not be pursued any further.
  prefs: []
  type: TYPE_NORMAL
- en: For each dataset, we highlight the highest accuracy cell in grey, and highlight
    the next highest in yellow.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the modeling dilemma. The naïve Bayes method turns out the best for
    the `German` and `Pima Indian Diabetes` datasets. The decision tree gives the
    highest accuracy for the `Hypothyroid` dataset, while SVM gives the best results
    for `Waveform`. The runner-up place is secured twice by logistic regression and
    twice by SVM. However, we also know that, depending on the initial seeds and maybe
    the number of hidden neurons, the neural networks are also expected to perform
    the best for some datasets. We then also have to consider whether the results
    will turn out differently for different partitions.
  prefs: []
  type: TYPE_NORMAL
- en: It is in such practical scenarios we would prefer to have a single approach
    that ensures reasonable properties. With the `Hypothyroid` dataset, the accuracy
    for each of the models is 97% or higher, and one might not go wrong with any of
    the models. However, in the `German` and `Pima Indian Diabetes` problems, the
    maximum accuracy is 80% and 78%, respectively. It would then be better if we can
    make good use of all the models and build a single unified one with increased
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble purview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `caret` R package is core to ensemble machine learning methods. It provides
    a large framework and we can also put different statistical and machine learning
    models together to create an ensemble. For the recent version of the package on
    the author''s laptop, the `caret` package provides access to the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You need to key in the number `1` and continue. The package will be installed
    and loaded, and the program will continue. It is good to know the host of options
    for ensemble methods. A brief method for stack ensembling analytical models is
    provided here, and the details will unfold later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `Hypothyroid` dataset, we had a high accuracy of an average of 98%
    between the five models. The `Waveform` dataset saw an average accuracy of approximately
    88%, while the average for `German` Credit data is 75%. We will try to increase
    the accuracy for this dataset. The accuracy improvement will be attempted using
    three models: naïve Bayes, logistic regression, and classification tree. First,
    we need to partition the data into three parts: train, test, and stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The model will be built on the training data first and accuracy will be assessed
    using the metric of Area Under Curve, the curve being the ROC. The control parameters
    will be set up first and the three models, naïve Bayes, classification tree, and
    logistic regression, will be created using the training dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Predictions for the test and stack blocks are carried out next. We store the
    predicted probabilities along the test and stack data frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The ROC is an important measure for model assessments. The higher the area
    under the ROC, the better the model would be. Note that these measures, or any
    other measure, will not be the same as the models fitted earlier since the data
    has changed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For the `test` dataset, we can see that the area under curve for the naïve
    Bayes, classification tree, and logistic regression are respectively `0.7543`,
    `0.6777`, and `0.7446`. If we put the predicted values together in some format,
    and that leads to an increase in the accuracy, the purpose of the ensemble technique
    has been accomplished. As such, we consider the new predicted probabilities under
    the three models and append them to the stacked data frame. These three columns
    will now be treated as new input vectors. We then build a naïve Bayes model, an
    arbitrary choice, and you can try any other model (not necessarily restricted
    to one of these) for the stacked data frame. The AUC can then be predicted and
    calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The AUC for the stacked data observations is higher than any of the earlier
    models, which is an improvement.
  prefs: []
  type: TYPE_NORMAL
- en: A host of questions should arise for the critical thinker. Why should this technique
    work? Will it lead to improvisations under all possible cases? If yes, will simply
    adding new model predictions lead to further improvements? If no, how does one
    pick the base models so that we can be reasonably assured of improvisations? What
    are the restrictions on the choice of models? We will provide solutions to most
    of these questions throughout this book. In the next section, we will quickly
    look at some useful statistical tests that will aid the assessment of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Complementary statistical tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, a model is selected over another plausible one. The accuracy of one model
    seems higher than the other. The **area under curve** (**AUC**) of the ROC of
    a model is greater than that of another. However, it is not appropriate to base
    the conclusion on pure numbers only. It is important to conclude whether the numbers
    hold significance from the point of view of statistical inference. In the analytical
    world, it is pivotal that we make use of statistical tests whenever they are available
    to validate claims/hypotheses. A reason for using statistical tests is that probability
    can be highly counterintuitive, and what appears on the surface might not be the
    case upon closer inspection, after incorporating the chance variation. For instance,
    if a fair coin is tossed 100 times, it is imprudent to think that the number of
    heads must be exactly 50\. Hence, if a fair coin shows up 45 heads, we need to
    incorporate the chance variation that the number of heads can be less than 50
    too. Caution must be exerted all the while when we are dealing with uncertain
    data. A few examples are in order here. Two variables might appear to be independent
    of each other, and the correlation might also be nearly equal to zero. However,
    applying the correlation test might result in the conclusion that the correlation
    is not significantly zero. Since we will be sampling and resampling a lot in this
    text, we will look at related tests.
  prefs: []
  type: TYPE_NORMAL
- en: Permutation test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that we have two processes, A and B, and the variances of these two
    processes are known to be equal, though unknown. Three independent observations
    from process A result in yields of 18, 20, and 22, while three independent observations
    from process B gives yields of 24, 26, and 28\. Under the assumption that the
    yield follows a normal distribution, we would like to test whether the means of
    processes A and B are the same. This is a suitable case for applying the t-test,
    since the number of observations is smaller. An application of the `t.test` function
    shows that the two means are different to each other, and this intuitively appears
    to be the case.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the assumption under the null hypothesis is that the means are equal, and
    that the variance is unknown and assumed to be equal under the two processes.
    Consequently, we have a genuine reason to believe that the observations from process
    A might well have occurred in process B too, and vice versa. We can therefore
    swap one observation in process B with process A, and recompute the t-test. The
    process can be repeated for all possible permutations of the two samples. In general,
    if we have m samples from population 1 and n samples from population 2, we can
    have
  prefs: []
  type: TYPE_NORMAL
- en: '![Permutation test](img/00043.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: different samples and as many tests. An overall test can be based on such permutation
    samples and such tests are called **permutation tests**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For process A and B observations, we will first apply the t-test and then the
    permutation test. The `t.test` is available in the core `stats` package and the
    permutation t-test is taken from the `perm` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The smaller p-value suggests that the means of processes A and B are not equal.
    Consequently, we now apply the permutation test `permTS` from the `perm` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The p-value is now at 0.1, which means that the permutation test concludes
    that the means of the processes are equal. Does this mean that the permutation
    test will always lead to this conclusion, contradicting the t-test? The answer
    is given in the next code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Chi-square and McNemar test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We had five models for the hypothyroid test. We then calculated the accuracy
    and were satisfied with the numbers. Let''s first look at the number of errors
    that the fitted model makes. We have 636 observations in the test partition and
    42 of them test positive for the hypothyroid problem. Note that if we mark all
    the patients as negative, we would be getting an accuracy of *1-42/636 = 0.934*,
    or about 93.4%. Using the table function, we pit the actuals against the predicted
    values and see how often the fitted model goes wrong. We remark here that identifying
    the hypothyroid cases as the same and the negative cases as negative is the correct
    prediction, while marking the hypothyroid case as negative and vice versa leads
    to errors. For each model, we look at the misclassification errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'From the misclassification table, we can see that the neural network identifies
    41 out of the 42 cases of hypothyroid correctly, but it identifies way more cases
    of hypothyroid incorrectly too. The question that arises is whether the correct
    predictions of the fitted models only occur by chance, or whether they depend
    on truth and can be explained. To test this, in the hypotheses framework we would
    like to test whether the actuals and predicted values of the actuals are independent
    of or dependent on each other. Technically, the null hypothesis is that the prediction
    is independent of the actual, and if a model explains the truth, the null hypothesis
    must be rejected. We should conclude that the fitted model predictions depend
    on the truth. We deploy two solutions here, the chi-square test and the McNemar
    test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The answer provided by the chi-square tests clearly shows that the predictions
    of each fitted model is not down to chance. It also shows that the prediction
    of hypothyroid cases, as well as the negative cases, is expected of the fitted
    models. The interpretation of and conclusions from the McNemar's test is left
    to the reader. The final important measure in classification problems is the ROC
    curve, which is considered next.
  prefs: []
  type: TYPE_NORMAL
- en: ROC test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ROC curve is an important improvement on the false positive and true negative
    measures of model performance. For a detailed explanation, refer to Chapter 9
    of Tattar et al. (2017). The ROC curve basically plots the true positive rate
    against the false positive rate, and we measure the AUC for the fitted model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main goal that the ROC test attempts to achieve is the following. Suppose
    that Model 1 gives an AUC of 0.89 and Model 2 gives 0.91\. Using the simple AUC
    criteria, we outright conclude that Model 2 is better than Model 1\. However,
    an important question that arises is whether 0.91 is significantly higher than
    0.89\. The `roc.test`, from the `pROC` R package, provides the answer here. For
    the neural network and classification tree, the following R segment gives the
    required answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Since the p-value is very large, we conclude that the AUC for the two models
    is not significantly different.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical tests are vital and we recommend that they be used whenever suitable.
    The concepts highlighted in this chapter will be drawn on in more detail in the
    rest of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter began with an introduction to some of the most important datasets
    that will be used in the rest of the book. The datasets covered a range of analytical
    problems including classification, regression, time series, survival, clustering,
    and a dataset in which identifying an outlier is important. Important families
    of classification models were then introduced in the statistical/machine learning
    models section. Following the introduction of a variety of models, we immediately
    saw the shortcoming, in that we don't have a model for all seasons. Model performance
    varies from dataset to dataset. Depending on the initialization, the performance
    of certain models (such as neural networks) is affected. Consequently, there is
    a need to find a way to ensure that the models can be improved upon in most scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: This paves the way for the ensemble method, which forms the title of this book.
    We will elaborate on this method in the rest of the book. This chapter closed
    with quick statistical tests that will help in carrying out model comparisons.
    Resampling forms the core of ensemble methods, and we will look at the important
    jackknife and bootstrap methods in the next chapter.
  prefs: []
  type: TYPE_NORMAL
