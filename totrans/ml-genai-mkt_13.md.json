["```py\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import TFAutoModel, AutoTokenizer\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = TFAutoModel.from_pretrained(model_name, output_attentions=True)\ntext = \"Write a product description for an eco-friendly kitchenware product focusing on brand ethics.\"\ninputs = tokenizer(text, return_tensors='tf')\noutputs = model(inputs)\nattention = outputs[-1][-1].numpy() \n```", "```py\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nattention = attention.squeeze(axis=0)\ntokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].numpy()[0])\ntokens = tokens[1:-1] \nattention = attention[:, 1:-1, 1:-1]\nfig, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(attention[0], annot=True, ax=ax, cmap=\"viridis\", xticklabels=tokens, yticklabels=tokens, fmt='.2f', annot_kws={\"size\": 8})\nax.set_title('Attention Weights for Marketing Prompt')\nplt.xticks(rotation=45, ha='right', fontsize=10)\nplt.yticks(fontsize=10)\nplt.show() \n```", "```py\nimport pandas as pd\nimport hashlib\ncustomer_data = pd.DataFrame({\n    'first_name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Hannah', 'Ivy', 'Jack'],\n    'last_name': ['Smith', 'Jones', 'Brown', 'Johnson', 'Davis', 'Wilson', 'Moore', 'Taylor', 'Anderson', 'Thomas'],\n    'age': [25, 30, 22, 40, 35, 28, 26, 33, 29, 37],\n    'income': [20000, 35000, 27000, 50000, 45000, 30000, 32000, 38000, 31000, 47000],\n    'purchase_amount': [100, 150, 200, 250, 220, 140, 180, 160, 190, 230]\n})\ndef pseudonymize_id(first_name, last_name):\n    return hashlib.sha256((first_name + last_name).encode()).hexdigest()\ncustomer_data['customer_id'] = customer_data.apply(lambda row: pseudonymize_id(row['first_name'], row['last_name']), axis=1)\nanonymized_data = customer_data.drop(columns=['first_name', 'last_name'])\ndisplay(anonymized_data) \n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nanonymized_data[['age', 'income', 'purchase_amount']] = scaler.fit_transform(anonymized_data[['age', 'income', 'purchase_amount']])\nepsilon = 5.0 \ndef add_noise(data, epsilon):\n    sensitivity = np.max(data) - np.min(data)\n    noise = np.random.laplace(0, sensitivity / epsilon, data.shape)\n    return data + noise\nnoisy_data = anonymized_data.copy()\nnoisy_data[['age', 'income', 'purchase_amount']] = add_noise(anonymized_data[['age', 'income', 'purchase_amount']].values, epsilon)\nnoisy_data[['age', 'income', 'purchase_amount']] = scaler.inverse_transform(noisy_data[['age', 'income', 'purchase_amount']])\nprint(\"Noisy Data: \\n\", noisy_data)\naverage_purchase = noisy_data['purchase_amount'].mean() \n```", "```py\ndef plot_data_with_trend_lines(original_data, anonymized_data, noisy_data):\n    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n    ax[0].scatter(original_data['age'], original_data['purchase_amount'], color='blue')\n    z = np.polyfit(original_data['age'], original_data['purchase_amount'], 1)\n    p = np.poly1d(z)\n    ax[0].plot(original_data['age'], p(original_data['age']), \"r--\")\n    ax[0].set_title('Original Data')\n    ax[0].set_xlabel('Age')\n    ax[0].set_ylabel('Purchase Amount')\n    ax[1].scatter(anonymized_data['age'], anonymized_data['purchase_amount'], color='green')\n    z = np.polyfit(anonymized_data['age'], anonymized_data['purchase_amount'], 1)\n    p = np.poly1d(z)\n    ax[1].plot(anonymized_data['age'], p(anonymized_data['age']), \"r--\")\n    ax[1].set_title('Anonymized Data')\n    ax[1].set_xlabel('Age')\n    ax[1].set_ylabel('Purchase Amount')\n    ax[2].scatter(noisy_data['age'], noisy_data['purchase_amount'], color='red')\n    z = np.polyfit(noisy_data['age'], noisy_data['purchase_amount'], 1)\n    p = np.poly1d(z)\n    ax[2].plot(noisy_data['age'], p(noisy_data['age']), \"r--\")\n    ax[2].set_title('Noisy Data with Differential Privacy')\n    ax[2].set_xlabel('Age')\n    ax[2].set_ylabel('Purchase Amount')\n    plt.tight_layout()\n    plt.show()\nplot_data_with_trend_lines(customer_data, anonymized_data, noisy_data) \n```"]