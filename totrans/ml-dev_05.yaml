- en: Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a developer, you have surely gained an interest in machine learning from
    looking at all the incredibly amazing applications that you see on your regular
    devices every day—automatic speech translation, picture style transfer, the ability
    to generate new pictures from sample ones, and so on. Brace yourself... we are
    heading directly into the technology that has made all these things possible.
  prefs: []
  type: TYPE_NORMAL
- en: Linear and logistic models, such as the ones we've observed, have certain limitations
    in terms of the complexity of the training dataset they train a model on, even
    when they are the basis of very articulated and efficient solutions.
  prefs: []
  type: TYPE_NORMAL
- en: How complex does a model have to be to capture the style of writing of an author,
    the concept of an image of a cat versus an image of a dog, or the classification
    of a plant based on visual elements? These things require the summation of a huge
    mix of low-level and high-level details captured, in the case of our brain by
    specialized sets of neurons, and in computer science by neural models.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to what's expected, in this book I'll spare you the typical introduction
    to the nervous system, its capabilities, the number of neurons in the nervous
    system, its chemical properties, and so on. I find that these topics add an aura
    of impossibility to problems, but the models we are about to learn are simple
    math equations with computational counterparts, which we will try to emphasize
    to allow you, a person with algorithmic interests, to easily understand them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The history of neural models, including perceptron and ADALINE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks and the kinds of problems they solve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multilayer perceptron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a simple neural layer to model a binary function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will allow you to use the building blocks of most of the awesome
    machine learning applications you see daily. So, let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: History of neural models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural models, in the sense of being disciplines that try to build representations
    of the internal workings of the brain, originated pretty distantly in the computer
    science timescale. They even date back to the time when the origins of modern
    computing were being invented, the mid-1940s.
  prefs: []
  type: TYPE_NORMAL
- en: At that time, the fields of neuroscience and computer science began to collaborate
    by researching ways of emulating the way the brain processes information, starting
    from its constituent unit—the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: The first mathematical method for representing the learning function of the
    human brain can be assigned to McCulloch and Pitts, in their 1943 paper *A Logical
    Calculus of Ideas Immanent in Nervous Activity:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e0ee661-6f00-42e3-87a4-35139415faa8.png)'
  prefs: []
  type: TYPE_IMG
- en: McCulloch and Pitts model
  prefs: []
  type: TYPE_NORMAL
- en: This simple model was a basic but realistic model of a learning algorithm. You
    will be surprised by what happens if we use a linear function as a transfer function;
    this is a simple linear model, as were the ones we saw in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noted that we are now using the `w` letter to specify the parameters
    to be tuned by the model. Take this as the new standard from now on. Our old `β`
    parameters in linear regression models will now be `w`.
  prefs: []
  type: TYPE_NORMAL
- en: But the model hasn't determined a way to tune the parameters. Let's go forward
    to the 1950s and review the **perceptron** model.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron model is one of the simplest ways to implement an artificial
    neuron. It was initially developed at the end of the 1950s, with the first hardware
    implementation being carried out in the 1960s. First, it was the name of a machine,
    and later became the name of an algorithm. Yes, perceptrons are not the strange
    entities we always thought they were, they're what you as a developer handle each
    day— algorithms!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following steps and learn how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function for a perceptron is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ef51780-af9a-41cd-8c1f-5749a0aa2e4e.png)'
  prefs: []
  type: TYPE_IMG
- en: If *y’ ≠ y*, modify all the connections, *wi*, by adding the changes *Δw =yxi*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to *step 2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is basically an algorithm that learns a binary classification function and
    maps a real function to a single binary function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s depict the new architecture of the perceptron and analyze the new scheme
    of the algorithm in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dd545c6-e350-4130-a7e7-45da7a1065c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Perceptron model (With changes from previous model highlighted)
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron builds on the ideas of its predecessors, but the novelty here
    is that we are adding a proper learning mechanism! In the following diagram, we
    have highlighted the new properties of the model—the feedback loop, where we calculate
    the error of our outcomes, and the adjustment of weights—with a predetermined
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db8ec5ff-a2b3-48bb-86ba-d7bb01b6c5b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Perceptron algorithm flowchart
  prefs: []
  type: TYPE_NORMAL
- en: Improving our predictions – the ADALINE algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ADALINE is another algorithm (yes, remember we are talking about algorithms)
    used to train neural networks. ADALINE is in some ways more advanced than the
    perceptron because it adds a new training method: gradient descent, which should
    be known to you by now. Additionally, it changes the point at which the error
    is measured before the activation output is applied to the summation of the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48d0f5a9-2fb4-4a18-a130-ab8d98268c49.png)'
  prefs: []
  type: TYPE_IMG
- en: Adaline model (With additions from Perceptron highlighted)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this is the standard way of representing the ADALINE algorithm in a structural
    way. As an algorithm consists of a series of steps, let''s aggregate those in
    a more detailed way, with some additional mathematical details:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output value that we will be taking will be the one after the summation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**y=Σ(xi * wi)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the error, comparing the model output with the right label *o*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*E=(o-y)²*'
  prefs: []
  type: TYPE_NORMAL
- en: Does it look similar to something we have already seen? Yes! We are basically
    resolving a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adjust the weights with the following gradient descent recursion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/58540fe4-1f28-4b5a-b899-7b20601ad6da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Return to *step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8429de2-2994-466a-b964-e181702e4f94.png)'
  prefs: []
  type: TYPE_IMG
- en: Adaline algorithm flowchart
  prefs: []
  type: TYPE_NORMAL
- en: Similarities and differences between a perceptron and ADALINE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a simplified explanation of the precursors of modern neural
    networks. As you can see, the elements of modern models were almost all laid out
    during the 1950s and the 1960s! Before continuing, let''s try to compare the approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarities**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are both algorithms (it's important to stress that)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are applied to single-layer neural models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are classifiers for binary classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both have a linear decision boundary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both can learn iteratively, sample by sample (the perceptron naturally, and
    ADALINE via stochastic gradient descent)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both use a threshold function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differences**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perceptron uses the final class decision to train weights
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ADALINE uses continuous predicted values (from the net input) to learn the model
    coefficients, and measures subtle changes in error with a continuous float point
    rank, not a Boolean or integer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we are done with our single-layer architectures and models, we will review
    some discoveries from the end of the 1960s that provoked quite a stir in the neural
    models community and were said to generate the first AI winter, or an abrupt collapse
    in the interest in machine learning research. Happily, some years after that,
    researchers found ways to overcome the limitations they faced, but this will be
    covered further on in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of early models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model itself now has most of the elements of any normal neural model, but
    it had its own problems. After some years of fast development, the publication
    of the book *P**erceptrons* by Minsky and Papert in 1969 prompted a stir in the
    field because of its main idea that perceptrons could only work on linearly separable
    problems, which were only a very tiny part of the problems that the practitioners
    thought were solvable. In a sense, the book suggested that perceptrons were verging
    on being useless, with the exception of the simplest classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This new-found deficiency could be represented by the inability of the model
    to represent the XOR function, which is a `Boolean` function with an output of
    `1` when the inputs are different, and `0` when they are equal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/935f5509-f277-4e86-b870-241374cfc73f.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem of modelling a XOR function. No single line will correctly separate
    the 0 and 1 values
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, the main problem is that neither class
    (cross nor point) is linearly separable; that is, we can not separate the two
    with any linear function on the plane.
  prefs: []
  type: TYPE_NORMAL
- en: This detected problem caused a decrease in activity in the field that lasted
    for more or less five years, until the development of the backpropagation algorithm,
    which started in the mid 1970s.
  prefs: []
  type: TYPE_NORMAL
- en: Single and multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will discuss more contemporary times, building on the previous concepts
    to allow more complex elements of reality to be modeled.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will directly study **multilayer perceptrons** (**MLPs**),
    which are the most common configuration used, and will consider a **single-layer
    perceptron** as a particular case of the former, highlighting the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Single-layer and multilayer perceptrons were the most commonly used architecture
    during the 1970s and 1980s and provided huge advances in terms of the capabilities
    of neural systems. The main innovations they bring to the table are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: They are feedforward networks because the calculations, starting from the inputs,
    flow from layer to layer without any cycling (information never returns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They use the backpropagation method to adjust their weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of the `step` function as a transfer function is replaced by non-linear
    ones such as the sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLP origins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After the power of single-unit neural models had been explored, an obvious
    step was to generate layers or sets of commonly connected units (we define a connection
    as the act of sending the output of one unit to be part of another unit''s summation):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9fa32993-2a1f-4143-ad6d-86ff94158092.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of a simple multilayer feed forward Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this phase of the operation of the network, the data will be input in the
    first layer and will flow from each unit to the corresponding units in the following
    layers. Then it will be summed and passed through in the hidden layers, and finally
    processed by the output layer. This process is totally unidirectional, so we are
    avoiding any recursive complications in the data flow.
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward mechanism, has in the MLP its counterpart in the training part
    of the modeling process, which will be in charge of improving the model's performance.
    The normally chosen algorithm is called **backpropagation**.
  prefs: []
  type: TYPE_NORMAL
- en: The chosen optimization algorithm – backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the perceptron algorithm onward, every neural architecture has had a means
    to optimize its internal parameters based on the comparison of the ground truth
    with the model output. The common assumption was to take the derivative of the
    (then simple) model function and iteratively work towards the minimum value.
  prefs: []
  type: TYPE_NORMAL
- en: For complex multilayer networks, there is an additional overhead, which has
    to do with the fact that the output layer's output is the result of a long chain
    of functions compositions, where each layer's output is wrapped by the next one's
    transfer function. So, the derivative of the output will involve the derivative
    of an exceedingly complex function. In this case, the backpropagation method was
    proposed, with excellent results.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation can be summarized as an algorithm used to calculate derivatives.
    The main attribute is that it is computationally efficient and works with complex
    functions. It is also a generalization of the least mean squares algorithm in
    the linear perceptron, which we already knew!
  prefs: []
  type: TYPE_NORMAL
- en: In the backpropagation algorithm, the responsibility of the error will be distributed
    among all the functions applied to the data in the whole architecture. So, the
    goal is to minimize the error, the gradient of the loss function, over a set of
    deeply compounded functions, which will again receive the help of the chain rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to define our general algorithm for our modern version of a
    neural network in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the feedforward signals from the input to the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate output error *E *based on the prediction *a[k]* and the target *t[k]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Backpropagate the error signals by weighting them by the weights in the previous
    layers and the gradients of the associated activation functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gradients 𝛿*E*/𝛿*θ* for the parameters based on the backpropagated
    error signal and the feedforward signals from the inputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the parameters using the calculated gradients  *θ ← θ - η* 𝛿E/𝛿θ *.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lets review this process now, in a graphical way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f3c94d8-c460-41e9-87c4-2af201ab0027.png)'
  prefs: []
  type: TYPE_IMG
- en: Step by step representation of the Feed Forward  and Back Propagation training
    process
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we will represent the whole process in an algorithmic
    way. You can see the number of coincidences with the previous optimization methods,
    and the short number of computation blocks involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png)'
  prefs: []
  type: TYPE_IMG
- en: Flowchart of the feedforward/backpropagation scheme
  prefs: []
  type: TYPE_NORMAL
- en: Types of problem to be tackled
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks can be used for both regression problems and classification
    problems. The common architectural difference resides in the output layer: in
    order to be able to bring a real number-based result, no standardization function,
    such as sigmoid, should be applied. In this manner, we won''t be changing the
    outcome of the variable to one of the many possible class values, getting a continuum
    of possible outcomes. Let''s take a look at the following types of problems to
    be tackled:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression/function approximation problems:** This type of problem uses a
    min squared error function, a linear output activation, and sigmoidal hidden activations.
    This will give us a real value for output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification problems (two classes, one output)**: In this kind of problem
    we normally have a cross-entropy cost function, a sigmoid output, and hidden activations.
    The sigmoid function will give us the probability of occurrence or nonoccurrence
    of one of the classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification problems (multiple-classes, one output per class)**: In this
    kind of problem, we will have a cross-entropy cost function with softmax outputs
    and sigmoid hidden activations, in order to have an output of the probabilities
    for any of the possible classes for a single input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a simple function with a single-layer perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Take a look at the following code snippet to implement a single function with
    a single-layer perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Defining and graphing transfer function types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The learning properties of a neural network would not be very good with just
    the help of a univariate linear classifier. Even some mildly complex problems
    in machine learning involve multiple non-linear variables, so many variants were
    developed as replacements for the transfer functions of the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: In order to represent non-linear models, a number of different non-linear functions
    can be used in the activation function. This implies changes in the way the neurons
    will react to changes in the input variables. In the following sections, we will
    define the main different transfer functions and define and represent them via
    code.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will start using some **object-oriented programming** (**OOP**)
    techniques from Python to represent entities of the problem domain. This will
    allow us to represent concepts in a much clearer way in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by creating a `TransferFunction` class, which will contain the
    following two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`getTransferFunction(x)`: This method will return an activation function determined
    by the class type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getTransferFunctionDerivative(x)`: This method will clearly return its derivative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For both functions, the input will be a NumPy array and the function will be
    applied element by element, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Representing and understanding the transfer functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code snippet to see how the transfer function
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Sigmoid or logistic function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sigmoid or logistic function is the canonical activation function and is well-suited
    for calculating probabilities in classification properties. Firstly, let's prepare
    a function that will be used to graph all the transfer functions with their derivatives,
    from a common range of `-2.0` to `2.0`, which will allow us to see the main characteristics
    of them around the *y* axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The classical formula for the sigmoid function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Playing with the sigmoid
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next, we will do an exercise to get an idea of how the sigmoid changes when
    multiplied by the weights and shifted by the bias to accommodate the final function
    towards its minimum. Let''s then vary the possible parameters of a single sigmoid
    first and see it stretch and move:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e172f3-3308-43d1-86cf-2ab429ddca97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take a look at the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Lets take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79183576-593f-47a5-ab50-a0cc6d1b8637.png)'
  prefs: []
  type: TYPE_IMG
- en: Rectified linear unit or ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ReLU** is called a rectified linear unit, and one of its main advantages
    is that it is not affected by vanishing gradient problems, which generally consist
    of the first layers of a network tending to be values of zero, or a tiny epsilon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe3f187c-9745-4477-b1dc-581078123250.png)'
  prefs: []
  type: TYPE_IMG
- en: Linear transfer function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code snippet to understand the linear transfer
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining loss functions for neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with every model in machine learning, we will explore the possible functions
    that we will use to determine how well our predictions and classification went.
  prefs: []
  type: TYPE_NORMAL
- en: The first type of distinction we will do is between the L1 and L2 error function
    types.
  prefs: []
  type: TYPE_NORMAL
- en: 'L1, also known as **least absolute deviations** (**LAD**) or **least absolute
    errors** (**LAE**), has very interesting properties, and it simply consists of
    the absolute difference between the final result of the model and the expected
    one, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b812faa4-4083-4230-9a61-0f17931c0979.png)![](img/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png)'
  prefs: []
  type: TYPE_IMG
- en: L1 versus L2 properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s time to do a head-to-head comparison between the two types of loss
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Robustness**: L1 is a more robust loss function, which can be expressed as
    the resistance of the function when being affected by outliers, which projects
    a quadratic function to very high values. Thus, in order to choose an L2 function,
    we should have very stringent data cleaning for it to be efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stability**: The stability property assesses how much the error curve jumps
    for a large error value. L1 is more unstable, especially for non-normalized datasets
    (because numbers in the `[-1, 1]` range diminish when squared).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution uniqueness**: As can be inferred by its quadratic nature, the L2
    function ensures that we will have a unique answer for our search for a minimum.
    L2 always has a unique solution, but L1 can have many solutions, due to the fact
    that we can find many paths with minimal length for our models in the form of
    piecewise linear functions, compared to the single line distance in the case of
    L2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regarding usage, the summation of the past properties allows us to use the L2
    error type in normal cases, especially because of the solution uniqueness, which
    gives us the required certainty when starting to minimize error values. In the
    first example, we will start with a simpler L1 error function for educational
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore these two approaches by graphing the error results for a sample
    L1 and L2 loss error function. In the next simple example, we will show you the
    very different nature of the two errors. In the first two examples, we have normalized
    the input between `-1` and `1` and then with values outside that range.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, from samples `0` to `3`, the quadratic error increases steadily
    and continuously, but with non-normalized data it can explode, especially with
    outliers, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ff32091-af2b-40f5-9e24-283a72afb96f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s define the loss functions in the form of a `LossFunction` class and
    a `getLoss` method for the L1 and L2 loss function types, receiving two NumPy
    arrays as parameters, `y_`, or the estimated function value, and `y`, the expected
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to define the goal function, which we will define as a simple
    `Boolean`. In order to allow faster convergence, it will have a direct relationship
    between the first input variable and the function''s outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The first model we will use is a very minimal neural network with three cells
    and a weight for each one, without bias, in order to keep the model''s complexity
    to a minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output generated by running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will define a set of variables to collect the model''s error, the weights,
    and training results progression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Then it's time to do the iterative error minimization. In this case, it will
    consist of feeding the whole true table 100 times via the weights and the neuron's
    transfer function, adjusting the weights in the direction of the error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this model doesn''t use a learning rate, so it should converge (or
    diverge) quickly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s simply review the last evaluation step by printing the output values
    at `l1`. Now we can see that we are reflecting quite literally the output of the
    original function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To better understand the process, let''s have a look at how the parameters
    change over time. First, let''s graph the neuron weights. As you can see, they
    go from a random state to accepting the whole values of the first column (which
    is always right), going to almost `0` for the second column (which is right 50%
    of the time), and then going to `-2` for the third (mainly because it has to trigger
    `0` in the first two elements of the table):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/448f7444-c937-497c-a880-31f062970776.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s also review how our solutions evolved (during the first 40 iterations)
    until we reached the last iteration; we can clearly see the convergence to the
    ideal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/450b72d6-0559-44d8-b979-24378328f577.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see how the error evolves and tends to be zero through the different
    epochs. In this case, we can observe that it swings from negative to positive,
    which is possible because we first used an L1 error function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6643314-6b1d-490c-937b-8c278c173ec0.png)'
  prefs: []
  type: TYPE_IMG
- en: Depiction of the decreasing training error of our simple Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a very important step towards solving complex problems
    together by means of implementing our first neural network. Now, the following
    architectures will have familiar elements, and we will be able to extrapolate
    the knowledge aquired on this chapter, to novel architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more complex models and problems, using
    more layers and special configurations, such as convolutional and dropout layers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following content:'
  prefs: []
  type: TYPE_NORMAL
- en: 'McCulloch, Warren S., and Walter Pitts*,. A logical calculus of the ideas immanent
    in nervous activity. The bulletin of mathematical biophysics 5.4 (1943): 115-133.*
    Kleene, Stephen Cole. Representation of events in nerve nets and finite automata.
    No. RAND-RM-704\. RAND PROJECT AIR FORCE SANTA MONICA CA, 1951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farley, B. W. A. C., and W. Clark*,* *Simulation of self-organizing systems
    by digital computer.* Transactions of the IRE Professional Group on Information
    Theory 4.4 (1954): 76-84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenblatt, Frank*, The perceptron: A probabilistic model for information storage
    and organization in the brain,* Psychological review 65.6 (1958): 386.Rosenblatt,
    Frank. x.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principles of Neurodynamics: perceptrons and the Theory of Brain Mechanisms.
    Spartan Books, Washington DC, 1961'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos, P.J. (1975), *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparata, Franco P., and Michael Ian Shamos*,. "Introduction." Computational
    Geometry. Springer New York, 1985\. 1-35.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart, David E., Geoffrey E. Hinton, and Ronald J*, Williams. Learning internal
    representations by error propagation. No. ICS-8506\. California Univ San Diego
    La Jolla Inst for Cognitive Science, 1985.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rumelhart, James L. McClelland, and the PDP research group. *Parallel distributed
    processing: Explorations in the microstructure of cognition, Volume 1: Foundation.
    MIT Press, 1986.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko, G. 1989*. Approximation by superpositions of a sigmoidal function Mathematics
    of Control, Signals, and Systems, 2(4), 303–314.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murtagh, Fionn*. Multilayer perceptrons for classification and regression.
    Neurocomputing 2.5 (1991): 183-197.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber, Jürgen*. Deep learning in neural networks: An overview. Neural
    networks 61 (2015): 85-117.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
