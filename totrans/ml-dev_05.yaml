- en: Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œ
- en: As a developer, you have surely gained an interest in machine learning from
    looking at all the incredibly amazing applications that you see on your regular
    devices every dayâ€”automatic speech translation, picture style transfer, the ability
    to generate new pictures from sample ones, and so on. Brace yourself... we are
    heading directly into the technology that has made all these things possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€åå¼€å‘è€…ï¼Œä½ ä¸€å®šä»æ¯å¤©åœ¨å¸¸è§„è®¾å¤‡ä¸Šçœ‹åˆ°çš„ä»¤äººéš¾ä»¥ç½®ä¿¡çš„ç¥å¥‡åº”ç”¨ä¸­äº§ç”Ÿäº†å¯¹æœºå™¨å­¦ä¹ çš„å…´è¶£â€”â€”è‡ªåŠ¨è¯­éŸ³ç¿»è¯‘ã€å›¾ç‰‡é£æ ¼è½¬æ¢ã€ä»æ ·æœ¬å›¾ç‰‡ç”Ÿæˆæ–°å›¾ç‰‡çš„èƒ½åŠ›ç­‰ç­‰ã€‚åšå¥½å‡†å¤‡å§...æˆ‘ä»¬å°†ç›´æ¥è¿›å…¥ä½¿æ‰€æœ‰è¿™äº›æˆä¸ºå¯èƒ½çš„æŠ€æœ¯ã€‚
- en: Linear and logistic models, such as the ones we've observed, have certain limitations
    in terms of the complexity of the training dataset they train a model on, even
    when they are the basis of very articulated and efficient solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: çº¿æ€§å’Œé€»è¾‘æ¨¡å‹ï¼Œå¦‚æˆ‘ä»¬æ‰€è§‚å¯Ÿåˆ°çš„ï¼Œåœ¨è®­ç»ƒæ•°æ®é›†çš„å¤æ‚åº¦æ–¹é¢å­˜åœ¨æŸäº›å±€é™æ€§ï¼Œå³ä½¿å®ƒä»¬æ˜¯é«˜åº¦ç»†è‡´å’Œé«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„åŸºç¡€ã€‚
- en: How complex does a model have to be to capture the style of writing of an author,
    the concept of an image of a cat versus an image of a dog, or the classification
    of a plant based on visual elements? These things require the summation of a huge
    mix of low-level and high-level details captured, in the case of our brain by
    specialized sets of neurons, and in computer science by neural models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹éœ€è¦æœ‰å¤šå¤æ‚æ‰èƒ½æ•æ‰åˆ°ä½œè€…çš„å†™ä½œé£æ ¼ã€çŒ«å’Œç‹—çš„å›¾åƒæ¦‚å¿µï¼Œæˆ–åŸºäºè§†è§‰å…ƒç´ çš„æ¤ç‰©åˆ†ç±»ï¼Ÿè¿™äº›äº‹æƒ…éœ€è¦å¤§é‡ä½çº§å’Œé«˜çº§ç»†èŠ‚çš„æ±‡æ€»ï¼Œåœ¨æˆ‘ä»¬çš„å¤§è„‘ä¸­æ˜¯é€šè¿‡ä¸“é—¨çš„ç¥ç»å…ƒé›†åˆæ•è·çš„ï¼Œåœ¨è®¡ç®—æœºç§‘å­¦ä¸­æ˜¯é€šè¿‡ç¥ç»ç½‘ç»œæ¨¡å‹æ•è·çš„ã€‚
- en: Contrary to what's expected, in this book I'll spare you the typical introduction
    to the nervous system, its capabilities, the number of neurons in the nervous
    system, its chemical properties, and so on. I find that these topics add an aura
    of impossibility to problems, but the models we are about to learn are simple
    math equations with computational counterparts, which we will try to emphasize
    to allow you, a person with algorithmic interests, to easily understand them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸é¢„æœŸç›¸åï¼Œåœ¨è¿™æœ¬ä¹¦ä¸­ï¼Œæˆ‘å°†çœç•¥å¯¹ç¥ç»ç³»ç»Ÿçš„å…¸å‹ä»‹ç»ï¼ŒåŒ…æ‹¬å…¶èƒ½åŠ›ã€ç¥ç»ç³»ç»Ÿä¸­çš„ç¥ç»å…ƒæ•°é‡ã€å…¶åŒ–å­¦ç‰¹æ€§ç­‰ç­‰ã€‚æˆ‘å‘ç°è¿™äº›è¯é¢˜ç»™é—®é¢˜å¢æ·»äº†ä¸€ç§ä¸å¯èƒ½æ€§ï¼Œä½†æˆ‘ä»¬å³å°†å­¦ä¹ çš„æ¨¡å‹æ˜¯ç®€å•çš„æ•°å­¦æ–¹ç¨‹å¼ï¼Œå…·æœ‰è®¡ç®—å¯¹åº”ç‰©ï¼Œæˆ‘ä»¬å°†å°è¯•å¼ºè°ƒè¿™ä¸€ç‚¹ï¼Œä»¥ä¾¿ä½ ï¼Œä¸€ä¸ªå¯¹ç®—æ³•æ„Ÿå…´è¶£çš„äººï¼Œèƒ½å¤Ÿè½»æ¾ç†è§£å®ƒä»¬ã€‚
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: The history of neural models, including perceptron and ADALINE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»æ¨¡å‹çš„å†å²ï¼ŒåŒ…æ‹¬æ„ŸçŸ¥å™¨å’ŒADALINE
- en: Neural networks and the kinds of problems they solve
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå’Œå®ƒä»¬è§£å†³çš„é—®é¢˜ç±»å‹
- en: The multilayer perceptron
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨
- en: Implementing a simple neural layer to model a binary function
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œå±‚æ¥æ¨¡æ‹ŸäºŒå…ƒå‡½æ•°
- en: This chapter will allow you to use the building blocks of most of the awesome
    machine learning applications you see daily. So, let's get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†ä½¿ä½ èƒ½å¤Ÿä½¿ç”¨ä½ æ¯å¤©çœ‹åˆ°çš„ç»å¤§å¤šæ•°ä»¤äººæƒŠå¹çš„æœºå™¨å­¦ä¹ åº”ç”¨çš„åŸºç¡€æ„å»ºå—ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: History of neural models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¥ç»æ¨¡å‹çš„å†å²
- en: Neural models, in the sense of being disciplines that try to build representations
    of the internal workings of the brain, originated pretty distantly in the computer
    science timescale. They even date back to the time when the origins of modern
    computing were being invented, the mid-1940s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—æœºç§‘å­¦çš„æ—¶é—´å°ºåº¦ä¸Šï¼Œç¥ç»æ¨¡å‹ä½œä¸ºä¸€ç§è¯•å›¾æ„å»ºå¤§è„‘å†…éƒ¨å·¥ä½œè¡¨ç¤ºçš„å­¦ç§‘ï¼Œèµ·æºç›¸å½“é¥è¿œã€‚å®ƒä»¬ç”šè‡³å¯ä»¥è¿½æº¯åˆ°ç°ä»£è®¡ç®—æœºèµ·æºçš„ä¸­æœŸâ€”â€”20ä¸–çºª40å¹´ä»£ä¸­æœŸã€‚
- en: At that time, the fields of neuroscience and computer science began to collaborate
    by researching ways of emulating the way the brain processes information, starting
    from its constituent unitâ€”the neuron.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é‚£æ—¶ï¼Œç¥ç»ç§‘å­¦å’Œè®¡ç®—æœºç§‘å­¦å¼€å§‹é€šè¿‡ç ”ç©¶æ¨¡æ‹Ÿå¤§è„‘å¤„ç†ä¿¡æ¯çš„æ–¹å¼å¼€å§‹åˆä½œï¼Œä»å…¶æ„æˆå•å…ƒâ€”â€”ç¥ç»å…ƒå¼€å§‹ã€‚
- en: The first mathematical method for representing the learning function of the
    human brain can be assigned to McCulloch and Pitts, in their 1943 paper *A Logical
    Calculus of Ideas Immanent in Nervous Activity:*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»å¤§è„‘å­¦ä¹ åŠŸèƒ½çš„ç¬¬ä¸€ç§æ•°å­¦æ–¹æ³•å¯ä»¥å½’åŠŸäºéº¦å¡æ´›å…‹å’Œçš®èŒ¨ï¼Œä»–ä»¬åœ¨1943å¹´çš„è®ºæ–‡ã€Šå†…åœ¨äºç¥ç»æ´»åŠ¨çš„é€»è¾‘æ¼”ç®—ï¼š*A Logical Calculus of
    Ideas Immanent in Nervous Activity:*ã€‹ä¸­æå‡ºã€‚
- en: '![](img/0e0ee661-6f00-42e3-87a4-35139415faa8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/0e0ee661-6f00-42e3-87a4-35139415faa8.png)'
- en: McCulloch and Pitts model
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: éº¦å¡æ´›å…‹å’Œçš®èŒ¨æ¨¡å‹
- en: This simple model was a basic but realistic model of a learning algorithm. You
    will be surprised by what happens if we use a linear function as a transfer function;
    this is a simple linear model, as were the ones we saw in the previous chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„æ¨¡å‹æ˜¯ä¸€ä¸ªåŸºæœ¬ä½†ç°å®çš„ç®—æ³•æ¨¡å‹ã€‚å¦‚æœä½ ä½¿ç”¨çº¿æ€§å‡½æ•°ä½œä¸ºä¼ é€’å‡½æ•°ï¼Œä½ ä¼šå¯¹å‘ç”Ÿçš„äº‹æƒ…æ„Ÿåˆ°æƒŠè®¶ï¼›è¿™æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: You may have noted that we are now using the `w` letter to specify the parameters
    to be tuned by the model. Take this as the new standard from now on. Our old `Î²`
    parameters in linear regression models will now be `w`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: But the model hasn't determined a way to tune the parameters. Let's go forward
    to the 1950s and review the **perceptron** model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron model
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron model is one of the simplest ways to implement an artificial
    neuron. It was initially developed at the end of the 1950s, with the first hardware
    implementation being carried out in the 1960s. First, it was the name of a machine,
    and later became the name of an algorithm. Yes, perceptrons are not the strange
    entities we always thought they were, they're what you as a developer handle each
    dayâ€” algorithms!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following steps and learn how it works:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function for a perceptron is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ef51780-af9a-41cd-8c1f-5749a0aa2e4e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: If *yâ€™ â‰  y*, modify all the connections,Â *wi*, by adding the changes *Î”w =yxi*.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to *step 2*.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is basically an algorithm that learns a binary classification function and
    maps a real function to a single binary function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s depict the new architecture of the perceptron and analyze the new scheme
    of the algorithm in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dd545c6-e350-4130-a7e7-45da7a1065c6.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Perceptron model (With changes from previous model highlighted)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron builds on the ideas of its predecessors, but the novelty here
    is that we are adding a proper learning mechanism! In the following diagram, we
    have highlighted the new properties of the modelâ€”the feedback loop, where we calculate
    the error of our outcomes, and the adjustment of weightsâ€”with a predetermined
    formula:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db8ec5ff-a2b3-48bb-86ba-d7bb01b6c5b8.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Perceptron algorithm flowchart
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Improving our predictions â€“ the ADALINE algorithm
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ADALINE is another algorithm (yes, remember we are talking about algorithms)
    used to train neural networks. ADALINE is in some ways more advanced than the
    perceptron because it adds a new training method: gradient descent, which should
    be known to you by now. Additionally, it changes the point at which the error
    is measured before the activation output is applied to the summation of the weights:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48d0f5a9-2fb4-4a18-a130-ab8d98268c49.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Adaline model (With additions from Perceptron highlighted)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this is the standard way of representing the ADALINE algorithm in a structural
    way. As an algorithm consists of a series of steps, let''s aggregate those in
    a more detailed way, with some additional mathematical details:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output value that we will be taking will be the one after the summation:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**y=Î£(xi * wi)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the error, comparing the model output with the right label *o*:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*E=(o-y)Â²*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Does it look similar to something we have already seen? Yes! We are basically
    resolving a regression problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Adjust the weights with the following gradient descent recursion:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/58540fe4-1f28-4b5a-b899-7b20601ad6da.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Return to *step 2*:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8429de2-2994-466a-b964-e181702e4f94.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Adaline algorithm flowchart
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Similarities and differences between a perceptron and ADALINE
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a simplified explanation of the precursors of modern neural
    networks. As you can see, the elements of modern models were almost all laid out
    during the 1950s and the 1960s! Before continuing, let''s try to compare the approaches:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarities**:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are both algorithms (it's important to stress that)
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are applied to single-layer neural models
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are classifiers for binary classification
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both have a linear decision boundary
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both can learn iteratively, sample by sample (the perceptron naturally, and
    ADALINE via stochastic gradient descent)
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both use a threshold function
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differences**:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perceptron uses the final class decision to train weights
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ADALINE uses continuous predicted values (from the net input) to learn the model
    coefficients, and measures subtle changes in error with a continuous float point
    rank, not a Boolean or integer
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we are done with our single-layer architectures and models, we will review
    some discoveries from the end of the 1960s that provoked quite a stir in the neural
    models community and were said to generate the first AI winter, or an abrupt collapse
    in the interest in machine learning research. Happily, some years after that,
    researchers found ways to overcome the limitations they faced, but this will be
    covered further on in the chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of early models
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model itself now has most of the elements of any normal neural model, but
    it had its own problems. After some years of fast development, the publication
    of the book *P**erceptrons* by Minsky and Papert in 1969 prompted a stir in the
    field because of its main idea that perceptrons could only work on linearly separable
    problems, which were only a very tiny part of the problems that the practitioners
    thought were solvable. In a sense, the book suggested that perceptrons were verging
    on being useless, with the exception of the simplest classification tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'This new-found deficiency could be represented by the inability of the model
    to represent the XOR function, which is a `Boolean` function with an output of
    `1` when the inputs are different, and `0` when they are equal:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/935f5509-f277-4e86-b870-241374cfc73f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: The problem of modelling a XOR function. No single line will correctly separate
    the 0 and 1 values
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, the main problem is that neither class
    (cross nor point) is linearly separable; that is, we can not separate the two
    with any linear function on the plane.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰å›¾æ‰€ç¤ºï¼Œä¸»è¦é—®é¢˜æ˜¯ä¸¤ä¸ªç±»åˆ«ï¼ˆäº¤å‰å’Œç‚¹ï¼‰éƒ½ä¸æ˜¯çº¿æ€§å¯åˆ†çš„ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æ— æ³•åœ¨å¹³é¢ä¸Šç”¨ä»»ä½•çº¿æ€§å‡½æ•°å°†å®ƒä»¬åˆ†å¼€ã€‚
- en: This detected problem caused a decrease in activity in the field that lasted
    for more or less five years, until the development of the backpropagation algorithm,
    which started in the mid 1970s.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ£€æµ‹åˆ°çš„é—®é¢˜å¯¼è‡´è¯¥é¢†åŸŸæ´»åŠ¨å‡å°‘ï¼ŒæŒç»­äº†å¤§çº¦äº”å¹´ï¼Œç›´åˆ°åå‘ä¼ æ’­ç®—æ³•çš„å‘å±•ï¼Œè¿™å§‹äº20ä¸–çºª70å¹´ä»£ä¸­æœŸã€‚
- en: Single and multilayer perceptrons
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å•å±‚å’Œå¤šå±‚æ„ŸçŸ¥å™¨
- en: Now we will discuss more contemporary times, building on the previous concepts
    to allow more complex elements of reality to be modeled.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ›´ç°ä»£çš„æ—¶æœŸï¼ŒåŸºäºå…ˆå‰æ¦‚å¿µï¼Œä»¥å…è®¸æ›´å¤æ‚çš„ç°å®å…ƒç´ è¢«å»ºæ¨¡ã€‚
- en: In this section, we will directly study **multilayer perceptrons** (**MLPs**),
    which are the most common configuration used, and will consider a **single-layer
    perceptron** as a particular case of the former, highlighting the differences.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç›´æ¥ç ”ç©¶**å¤šå±‚æ„ŸçŸ¥å™¨**ï¼ˆ**MLPs**ï¼‰ï¼Œè¿™æ˜¯æœ€å¸¸ç”¨çš„é…ç½®ï¼Œå¹¶å°†**å•å±‚æ„ŸçŸ¥å™¨**è§†ä¸ºå‰è€…çš„ä¸€ä¸ªç‰¹ä¾‹ï¼Œçªå‡ºå®ƒä»¬ä¹‹é—´çš„å·®å¼‚ã€‚
- en: 'Single-layer and multilayer perceptrons were the most commonly used architecture
    during the 1970s and 1980s and provided huge advances in terms of the capabilities
    of neural systems. The main innovations they bring to the table are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å•å±‚å’Œå¤šå±‚æ„ŸçŸ¥å™¨åœ¨20ä¸–çºª70å¹´ä»£å’Œ80å¹´ä»£æ˜¯æœ€å¸¸ç”¨çš„æ¶æ„ï¼Œåœ¨ç¥ç»ç½‘ç»œçš„èƒ½åŠ›æ–¹é¢å–å¾—äº†å·¨å¤§çš„è¿›æ­¥ã€‚å®ƒä»¬å¸¦æ¥çš„ä¸»è¦åˆ›æ–°å¦‚ä¸‹ï¼š
- en: They are feedforward networks because the calculations, starting from the inputs,
    flow from layer to layer without any cycling (information never returns)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒä»¬æ˜¯å‰é¦ˆç½‘ç»œï¼Œå› ä¸ºè®¡ç®—ä»è¾“å…¥å¼€å§‹ï¼Œä»ä¸€å±‚æµå‘å¦ä¸€å±‚ï¼Œæ²¡æœ‰ä»»ä½•å¾ªç¯ï¼ˆä¿¡æ¯æ°¸è¿œä¸ä¼šè¿”å›ï¼‰
- en: They use the backpropagation method to adjust their weights
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»–ä»¬ä½¿ç”¨åå‘ä¼ æ’­æ–¹æ³•æ¥è°ƒæ•´ä»–ä»¬çš„æƒé‡
- en: The use of the `step` function as a transfer function is replaced by non-linear
    ones such as the sigmoid
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†`step`å‡½æ•°ä½œä¸ºä¼ é€’å‡½æ•°çš„ä½¿ç”¨è¢«éçº¿æ€§å‡½æ•°å¦‚sigmoidæ‰€å–ä»£
- en: MLP origins
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLPèµ·æº
- en: 'After the power of single-unit neural models had been explored, an obvious
    step was to generate layers or sets of commonly connected units (we define a connection
    as the act of sending the output of one unit to be part of another unit''s summation):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¢ç´¢äº†å•å•å…ƒç¥ç»ç½‘ç»œæ¨¡å‹çš„åŠ›é‡ä¹‹åï¼Œä¸€ä¸ªæ˜æ˜¾çš„æ­¥éª¤æ˜¯ç”Ÿæˆå±‚æˆ–ä¸€ç»„é€šå¸¸ç›¸äº’è¿æ¥çš„å•å…ƒï¼ˆæˆ‘ä»¬å®šä¹‰è¿æ¥ä¸ºå°†ä¸€ä¸ªå•å…ƒçš„è¾“å‡ºå‘é€ç»™å¦ä¸€ä¸ªå•å…ƒçš„æ±‚å’Œéƒ¨åˆ†ï¼‰ï¼š
- en: '![](img/9fa32993-2a1f-4143-ad6d-86ff94158092.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/9fa32993-2a1f-4143-ad6d-86ff94158092.png)'
- en: Depiction of a simple multilayer feed forward Neural Network
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€å•å¤šå±‚å‰é¦ˆç¥ç»ç½‘ç»œç¤ºæ„å›¾
- en: The feedforward mechanism
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‰é¦ˆæœºåˆ¶
- en: In this phase of the operation of the network, the data will be input in the
    first layer and will flow from each unit to the corresponding units in the following
    layers. Then it will be summed and passed through in the hidden layers, and finally
    processed by the output layer. This process is totally unidirectional, so we are
    avoiding any recursive complications in the data flow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç½‘ç»œæ“ä½œçš„è¿™ä¸€é˜¶æ®µï¼Œæ•°æ®å°†è¢«è¾“å…¥åˆ°ç¬¬ä¸€å±‚ï¼Œå¹¶ä»æ¯ä¸ªå•å…ƒæµå‘ä¸‹ä¸€å±‚çš„ç›¸åº”å•å…ƒã€‚ç„¶åå®ƒå°†åœ¨éšè—å±‚ä¸­è¿›è¡Œæ±‚å’Œå¹¶ä¼ é€’ï¼Œæœ€åç”±è¾“å‡ºå±‚å¤„ç†ã€‚è¿™ä¸ªè¿‡ç¨‹æ˜¯å®Œå…¨å•å‘çš„ï¼Œå› æ­¤æˆ‘ä»¬é¿å…äº†æ•°æ®æµä¸­çš„ä»»ä½•é€’å½’å¤æ‚æ€§ã€‚
- en: The feedforward mechanism, has in the MLP its counterpart in the training part
    of the modeling process, which will be in charge of improving the model's performance.
    The normally chosen algorithm is called **backpropagation**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¦ˆæœºåˆ¶åœ¨MLPä¸­å¯¹åº”äºå»ºæ¨¡è¿‡ç¨‹çš„è®­ç»ƒéƒ¨åˆ†ï¼Œè´Ÿè´£æé«˜æ¨¡å‹æ€§èƒ½ã€‚é€šå¸¸é€‰æ‹©çš„ç®—æ³•è¢«ç§°ä¸º**åå‘ä¼ æ’­**ã€‚
- en: The chosen optimization algorithm â€“ backpropagation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€‰æ‹©çš„ä¼˜åŒ–ç®—æ³•â€”â€”åå‘ä¼ æ’­
- en: From the perceptron algorithm onward, every neural architecture has had a means
    to optimize its internal parameters based on the comparison of the ground truth
    with the model output. The common assumption was to take the derivative of the
    (then simple) model function and iteratively work towards the minimum value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ„ŸçŸ¥å™¨ç®—æ³•å¼€å§‹ï¼Œæ¯ä¸ªç¥ç»ç½‘ç»œæ¶æ„éƒ½æœ‰ä¸€ç§åŸºäºçœŸå®å€¼ä¸æ¨¡å‹è¾“å‡ºæ¯”è¾ƒæ¥ä¼˜åŒ–å…¶å†…éƒ¨å‚æ•°çš„æ–¹æ³•ã€‚å¸¸è§çš„å‡è®¾æ˜¯å–ï¼ˆå½“æ—¶ç®€å•çš„ï¼‰æ¨¡å‹å‡½æ•°çš„å¯¼æ•°ï¼Œå¹¶è¿­ä»£åœ°æœç€æœ€å°å€¼å·¥ä½œã€‚
- en: For complex multilayer networks, there is an additional overhead, which has
    to do with the fact that the output layer's output is the result of a long chain
    of functions compositions, where each layer's output is wrapped by the next one's
    transfer function. So, the derivative of the output will involve the derivative
    of an exceedingly complex function. In this case, the backpropagation method was
    proposed, with excellent results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤æ‚çš„å¤šå±‚ç½‘ç»œï¼Œå­˜åœ¨é¢å¤–çš„å¼€é”€ï¼Œè¿™ä¸è¾“å‡ºå±‚çš„è¾“å‡ºæ˜¯å‡½æ•°å¤åˆçš„é•¿é“¾çš„ç»“æœæœ‰å…³ï¼Œå…¶ä¸­æ¯ä¸€å±‚çš„è¾“å‡ºéƒ½è¢«ä¸‹ä¸€å±‚çš„ä¼ é€’å‡½æ•°æ‰€åŒ…è£¹ã€‚å› æ­¤ï¼Œè¾“å‡ºå¯¼æ•°å°†æ¶‰åŠä¸€ä¸ªæå…¶å¤æ‚çš„å‡½æ•°çš„å¯¼æ•°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæå‡ºäº†åå‘ä¼ æ’­æ–¹æ³•ï¼Œå¹¶å–å¾—äº†ä¼˜å¼‚çš„ç»“æœã€‚
- en: Backpropagation can be summarized as an algorithm used to calculate derivatives.
    The main attribute is that it is computationally efficient and works with complex
    functions. It is also a generalization of the least mean squares algorithm in
    the linear perceptron, which we already knew!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åå‘ä¼ æ’­å¯ä»¥æ€»ç»“ä¸ºä¸€ç§ç”¨äºè®¡ç®—å¯¼æ•°çš„ç®—æ³•ã€‚å…¶ä¸»è¦å±æ€§æ˜¯è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚ç”¨äºå¤æ‚å‡½æ•°ã€‚å®ƒä¹Ÿæ˜¯çº¿æ€§æ„ŸçŸ¥å™¨ä¸­æœ€å°å‡æ–¹è¯¯å·®ç®—æ³•çš„æ¨å¹¿ï¼Œè¿™æ˜¯æˆ‘ä»¬å·²çŸ¥çš„ï¼
- en: In the backpropagation algorithm, the responsibility of the error will be distributed
    among all the functions applied to the data in the whole architecture. So, the
    goal is to minimize the error, the gradient of the loss function, over a set of
    deeply compounded functions, which will again receive the help of the chain rule.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åå‘ä¼ æ’­ç®—æ³•ä¸­ï¼Œé”™è¯¯çš„èŒè´£å°†åœ¨æ•´ä¸ªæ¶æ„ä¸­åº”ç”¨äºæ‰€æœ‰åº”ç”¨äºæ•°æ®çš„å‡½æ•°ã€‚å› æ­¤ï¼Œç›®æ ‡æ˜¯æœ€å°åŒ–è¯¯å·®ï¼Œå³æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œåœ¨ä¸€ç³»åˆ—æ·±åº¦å¤åˆå‡½æ•°ä¸Šï¼Œè¿™åˆå°†å†æ¬¡å¾—åˆ°é“¾å¼æ³•åˆ™çš„å¸®åŠ©ã€‚
- en: 'Now it''s time to define our general algorithm for our modern version of a
    neural network in the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å®šä¹‰æˆ‘ä»¬ç°ä»£ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬ç®—æ³•äº†ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›æ­¥éª¤ï¼š
- en: Calculate the feedforward signals from the input to the output.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—ä»è¾“å…¥åˆ°è¾“å‡ºçš„å‰å‘ä¿¡å·ã€‚
- en: Calculate output error *EÂ *based on the prediction *a[k]* and the target *t[k]*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®é¢„æµ‹å€¼ *a[k]* å’Œç›®æ ‡å€¼ *t[k]* è®¡ç®—è¾“å‡ºè¯¯å·® *E*ã€‚
- en: Backpropagate the error signals by weighting them by the weights in the previous
    layers and the gradients of the associated activation functions.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡åŠ æƒå‰ä¸€å±‚ä¸­çš„æƒé‡å’Œå…³è”æ¿€æ´»å‡½æ•°çš„æ¢¯åº¦æ¥åå‘ä¼ æ’­é”™è¯¯ä¿¡å·ã€‚
- en: Calculate the gradients ğ›¿*E*/ğ›¿*Î¸*Â for the parameters based on the backpropagated
    error signal and the feedforward signals from the inputs.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ ¹æ®åå‘ä¼ æ’­çš„é”™è¯¯ä¿¡å·å’Œä»è¾“å…¥çš„å‰å‘ä¿¡å·ï¼Œè®¡ç®—å‚æ•°çš„æ¢¯åº¦ ğ›¿*E*/ğ›¿*Î¸*ã€‚
- en: Update the parameters using the calculated gradientsÂ  *Î¸Â â†Â Î¸ -Â Î·* ğ›¿E/ğ›¿Î¸ *.*
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è®¡ç®—å‡ºçš„æ¢¯åº¦æ›´æ–°å‚æ•° *Î¸ â† Î¸ - Î·* ğ›¿E/ğ›¿Î¸ *.
- en: 'Lets review this process now, in a graphical way:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥å›¾å½¢æ–¹å¼å›é¡¾è¿™ä¸ªè¿‡ç¨‹ï¼š
- en: '![](img/6f3c94d8-c460-41e9-87c4-2af201ab0027.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/6f3c94d8-c460-41e9-87c4-2af201ab0027.png)'
- en: Step by step representation of the Feed ForwardÂ  and Back Propagation training
    process
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: é€æ­¥å±•ç¤ºå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è®­ç»ƒè¿‡ç¨‹
- en: 'In the following diagram, we will represent the whole process in an algorithmic
    way. You can see the number of coincidences with the previous optimization methods,
    and the short number of computation blocks involved:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªè¿‡ç¨‹ä»¥ç®—æ³•æ–¹å¼è¡¨ç¤ºã€‚æ‚¨å¯ä»¥çœ‹åˆ°ä¸å…ˆå‰ä¼˜åŒ–æ–¹æ³•çš„å·§åˆä¹‹å¤„ï¼Œä»¥åŠæ¶‰åŠçš„è®¡ç®—å—æ•°é‡è¾ƒå°‘ï¼š
- en: '![](img/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png)'
- en: Flowchart of the feedforward/backpropagation scheme
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‰å‘/åå‘ä¼ æ’­æ–¹æ¡ˆçš„æµç¨‹å›¾
- en: Types of problem to be tackled
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¦è§£å†³çš„é—®é¢˜ç±»å‹
- en: 'Neural networks can be used for both regression problems and classification
    problems. The common architectural difference resides in the output layer: in
    order to be able to bring a real number-based result, no standardization function,
    such as sigmoid, should be applied. In this manner, we won''t be changing the
    outcome of the variable to one of the many possible class values, getting a continuum
    of possible outcomes. Let''s take a look at the following types of problems to
    be tackled:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå¯ä»¥ç”¨äºå›å½’é—®é¢˜å’Œåˆ†ç±»é—®é¢˜ã€‚å¸¸è§çš„æ¶æ„å·®å¼‚åœ¨äºè¾“å‡ºå±‚ï¼šä¸ºäº†èƒ½å¤Ÿå¾—åˆ°åŸºäºå®æ•°çš„ç»“æœï¼Œä¸åº”åº”ç”¨ä»»ä½•æ ‡å‡†åŒ–å‡½æ•°ï¼Œå¦‚sigmoidå‡½æ•°ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬ä¸ä¼šå°†å˜é‡çš„ç»“æœæ”¹å˜ä¸ºè®¸å¤šå¯èƒ½çš„ç±»åˆ«å€¼ä¹‹ä¸€ï¼Œä»è€Œå¾—åˆ°ä¸€ç³»åˆ—å¯èƒ½çš„è¿ç»­ç»“æœã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»¥ä¸‹è¦è§£å†³çš„é—®é¢˜ç±»å‹ï¼š
- en: '**Regression/function approximation problems:**Â This type of problem uses a
    min squared error function, a linear output activation, and sigmoidalÂ hidden activations.
    This will give us a real value for output.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å›å½’/å‡½æ•°é€¼è¿‘é—®é¢˜**ï¼šè¿™ç±»é—®é¢˜ä½¿ç”¨æœ€å°å¹³æ–¹è¯¯å·®å‡½æ•°ã€çº¿æ€§è¾“å‡ºæ¿€æ´»å’Œsigmoidéšè—æ¿€æ´»ã€‚è¿™å°†ç»™å‡ºä¸€ä¸ªè¾“å‡ºå®æ•°å€¼ã€‚'
- en: '**Classification problems (two classes, one output)**: In this kind of problem
    we normally have a cross-entropy cost function, a sigmoid output, and hidden activations.
    The sigmoid function will give us the probability of occurrence or nonoccurrence
    of one of the classes.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†ç±»é—®é¢˜ï¼ˆä¸¤ç±»ï¼Œæ¯ç±»ä¸€ä¸ªè¾“å‡ºï¼‰**ï¼šåœ¨è¿™ç§é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰ä¸€ä¸ªäº¤å‰ç†µæŸå¤±å‡½æ•°ã€ä¸€ä¸ªSå‹è¾“å‡ºå’Œéšè—æ¿€æ´»ã€‚Så‹å‡½æ•°å°†ç»™æˆ‘ä»¬ä¸€ä¸ªç±»å‘ç”Ÿçš„æ¦‚ç‡æˆ–éå‘ç”Ÿçš„æ¦‚ç‡ã€‚'
- en: '**Classification problems (multiple-classes, one output per class)**: In this
    kind of problem, we will have a cross-entropyÂ cost function with softmax outputs
    and sigmoid hidden activations, in order to have an output of the probabilities
    for any of the possible classes for a single input.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†ç±»é—®é¢˜ï¼ˆå¤šç±»ï¼Œæ¯ç±»ä¸€ä¸ªè¾“å‡ºï¼‰**ï¼šåœ¨è¿™ç§é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ªäº¤å‰ç†µæŸå¤±å‡½æ•°ã€softmaxè¾“å‡ºå’Œsigmoidéšè—æ¿€æ´»ï¼Œä»¥ä¾¿å¯¹å•ä¸ªè¾“å…¥çš„ä»»ä½•å¯èƒ½çš„ç±»è¾“å‡ºæ¦‚ç‡ã€‚'
- en: Implementing a simple function with a single-layer perceptron
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å•å±‚æ„ŸçŸ¥å™¨å®ç°ç®€å•å‡½æ•°
- en: 'Take a look at the following code snippet to implement a single function with
    a single-layer perceptron:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼Œä»¥å®ç°ä¸€ä¸ªå•å±‚æ„ŸçŸ¥å™¨çš„å•ä¸ªå‡½æ•°ï¼š
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining and graphing transfer function types
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰å’Œç»˜åˆ¶ä¼ é€’å‡½æ•°ç±»å‹
- en: The learning properties of a neural network would not be very good with just
    the help of a univariate linear classifier. Even some mildly complex problems
    in machine learning involve multiple non-linear variables, so many variants were
    developed as replacements for the transfer functions of the perceptron.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä»…é å•å˜é‡çº¿æ€§åˆ†ç±»å™¨ï¼Œç¥ç»ç½‘ç»œçš„å­¦ä¹ èƒ½åŠ›ä¸ä¼šå¾ˆå¥½ã€‚ç”šè‡³åœ¨æœºå™¨å­¦ä¹ ä¸­çš„ä¸€äº›è½»å¾®å¤æ‚çš„é—®é¢˜ä¹Ÿæ¶‰åŠå¤šä¸ªéçº¿æ€§å˜é‡ï¼Œå› æ­¤å¼€å‘äº†å¤šç§å˜ä½“æ¥æ›¿ä»£æ„ŸçŸ¥å™¨çš„ä¼ é€’å‡½æ•°ã€‚
- en: In order to represent non-linear models, a number of different non-linear functions
    can be used in the activation function. This implies changes in the way the neurons
    will react to changes in the input variables. In the following sections, we will
    define the main different transfer functions and define and represent them via
    code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¡¨ç¤ºéçº¿æ€§æ¨¡å‹ï¼Œæ¿€æ´»å‡½æ•°ä¸­å¯ä»¥ä½¿ç”¨å¤šç§ä¸åŒçš„éçº¿æ€§å‡½æ•°ã€‚è¿™æ„å‘³ç€ç¥ç»å…ƒå¯¹è¾“å…¥å˜é‡å˜åŒ–çš„ååº”æ–¹å¼å°†å‘ç”Ÿå˜åŒ–ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸»è¦çš„ä¸åŒçš„ä¼ é€’å‡½æ•°ï¼Œå¹¶é€šè¿‡ä»£ç å®šä¹‰å’Œè¡¨ç¤ºå®ƒä»¬ã€‚
- en: In this section, we will start using some **object-oriented programming** (**OOP**)
    techniques from Python to represent entities of the problem domain. This will
    allow us to represent concepts in a much clearer way in the examples.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¼€å§‹ä½¿ç”¨ä¸€äº› **é¢å‘å¯¹è±¡ç¼–ç¨‹**ï¼ˆ**OOP**ï¼‰æŠ€æœ¯æ¥è¡¨ç¤ºé—®é¢˜åŸŸçš„å®ä½“ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç¤ºä¾‹ä¸­ä»¥æ›´æ¸…æ™°çš„æ–¹å¼è¡¨ç¤ºæ¦‚å¿µã€‚
- en: 'Let''s start by creating a `TransferFunction` class, which will contain the
    following two methods:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å…ˆåˆ›å»ºä¸€ä¸ª `TransferFunction` ç±»ï¼Œå®ƒå°†åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªæ–¹æ³•ï¼š
- en: '`getTransferFunction(x)`: This method will return an activation function determined
    by the class type'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getTransferFunction(x)`: æ­¤æ–¹æ³•å°†è¿”å›ç”±ç±»ç±»å‹ç¡®å®šçš„æ¿€æ´»å‡½æ•°'
- en: '`getTransferFunctionDerivative(x)`: This method will clearly return its derivative'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getTransferFunctionDerivative(x)`: æ­¤æ–¹æ³•å°†æ˜ç¡®è¿”å›å…¶å¯¼æ•°'
- en: 'For both functions, the input will be a NumPy array and the function will be
    applied element by element, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸¤ä¸ªå‡½æ•°ï¼Œè¾“å…¥å°†æ˜¯ä¸€ä¸ªNumPyæ•°ç»„ï¼Œå‡½æ•°å°†é€å…ƒç´ åº”ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Representing and understanding the transfer functions
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºå’Œç†è§£ä¼ é€’å‡½æ•°
- en: 'Let''s take a look at the following code snippet to see how the transfer function
    works:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼Œä»¥äº†è§£ä¼ é€’å‡½æ•°æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼š
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Sigmoid or logistic function
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Så‹æˆ–é€»è¾‘å‡½æ•°
- en: A sigmoid or logistic function is the canonical activation function and is well-suited
    for calculating probabilities in classification properties. Firstly, let's prepare
    a function that will be used to graph all the transfer functions with their derivatives,
    from a common range of `-2.0` to `2.0`, which will allow us to see the main characteristics
    of them around the *y* axis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Så‹æˆ–é€»è¾‘å‡½æ•°æ˜¯æ ‡å‡†çš„æ¿€æ´»å‡½æ•°ï¼Œéå¸¸é€‚åˆç”¨äºè®¡ç®—åˆ†ç±»å±æ€§ä¸­çš„æ¦‚ç‡ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†ç”¨äºç»˜åˆ¶æ‰€æœ‰ä¼ é€’å‡½æ•°åŠå…¶å¯¼æ•°ï¼ŒèŒƒå›´ä»å…¬å…±çš„ `-2.0`
    åˆ° `2.0`ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿçœ‹åˆ°å®ƒä»¬åœ¨ *y* è½´é™„è¿‘çš„ä¸»è¦ç‰¹å¾ã€‚
- en: 'The classical formula for the sigmoid function is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Så‹å‡½æ•°çš„ç»å…¸å…¬å¼å¦‚ä¸‹ï¼š
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at the following graph:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![](img/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png)'
- en: Playing with the sigmoid
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç©è½¬Så‹å‡½æ•°
- en: 'Next, we will do an exercise to get an idea of how the sigmoid changes when
    multiplied by the weights and shifted by the bias to accommodate the final function
    towards its minimum. Let''s then vary the possible parameters of a single sigmoid
    first and see it stretch and move:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿›è¡Œä¸€é¡¹ç»ƒä¹ ï¼Œä»¥äº†è§£sigmoidå‡½æ•°åœ¨ä¹˜ä»¥æƒé‡å¹¶åç§»ä»¥é€‚åº”æœ€ç»ˆå‡½æ•°çš„æœ€å°å€¼æ—¶æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚ç„¶åæˆ‘ä»¬é¦–å…ˆæ”¹å˜å•ä¸ªsigmoidå‡½æ•°çš„å¯èƒ½å‚æ•°ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦‚ä½•æ‹‰ä¼¸å’Œç§»åŠ¨çš„ï¼š
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at the following graph:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![](img/21e172f3-3308-43d1-86cf-2ab429ddca97.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡4](img/21e172f3-3308-43d1-86cf-2ab429ddca97.png)'
- en: 'Let''s take a look at the following code snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ç‰‡æ®µï¼š
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lets take a look at the following graph:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![](img/79183576-593f-47a5-ab50-a0cc6d1b8637.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡5](img/79183576-593f-47a5-ab50-a0cc6d1b8637.png)'
- en: Rectified linear unit or ReLU
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ©å½¢çº¿æ€§å•å…ƒæˆ–ReLU
- en: '**ReLU** is called a rectified linear unit, and one of its main advantages
    is that it is not affected by vanishing gradient problems, which generally consist
    of the first layers of a network tending to be values of zero, or a tiny epsilon:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** è¢«ç§°ä¸ºçŸ©å½¢çº¿æ€§å•å…ƒï¼Œå®ƒçš„ä¸€å¤§ä¼˜ç‚¹æ˜¯å®ƒä¸å—æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„å½±å“ï¼Œè¿™ä¸ªé—®é¢˜é€šå¸¸åŒ…æ‹¬ç½‘ç»œçš„ç¬¬ä¸€å±‚è¶‹å‘äºé›¶å€¼ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªéå¸¸å°çš„epsilonï¼š'
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![](img/fe3f187c-9745-4477-b1dc-581078123250.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡3](img/fe3f187c-9745-4477-b1dc-581078123250.png)'
- en: Linear transfer function
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§ä¼ é€’å‡½æ•°
- en: 'Let''s take a look at the following code snippet to understand the linear transfer
    function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ç‰‡æ®µï¼Œä»¥äº†è§£çº¿æ€§ä¼ é€’å‡½æ•°ï¼š
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![](img/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡6](img/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png)'
- en: Defining loss functions for neural networks
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°
- en: As with every model in machine learning, we will explore the possible functions
    that we will use to determine how well our predictions and classification went.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæœºå™¨å­¦ä¹ ä¸­çš„æ¯ä¸€ä¸ªæ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ¢è®¨æˆ‘ä»¬å°†ä½¿ç”¨çš„å¯èƒ½å‡½æ•°ï¼Œä»¥ç¡®å®šæˆ‘ä»¬çš„é¢„æµ‹å’Œåˆ†ç±»åšå¾—æœ‰å¤šå¥½ã€‚
- en: The first type of distinction we will do is between the L1 and L2 error function
    types.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦åšçš„ç¬¬ä¸€ç§åŒºåˆ†æ˜¯L1å’ŒL2è¯¯å·®å‡½æ•°ç±»å‹ã€‚
- en: 'L1, also known as **least absolute deviations** (**LAD**) or **least absolute
    errors** (**LAE**), has very interesting properties, and it simply consists of
    the absolute difference between the final result of the model and the expected
    one, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: L1ï¼Œä¹Ÿç§°ä¸º**æœ€å°ç»å¯¹åå·®**ï¼ˆ**LAD**ï¼‰æˆ–**æœ€å°ç»å¯¹è¯¯å·®**ï¼ˆ**LAE**ï¼‰ï¼Œå…·æœ‰éå¸¸æœ‰è¶£çš„å±æ€§ï¼Œå®ƒç®€å•åœ°ç”±æ¨¡å‹æœ€ç»ˆç»“æœä¸é¢„æœŸç»“æœä¹‹é—´çš„ç»å¯¹å·®å€¼ç»„æˆï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/b812faa4-4083-4230-9a61-0f17931c0979.png)![](img/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡1](img/b812faa4-4083-4230-9a61-0f17931c0979.png)![å›¾ç‰‡2](img/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png)'
- en: L1 versus L2 properties
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1ä¸L2å±æ€§
- en: 'Now it''s time to do a head-to-head comparison between the two types of loss
    function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æ—¶å€™å¯¹ä¸¤ç§ç±»å‹çš„æŸå¤±å‡½æ•°è¿›è¡Œé¢å¯¹é¢æ¯”è¾ƒäº†ï¼š
- en: '**Robustness**: L1 is a more robust loss function, which can be expressed as
    the resistance of the function when being affected by outliers, which projects
    a quadratic function to very high values. Thus, in order to choose an L2 function,
    we should have very stringent data cleaning for it to be efficient.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é²æ£’æ€§**ï¼šL1æ˜¯ä¸€ä¸ªæ›´é²æ£’çš„æŸå¤±å‡½æ•°ï¼Œå®ƒå¯ä»¥è¡¨ç¤ºä¸ºå‡½æ•°åœ¨å—åˆ°å¼‚å¸¸å€¼å½±å“æ—¶çš„æŠµæŠ—åŠ›ï¼Œå®ƒå°†äºŒæ¬¡å‡½æ•°æŠ•å½±åˆ°éå¸¸é«˜çš„å€¼ã€‚å› æ­¤ï¼Œä¸ºäº†é€‰æ‹©L2å‡½æ•°ï¼Œæˆ‘ä»¬åº”è¯¥å¯¹å…¶è¿›è¡Œéå¸¸ä¸¥æ ¼çš„æ•°æ®æ¸…ç†ï¼Œä»¥ä¾¿å®ƒèƒ½å¤Ÿé«˜æ•ˆè¿è¡Œã€‚'
- en: '**Stability**: The stability property assesses how much the error curve jumps
    for a largeÂ error value. L1 is more unstable, especially for non-normalized datasets
    (because numbers in the `[-1, 1]` range diminish when squared).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¨³å®šæ€§**ï¼šç¨³å®šæ€§å±æ€§è¯„ä¼°äº†å½“å‡ºç°å¤§è¯¯å·®å€¼æ—¶ï¼Œè¯¯å·®æ›²çº¿è·³åŠ¨çš„ç¨‹åº¦ã€‚L1æ›´ä¸ç¨³å®šï¼Œå°¤å…¶æ˜¯åœ¨éå½’ä¸€åŒ–æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼ˆå› ä¸º`[-1, 1]`èŒƒå›´å†…çš„æ•°å­—åœ¨å¹³æ–¹åä¼šå‡å°ï¼‰ã€‚'
- en: '**Solution uniqueness**: As can be inferred by its quadratic nature, the L2
    function ensures that we will have a unique answer for our search for a minimum.
    L2 always has a unique solution, but L1 can have many solutions, due to the fact
    that we can find many paths with minimal lengthÂ for our models in the form of
    piecewise linear functions, compared to the single line distance in the case of
    L2.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£çš„å”¯ä¸€æ€§**ï¼šæ­£å¦‚å…¶äºŒæ¬¡æ€§è´¨æ‰€æš—ç¤ºçš„ï¼ŒL2å‡½æ•°ç¡®ä¿æˆ‘ä»¬åœ¨å¯»æ‰¾æœ€å°å€¼æ—¶å°†æœ‰ä¸€ä¸ªå”¯ä¸€çš„ç­”æ¡ˆã€‚L2æ€»æ˜¯æœ‰ä¸€ä¸ªå”¯ä¸€çš„è§£ï¼Œä½†L1å¯èƒ½æœ‰å¤šä¸ªè§£ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°è®¸å¤šè·¯å¾„ï¼Œè¿™äº›è·¯å¾„ä»¥åˆ†æ®µçº¿æ€§å‡½æ•°çš„å½¢å¼å…·æœ‰æœ€å°é•¿åº¦ï¼Œä¸L2æƒ…å†µä¸‹çš„å•çº¿è·ç¦»ç›¸æ¯”ã€‚'
- en: Regarding usage, the summation of the past properties allows us to use the L2
    error type in normal cases, especially because of the solution uniqueness, which
    gives us the required certainty when starting to minimize error values. In the
    first example, we will start with a simpler L1 error function for educational
    purposes.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºä½¿ç”¨ï¼Œè¿‡å»å±æ€§çš„æ€»å’Œå…è®¸æˆ‘ä»¬åœ¨æ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨L2è¯¯å·®ç±»å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è§£å”¯ä¸€æ€§æ–¹é¢ï¼Œè¿™åœ¨æˆ‘ä»¬å¼€å§‹æœ€å°åŒ–è¯¯å·®å€¼æ—¶ç»™æˆ‘ä»¬æä¾›äº†æ‰€éœ€çš„ç¡®å®šæ€§ã€‚åœ¨ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»ç®€å•çš„L1è¯¯å·®å‡½æ•°å¼€å§‹ï¼Œç”¨äºæ•™è‚²ç›®çš„ã€‚
- en: Let's explore these two approaches by graphing the error results for a sample
    L1 and L2 loss error function. In the next simple example, we will show you the
    very different nature of the two errors. In the first two examples, we have normalized
    the input between `-1` and `1` and then with values outside that range.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ç»˜åˆ¶æ ·æœ¬L1å’ŒL2æŸå¤±è¯¯å·®å‡½æ•°çš„è¯¯å·®ç»“æœæ¥æ¢ç´¢è¿™ä¸¤ç§æ–¹æ³•ã€‚åœ¨ä¸‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºä¸¤ç§è¯¯å·®çš„éå¸¸ä¸åŒçš„æ€§è´¨ã€‚åœ¨å‰ä¸¤ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥è§„èŒƒåŒ–åœ¨`-1`å’Œ`1`ä¹‹é—´ï¼Œç„¶åæ˜¯è¶…å‡ºè¯¥èŒƒå›´çš„å€¼ã€‚
- en: 'As you can see, from samplesÂ `0` to `3`, the quadratic error increases steadily
    and continuously, but with non-normalized data it can explode, especially with
    outliers, as shown in the following code snippet:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æ‚¨æ‰€è§ï¼Œä»æ ·æœ¬`0`åˆ°`3`ï¼ŒäºŒæ¬¡è¯¯å·®ç¨³æ­¥ä¸”è¿ç»­åœ°å¢åŠ ï¼Œä½†ä¸éå½’ä¸€åŒ–æ•°æ®ç›¸æ¯”ï¼Œå®ƒå¯èƒ½ä¼šçˆ†ç‚¸ï¼Œå°¤å…¶æ˜¯åœ¨å¼‚å¸¸å€¼çš„æƒ…å†µä¸‹ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µæ‰€ç¤ºï¼š
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„å›¾è¡¨ï¼š
- en: '![](img/1ff32091-af2b-40f5-9e24-283a72afb96f.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/1ff32091-af2b-40f5-9e24-283a72afb96f.png)'
- en: 'Let''s define the loss functions in the form of a `LossFunction` class and
    a `getLoss` method for the L1 and L2 loss function types, receiving two NumPy
    arrays as parameters, `y_`, or the estimated function value, and `y`, the expected
    value:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥`LossFunction`ç±»å’Œ`getLoss`æ–¹æ³•çš„å½¢å¼å®šä¹‰æŸå¤±å‡½æ•°ï¼Œè¯¥æ–¹æ³•æ¥æ”¶ä¸¤ä¸ªNumPyæ•°ç»„ä½œä¸ºå‚æ•°ï¼Œ`y_`æˆ–ä¼°è®¡çš„å‡½æ•°å€¼ï¼Œä»¥åŠ`y`ï¼ŒæœŸæœ›å€¼ï¼š
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now it''s time to define the goal function, which we will define as a simple
    `Boolean`. In order to allow faster convergence, it will have a direct relationship
    between the first input variable and the function''s outcome:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯å®šä¹‰ç›®æ ‡å‡½æ•°çš„æ—¶å€™äº†ï¼Œæˆ‘ä»¬å°†å°†å…¶å®šä¹‰ä¸ºç®€å•çš„`Boolean`ã€‚ä¸ºäº†å…è®¸æ›´å¿«åœ°æ”¶æ•›ï¼Œå®ƒå°†ç¬¬ä¸€ä¸ªè¾“å…¥å˜é‡å’Œå‡½æ•°çš„è¾“å‡ºä¹‹é—´æœ‰ä¸€ä¸ªç›´æ¥çš„å…³ç³»ï¼š
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first model we will use is a very minimal neural network with three cells
    and a weight for each one, without bias, in order to keep the model''s complexity
    to a minimum:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œå…·æœ‰ä¸‰ä¸ªç»†èƒå’Œæ¯ä¸ªç»†èƒçš„æƒé‡ï¼Œæ²¡æœ‰åå·®ï¼Œä»¥ä¿æŒæ¨¡å‹å¤æ‚åº¦æœ€å°ï¼š
- en: '[PRE11]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Take a look at the following output generated by running the preceding code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä»¥ä¸‹ç”±è¿è¡Œå‰é¢çš„ä»£ç ç”Ÿæˆçš„è¾“å‡ºï¼š
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we will define a set of variables to collect the model''s error, the weights,
    and training results progression:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å®šä¹‰ä¸€ç»„å˜é‡æ¥æ”¶é›†æ¨¡å‹çš„è¯¯å·®ã€æƒé‡å’Œè®­ç»ƒç»“æœè¿›åº¦ï¼š
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then it's time to do the iterative error minimization. In this case, it will
    consist of feeding the whole true tableÂ 100 timesÂ via the weights and the neuron's
    transfer function, adjusting the weights in the direction of the error.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°±æ˜¯è¿›è¡Œè¿­ä»£è¯¯å·®æœ€å°åŒ–çš„æ—¶å€™äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒå°†åŒ…æ‹¬é€šè¿‡æƒé‡å’Œç¥ç»å…ƒçš„ä¼ é€’å‡½æ•°100æ¬¡ä¼ é€’æ•´ä¸ªçœŸå®è¡¨ï¼Œè°ƒæ•´æƒé‡çš„æ–¹å‘ä»¥å‡å°è¯¯å·®ã€‚
- en: 'Note that this model doesn''t use a learning rate, so it should converge (or
    diverge) quickly:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œè¿™ä¸ªæ¨¡å‹æ²¡æœ‰ä½¿ç”¨å­¦ä¹ ç‡ï¼Œæ‰€ä»¥å®ƒåº”è¯¥å¿«é€Ÿæ”¶æ•›ï¼ˆæˆ–å‘æ•£ï¼‰ï¼š
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s simply review the last evaluation step by printing the output values
    at `l1`. Now we can see that we are reflecting quite literally the output of the
    original function:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç®€å•åœ°å›é¡¾ä¸€ä¸‹æœ€åçš„è¯„ä¼°æ­¥éª¤ï¼Œé€šè¿‡æ‰“å°`l1`çš„è¾“å‡ºå€¼ã€‚ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æ­£åœ¨éå¸¸ç›´æ¥åœ°åæ˜ åŸå§‹å‡½æ•°çš„è¾“å‡ºï¼š
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä»¥ä¸‹ç”±è¿è¡Œå‰é¢çš„ä»£ç ç”Ÿæˆçš„è¾“å‡ºï¼š
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To better understand the process, let''s have a look at how the parameters
    change over time. First, let''s graph the neuron weights. As you can see, they
    go from a random state to accepting the whole values of the first column (which
    is always right), going to almost `0` for the second column (which is right 50%
    of the time), and then going to `-2` for the third (mainly because it has to trigger
    `0` in the first two elements of the table):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸ªè¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å‚æ•°éšæ—¶é—´çš„å˜åŒ–ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶ç¥ç»å…ƒæƒé‡å›¾ã€‚å¦‚æ‚¨æ‰€è§ï¼Œå®ƒä»¬ä»éšæœºçŠ¶æ€å˜ä¸ºæ¥å—ç¬¬ä¸€åˆ—çš„æ•´ä¸ªå€¼ï¼ˆè¿™æ€»æ˜¯æ­£ç¡®çš„ï¼‰ï¼Œç„¶åå˜ä¸ºç¬¬äºŒåˆ—å‡ ä¹ä¸º`0`ï¼ˆæ­£ç¡®50%çš„æ—¶é—´ï¼‰ï¼Œç„¶åå˜ä¸ºç¬¬ä¸‰åˆ—çš„`-2`ï¼ˆä¸»è¦æ˜¯å› ä¸ºå®ƒå¿…é¡»è§¦å‘è¡¨ä¸­çš„å‰ä¸¤ä¸ªå…ƒç´ çš„`0`ï¼‰ï¼š
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹ä¸€ä¸‹ä»¥ä¸‹è¾“å‡ºï¼Œè¿™æ˜¯é€šè¿‡è¿è¡Œå‰é¢çš„ä»£ç ç”Ÿæˆçš„ï¼š
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ä¸‹é¢çš„æˆªå›¾ï¼š
- en: '![](img/448f7444-c937-497c-a880-31f062970776.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/448f7444-c937-497c-a880-31f062970776.png)'
- en: 'Let''s also review how our solutions evolved (during the first 40 iterations)
    until we reached the last iteration; we can clearly see the convergence to the
    ideal values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/450b72d6-0559-44d8-b979-24378328f577.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'We can see how the error evolves and tends to be zero through the different
    epochs. In this case, we can observe that it swings from negative to positive,
    which is possible because we first used an L1 error function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6643314-6b1d-490c-937b-8c278c173ec0.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Depiction of the decreasing training error of our simple Neural Network
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a very important step towards solving complex problems
    together by means of implementing our first neural network. Now, the following
    architectures will have familiar elements, and we will be able to extrapolate
    the knowledge aquired on this chapter, to novel architectures.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more complex models and problems, using
    more layers and special configurations, such as convolutional and dropout layers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following content:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'McCulloch, Warren S., and Walter Pitts*,. A logical calculus of the ideas immanent
    in nervous activity. The bulletin of mathematical biophysics 5.4 (1943): 115-133.*
    Kleene, Stephen Cole. Representation of events in nerve nets and finite automata.
    No. RAND-RM-704\. RAND PROJECT AIR FORCE SANTA MONICA CA, 1951.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farley, B. W. A. C., and W. Clark*,* *Simulation of self-organizing systems
    by digital computer.* Transactions of the IRE Professional Group on Information
    Theory 4.4 (1954): 76-84.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenblatt, Frank*, The perceptron: A probabilistic model for information storage
    and organization in the brain,* Psychological review 65.6 (1958): 386.Rosenblatt,
    Frank. x.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principles of Neurodynamics: perceptrons and the Theory of Brain Mechanisms.
    Spartan Books, Washington DC, 1961'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos, P.J. (1975), *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences.*'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparata, Franco P., and Michael Ian Shamos*,. "Introduction." Computational
    Geometry. Springer New York, 1985\. 1-35.*
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart, David E., Geoffrey E. Hinton, and Ronald J*, Williams. Learning internal
    representations by error propagation. No. ICS-8506\. California Univ San Diego
    La Jolla Inst for Cognitive Science, 1985.*
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rumelhart, James L. McClelland, and the PDP research group. *Parallel distributed
    processing: Explorations in the microstructure of cognition, Volume 1: Foundation.
    MIT Press, 1986.*'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko, G. 1989*. Approximation by superpositions of a sigmoidal function Mathematics
    of Control, Signals, and Systems, 2(4), 303â€“314.*
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murtagh, Fionn*. Multilayer perceptrons for classification and regression.
    Neurocomputing 2.5 (1991): 183-197.*'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber, JÃ¼rgen*. Deep learning in neural networks: An overview. Neural
    networks 61 (2015): 85-117.*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
