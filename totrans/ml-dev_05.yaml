- en: Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: As a developer, you have surely gained an interest in machine learning from
    looking at all the incredibly amazing applications that you see on your regular
    devices every day—automatic speech translation, picture style transfer, the ability
    to generate new pictures from sample ones, and so on. Brace yourself... we are
    heading directly into the technology that has made all these things possible.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名开发者，你一定从每天在常规设备上看到的令人难以置信的神奇应用中产生了对机器学习的兴趣——自动语音翻译、图片风格转换、从样本图片生成新图片的能力等等。做好准备吧...我们将直接进入使所有这些成为可能的技术。
- en: Linear and logistic models, such as the ones we've observed, have certain limitations
    in terms of the complexity of the training dataset they train a model on, even
    when they are the basis of very articulated and efficient solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 线性和逻辑模型，如我们所观察到的，在训练数据集的复杂度方面存在某些局限性，即使它们是高度细致和高效解决方案的基础。
- en: How complex does a model have to be to capture the style of writing of an author,
    the concept of an image of a cat versus an image of a dog, or the classification
    of a plant based on visual elements? These things require the summation of a huge
    mix of low-level and high-level details captured, in the case of our brain by
    specialized sets of neurons, and in computer science by neural models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型需要有多复杂才能捕捉到作者的写作风格、猫和狗的图像概念，或基于视觉元素的植物分类？这些事情需要大量低级和高级细节的汇总，在我们的大脑中是通过专门的神经元集合捕获的，在计算机科学中是通过神经网络模型捕获的。
- en: Contrary to what's expected, in this book I'll spare you the typical introduction
    to the nervous system, its capabilities, the number of neurons in the nervous
    system, its chemical properties, and so on. I find that these topics add an aura
    of impossibility to problems, but the models we are about to learn are simple
    math equations with computational counterparts, which we will try to emphasize
    to allow you, a person with algorithmic interests, to easily understand them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与预期相反，在这本书中，我将省略对神经系统的典型介绍，包括其能力、神经系统中的神经元数量、其化学特性等等。我发现这些话题给问题增添了一种不可能性，但我们即将学习的模型是简单的数学方程式，具有计算对应物，我们将尝试强调这一点，以便你，一个对算法感兴趣的人，能够轻松理解它们。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The history of neural models, including perceptron and ADALINE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经模型的历史，包括感知器和ADALINE
- en: Neural networks and the kinds of problems they solve
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和它们解决的问题类型
- en: The multilayer perceptron
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器
- en: Implementing a simple neural layer to model a binary function
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个简单的神经网络层来模拟二元函数
- en: This chapter will allow you to use the building blocks of most of the awesome
    machine learning applications you see daily. So, let's get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使你能够使用你每天看到的绝大多数令人惊叹的机器学习应用的基础构建块。那么，让我们开始吧！
- en: History of neural models
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经模型的历史
- en: Neural models, in the sense of being disciplines that try to build representations
    of the internal workings of the brain, originated pretty distantly in the computer
    science timescale. They even date back to the time when the origins of modern
    computing were being invented, the mid-1940s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学的时间尺度上，神经模型作为一种试图构建大脑内部工作表示的学科，起源相当遥远。它们甚至可以追溯到现代计算机起源的中期——20世纪40年代中期。
- en: At that time, the fields of neuroscience and computer science began to collaborate
    by researching ways of emulating the way the brain processes information, starting
    from its constituent unit—the neuron.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在那时，神经科学和计算机科学开始通过研究模拟大脑处理信息的方式开始合作，从其构成单元——神经元开始。
- en: The first mathematical method for representing the learning function of the
    human brain can be assigned to McCulloch and Pitts, in their 1943 paper *A Logical
    Calculus of Ideas Immanent in Nervous Activity:*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑学习功能的第一种数学方法可以归功于麦卡洛克和皮茨，他们在1943年的论文《内在于神经活动的逻辑演算：*A Logical Calculus of
    Ideas Immanent in Nervous Activity:*》中提出。
- en: '![](img/0e0ee661-6f00-42e3-87a4-35139415faa8.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0e0ee661-6f00-42e3-87a4-35139415faa8.png)'
- en: McCulloch and Pitts model
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 麦卡洛克和皮茨模型
- en: This simple model was a basic but realistic model of a learning algorithm. You
    will be surprised by what happens if we use a linear function as a transfer function;
    this is a simple linear model, as were the ones we saw in the previous chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的模型是一个基本但现实的算法模型。如果你使用线性函数作为传递函数，你会对发生的事情感到惊讶；这是一个简单的线性模型，就像我们在上一章中看到的那样。
- en: You may have noted that we are now using the `w` letter to specify the parameters
    to be tuned by the model. Take this as the new standard from now on. Our old `β`
    parameters in linear regression models will now be `w`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: But the model hasn't determined a way to tune the parameters. Let's go forward
    to the 1950s and review the **perceptron** model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron model
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron model is one of the simplest ways to implement an artificial
    neuron. It was initially developed at the end of the 1950s, with the first hardware
    implementation being carried out in the 1960s. First, it was the name of a machine,
    and later became the name of an algorithm. Yes, perceptrons are not the strange
    entities we always thought they were, they're what you as a developer handle each
    day— algorithms!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following steps and learn how it works:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The function for a perceptron is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ef51780-af9a-41cd-8c1f-5749a0aa2e4e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: If *y’ ≠ y*, modify all the connections, *wi*, by adding the changes *Δw =yxi*.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return to *step 2*.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is basically an algorithm that learns a binary classification function and
    maps a real function to a single binary function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s depict the new architecture of the perceptron and analyze the new scheme
    of the algorithm in the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8dd545c6-e350-4130-a7e7-45da7a1065c6.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Perceptron model (With changes from previous model highlighted)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron builds on the ideas of its predecessors, but the novelty here
    is that we are adding a proper learning mechanism! In the following diagram, we
    have highlighted the new properties of the model—the feedback loop, where we calculate
    the error of our outcomes, and the adjustment of weights—with a predetermined
    formula:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db8ec5ff-a2b3-48bb-86ba-d7bb01b6c5b8.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Perceptron algorithm flowchart
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Improving our predictions – the ADALINE algorithm
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ADALINE is another algorithm (yes, remember we are talking about algorithms)
    used to train neural networks. ADALINE is in some ways more advanced than the
    perceptron because it adds a new training method: gradient descent, which should
    be known to you by now. Additionally, it changes the point at which the error
    is measured before the activation output is applied to the summation of the weights:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48d0f5a9-2fb4-4a18-a130-ab8d98268c49.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
- en: Adaline model (With additions from Perceptron highlighted)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this is the standard way of representing the ADALINE algorithm in a structural
    way. As an algorithm consists of a series of steps, let''s aggregate those in
    a more detailed way, with some additional mathematical details:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights with a random (low value) distribution.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an input vector and present it to the network.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the output *y'* of the network for the input vector specified and the
    values of the weights.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output value that we will be taking will be the one after the summation:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**y=Σ(xi * wi)**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the error, comparing the model output with the right label *o*:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*E=(o-y)²*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Does it look similar to something we have already seen? Yes! We are basically
    resolving a regression problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Adjust the weights with the following gradient descent recursion:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/58540fe4-1f28-4b5a-b899-7b20601ad6da.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Return to *step 2*:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e8429de2-2994-466a-b964-e181702e4f94.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: Adaline algorithm flowchart
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Similarities and differences between a perceptron and ADALINE
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered a simplified explanation of the precursors of modern neural
    networks. As you can see, the elements of modern models were almost all laid out
    during the 1950s and the 1960s! Before continuing, let''s try to compare the approaches:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarities**:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are both algorithms (it's important to stress that)
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are applied to single-layer neural models
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: They are classifiers for binary classification
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both have a linear decision boundary
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both can learn iteratively, sample by sample (the perceptron naturally, and
    ADALINE via stochastic gradient descent)
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Both use a threshold function
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Differences**:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perceptron uses the final class decision to train weights
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ADALINE uses continuous predicted values (from the net input) to learn the model
    coefficients, and measures subtle changes in error with a continuous float point
    rank, not a Boolean or integer
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we are done with our single-layer architectures and models, we will review
    some discoveries from the end of the 1960s that provoked quite a stir in the neural
    models community and were said to generate the first AI winter, or an abrupt collapse
    in the interest in machine learning research. Happily, some years after that,
    researchers found ways to overcome the limitations they faced, but this will be
    covered further on in the chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of early models
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model itself now has most of the elements of any normal neural model, but
    it had its own problems. After some years of fast development, the publication
    of the book *P**erceptrons* by Minsky and Papert in 1969 prompted a stir in the
    field because of its main idea that perceptrons could only work on linearly separable
    problems, which were only a very tiny part of the problems that the practitioners
    thought were solvable. In a sense, the book suggested that perceptrons were verging
    on being useless, with the exception of the simplest classification tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'This new-found deficiency could be represented by the inability of the model
    to represent the XOR function, which is a `Boolean` function with an output of
    `1` when the inputs are different, and `0` when they are equal:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/935f5509-f277-4e86-b870-241374cfc73f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: The problem of modelling a XOR function. No single line will correctly separate
    the 0 and 1 values
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, the main problem is that neither class
    (cross nor point) is linearly separable; that is, we can not separate the two
    with any linear function on the plane.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，主要问题是两个类别（交叉和点）都不是线性可分的；也就是说，我们无法在平面上用任何线性函数将它们分开。
- en: This detected problem caused a decrease in activity in the field that lasted
    for more or less five years, until the development of the backpropagation algorithm,
    which started in the mid 1970s.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个检测到的问题导致该领域活动减少，持续了大约五年，直到反向传播算法的发展，这始于20世纪70年代中期。
- en: Single and multilayer perceptrons
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单层和多层感知器
- en: Now we will discuss more contemporary times, building on the previous concepts
    to allow more complex elements of reality to be modeled.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论更现代的时期，基于先前概念，以允许更复杂的现实元素被建模。
- en: In this section, we will directly study **multilayer perceptrons** (**MLPs**),
    which are the most common configuration used, and will consider a **single-layer
    perceptron** as a particular case of the former, highlighting the differences.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将直接研究**多层感知器**（**MLPs**），这是最常用的配置，并将**单层感知器**视为前者的一个特例，突出它们之间的差异。
- en: 'Single-layer and multilayer perceptrons were the most commonly used architecture
    during the 1970s and 1980s and provided huge advances in terms of the capabilities
    of neural systems. The main innovations they bring to the table are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 单层和多层感知器在20世纪70年代和80年代是最常用的架构，在神经网络的能力方面取得了巨大的进步。它们带来的主要创新如下：
- en: They are feedforward networks because the calculations, starting from the inputs,
    flow from layer to layer without any cycling (information never returns)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是前馈网络，因为计算从输入开始，从一层流向另一层，没有任何循环（信息永远不会返回）
- en: They use the backpropagation method to adjust their weights
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们使用反向传播方法来调整他们的权重
- en: The use of the `step` function as a transfer function is replaced by non-linear
    ones such as the sigmoid
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`step`函数作为传递函数的使用被非线性函数如sigmoid所取代
- en: MLP origins
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP起源
- en: 'After the power of single-unit neural models had been explored, an obvious
    step was to generate layers or sets of commonly connected units (we define a connection
    as the act of sending the output of one unit to be part of another unit''s summation):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了单单元神经网络模型的力量之后，一个明显的步骤是生成层或一组通常相互连接的单元（我们定义连接为将一个单元的输出发送给另一个单元的求和部分）：
- en: '![](img/9fa32993-2a1f-4143-ad6d-86ff94158092.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9fa32993-2a1f-4143-ad6d-86ff94158092.png)'
- en: Depiction of a simple multilayer feed forward Neural Network
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 简单多层前馈神经网络示意图
- en: The feedforward mechanism
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈机制
- en: In this phase of the operation of the network, the data will be input in the
    first layer and will flow from each unit to the corresponding units in the following
    layers. Then it will be summed and passed through in the hidden layers, and finally
    processed by the output layer. This process is totally unidirectional, so we are
    avoiding any recursive complications in the data flow.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络操作的这一阶段，数据将被输入到第一层，并从每个单元流向下一层的相应单元。然后它将在隐藏层中进行求和并传递，最后由输出层处理。这个过程是完全单向的，因此我们避免了数据流中的任何递归复杂性。
- en: The feedforward mechanism, has in the MLP its counterpart in the training part
    of the modeling process, which will be in charge of improving the model's performance.
    The normally chosen algorithm is called **backpropagation**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈机制在MLP中对应于建模过程的训练部分，负责提高模型性能。通常选择的算法被称为**反向传播**。
- en: The chosen optimization algorithm – backpropagation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择的优化算法——反向传播
- en: From the perceptron algorithm onward, every neural architecture has had a means
    to optimize its internal parameters based on the comparison of the ground truth
    with the model output. The common assumption was to take the derivative of the
    (then simple) model function and iteratively work towards the minimum value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从感知器算法开始，每个神经网络架构都有一种基于真实值与模型输出比较来优化其内部参数的方法。常见的假设是取（当时简单的）模型函数的导数，并迭代地朝着最小值工作。
- en: For complex multilayer networks, there is an additional overhead, which has
    to do with the fact that the output layer's output is the result of a long chain
    of functions compositions, where each layer's output is wrapped by the next one's
    transfer function. So, the derivative of the output will involve the derivative
    of an exceedingly complex function. In this case, the backpropagation method was
    proposed, with excellent results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的多层网络，存在额外的开销，这与输出层的输出是函数复合的长链的结果有关，其中每一层的输出都被下一层的传递函数所包裹。因此，输出导数将涉及一个极其复杂的函数的导数。在这种情况下，提出了反向传播方法，并取得了优异的结果。
- en: Backpropagation can be summarized as an algorithm used to calculate derivatives.
    The main attribute is that it is computationally efficient and works with complex
    functions. It is also a generalization of the least mean squares algorithm in
    the linear perceptron, which we already knew!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播可以总结为一种用于计算导数的算法。其主要属性是计算效率高，适用于复杂函数。它也是线性感知器中最小均方误差算法的推广，这是我们已知的！
- en: In the backpropagation algorithm, the responsibility of the error will be distributed
    among all the functions applied to the data in the whole architecture. So, the
    goal is to minimize the error, the gradient of the loss function, over a set of
    deeply compounded functions, which will again receive the help of the chain rule.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播算法中，错误的职责将在整个架构中应用于所有应用于数据的函数。因此，目标是最小化误差，即损失函数的梯度，在一系列深度复合函数上，这又将再次得到链式法则的帮助。
- en: 'Now it''s time to define our general algorithm for our modern version of a
    neural network in the following steps:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候定义我们现代神经网络的一般算法了，以下是一些步骤：
- en: Calculate the feedforward signals from the input to the output.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算从输入到输出的前向信号。
- en: Calculate output error *E *based on the prediction *a[k]* and the target *t[k]*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据预测值 *a[k]* 和目标值 *t[k]* 计算输出误差 *E*。
- en: Backpropagate the error signals by weighting them by the weights in the previous
    layers and the gradients of the associated activation functions.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过加权前一层中的权重和关联激活函数的梯度来反向传播错误信号。
- en: Calculate the gradients 𝛿*E*/𝛿*θ* for the parameters based on the backpropagated
    error signal and the feedforward signals from the inputs.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据反向传播的错误信号和从输入的前向信号，计算参数的梯度 𝛿*E*/𝛿*θ*。
- en: Update the parameters using the calculated gradients  *θ ← θ - η* 𝛿E/𝛿θ *.*
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用计算出的梯度更新参数 *θ ← θ - η* 𝛿E/𝛿θ *.
- en: 'Lets review this process now, in a graphical way:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以图形方式回顾这个过程：
- en: '![](img/6f3c94d8-c460-41e9-87c4-2af201ab0027.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f3c94d8-c460-41e9-87c4-2af201ab0027.png)'
- en: Step by step representation of the Feed Forward  and Back Propagation training
    process
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步展示前向传播和反向传播训练过程
- en: 'In the following diagram, we will represent the whole process in an algorithmic
    way. You can see the number of coincidences with the previous optimization methods,
    and the short number of computation blocks involved:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们将整个过程以算法方式表示。您可以看到与先前优化方法的巧合之处，以及涉及的计算块数量较少：
- en: '![](img/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b02eecc6-ef04-42f1-be66-5a4a7daa093f.png)'
- en: Flowchart of the feedforward/backpropagation scheme
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前向/反向传播方案的流程图
- en: Types of problem to be tackled
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要解决的问题类型
- en: 'Neural networks can be used for both regression problems and classification
    problems. The common architectural difference resides in the output layer: in
    order to be able to bring a real number-based result, no standardization function,
    such as sigmoid, should be applied. In this manner, we won''t be changing the
    outcome of the variable to one of the many possible class values, getting a continuum
    of possible outcomes. Let''s take a look at the following types of problems to
    be tackled:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以用于回归问题和分类问题。常见的架构差异在于输出层：为了能够得到基于实数的结果，不应应用任何标准化函数，如sigmoid函数。这样，我们不会将变量的结果改变为许多可能的类别值之一，从而得到一系列可能的连续结果。让我们看看以下要解决的问题类型：
- en: '**Regression/function approximation problems:** This type of problem uses a
    min squared error function, a linear output activation, and sigmoidal hidden activations.
    This will give us a real value for output.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归/函数逼近问题**：这类问题使用最小平方误差函数、线性输出激活和sigmoid隐藏激活。这将给出一个输出实数值。'
- en: '**Classification problems (two classes, one output)**: In this kind of problem
    we normally have a cross-entropy cost function, a sigmoid output, and hidden activations.
    The sigmoid function will give us the probability of occurrence or nonoccurrence
    of one of the classes.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类问题（两类，每类一个输出）**：在这种问题中，我们通常有一个交叉熵损失函数、一个S型输出和隐藏激活。S型函数将给我们一个类发生的概率或非发生的概率。'
- en: '**Classification problems (multiple-classes, one output per class)**: In this
    kind of problem, we will have a cross-entropy cost function with softmax outputs
    and sigmoid hidden activations, in order to have an output of the probabilities
    for any of the possible classes for a single input.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类问题（多类，每类一个输出）**：在这种问题中，我们将有一个交叉熵损失函数、softmax输出和sigmoid隐藏激活，以便对单个输入的任何可能的类输出概率。'
- en: Implementing a simple function with a single-layer perceptron
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单层感知器实现简单函数
- en: 'Take a look at the following code snippet to implement a single function with
    a single-layer perceptron:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下代码片段，以实现一个单层感知器的单个函数：
- en: '[PRE0]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining and graphing transfer function types
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义和绘制传递函数类型
- en: The learning properties of a neural network would not be very good with just
    the help of a univariate linear classifier. Even some mildly complex problems
    in machine learning involve multiple non-linear variables, so many variants were
    developed as replacements for the transfer functions of the perceptron.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 仅靠单变量线性分类器，神经网络的学习能力不会很好。甚至在机器学习中的一些轻微复杂的问题也涉及多个非线性变量，因此开发了多种变体来替代感知器的传递函数。
- en: In order to represent non-linear models, a number of different non-linear functions
    can be used in the activation function. This implies changes in the way the neurons
    will react to changes in the input variables. In the following sections, we will
    define the main different transfer functions and define and represent them via
    code.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示非线性模型，激活函数中可以使用多种不同的非线性函数。这意味着神经元对输入变量变化的反应方式将发生变化。在接下来的章节中，我们将定义主要的不同的传递函数，并通过代码定义和表示它们。
- en: In this section, we will start using some **object-oriented programming** (**OOP**)
    techniques from Python to represent entities of the problem domain. This will
    allow us to represent concepts in a much clearer way in the examples.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始使用一些 **面向对象编程**（**OOP**）技术来表示问题域的实体。这将使我们能够在示例中以更清晰的方式表示概念。
- en: 'Let''s start by creating a `TransferFunction` class, which will contain the
    following two methods:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先创建一个 `TransferFunction` 类，它将包含以下两个方法：
- en: '`getTransferFunction(x)`: This method will return an activation function determined
    by the class type'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getTransferFunction(x)`: 此方法将返回由类类型确定的激活函数'
- en: '`getTransferFunctionDerivative(x)`: This method will clearly return its derivative'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`getTransferFunctionDerivative(x)`: 此方法将明确返回其导数'
- en: 'For both functions, the input will be a NumPy array and the function will be
    applied element by element, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个函数，输入将是一个NumPy数组，函数将逐元素应用，如下所示：
- en: '[PRE1]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Representing and understanding the transfer functions
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示和理解传递函数
- en: 'Let''s take a look at the following code snippet to see how the transfer function
    works:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下代码片段，以了解传递函数是如何工作的：
- en: '[PRE2]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Sigmoid or logistic function
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: S型或逻辑函数
- en: A sigmoid or logistic function is the canonical activation function and is well-suited
    for calculating probabilities in classification properties. Firstly, let's prepare
    a function that will be used to graph all the transfer functions with their derivatives,
    from a common range of `-2.0` to `2.0`, which will allow us to see the main characteristics
    of them around the *y* axis.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: S型或逻辑函数是标准的激活函数，非常适合用于计算分类属性中的概率。首先，让我们准备一个函数，该函数将用于绘制所有传递函数及其导数，范围从公共的 `-2.0`
    到 `2.0`，这将使我们能够看到它们在 *y* 轴附近的主要特征。
- en: 'The classical formula for the sigmoid function is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: S型函数的经典公式如下：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at the following graph:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下图表：
- en: '![](img/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b6c520-45f2-464e-9f60-3cdc1612b2f6.png)'
- en: Playing with the sigmoid
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转S型函数
- en: 'Next, we will do an exercise to get an idea of how the sigmoid changes when
    multiplied by the weights and shifted by the bias to accommodate the final function
    towards its minimum. Let''s then vary the possible parameters of a single sigmoid
    first and see it stretch and move:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一项练习，以了解sigmoid函数在乘以权重并偏移以适应最终函数的最小值时是如何变化的。然后我们首先改变单个sigmoid函数的可能参数，看看它是如何拉伸和移动的：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at the following graph:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图表：
- en: '![](img/21e172f3-3308-43d1-86cf-2ab429ddca97.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片4](img/21e172f3-3308-43d1-86cf-2ab429ddca97.png)'
- en: 'Let''s take a look at the following code snippet:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的代码片段：
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Lets take a look at the following graph:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图表：
- en: '![](img/79183576-593f-47a5-ab50-a0cc6d1b8637.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片5](img/79183576-593f-47a5-ab50-a0cc6d1b8637.png)'
- en: Rectified linear unit or ReLU
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩形线性单元或ReLU
- en: '**ReLU** is called a rectified linear unit, and one of its main advantages
    is that it is not affected by vanishing gradient problems, which generally consist
    of the first layers of a network tending to be values of zero, or a tiny epsilon:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLU** 被称为矩形线性单元，它的一大优点是它不受梯度消失问题的影响，这个问题通常包括网络的第一层趋向于零值，或者是一个非常小的epsilon：'
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图表：
- en: '![](img/fe3f187c-9745-4477-b1dc-581078123250.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片3](img/fe3f187c-9745-4477-b1dc-581078123250.png)'
- en: Linear transfer function
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性传递函数
- en: 'Let''s take a look at the following code snippet to understand the linear transfer
    function:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的代码片段，以了解线性传递函数：
- en: '[PRE7]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图表：
- en: '![](img/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片6](img/a3137a1f-113f-4dd6-9482-0e0d3b2a2f3f.png)'
- en: Defining loss functions for neural networks
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义神经网络的损失函数
- en: As with every model in machine learning, we will explore the possible functions
    that we will use to determine how well our predictions and classification went.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 就像机器学习中的每一个模型一样，我们将探讨我们将使用的可能函数，以确定我们的预测和分类做得有多好。
- en: The first type of distinction we will do is between the L1 and L2 error function
    types.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的第一种区分是L1和L2误差函数类型。
- en: 'L1, also known as **least absolute deviations** (**LAD**) or **least absolute
    errors** (**LAE**), has very interesting properties, and it simply consists of
    the absolute difference between the final result of the model and the expected
    one, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: L1，也称为**最小绝对偏差**（**LAD**）或**最小绝对误差**（**LAE**），具有非常有趣的属性，它简单地由模型最终结果与预期结果之间的绝对差值组成，如下所示：
- en: '![](img/b812faa4-4083-4230-9a61-0f17931c0979.png)![](img/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](img/b812faa4-4083-4230-9a61-0f17931c0979.png)![图片2](img/9ce333ae-4b4d-49f1-9382-a44a174d8b71.png)'
- en: L1 versus L2 properties
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1与L2属性
- en: 'Now it''s time to do a head-to-head comparison between the two types of loss
    function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候对两种类型的损失函数进行面对面比较了：
- en: '**Robustness**: L1 is a more robust loss function, which can be expressed as
    the resistance of the function when being affected by outliers, which projects
    a quadratic function to very high values. Thus, in order to choose an L2 function,
    we should have very stringent data cleaning for it to be efficient.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鲁棒性**：L1是一个更鲁棒的损失函数，它可以表示为函数在受到异常值影响时的抵抗力，它将二次函数投影到非常高的值。因此，为了选择L2函数，我们应该对其进行非常严格的数据清理，以便它能够高效运行。'
- en: '**Stability**: The stability property assesses how much the error curve jumps
    for a large error value. L1 is more unstable, especially for non-normalized datasets
    (because numbers in the `[-1, 1]` range diminish when squared).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性**：稳定性属性评估了当出现大误差值时，误差曲线跳动的程度。L1更不稳定，尤其是在非归一化数据集的情况下（因为`[-1, 1]`范围内的数字在平方后会减小）。'
- en: '**Solution uniqueness**: As can be inferred by its quadratic nature, the L2
    function ensures that we will have a unique answer for our search for a minimum.
    L2 always has a unique solution, but L1 can have many solutions, due to the fact
    that we can find many paths with minimal length for our models in the form of
    piecewise linear functions, compared to the single line distance in the case of
    L2.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解的唯一性**：正如其二次性质所暗示的，L2函数确保我们在寻找最小值时将有一个唯一的答案。L2总是有一个唯一的解，但L1可能有多个解，因为我们可以找到许多路径，这些路径以分段线性函数的形式具有最小长度，与L2情况下的单线距离相比。'
- en: Regarding usage, the summation of the past properties allows us to use the L2
    error type in normal cases, especially because of the solution uniqueness, which
    gives us the required certainty when starting to minimize error values. In the
    first example, we will start with a simpler L1 error function for educational
    purposes.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用，过去属性的总和允许我们在正常情况下使用L2误差类型，特别是在解唯一性方面，这在我们开始最小化误差值时给我们提供了所需的确定性。在第一个例子中，我们将从简单的L1误差函数开始，用于教育目的。
- en: Let's explore these two approaches by graphing the error results for a sample
    L1 and L2 loss error function. In the next simple example, we will show you the
    very different nature of the two errors. In the first two examples, we have normalized
    the input between `-1` and `1` and then with values outside that range.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过绘制样本L1和L2损失误差函数的误差结果来探索这两种方法。在下一个简单的例子中，我们将展示两种误差的非常不同的性质。在前两个例子中，我们将输入规范化在`-1`和`1`之间，然后是超出该范围的值。
- en: 'As you can see, from samples `0` to `3`, the quadratic error increases steadily
    and continuously, but with non-normalized data it can explode, especially with
    outliers, as shown in the following code snippet:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，从样本`0`到`3`，二次误差稳步且连续地增加，但与非归一化数据相比，它可能会爆炸，尤其是在异常值的情况下，如下面的代码片段所示：
- en: '[PRE8]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的图表：
- en: '![](img/1ff32091-af2b-40f5-9e24-283a72afb96f.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1ff32091-af2b-40f5-9e24-283a72afb96f.png)'
- en: 'Let''s define the loss functions in the form of a `LossFunction` class and
    a `getLoss` method for the L1 and L2 loss function types, receiving two NumPy
    arrays as parameters, `y_`, or the estimated function value, and `y`, the expected
    value:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以`LossFunction`类和`getLoss`方法的形式定义损失函数，该方法接收两个NumPy数组作为参数，`y_`或估计的函数值，以及`y`，期望值：
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now it''s time to define the goal function, which we will define as a simple
    `Boolean`. In order to allow faster convergence, it will have a direct relationship
    between the first input variable and the function''s outcome:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是定义目标函数的时候了，我们将将其定义为简单的`Boolean`。为了允许更快地收敛，它将第一个输入变量和函数的输出之间有一个直接的关系：
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The first model we will use is a very minimal neural network with three cells
    and a weight for each one, without bias, in order to keep the model''s complexity
    to a minimum:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的第一个模型是一个非常简单的神经网络，具有三个细胞和每个细胞的权重，没有偏差，以保持模型复杂度最小：
- en: '[PRE11]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Take a look at the following output generated by running the preceding code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下由运行前面的代码生成的输出：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we will define a set of variables to collect the model''s error, the weights,
    and training results progression:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将定义一组变量来收集模型的误差、权重和训练结果进度：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then it's time to do the iterative error minimization. In this case, it will
    consist of feeding the whole true table 100 times via the weights and the neuron's
    transfer function, adjusting the weights in the direction of the error.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然后就是进行迭代误差最小化的时候了。在这种情况下，它将包括通过权重和神经元的传递函数100次传递整个真实表，调整权重的方向以减小误差。
- en: 'Note that this model doesn''t use a learning rate, so it should converge (or
    diverge) quickly:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个模型没有使用学习率，所以它应该快速收敛（或发散）：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s simply review the last evaluation step by printing the output values
    at `l1`. Now we can see that we are reflecting quite literally the output of the
    original function:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简单地回顾一下最后的评估步骤，通过打印`l1`的输出值。现在我们可以看到，我们正在非常直接地反映原始函数的输出：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下由运行前面的代码生成的输出：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To better understand the process, let''s have a look at how the parameters
    change over time. First, let''s graph the neuron weights. As you can see, they
    go from a random state to accepting the whole values of the first column (which
    is always right), going to almost `0` for the second column (which is right 50%
    of the time), and then going to `-2` for the third (mainly because it has to trigger
    `0` in the first two elements of the table):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个过程，让我们看看参数随时间的变化。首先，让我们绘制神经元权重图。如您所见，它们从随机状态变为接受第一列的整个值（这总是正确的），然后变为第二列几乎为`0`（正确50%的时间），然后变为第三列的`-2`（主要是因为它必须触发表中的前两个元素的`0`）：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Take a look at the following output, which is generated by running the preceding
    code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下以下输出，这是通过运行前面的代码生成的：
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下面的截图：
- en: '![](img/448f7444-c937-497c-a880-31f062970776.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/448f7444-c937-497c-a880-31f062970776.png)'
- en: 'Let''s also review how our solutions evolved (during the first 40 iterations)
    until we reached the last iteration; we can clearly see the convergence to the
    ideal values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/450b72d6-0559-44d8-b979-24378328f577.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'We can see how the error evolves and tends to be zero through the different
    epochs. In this case, we can observe that it swings from negative to positive,
    which is possible because we first used an L1 error function:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6643314-6b1d-490c-937b-8c278c173ec0.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Depiction of the decreasing training error of our simple Neural Network
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took a very important step towards solving complex problems
    together by means of implementing our first neural network. Now, the following
    architectures will have familiar elements, and we will be able to extrapolate
    the knowledge aquired on this chapter, to novel architectures.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore more complex models and problems, using
    more layers and special configurations, such as convolutional and dropout layers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Refer to the following content:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'McCulloch, Warren S., and Walter Pitts*,. A logical calculus of the ideas immanent
    in nervous activity. The bulletin of mathematical biophysics 5.4 (1943): 115-133.*
    Kleene, Stephen Cole. Representation of events in nerve nets and finite automata.
    No. RAND-RM-704\. RAND PROJECT AIR FORCE SANTA MONICA CA, 1951.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farley, B. W. A. C., and W. Clark*,* *Simulation of self-organizing systems
    by digital computer.* Transactions of the IRE Professional Group on Information
    Theory 4.4 (1954): 76-84.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenblatt, Frank*, The perceptron: A probabilistic model for information storage
    and organization in the brain,* Psychological review 65.6 (1958): 386.Rosenblatt,
    Frank. x.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Principles of Neurodynamics: perceptrons and the Theory of Brain Mechanisms.
    Spartan Books, Washington DC, 1961'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos, P.J. (1975), *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences.*'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparata, Franco P., and Michael Ian Shamos*,. "Introduction." Computational
    Geometry. Springer New York, 1985\. 1-35.*
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart, David E., Geoffrey E. Hinton, and Ronald J*, Williams. Learning internal
    representations by error propagation. No. ICS-8506\. California Univ San Diego
    La Jolla Inst for Cognitive Science, 1985.*
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rumelhart, James L. McClelland, and the PDP research group. *Parallel distributed
    processing: Explorations in the microstructure of cognition, Volume 1: Foundation.
    MIT Press, 1986.*'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko, G. 1989*. Approximation by superpositions of a sigmoidal function Mathematics
    of Control, Signals, and Systems, 2(4), 303–314.*
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murtagh, Fionn*. Multilayer perceptrons for classification and regression.
    Neurocomputing 2.5 (1991): 183-197.*'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber, Jürgen*. Deep learning in neural networks: An overview. Neural
    networks 61 (2015): 85-117.*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
