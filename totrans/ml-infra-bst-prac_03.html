<html><head></head><body>
		<div id="_idContainer028">
			<h1 class="chapter-number"><a id="_idTextAnchor023"/>2</h1>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor024"/>Elements of a Machine  Learning System</h1>
			<p>Data and algorithms are <a id="_idIndexMarker043"/>crucial for machine learning systems, but they are far from sufficient. Algorithms are the smallest part of a production machine learning system. Machine learning systems also require data, infrastructure, monitoring, and storage to function efficiently. For a large-scale machine learning system, we need to ensure that we can include a good user interface or package model <span class="No-Break">in microservices.</span></p>
			<p>In modern software systems, combining all necessary elements requires different professional competencies – including machine learning/data science engineering expertise, database engineering, software engineering, and finally interaction design. In these professional systems, it is more important to provide reliable results that bring value to users rather than include a lot of unnecessary functionality. It is also important to orchestrate all elements of machine learning together (data, algorithms, storage, configuration, and infrastructure) rather than optimize each one of them separately – all to provide the most optimal system for one or more use cases from <span class="No-Break">end users.</span></p>
			<p>In this chapter, we’ll review each element of a professional machine learning system. We’ll start by understanding which elements are important and why. Then, we’ll explore how to create such elements and how to put them together into a single machine learning system – a so-called machine <span class="No-Break">learning pipeline.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Machine learning is more than just algorithms <span class="No-Break">and data</span></li>
				<li>Data <span class="No-Break">and algorithms</span></li>
				<li>Configuration <span class="No-Break">and monitoring</span></li>
				<li>Infrastructure and <span class="No-Break">resource management</span></li>
				<li>Machine <span class="No-Break">learning pipelines</span></li>
			</ul>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor025"/>Elements of a production machine learning system</h1>
			<p>Modern machine learning <a id="_idIndexMarker044"/>algorithms are very capable because they use large quantities of data and consist of a large number of trainable parameters. The largest available models<a id="_idIndexMarker045"/> are <strong class="bold">Generative Pre-trained Transformer-3</strong> (<strong class="bold">GPT-3</strong>) from OpenAI (with 175 billion parameters) and Megatron-Turing from NVidia (356 billion parameters). These models can create texts (novels) and make conversations but also write program code, create user interfaces, or <span class="No-Break">write requirements.</span></p>
			<p>Now, such large models cannot be used on a desktop computer, laptop, or even in a dedicated server. They need advanced computing infrastructure, which can withstand long-term training and evaluation of such large models. Such infrastructure also needs to provide means to automatically provide these models with data, monitor the training process, and, finally, provide the possibility for the users to access the models to make inferences. One of the modern ways of providing such infrastructure is the concept of <strong class="bold">Machine learning as a service</strong> (<strong class="bold">MLaaS</strong>). MLaaS <a id="_idIndexMarker046"/>provides an easy way to use machine learning from the perspective of data analysts of software integrators since it delegates the management, monitoring, and configuration of the infrastructure to <span class="No-Break">specialized companies.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> shows elements of modern machine learning-based software systems. Google has used these to describe production machine learning systems since then. Although variations in this setup exist, the <span class="No-Break">principles remain:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="Figure 2.1 – Elements of a production machine learning system" src="image/B19548_02_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – Elements of a production machine learning system</p>
			<p>Here, the <a id="_idIndexMarker047"/>machine learning model (<strong class="bold">ML code</strong>) is the smallest of these elements (Google, under the Creative Commons 4.0 Attribution License, <a href="https://developers.google.com/machine-learning/crash-course/production-ml-systems">https://developers.google.com/machine-learning/crash-course/production-ml-systems</a>). In terms of the actual source code, in Python, model creation, training, and validation are just three lines of code (at least for some of <span class="No-Break">the models):</span></p>
			<pre class="source-code">
model = RandomForestRegressor(n_estimators=10, max_depth=2)
model.fit(X_train, Y_train)
Y_pred = model.predict(X_test)</pre>			<p>The first line creates the model from a template – in this case, it is a random forest model with 10 trees, each of which has a maximum of two splits. Random forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) of the individual trees for a given input. By aggregating the results of multiple trees, it reduces overfitting and provides higher accuracy compared to a single decision tree. The second line trains the model based on the training data (<strong class="source-inline">X_train</strong>, which contains only the preditors/input features, and <strong class="source-inline">Y_train</strong>, which contains the predicted class/output features). Finally, the last line makes predictions for the test data (<strong class="source-inline">X_test</strong>) to compare it to the oracle (the expected value) in subsequent steps. Even though this <strong class="source-inline">model.predict(X_test)</strong> line is not part of a production system, we still need to make inferences, so there is always a similar line in <span class="No-Break">our software.</span></p>
			<p>Therefore, we can introduce the next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #5</p>
			<p class="callout">When designing machine learning software, prioritize your data and the problem to solve over <span class="No-Break">the algorithm.</span></p>
			<p>In this example, we<a id="_idIndexMarker048"/> saw that the machine learning code, from the perspective of the software engineer, is rather small. Before applying algorithms, we need to prepare the data correctly as the algorithms (<strong class="source-inline">model.fit(X_train, Y_train)</strong>) require the data to be in a specific format – the first parameter is the data that’s used to make inferences (so-called feature vectors or input data samples), while the second parameter is the target values (so-called decision classes, reference values, or target values, depending on <span class="No-Break">the algorithm).</span></p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor026"/>Data and algorithms</h1>
			<p>Now, if using the<a id="_idIndexMarker049"/> algorithms is not the main part of the machine learning code, then something else must be – that is, data handling. Managing data in machine learning software, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>, consists of <span class="No-Break">three areas:</span></p>
			<ol>
				<li><span class="No-Break">Data collection.</span></li>
				<li><span class="No-Break">Feature extraction.</span></li>
				<li><span class="No-Break">Data validation.</span></li>
			</ol>
			<p>Although we will go back to these areas throughout this book, let’s explore what they contain. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em> shows the processing pipeline for <span class="No-Break">these areas:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer019">
					<img alt="Figure 2.2 – Data collection and preparation pipeline" src="image/B19548_02_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Data collection and preparation pipeline</p>
			<p>Note that the<a id="_idIndexMarker050"/> process of preparing the data for the algorithms can become quite complex. First, we need to extract data from its source, which is usually a database. It can be a database of measurements, images, texts, or any other raw data. Once we’ve exported/extracted the data we need, we must store it in a raw data format. This can be in the form of a table, as shown in the preceding figure, or it can be in a set of raw files, such <span class="No-Break">as images.</span></p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor027"/>Data collection</h1>
			<p>Data collection<a id="_idIndexMarker051"/> is a procedure of transforming data from its raw format to a format that a machine learning algorithm can take as input. Depending on the data and the algorithm, this process can take different forms, as illustrated in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer020">
					<img alt="Figure 2.3 – Different forms of data collection – examples" src="image/B19548_02_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Different forms of data collection – examples</p>
			<p>Data from images and <a id="_idIndexMarker052"/>measurements such as time series is usually collected to make classifications and predictions. These two classes of problems require the ground truth to be available, which we saw as <strong class="source-inline">Y_train</strong> in the previous code example. These target labels are either extracted automatically from the raw data or added manually through the process of labeling. The manual process is time-consuming, so the automated one <span class="No-Break">is preferred.</span></p>
			<p>The data that’s<a id="_idIndexMarker053"/> used in non-supervised learning and reinforcement learning models is often extracted as tabular data without labels. This data is used in the decision process or the optimization process to find the best solution to the given problem. Without optimization, there is a risk that our results are not representative of new data. The preceding figure shows two examples of such problems – optimizations of smart factories of Industry 4.0 and self-driving vehicles. In smart factories, reinforcement learning models are used to optimize production processes or control so-called <em class="italic">dark factories</em>, which<a id="_idIndexMarker054"/> operate entirely without human intervention (the name <em class="italic">dark factories</em> comes from the fact that there is no need for lights in these factories; robots do not need light <span class="No-Break">to work).</span></p>
			<p>The data that’s used for modern self-supervised models often comes from such sources as text or speech. These models do not require a tabular form of the data, but they require structure. For example, to train text transformer models, we need to tokenize the text per sentence (or per paragraph) for the model to learn the context of <span class="No-Break">the words.</span></p>
			<p>Hence, here comes my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #6</p>
			<p class="callout">Once you’ve explored the problem you wish to solve and understood the data availability, decide whether you want to use supervised, self-supervised, unsupervised, or reinforcement <span class="No-Break">learning algorithms.</span></p>
			<p>The fact that we need different data for different algorithms is natural. However, we have not discussed how to decide upon the algorithm. Choosing supervised learning only makes sense when we want to predict or classify data statically – that is, we train the model and then we use it to make inferences. When we train and then make inferences, the model does not change. There is no adjustment as we go and re-training the algorithm is done <a id="_idIndexMarker055"/>periodically – I call it a <em class="italic">train once, predict </em><span class="No-Break"><em class="italic">many</em></span><span class="No-Break"> principle.</span></p>
			<p>We can choose unsupervised methods when we use/train and apply the algorithm without the target class. Some of these algorithms are also used to group data based on the data’s property, for example, to cluster it. I call this the <em class="italic">train once, predict </em><span class="No-Break"><em class="italic">once</em></span><span class="No-Break"> principle.</span></p>
			<p>For self-supervised models, the<a id="_idIndexMarker056"/> situation is a bit more interesting. There, we can use something called <em class="italic">pre-training</em>. Pre-training<a id="_idIndexMarker057"/> means that we can train a model on a large corpus of data without any specific context – for example, we train language models on large corpora of English texts from Wikipedia. Then, when we want to use the model for a specific task, such as to write new text, we train it a bit more on that task. I call<a id="_idIndexMarker058"/> this the <em class="italic">train many, predict once</em> principle as we must pre-train and train the model for <span class="No-Break">each task.</span></p>
			<p>Finally, reinforcement learning needs data that is changed every time the model is used.  For example, when we use a reinforcement learning algorithm to optimize a process, it updates the model each time it is used – we could say it learns from its mistakes. I call this the <em class="italic">train many, predict </em><span class="No-Break"><em class="italic">many</em></span><span class="No-Break"> principle.</span></p>
			<p>Usually, the raw data is not ready to be used with machine learning as it can contain empty data points, noise, or broken files. Therefore, we need to clean up these erroneous data points, such as by removing empty data points (using Python commands such as <strong class="source-inline">dataFrame.dropna(…)</strong>) or using data <span class="No-Break">imputation techniques.</span></p>
			<p>Now, there is a fundamental difference between the removal of data points and their imputation. The data imputation process is when we add missing properties of data based on similar data points. It’s like filling in blanks in a sequence of numbers – 1, 2, 3, …, 5, where we fill in the number 4. Although filling in the data increases the number of data points available (thus making the models better), it can strengthen certain properties of the data, which can cause the model to learn. Imputation is also relevant when the size of the data is small; in large datasets, it is better (more resource-efficient and fair) to drop the missing data points. With that, we’ve come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #7</p>
			<p class="callout">Use data imputation only when you know which properties of data you wish to strengthen and only do so for <span class="No-Break">small datasets.</span></p>
			<p>Finally, once <a id="_idIndexMarker059"/>we have clean data to work with, we can extract features. There, we can use algorithms that are specific to our problem at hand. For example, when we work with textual data, we could use a simple bag-of-words to count the frequencies of words, though we can also use the word2vec algorithm to embed the frequencies of the co-occurrence of words (algorithms that we’ll discuss in the next few chapters). Once we’ve extracted features, we can start validating the data. The features can emphasize certain properties of data that we didn’t <span class="No-Break">see before.</span></p>
			<p>One such example is noise – when we have data in a feature format, we can check whether there is <em class="italic">attribute</em> or <em class="italic">class noise</em> in the data. Class noise is a phenomenon that is related to labeling errors – one or more data points have been labeled incorrectly. Class noise can be either contradictory examples or wrongly labeled data points. It is a dangerous phenomenon since it can cause low performance when training and using machine <span class="No-Break">learning models.</span></p>
			<p>Attribute noise is when one (or more) attributes is corrupted with wrong values. Examples include wrong values, missing attribute (feature) values, and <span class="No-Break">irrelevant values.</span></p>
			<p>Once the data has been validated, it can be used in algorithms. So, let’s dive a bit deeper into each step of the data <span class="No-Break">processing pipeline.</span></p>
			<p>Now, since different algorithms use data in different ways, the data has a different form. Let’s explore how the data should be structured for each of <span class="No-Break">the algorithms.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/>Feature extraction</h2>
			<p>The<a id="_idIndexMarker060"/> process of transforming raw data into a format that can be used by algorithms is called feature extraction. It is a process where we apply a feature extraction algorithm to find properties of interest in the data. The algorithm for extracting features varies depending on the problem and the <span class="No-Break">data type.</span></p>
			<p>When we work with textual data, we can use several algorithms, but let me illustrate the use of one of the simplest ones – bag-of-words. The algorithm simply counts the occurrence of words in the sentence – it either counts a pre-defined set of words or uses statistics to find the most frequent words. Let’s consider the <span class="No-Break">following sentence:</span></p>
			<p><strong class="source-inline">Mike is a </strong><span class="No-Break"><strong class="source-inline">tall boy.</strong></span></p>
			<p>When we use the bag-of-words algorithm without any constraints, it provides us with the <span class="No-Break">following table:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Sentence ID</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Mike</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Is</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">A</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">tall</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Boy</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Features extracted using bag-of-words</p>
			<p>The <a id="_idIndexMarker061"/>table contains all the words in the sentence as features. It is not very useful for just one sentence, but if we add another one (sentence 1), things become more obvious. So, let’s add the <span class="No-Break">following sentence:</span></p>
			<p><strong class="source-inline">Mary is a </strong><span class="No-Break"><strong class="source-inline">smart girl.</strong></span></p>
			<p>This will result in the following <span class="No-Break">feature table:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table002">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Sentence ID</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Mike</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Is</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">A</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Tall</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">boy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">smart</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">girl</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Features extracted from two sentences</p>
			<p>We are now ready to add the label column to the data. Let’s say that we want to label each sentence as being positive or negative. The table then gets one more column – <strong class="source-inline">label</strong> – where <strong class="bold">1</strong> means that the sentence is positive and <span class="No-Break"><strong class="bold">0</strong></span><span class="No-Break"> otherwise:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table003">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Sentence ID</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Mike</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Is</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">A</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Tall</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">boy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">smart</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">girl</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Label</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Labels added to the data</p>
			<p>Now, these features allow us to see the difference between two sentences, which we can then use to train and test machine <span class="No-Break">learning algorithms.</span></p>
			<p>There are, however, two important limitations of this approach. The first one is that it is impractical (if not impossible) to have all words from all sentences as columns/features. For any non-trivial text, this would result in large and sparse matrices – a lot of wasted space. The second limitation is the fact that we usually lose important information – for example, the sentence “Is Mike a boy?” would result in the same feature vector as the first sentence. A feature vector is an <em class="italic">n</em>-dimensional vector of numerical features that describe some object in pattern recognition in machine learning. Although these sentences are not identical, they become undistinguishable, which can lead to class noise if they are <span class="No-Break">labeled differently.</span></p>
			<p>The problem<a id="_idIndexMarker062"/> with adding this kind of noise becomes even more evident if we use statistics to choose the most frequent words as features. Here, we can lose important features that discriminate data points in a useful way. Therefore, this bag-of-words approach is only for illustration here. Later in this book, we’ll dive deeper into so-called transformers, which use more advanced techniques to <span class="No-Break">extract features.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor029"/>Data validation</h2>
			<p>Feature vectors<a id="_idIndexMarker063"/> are the core of machine learning algorithms. They are most prominently, and directly, used by supervised machine learning algorithms. However, the same concepts of data validation apply to data used in other types <span class="No-Break">of validation.</span></p>
			<p>Every form of data validation is a set of checks that ensure the data contains the desired properties. An example of such a set of checks is presented in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer021">
					<img alt="Figure 2.7 – An example of data quality checks" src="image/B19548_02_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – An example of data quality checks</p>
			<p>Completeness of data is a property that describes how much of the total distribution our data covers. This can be measured in terms of object distribution – for example, how many types/models/colors of cars we have in our image dataset – or it can be measured in terms of properties – for example, how many of the words in the language our <span class="No-Break">data contains.</span></p>
			<p>Accuracy<a id="_idIndexMarker064"/> is a property that describes how well our data is related to the empirical (real) world. For example, we may want to check whether all images in our dataset are linked to an object or whether all objects in the image <span class="No-Break">are annotated.</span></p>
			<p>Consistency describes how well the data is structured internally and whether the same data points are annotated in the same way. For example, in binary classification, we want all data points to be labeled “0” and “1” or “True” and “False,” but <span class="No-Break">not both.</span></p>
			<p>Integrity is the property where we check that the data can be integrated with other data. The integration can be done through common keys or other means. For example, we can check if our images contain metadata that will allow us to know where the image <span class="No-Break">was taken.</span></p>
			<p>Finally, timeliness is a property that describes how fresh the data is. It checks whether the data contains the latest required information. For example, when we design a recommendation system, we would like to recommend both the new items and the old ones, so timeliness <span class="No-Break">is important.</span></p>
			<p>So, here is the next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #8</p>
			<p class="callout">Choose the data validation attributes that are the most relevant for <span class="No-Break">your system.</span></p>
			<p>Since every check requires additional steps in the workflow and can slow down the processing of the data, we should choose the data quality checks that impact our business and our architecture. If we develop a system where we want to provide up-to-date recommendations, then timeliness is our top priority, and we should focus on that rather than <span class="No-Break">on completeness.</span></p>
			<p>Although there are a lot of frameworks for data validation and assessing data quality, I usually use a subset of data quality attributes from the AIMQ framework. The AIMQ framework has been designed to quantify data quality based on several quality attributes, similar to quality frameworks in software engineering such as the ISO 25000 series. I find the following properties of data to be the most important <span class="No-Break">to validate:</span></p>
			<ul>
				<li>The data is free <span class="No-Break">of noise</span></li>
				<li>The data <span class="No-Break">is fresh</span></li>
				<li>The data is fit <span class="No-Break">for purpose</span></li>
			</ul>
			<p>The <a id="_idIndexMarker065"/>first property is the most important as noisy data can cause low performance in machine learning algorithms. For class noise, which we introduced previously, it is important to check if the data labels are not contradictory and to check whether the labels are assigned correctly. Contradictory labels can be found automatically, but wrongly annotated data points need manual assessment. For attribute noise, we can use statistical approaches to identify attributes that have low variability (or even the ones that are constant) or attributes that are completely random and do not contribute to the model’s learning. Let’s consider an example of <span class="No-Break">a sentence:</span></p>
			<p><strong class="source-inline">Mike is not a </strong><span class="No-Break"><strong class="source-inline">tall boy.</strong></span></p>
			<p>If we use the same feature extraction technique as for the previous sentences, our feature matrix looks like what’s shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.8</em>. We use the same features as for sentences 0 and 1 for sentence 2. Since the last sentence differs only in terms of one word (not), this can lead to class noise. The last column has a label, <strong class="bold">0</strong>, which indicates that the sentence is negative. However, since we use the same feature extraction algorithm, the feature vector does not include the word <strong class="source-inline">not</strong>. This can happen when we train the model on one dataset and apply it to another one. This means that the first sentence and the last sentence have identical feature vectors, but <span class="No-Break">different labels:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table004">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Sentence ID</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Mike</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Is</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">A</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">tall</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">boy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">smart</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">girl</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Label</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">2</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">0</strong></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Noisy dataset</p>
			<p>Having two different annotations for the same feature vectors is problematic as the machine learning algorithms cannot learn the pattern since there isn’t one for these noisy data points. Therefore, we need to validate the data in terms of it being free <span class="No-Break">from noise.</span></p>
			<p>Another <a id="_idIndexMarker066"/>property that the data needs to possess is its timeliness – that is, being fresh. We must use current, not old, data. One of the areas where this is of utmost importance is autonomous driving, where we need to keep the models up to date with the latest conditions (for example, the latest <span class="No-Break">traffic signs).</span></p>
			<p>Finally, the most important part of validation is assessing whether the data is fit for purpose. Note that this assessment cannot be done automatically as it needs to be <span class="No-Break">done expertly.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor030"/>Configuration and monitoring</h1>
			<p>Machine learning software is meant to be professionally engineered, deployed, and maintained. Modern companies call this process <em class="italic">MLOps</em>, which means that the same team needs to take responsibility for both the development and operations of the machine learning system. The rationale behind this extended responsibility is that the team knows the system best and therefore can configure, monitor, and maintain it in the best possible way. The teams know the design decisions that must be taken when developing the system, assumptions made about the data, and potential risks to monitor after <span class="No-Break">the deployment.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor031"/>Configuration</h2>
			<p>Configuration<a id="_idIndexMarker067"/> is one such design decision that’s made by the development team. The team configures the parameters of the machine learning models, the execution environment, and the monitoring infrastructure. Let’s explore the first one; the latter two will be discussed in the next <span class="No-Break">few sections.</span></p>
			<p>To exemplify this challenge, let’s look at a random forest classifier for a dataset for classifying events during a specific surgery. The classifier, at least in its Python implementation, has 16 hyperparameters (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>). Each of these hyperparameters has several values, which means that finding the optimal set of hyperparameters can be a <span class="No-Break">tedious task.</span></p>
			<p>However, in practice, we do not need to explore all hyperparameters’ values and we do not need to explore all combinations. We should only explore the ones that are the most relevant for our task and, by extension, the dataset. I usually explore only two hyperparameters <a id="_idIndexMarker068"/>as these are the most important: the number of trees and the depth of the tree. The first determines how broad the forest is, while the second determines how deep it is. The code to specify these parameters can look something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
rnd_clf = RandomForestClassifier(n_estimators=2,
                                    max_leaf_nodes=10,
                                    n_jobs=-1)</pre>			<p>The <strong class="source-inline">n_estimators</strong> hyperparameter is the number of trees, while <strong class="source-inline">max_depth</strong> hyperparameter is the depth of each tree. The values of these parameters depend on the dataset – how many features we have and how many data points we have. If we have too many trees and too many leaves compared to the number of features and data points, the classifier gets overfitted and cannot generalize from the data. This means that the classifier has learned to recognize each instance rather than recognize patterns in the data – we<a id="_idIndexMarker069"/> call <span class="No-Break">this </span><span class="No-Break"><em class="italic">overfitting</em></span><span class="No-Break">.</span></p>
			<p>If we have too few trees or leaves, then the generalized patterns will be too broad and therefore we observe errors in classification – at least more errors than the optimal classifier. We call <a id="_idIndexMarker070"/>this <em class="italic">underfitting</em> as the model does not learn the <span class="No-Break">pattern correctly.</span></p>
			<p>So, we can write a piece of code that would search for the best combination of these two parameters based on the pre-defined set of values. The code to find the best parameters manually would look something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
numEstimators = [2, 4, 8, 16, 32, 64, 128, 256, 512]
numLeaves = [2, 4, 8, 16, 32, 64, 128]
for nEst in numEstimators:
  for nLeaves in numLeaves:
    rnd_clf = RandomForestClassifier(n_estimators=nEst,
                                    max_leaf_nodes=nLeaves,
                                    n_jobs=-1)
    rnd_clf.fit(X_train, y_train)
    y_pred_rf = rnd_clf.predict(X_test)
    accuracy_rf = accuracy_score(y_test, y_pred_rf)
    print(f'Trees: {nEst}, Leaves: {nLeaves}, Acc: {accuracy_rf:.2f}')</pre>			<p>The two lines emphasized in orange show two loops that explore these parameters – the content of the inner loop trains the classifier with these parameters and prints out <span class="No-Break">the output.</span></p>
			<p>Let’s apply this<a id="_idIndexMarker071"/> algorithm to physiological data from patients who have undergone operations. When we plot the output on the diagram, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.9</em>, we can observe how the accuracy evolves. If we set the number of trees to 2, the classifier’s best performance is the best for 8 leaves, but even then, it does not classify the events perfectly. For four trees, the classifier achieves the best performance with 128 leaves, and the accuracy is 1.0. From the following diagram, we can see that adding more trees does not improve the <span class="No-Break">results significantly:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer022">
					<img alt="Figure 2.9 – Accuracy per number of estimators and leaves. The labels of the ﻿x axis show the number of trees (before the underscore) and the number of leaves (after the underscore)" src="image/B19548_02_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Accuracy per number of estimators and leaves. The labels of the x axis show the number of trees (before the underscore) and the number of leaves (after the underscore)</p>
			<p>For this example, the time required to search for the best result is relatively short – it takes up to 1-2 minutes on a standard laptop. However, if we want to find the optimal combination of all 16 parameters, we will spend a significant amount of time <span class="No-Break">doing this.</span></p>
			<p>There is a<a id="_idIndexMarker072"/> more automated way of finding the optimal parameters of machine learning classifiers – different types of search algorithms. One of the most popular ones is the GridSearch algorithm (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</a>), which works similarly to our manual script, except that it can do cross-validation with multiple splits and many other statistical tricks to improve the search. Searching for the optimal solution with the GridSearch algorithm can look something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
# Create the parameter grid based on the results of random search
param_grid = {
    'max_depth': [2, 4, 8, 16, 32, 64, 128],
    'n_estimators': [2, 4, 8, 16, 32, 64, 128, 256, 512]
}
# Create a base model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf,
                           param_grid = param_grid,
                           cv = 3,
                           n_jobs = -1)
# Fit the grid search to the data
grid_search.fit(X_train, y_train)
# get the best parameters
best_grid = grid_search.best_estimator_
# print the best parameters
print(grid_search.best_params_)</pre>			<p>The preceding <a id="_idIndexMarker073"/>code finds the best solution and saves it as the <strong class="source-inline">best_estimator_</strong> parameter of the GridSearch model. In the case of this dataset and this model, the algorithm finds the best random forest to be the one with 128 trees (<strong class="source-inline">n_estimators</strong>) and 4 levels (<strong class="source-inline">max_depth</strong>). The results are a bit different than the ones found manually, but this does not mean that one of the methods <span class="No-Break">is superior.</span></p>
			<p>However, having too many trees can result in overfitting, so I would choose the model with 4 trees and 128 leaves over the one with 128 trees and 4 levels. Or maybe I would also use a model that is somewhere in-between – that is, a model that has the same accuracy but is less prone to overfitting in either the depth or <span class="No-Break">the width.</span></p>
			<p>This leads to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #9</p>
			<p class="callout">Use GridSearch and other algorithms after you have explored the parameter search <span class="No-Break">space manually.</span></p>
			<p>Although the automated parameter search algorithms are very useful, they hide properties of the data from us and do not allow us to explore the data and the parameters ourselves. From my experience, understanding the data, the model, its parameters, and its configuration is crucial to the successful deployment of machine learning software. I only use GridSearch (or other optimization algorithms) after I’ve tried to find some optima manually since I would like to understand <span class="No-Break">the data.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor032"/>Monitoring</h2>
			<p>Once the<a id="_idIndexMarker074"/> machine learning system has been configured, it is set into production, often as part of the larger software system. The inferences that are made by machine learning are the basis for the features of the product and the business model behind this. Therefore, the machine learning component should make as few mistakes as possible. Unfortunately, the customers take failures and mistakes more seriously than correctly <span class="No-Break">functioning products.</span></p>
			<p>However, the performance of the machine learning system degrades over time, but not because of low-quality programming or design – this is the nature of probabilistic computing. Therefore, all machine learning systems need to be monitored <span class="No-Break">and maintained.</span></p>
			<p>One of <a id="_idIndexMarker075"/>the aspects that needs to be monitored is called <em class="italic">concept drift</em>. Concept drift is a phenomenon in the data, which means that the distribution of entities in the data changes over time for natural reasons. <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.10</em> illustrates concept drift for a machine learning classifier (blue and red lines) of images of yellow and <span class="No-Break">orange trucks:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer023">
					<img alt="Figure 2.10 – Illustration of concept drift. The original distribution of the objects (on the left) changes over time (on the right), so the classifier must be retrained (blue versus red lines)" src="image/B19548_02_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Illustration of concept drift. The original distribution of the objects (on the left) changes over time (on the right), so the classifier must be retrained (blue versus red lines)</p>
			<p>The left-hand side shows the original distribution of data that was used to train the model. The model is conceptually shown as the blue dotted line. The model recognizes the differences between the two classes of images. However, over time, the data can change. New images can appear in the dataset and the distribution can change. The original model starts to make mistakes in inference and therefore needs to be adjusted. The re-trained model – that is, the solid red line – captures the new distribution of <span class="No-Break">the data.</span></p>
			<p>It is this change in the dataset that we call concept drift. It is more common in complex datasets and supervised learning models, but the effects of concept drift are equally problematic for non-supervised models and reinforcement <span class="No-Break">learning models.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.11</em> presents the performance of the same random forest model applied to the data from the same distribution (directly after training) and on the data after some operation. Concept drift is visible in the accuracy reduction from 1.0 to 0.44. The model has been trained<a id="_idIndexMarker076"/> on the same data as in the example in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.9</em> but has been applied to data from <span class="No-Break">another patient:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer024">
					<img alt="Figure 2.11 – An example of performance decrease before and after concept drift" src="image/B19548_02_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – An example of performance decrease before and after concept drift</p>
			<p>Therefore, I would like to introduce my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #10</p>
			<p class="callout">Always include monitoring mechanisms in your machine <span class="No-Break">learning systems.</span></p>
			<p>Including mechanisms to monitor concept drift, even a simple mechanism such as using a Chi-square statistical test for the similarity of distributions, makes a lot of difference. It allows us to identify problems in the system, troubleshoot them, and prevent them from spreading to other parts of the software, or even to the end user of <span class="No-Break">the software.</span></p>
			<p>Professional machine learning engineers set up monitoring mechanisms for concept drift in production, which indicates the degradation of <span class="No-Break">AI software.</span></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor033"/>Infrastructure and resource management</h1>
			<p>The<a id="_idIndexMarker077"/> infrastructure and resources needed for the machine learning software are organized into two areas – data serving infrastructure (for example, databases) and computational infrastructure (for example, GPU computing platforms). There is also serving infrastructure, which is used to provide the services to the end users. The serving infrastructure could be in the form of desktop applications, embedded software (such as the one in autonomous vehicles), add-ins to tools (as in the case of GitHub Co-pilot), or websites (such as ChatGPT). However, in this book, we’ll focus on the data-serving infrastructure and the <span class="No-Break">computational infrastructure.</span></p>
			<p>Both areas can be deployed locally or remotely. Local deployment means that we use our own infrastructure at the company, while remote infrastructure means that we use cloud services or services of <span class="No-Break">another supplier.</span></p>
			<p>Conceptually, we could see these two areas as co-dependent, as depicted in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer025">
					<img alt="Figure 2.12 – Co-dependency between serving, computing, and data-serving infrastructure" src="image/B19548_02_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Co-dependency between serving, computing, and data-serving infrastructure</p>
			<p>The data serving infrastructure provisions data that’s used for the computation infrastructure. It consists of databases and other data sources (for example, raw files). The computation infrastructure consists of computing infrastructure to train and test machine learning models. Finally, the user-serving infrastructure uses the models to make inferences and provides services and functionality to the <span class="No-Break">end users.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor034"/>Data serving infrastructure</h2>
			<p>Data serving infrastructure is<a id="_idIndexMarker078"/> one of the fundamental parts of the machine learning software because there is no machine learning if there is no data. Data-hungry machine learning applications pose new requirements for the infrastructure in terms of performance, reliability, and traceability. The last requirement is very important as the machine learning training data determines how the trained machine learning model makes inferences. In the case of defects in the end user function, software engineers need to scrutinize the algorithms, the models, and the data that were used to construct the failing machine <span class="No-Break">learning system.</span></p>
			<p>In contrast to traditional software, the data-serving infrastructure is often composed of three different parts, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 2.13 – Data serving infrastructure" src="image/B19548_02_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Data serving infrastructure</p>
			<p>The data is stored in persistent storage, such as in a database on a hard drive. It can be stored locally or on a cloud server. The most important part is that this data is secure and can be accessed for further processing. The persistently stored data needs to be extracted so that it can be used in machine learning. The first step is to find the snapshot of the data that is needed – for example, by selecting data for training. The snapshot needs to be prepared and formatted in tabular form so that the machine learning algorithms can use the data to <span class="No-Break">make inferences.</span></p>
			<p>There are several different types of databases today that machine learning systems use. First, there are standard relational databases, where data is stored in the form of tables. These are the databases that are well known and used widely both in traditional and machine <span class="No-Break">learning software.</span></p>
			<p>Newer types of databases are non-SQL databases such as Elasticsearch (<a href="https://www.elastic.co">https://www.elastic.co</a>), which<a id="_idIndexMarker079"/> are designed to store documents, not tables. These documents are indexed flexibly so that data can be stored and retrieved based on these documents. In the case of machine learning software, these documents can be images, entire text documents, or even sound data. This is important for storing data in the same format as it is collected so that we can trace the data <span class="No-Break">when needed.</span></p>
			<p>Regardless<a id="_idIndexMarker080"/> of the format of the data in the databases, it is retrieved from the database and transformed into tabular form; we’ll discuss this in <a href="B19548_03.xhtml#_idTextAnchor038"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. This tabular form is required for the data to be processed by the <span class="No-Break">computational infrastructure.</span></p>
			<p>With that, we’ve come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #11</p>
			<p class="callout">Choose the right database for your data – look at this from the perspective of the data, not <span class="No-Break">the system.</span></p>
			<p>Although it sounds obvious that we should choose the right database for the data that we have, for machine learning systems, it is important to select the database that works best for the data at hand, not the system. For example, when we use natural language processing models, we should store the data in documents that we can easily retrieve in an <span class="No-Break">organized form.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor035"/>Computational infrastructure</h2>
			<p>The <a id="_idIndexMarker081"/>computational infrastructure can change over time. In the early phases of the development of machine learning systems, software developers often use pre-configured experimentation environments. These can be in the form of Jupyter Notebooks on their computers or in the form of pre-configured services such as Google Colab or Microsoft Azure Notebooks. This kind of infrastructure supports rapid prototyping of machine learning, easy provisioning of data, and no need for setting up advanced features. They also allow us to easily scale the computational resources up and down without the need to add or remove <span class="No-Break">extra hardware.</span></p>
			<p>An alternative to this approach is to use our own infrastructure, where we set up your own servers and runtime environments. It requires more effort, but it provides us with full control <a id="_idIndexMarker082"/>over the computational resources, as well as full control over data processing. Having full control over data processing can sometimes be the most important factor for selecting <span class="No-Break">the infrastructure.</span></p>
			<p>Hence, my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #12</p>
			<p class="callout">Use cloud infrastructure if you can as it saves resources and reduces the need for <span class="No-Break">specialized competence.</span></p>
			<p>Professional AI engineers use self-owned infrastructure for prototyping and training and cloud-based infrastructure for production as it scales better with the number of users. The opposite, which is to use our own infrastructure, is true only if we need to retain full control over data or infrastructure. Full control can be required for applications that use sensitive customer data, military applications, security applications, and other applications where data is <span class="No-Break">extremely sensitive.</span></p>
			<p>Luckily, we have several large actors providing the computational infrastructure, as well as a large ecosystem of small actors. The three largest – Amazon Web Services, Google Cloud, and Microsoft Azure – can provide all kinds of services for both small and large enterprises. Amazon Web Services (<a href="https://aws.amazon.com">https://aws.amazon.com</a>) specializes in provisioning<a id="_idIndexMarker083"/> data storage and processing infrastructure. It is often used for applications that must quickly process large quantities of data. The infrastructure is professionally maintained and can be used to achieve near-perfect reliability of the products built on that platform. To use it efficiently, you must usually work with containers and virtual machines that execute the code of the machine <span class="No-Break">learning application.</span></p>
			<p>Google Cloud (<a href="https://cloud.google.com">https://cloud.google.com</a>) specializes<a id="_idIndexMarker084"/> in provisioning platforms for data-intensive applications and computation-intensive solutions. Thanks to <a id="_idIndexMarker085"/>Google’s processors (<strong class="bold">tensor processing units</strong> (<strong class="bold">TPUs</strong>)), the platform provides a very efficient environment for both training and using machine learning solutions. Google Cloud also provides free solutions for learning machine learning in the form of Google Colab, which is an extension of the<a id="_idIndexMarker086"/> Jupyter Notebook (<a href="https://jupyter.org">https://jupyter.org</a>) platform <span class="No-Break">on Python.</span></p>
			<p>Microsoft Azure (<a href="https://azure.microsoft.com">https://azure.microsoft.com</a>) specializes<a id="_idIndexMarker087"/> in provisioning platforms for training and deploying machine learning systems in the form of virtual machines. It also provides ready-to-deploy models for image recognition, classification, and natural language processing, and even platforms for training generic machine learning models. It is the most flexible of the available platforms and the <span class="No-Break">most scalable.</span></p>
			<p>In addition <a id="_idIndexMarker088"/>to these platforms, you can use several specialized ones, such as Facebook’s platform for machine learning, which specializes in recommender systems. However, since the specialized platforms are often narrow, we need to remember the potential issues if we want to port our software from one platform <span class="No-Break">to another.</span></p>
			<p>Hence, my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #13</p>
			<p class="callout">Decide on which production environment you wish to use early and align your process with <span class="No-Break">that environment.</span></p>
			<p>We need to decide if we want to use Amazon’s, Google’s, or Microsoft’s cloud environment or whether we want to use our own infrastructure to reduce the cost of software development. Although it is possible to move our software between these environments, it is not straightforward and requires (at best) significant testing and pre-deployment validation, which often comes with <span class="No-Break">significant costs.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor036"/>How this all comes together – machine learning pipelines</h1>
			<p>In this chapter, we <a id="_idIndexMarker089"/>explored the main characteristics of machine learning systems and compared them to traditional software systems. Let’s finish this comparison by summarizing how we usually design and describe machine learning systems – by using pipelines. A pipeline is a sequence of data processing steps, including the machine learning models. The typical set of steps (also called phases) is shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer027">
					<img alt="Figure 2.14 – A typical sequence of steps in a machine learning pipeline" src="image/B19548_02_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – A typical sequence of steps in a machine learning pipeline</p>
			<p>This<a id="_idIndexMarker090"/> kind of pipeline, although drawn linearly, is usually processed in cycles, where, for example, monitoring for concept drift can trigger re-training, re-testing, <span class="No-Break">and re-deployment.</span></p>
			<p>Machine learning pipelines, just like the one presented in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.14</em>, are often depicted as a set of components as parts of the entire system. However, presenting it using the pipeline analogy helps us understand that machine learning systems process data <span class="No-Break">in steps.</span></p>
			<p>In the next chapter, we’ll explore the first part of the pipeline – working with data. We’ll start by exploring different types of data and how these types of data are collected, processed, and used in modern <span class="No-Break">software systems.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor037"/>References</h1>
			<ul>
				<li><em class="italic">Shortliffe, E.H., et al., Computer-based consultations in clinical therapeutics: explanation and rule acquisition capabilities of the MYCIN system. Computers and biomedical research, 1975. 8(4): </em><span class="No-Break"><em class="italic">p. 303-320.</em></span></li>
				<li><em class="italic">Vaswani, A., et al., Attention is all you need. Advances in neural information processing systems, </em><span class="No-Break"><em class="italic">2017. 30.</em></span></li>
				<li><em class="italic">Dale, R., GPT-3: What’s it good for? Natural Language Engineering, 2021. 27(1): </em><span class="No-Break"><em class="italic">p. 113-118.</em></span></li>
				<li><em class="italic">Smith, S., et al., Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:2201.11990, 2022.</em></span></li>
				<li><em class="italic">Lee, Y.W., et al., AIMQ: a methodology for information quality assessment. Information &amp; management, 2002. 40(2): </em><span class="No-Break"><em class="italic">p. 133-146.</em></span></li>
				<li><em class="italic">Zenisek, J., F. Holzinger, and M. Affenzeller, Machine learning based concept drift detection for predictive maintenance. Computers &amp; Industrial Engineering, 2019. 137: </em><span class="No-Break"><em class="italic">p. 106031.</em></span></li>
				<li><em class="italic">Amershi, S., et al. Software engineering for machine learning: A case study. in 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). </em><span class="No-Break"><em class="italic">2019. IEEE.</em></span></li>
			</ul>
		</div>
	</body></html>