- en: Chapter 10. Probabilistic Graphical Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章。概率图模型
- en: Probabilistic graphical models, or simply graphical models as we will refer
    to them in this chapter, are models that use the representation of a graph to
    describe the conditional independence relationships between a series of random
    variables. This topic has received an increasing amount of attention in recent
    years and probabilistic graphical models have been successfully applied to tasks
    ranging from medical diagnosis to image segmentation. In this chapter, we'll present
    some of the necessary background that will pave the way to understanding the most
    basic graphical model, the Naïve Bayes classifier. We will then look at a slightly
    more complicated graphical model, known as the **Hidden Markov Model** (**HMM**).
    To get started in this field, we must first learn about graphs and why they are
    useful.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概率图模型，或简称为我们在本章中将要提到的图模型，是使用图的表示来描述一系列随机变量之间的条件独立性关系的模型。近年来，这一主题受到了越来越多的关注，概率图模型已经在从医学诊断到图像分割等任务中得到了成功的应用。在本章中，我们将介绍一些必要的背景知识，这将有助于理解最基本的图模型——朴素贝叶斯分类器。然后，我们将探讨一个稍微复杂一些的图模型，称为**隐马尔可夫模型**（**HMM**）。要进入这个领域，我们首先必须了解图以及它们为什么有用。
- en: A little graph theory
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一点图论
- en: Graph theory is a branch of mathematics that deals with mathematical objects
    known as **graphs**. Here, a graph does not have the everyday meaning that we
    are more used to talking about, in the sense of a diagram or plot with an *x*
    and *y* axis. In graph theory, a graph consists of two sets. The first is a set
    of vertices, which are also referred to as **nodes**. We typically use integers
    to label and enumerate the vertices. The second set consists of **edges** between
    these vertices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图论是数学的一个分支，它处理称为**图**的数学对象。在这里，图没有我们更习惯于谈论的日常意义，即具有 *x* 和 *y* 轴的图表或图解。在图论中，一个图由两个集合组成。第一个是顶点集合，也被称为**节点**。我们通常使用整数来标记和列举顶点。第二个集合由这些顶点之间的**边**组成。
- en: Thus, a graph is nothing more than a description of some points and the connections
    between them. The connections can have a direction so that an edge goes from the
    **source** or **tail vertex** to the **target** or **head vertex**. In this case,
    we have a **directed graph**. Alternatively, the edges can have no direction,
    so that the graph is **undirected**.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，图不过是一些点的描述以及它们之间的连接。这些连接可以有方向，使得边从**源**或**尾**顶点指向**目标**或**头**顶点。在这种情况下，我们有一个**有向图**。或者，边可以没有方向，这样图就是**无向图**。
- en: A common way to describe a graph is via the **adjacency matrix**. If we have
    *V* vertices in the graph, an adjacency matrix is a *V×V* matrix whose entries
    are 0 if the vertex represented by the row number is not connected to the vertex
    represented by the column number. If there is a connection, the entry is 1 (however,
    when you are using weighted graphs, the entry value is not always 1).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 描述图的一种常见方式是通过**邻接矩阵**。如果我们有一个图中的 *V* 个顶点，那么邻接矩阵是一个 *V×V* 矩阵，其条目如果行号表示的顶点与列号表示的顶点不相连，则为
    0。如果存在连接，则条目为 1（然而，当你在使用加权图时，条目值不总是 1）。
- en: 'With undirected graphs, both nodes at each edge are connected to each other
    so the adjacency matrix is symmetric. For directed graphs, a vertex *v[i]* is
    connected to a vertex *v[j]* via an edge (*v[i]*,*v[j]*); that is, an edge where
    *v[i]* is the tail and *v[j]* is the head. Here is an example adjacency matrix
    for a graph with seven nodes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在无向图中，每条边上的节点都相互连接，因此邻接矩阵是对称的。对于有向图，一个顶点 *v[i]* 通过边 (*v[i]*,*v[j]*) 与顶点 *v[j]*
    相连；也就是说，其中 *v[i]* 是尾节点，*v[j]* 是头节点。以下是一个具有七个节点的图的邻接矩阵示例：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This matrix is not symmetric, so we know that we are dealing with a directed
    graph. The first `1` value in the first row of the matrix denotes the fact that
    there is an edge starting from vertex 1 and ending on vertex 6\. When the number
    of nodes is small, it is easy to visualize a graph. We simply draw circles to
    represent the vertices and lines between them to represent the edges.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵不是对称的，因此我们知道我们正在处理一个有向图。矩阵第一行中的第一个 `1` 值表示从顶点 1 出发并结束在顶点 6 的边。当节点数量较少时，很容易可视化一个图。我们只需画圆圈来表示顶点，并在它们之间画线来表示边。
- en: 'For directed graphs, we use arrows on the lines to denote the directions of
    the edges. It is important to note that we can draw the same graph in an infinite
    number of different ways on the page. This is because the graph tells us nothing
    about the positioning of the nodes in space; we only care about how they are connected
    to each other. Here are two different, but equally valid, ways to draw the graph
    described by the adjacency matrix we just saw:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有向图，我们在线段上使用箭头来表示边的方向。需要注意的是，我们可以在页面上以无限多种不同的方式绘制相同的图。这是因为图告诉我们关于节点在空间中的位置信息很少；我们只关心它们是如何相互连接的。以下是我们刚才看到的邻接矩阵描述的图的不同但同样有效的方式：
- en: '![A little graph theory](img/00167.jpeg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![一点图论](img/00167.jpeg)'
- en: Two vertices are said to be connected with each other if there is an edge between
    them (taking note of the order when talking about directed graphs). If we can
    move from vertex *v[i]* to vertex *v[j]* by starting at the first vertex and finishing
    at the second vertex, by moving on the graph along the edges and passing through
    an arbitrary number of graph vertices, then these intermediate edges form a **path**
    between these two vertices. Note that this definition requires that all the vertices
    and edges along the path are distinct from each other (with the possible exception
    of the first and last vertex).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个顶点之间存在边（在有向图的情况下，注意顺序），则称这两个顶点相互连接。如果我们可以从顶点 *v[i]* 通过移动到顶点 *v[j]*，从第一个顶点开始，在第二个顶点结束，沿着图中的边移动并通过任意数量的图顶点，那么这些中间边形成这两个顶点之间的**路径**。请注意，这个定义要求路径上的所有顶点和边彼此不同（可能只有第一个和最后一个顶点除外）。
- en: For example, in our graph, vertex 6 can be reached from vertex 2 by a path leading
    through vertex 1\. Sometimes, there can be many such possible paths through the
    graph, and we are often interested in the shortest path, which moves through the
    fewest number of intermediary vertices. We can define the distance between two
    nodes in the graph as the length of the shortest path between them. A path that
    begins and ends at the same vertex is known as a **cycle**. A graph that does
    not have any cycles in it is known as an **acyclic graph**. If an acyclic graph
    has directed edges, it is known as a **directed acyclic graph**, which is often
    abbreviated to **DAG**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的图中，顶点6可以通过通过顶点1的路径从顶点2到达。有时，图中可能会有许多这样的可能路径，而我们通常对最短路径感兴趣，即通过最少数量的中间顶点。我们可以在图中定义两个节点之间的距离为它们之间最短路径的长度。起点和终点相同的路径称为**环**。没有环的图称为**无环图**。如果一个无环图有有向边，则称为**有向无环图**，通常缩写为**DAG**。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are many excellent references on graph theory available. One such reference
    that is available online is *Graph Theory*, *Reinhard Diestel*, *Springer*. This
    landmark reference is now in its *4th* edition and can be found at [http://diestel-graph-theory.com/](http://diestel-graph-theory.com/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图论有许多优秀的参考资料。其中之一是可在网上找到的《图论》，作者为*Reinhard Diestel*，*Springer*出版社。这本里程碑式的参考书现在已出到*第4版*，可在[http://diestel-graph-theory.com/](http://diestel-graph-theory.com/)找到。
- en: It might not seem obvious at first, but it turns out that a large number of
    real-world situations can be conveniently described using graphs. For example,
    the network of friendships on social media sites, such as Facebook, or followers
    on Twitter, can be represented as graphs. On Facebook, the friendship relation
    is reciprocal, and so the graph is undirected. On Twitter, the follower relation
    is not, and so the graph is directed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 起初可能并不明显，但事实是，许多现实世界的情况可以方便地使用图来描述。例如，社交媒体网站上的友谊网络，如Facebook，或Twitter上的关注者，都可以表示为图。在Facebook上，友谊关系是相互的，因此图是无向的。在Twitter上，关注者关系则不是，因此图是有向的。
- en: Another graph is the network of websites on the web, where links from one web
    page to the next form directed edges. Transport networks, communication networks,
    and electricity grids can be represented as graphs. For the predictive modeler,
    it turns out that a special class of models known as **probabilistic graphical
    models**, or **graphical models** for short, are models that involve a graph structure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个图是网络上的网站网络，其中从一个网页到下一个网页的链接形成有向边。运输网络、通信网络和电网都可以表示为图。对于预测模型师来说，结果是存在一类称为**概率图模型**或简称**图模型**的特殊模型，这些模型涉及图结构。
- en: In a graphical model, the nodes represent random variables and the edges in
    between represent the dependencies between them. Before we can go into further
    detail, we'll need to take a short detour in order to visit Bayes' theorem, a
    classic theorem in statistics that, despite its simplicity, has implications both
    profound and practical when it comes to statistical inference and prediction.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图形模型中，节点代表随机变量，节点之间的边代表它们之间的依赖关系。在我们可以进一步详细说明之前，我们需要短暂地偏离一下，以便访问贝叶斯定理，这是统计学中的一个经典定理，尽管它很简单，但在统计推断和预测方面具有深远和实用的意义。
- en: Bayes' theorem
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: Suppose we are interested in two events, *A* and *B*. In this case, event *A*
    might represent the event that a patient has appendicitis and event *B* might
    represent a patient having a high white blood cell count. The **conditional probability**
    of event *A* given event *B* is essentially the probability that event *A* will
    occur when we know that event *B* has already happened.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们感兴趣的两个事件是 *A* 和 *B*。在这种情况下，事件 *A* 可能代表患者患有阑尾炎，而事件 *B* 可能代表患者有高白细胞计数。事件 *A*
    在事件 *B* 发生条件下的**条件概率**实际上是在我们知道事件 *B* 已经发生时事件 *A* 发生的概率。
- en: 'Formally, we define the conditional probability of event *A* given event *B*
    as the joint probability of both events occurring divided by the probability of
    event *B* occurring:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，我们定义事件 *A* 在事件 *B* 发生条件下的条件概率为两个事件同时发生的联合概率除以事件 *B* 发生的概率：
- en: '![Bayes'' theorem](img/00168.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](img/00168.jpeg)'
- en: 'Note that this is consistent with the way in which we define statistical independence.
    Statistical independence occurs when the joint probability of two events occurring
    is just the product of the individual probabilities of the two events. If we substitute
    this in our previous equation, we have:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与我们定义的统计独立性是一致的。当两个事件同时发生的联合概率只是这两个事件各自概率的乘积时，就发生了统计独立性。如果我们用这个替换我们之前的方程，我们得到：
- en: '![Bayes'' theorem](img/00169.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](img/00169.jpeg)'
- en: 'This makes sense intuitively because if we know that two events are independent
    of each other, knowing that event *B* has occurred does not change the probability
    of event *A* occurring. Now, we can rearrange our equation for conditional probability
    as follows, and note that we can switch over events *A* and *B* to get an alternative
    form:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这从直观上是有意义的，因为如果我们知道两个事件是相互独立的，那么知道事件 *B* 已经发生并不会改变事件 *A* 发生的概率。现在，我们可以重新排列我们的条件概率方程，并注意我们可以交换事件
    *A* 和 *B* 来得到另一种形式：
- en: '![Bayes'' theorem](img/00170.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](img/00170.jpeg)'
- en: 'This last step allows us to state Bayes'' theorem in its simplest form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一步使我们能够以最简单形式表述贝叶斯定理：
- en: '![Bayes'' theorem](img/00171.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯定理](img/00171.jpeg)'
- en: In the previous equation, *P(A)* is referred to as the **prior probability**
    of event *A*, as it represents the probability of event *A* occurring prior to
    any new information. *P(A|B)*, which is the conditional probability of event *A*
    given that event *B* has occurred, is often also referred to as the **posterior
    probability** of *A*. It is the probability of event *A* occurring after receiving
    some new information; in this case, the fact that event *B* has occurred.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，*P(A)* 被称为事件 *A* 的**先验概率**，因为它代表了在获得任何新信息之前事件 *A* 发生的概率。*P(A|B)*，即在事件
    *B* 发生的情况下事件 *A* 的条件概率，通常也被称为 *A* 的**后验概率**。这是在接收到一些新信息后事件 *A* 发生的概率；在这种情况下，事件
    *B* 已经发生的事实。
- en: All of this might seem like algebraic trickery, but if we revisit our example
    of event *A* representing a patient having appendicitis and event *B* representing
    a patient having a high white blood cell count, the usefulness of Bayes' theorem
    will be revealed. Knowing *P(A|B)*, the conditional probability of having appendicitis,
    given that we observe that a patient has a high white blood cell count (and similarly
    for other symptoms), is knowledge that would be very useful to doctors. This would
    allow them to make a diagnosis about something that isn't easily observable (appendicitis)
    using something that is (high white blood cell count).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可能看起来像是代数技巧，但如果我们回顾一下事件 *A* 代表患者患有阑尾炎，事件 *B* 代表患者有高白细胞计数的例子，贝叶斯定理的有用性将得到揭示。知道
    *P(A|B)*，即在观察到患者有高白细胞计数（以及其他症状）的情况下患有阑尾炎的条件概率，对于医生来说是非常有用的知识。这将使他们能够利用可以观察到的（高白细胞计数）来对不易观察到的（阑尾炎）进行诊断。
- en: Unfortunately, this is something that is very hard to estimate because a high
    white blood cell count might occur as a symptom of a host of other diseases or
    pathologies. The reverse probability, *P(B|A)*, however (namely, the conditional
    probability of having a high white blood cell count given that a patient already
    has appendicitis), is much easier to estimate. One simply needs to examine records
    of past cases with appendicitis and inspect the blood tests of those cases. Bayes'
    theorem is a fundamental boon to predictive modeling because it allows us to estimate
    cause by observing effect.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这很难估计，因为高白细胞计数可能是一系列其他疾病或病理症状的表现。然而，逆概率 *P(B|A)*（即在患者已经患有阑尾炎的情况下，高白细胞计数的条件概率）则容易得多。只需要检查过去阑尾炎病例的记录并检查这些病例的血液检查结果。贝叶斯定理是预测建模的基本福音，因为它允许我们通过观察效果来估计原因。
- en: Conditional independence
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件独立性
- en: 'We know from statistics that the notion of statistical independence says that
    the joint probability of two random variables, *A* and *B*, is just the product
    of their (marginal) probabilities. Sometimes, two variables may not be statistically
    independent of each other to begin with, but observing a third variable, *C*,
    might result in them becoming independent of each other. In short, we say that
    events *A* and *B* are **conditionally independent** given *C*, and we can express
    this as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学我们知道，统计独立性的概念表明两个随机变量 *A* 和 *B* 的联合概率只是它们（边缘）概率的乘积。有时，两个变量可能一开始就不相互统计独立，但观察第三个变量
    *C* 可能会导致它们相互独立。简而言之，我们说在 *C* 的条件下，事件 *A* 和 *B* 是**条件独立的**，我们可以用以下方式表示：
- en: '![Conditional independence](img/00172.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![条件独立性](img/00172.jpeg)'
- en: 'For example, suppose that *J* represents the probability of being given a job
    offer at a particular company and *G* represents the probability of being accepted
    into graduate school at a particular university. Both of these might depend on
    a variable *U*, a person''s performance on their undergraduate degree. This can
    be summarized in a graph as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 *J* 代表在特定公司获得工作机会的概率，而 *G* 代表在特定大学被录取的概率。这两个概率都可能依赖于一个变量 *U*，即一个人在本科学习中的表现。这可以用以下图表示：
- en: '![Conditional independence](img/00173.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![条件独立性](img/00173.jpeg)'
- en: When we don't know *U*, a person's performance on their undergraduate degree,
    knowing that they were accepted into graduate school might increase our belief
    in their chances of getting a job and vice versa. This is because we are inclined
    to believe that they did well in their undergraduate degree, which influences
    that person's chances of getting a job. Thus, the two events *J* and *G* are not
    independent of each other.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们不知道 *U*，即一个人的本科学习成绩时，知道他们被研究生院录取可能会增加我们对其获得工作机会的信心，反之亦然。这是因为我们倾向于相信他们在本科学习表现良好，这影响了那个人获得工作的机会。因此，这两个事件
    *J* 和 *G* 并不相互独立。
- en: If we are told the performance of a person on their undergraduate degree, however,
    we might assume that the person's chance of getting a job offer might be independent
    of their chance of getting into graduate school. This is because of other factors
    that might affect this, such as the person's job interview on a particular day
    or the quality of other potential candidates for the job, which are not influenced
    by the person's application to graduate school.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们被告知一个人的本科学习成绩，我们可能会假设这个人获得工作机会的机会可能与进入研究生院的机会独立。这是因为可能影响这一点的其他因素，例如某一天这个人的工作面试或其他潜在候选人的工作质量，这些因素不受这个人申请研究生院的影响。
- en: Bayesian networks
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯网络
- en: '**Bayesian networks** are a type of graphical model that involve a directed
    acyclic graph structure. We often refer to the tail node of a directed edge in
    a graphical model as the **parent** and the head node as the **child** or **descendant**.
    In fact, we generalize this latter notion so that, if there is a path from node
    *A* to node *B* in the model, node *B* is a descendant of node *A*. We can distinguish
    the special case of node *A* connected to node *B* by saying that the latter is
    a **direct descendant**.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯网络**是一种涉及有向无环图结构的图形模型。我们通常将图形模型中一条有向边的尾节点称为**父节点**，而头节点称为**子节点**或**后代节点**。实际上，我们推广了这种后一种概念，即如果模型中从节点
    *A* 到节点 *B* 存在一条路径，那么节点 *B* 就是节点 *A* 的后代。我们可以通过说后者是**直接后代**来区分节点 *A* 与节点 *B* 直接相连的特殊情况。'
- en: The parent relationship and the descendant relationship are mutually exclusive
    in a Bayesian network because it has no cycles. Bayesian networks have the distinguishing
    property that, given its parents, every node in the network is conditionally independent
    of all other nodes in the network that are not its descendants. This is sometimes
    referred to as the **local Markov property**. It is an important property because
    it means that we can easily factorize the joint probability function of all the
    random variables in the model by simply taking note of the edges in the graph.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯网络中，父节点关系和子节点关系是互斥的，因为它没有循环。贝叶斯网络具有一个显著特性，即给定其父节点，网络中的每个节点在条件上独立于网络中所有不是其子节点的其他节点。这有时被称为**局部马尔可夫性质**。这是一个重要的特性，因为它意味着我们可以通过简单地注意图中的边来轻松分解模型中所有随机变量的联合概率函数。
- en: 'To understand how this works, we will begin with the product rule of probability
    for three variables that says the following (taking *G*, *J*, and *U* as example
    variables):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这是如何工作的，我们将从三个变量的概率乘法定律开始，该定律如下（以 *G*、*J* 和 *U* 作为示例变量）：
- en: '![Bayesian networks](img/00174.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络](img/00174.jpeg)'
- en: 'This rule is a general rule and always holds without any loss of generality.
    Let''s return to our student applicant example. This is actually a simple Bayesian
    network where *G* and *J* have *U* as a parent. Using the local Markov property
    of Bayesian networks, we can simplify the equation for the joint probability distribution
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一条普遍适用的规则，在没有任何普遍性损失的情况下始终成立。让我们回到我们的学生申请人示例。这实际上是一个简单的贝叶斯网络，其中 *G* 和 *J*
    以 *U* 为父节点。利用贝叶斯网络的局部马尔可夫性质，我们可以简化联合概率分布的方程如下：
- en: '![Bayesian networks](img/00175.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络](img/00175.jpeg)'
- en: The ability to factorize a probability distribution in this way is useful as
    it simplifies the computations we need to make. It can also allow us to represent
    the entire distribution in a more compact form. Suppose that the distribution
    of each random variable is discrete and takes on a finite set of values, for example,
    random variables *G* and *J* could each take on the two discrete values {yes,
    no}. To store a joint probability distribution without factorizing, and taking
    into account independence relations, we need to consider all possible combinations
    of every random variable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式分解概率分布的能力是有用的，因为它简化了我们需要的计算。它还可以使我们能够以更紧凑的形式表示整个分布。假设每个随机变量的分布是离散的，并取有限集合中的值，例如，随机变量
    *G* 和 *J* 可以分别取两个离散值 {是，否}。为了在不分解的情况下存储联合概率分布，并考虑独立性关系，我们需要考虑每个随机变量的所有可能组合。
- en: By contrast, if the distribution factorizes into a product of simpler distributions
    as we saw earlier, the total number of random variable combinations we need to
    consider are far fewer. For networks with several random variables that take on
    many values, the savings are very substantial indeed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果分布分解为更简单分布的乘积，如我们之前所见，我们需要考虑的随机变量组合总数要少得多。对于具有多个随机变量且取许多值的网络，这种节省确实是实质性的。
- en: Besides computation and storage, another significant benefit is that when we
    want to determine the joint probability distribution of our random variables given
    some data, it becomes much simpler to do so when we can factorize it because of
    known independence relations. We will see this in detail when, in the next section,
    we study an important example of a Bayesian network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算和存储之外，另一个显著的好处是，当我们想要确定给定某些数据时随机变量的联合概率分布，由于已知的独立性关系，当我们能够分解它时，这样做会变得简单得多。我们将在下一节详细研究贝叶斯网络的一个重要示例时看到这一点。
- en: 'To wrap up this section, we''ll note the factorization of the joint probability
    function of the Bayesian network, represented by the graph we saw in the first
    diagram in this chapter, and leave it as an exercise for the reader to verify:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本节，我们将注意贝叶斯网络的联合概率函数的分解，该网络由本章第一幅图中看到的图表示，并将其留作读者的练习以验证：
- en: '![Bayesian networks](img/00176.jpeg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![贝叶斯网络](img/00176.jpeg)'
- en: The Naïve Bayes classifier
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真贝叶斯分类器
- en: 'We now have the necessary tools to learn about our first and simplest graphical
    model, the **Naïve Bayes classifier**. This is a directed graphical model that
    contains a single parent node and a series of child nodes representing random
    variables that are dependent only on this node with no dependencies between them.
    Here is an example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了学习我们第一个也是最简单的图形模型——**朴素贝叶斯分类器**所必需的工具。这是一个包含单个父节点和一系列子节点的有向图形模型，这些子节点代表仅依赖于该节点的随机变量，它们之间没有依赖关系。以下是一个示例：
- en: '![The Naïve Bayes classifier](img/00177.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/00177.jpeg)'
- en: We usually interpret our single parent node as the causal node, so in our particular
    example, the value of the *Sentiment* node will influence the value of the *sad*
    node, the *fun* node, and so on. As this is a Bayesian network, the local Markov
    property can be used to explain the core assumption of the model. Given the *Sentiment*
    node, all other nodes are independent of each other.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将单个父节点解释为因果节点，因此在我们特定的例子中，*Sentiment* 节点的值将影响 *sad* 节点、*fun* 节点等的值。由于这是一个贝叶斯网络，局部马尔可夫性质可以用来解释模型的核心假设。给定
    *Sentiment* 节点，所有其他节点都是相互独立的。
- en: In practice, we use the Naïve Bayes classifier in a context where we can observe
    and measure the child nodes and attempt to estimate the parent node as our output.
    Thus, the child nodes will be the input features of our model, and the parent
    node will be the output variable. For example, the child nodes may represent various
    medical symptoms and the parent node might be whether a particular disease is
    present.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们使用朴素贝叶斯分类器在一个可以观察和测量子节点并尝试估计父节点作为我们的输出的环境中。因此，子节点将成为我们模型的输入特征，而父节点将是输出变量。例如，子节点可能代表各种医疗症状，而父节点可能表示是否存在特定的疾病。
- en: 'To understand how the model works in practice, we make recourse to Bayes''
    theorem, where *C* is the parent node and *F[i]* are the children or feature nodes:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解模型在实际中的工作方式，我们求助于贝叶斯定理，其中 *C* 是父节点，*F[i]* 是子节点或特征节点：
- en: '![The Naïve Bayes classifier](img/00178.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/00178.jpeg)'
- en: 'We can simplify this using the conditional independence assumptions of the
    network:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用网络的条件独立性假设来简化这一点：
- en: '![The Naïve Bayes classifier](img/00179.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/00179.jpeg)'
- en: 'To make a classifier out of this probability model, our objective is to choose
    the class *C[i]* which maximizes the posterior probability *P(Ci|F[1]* *…F[n]*
    *)*; that is, the posterior probability of that class given the observed features.
    The denominator is the joint probability of the observed features, which is not
    influenced by the class that is chosen. Consequently, maximizing the posterior
    class probability amounts to maximizing the numerator of the previous equation:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这个概率模型中构建一个分类器，我们的目标是选择最大化后验概率 *P(Ci|F[1]* *…F[n]* *)* 的类 *C[i]*；即给定观察到的特征的这个类的后验概率。分母是观察特征的联合概率，它不受所选类的影响。因此，最大化后验类概率等同于最大化前一个方程的分子：
- en: '![The Naïve Bayes classifier](img/00180.jpeg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![朴素贝叶斯分类器](img/00180.jpeg)'
- en: Given some data, we can estimate the probabilities, *P(F[i]* *|C[j]* *)*, for
    all the different values of the feature *F[i]* as the relative proportion of the
    observations of class *C[j]* that have each different value of feature *F[i]*.
    We can also estimate *P(C[j]* *)* as the relative proportion of the observations
    that are assigned to class *C[j]*. These are the maximum likelihood estimates.
    In the next section, we will see how the Naïve Bayes classifier works in a real
    example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一些数据，我们可以估计特征 *F[i]* 的所有不同值的概率 *P(F[i]* *|C[j]* *)*，作为类 *C[j]* 中具有每个不同特征 *F[i]*
    的观察值的相对比例。我们还可以估计 *P(C[j]* *)*，作为分配给类 *C[j]* 的观察值的相对比例。这些都是最大似然估计。在下一节中，我们将看到朴素贝叶斯分类器在真实示例中的工作方式。
- en: Predicting the sentiment of movie reviews
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测电影评论的情感
- en: In a world of online reviews, forums, and social media, a task that has received,
    and continues to receive, a growing amount of interest is the task of **sentiment
    analysis**. Put simply, the task is to analyze a piece of text to determine the
    sentiment that is being expressed by the author. A typical scenario involves collecting
    online reviews, blog posts, or tweets and building a model that predicts whether
    the user is trying to express a positive or a negative feeling. Sometimes, the
    task can be framed to capture a wider variety of sentiments, such as a neutral
    sentiment or the degree of sentiment, such as mildly negative versus very negative.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线评论、论坛和社交媒体的世界中，一个已经并且继续受到越来越多关注的是**情感分析**的任务。简单来说，这个任务就是分析一段文本，以确定作者所表达的情感。一个典型的场景是收集在线评论、博客文章或推文，并构建一个模型来预测用户是否试图表达正面或负面的感受。有时，这个任务可以扩展以捕捉更广泛的情感，例如中性情感或情感程度，如轻微负面与非常负面。
- en: In this section, we will limit ourselves to the simpler task of discerning positive
    from negative sentiments. We will do this by modeling sentiment using a similar
    Bayesian network to the one that we saw in the previous section. The sentiment
    is our target output variable, which is either positive or negative. Our input
    features are all binary features that describe whether a particular word is present
    in a movie review. The key idea here is that users expressing a negative sentiment
    will tend to choose from a characteristic set of words in their review that is
    different from the characteristic set that users would pick from when writing
    a positive review.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将限制自己只进行区分正面和负面情感这一更简单的任务。我们将通过使用与上一节中看到类似的贝叶斯网络来建模情感。情感是我们的目标输出变量，可以是正面或负面。我们的输入特征是所有二元特征，描述特定单词是否出现在电影评论中。关键思想是，表达负面情感的用户倾向于在他们评论中选择一组具有特征性的单词，这与用户在撰写正面评论时选择的特征性单词集不同。
- en: By using the Naïve Bayes model, our assumption will be that if we know the sentiment
    being expressed, the presence of each word in the text is independent from all
    the other words. Of course, this is a very strict assumption to use and doesn't
    speak at all to the process of how real text is written. Nonetheless, we will
    show that even under these strict assumptions, we can build a model that performs
    reasonably well.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用朴素贝叶斯模型，我们的假设是，如果我们知道表达的情感，文本中每个词的存在将独立于所有其他词。当然，这是一个非常严格的假设，并且根本无法说明真实文本的写作过程。尽管如此，我们将展示即使在这些严格的假设下，我们也能构建一个表现合理的模型。
- en: We will use the *Large Movie Review Data Set*, first presented in the paper
    titled *Learning Word Vectors for Sentiment Analysis*, *Andrew L. Maas*, *Raymond
    E. Daly*, *Peter T. Pham*, *Dan Huang*, *Andrew Y. Ng*, and *Christopher Potts*,
    published in *The 49th Annual Meeting of the Association for Computational Linguistics*
    (*ACL 2011*). The data is hosted at [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    and is comprised of a training set of 25,000 movie reviews and a test set of another
    25,000 movie reviews.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*大型电影评论数据集*，该数据集首次在题为*学习用于情感分析的词向量*的论文中提出，该论文由*安德鲁·L·马斯*、*雷蒙德·E·戴利*、*彼得·T·范*、*丹·黄*、*安德鲁·Y·吴*和*克里斯托弗·波茨*撰写，发表于*第49届计算语言学协会年会*（*ACL
    2011*）。数据托管在[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)，包含一个由25,000条电影评论组成的训练集和一个由另外25,000条电影评论组成的测试集。
- en: In order to demonstrate how the model works, we would like to keep the training
    time of our model low. For this reason, we are going to partition the original
    training set into a new training and test set, but the reader is very strongly
    encouraged to repeat the exercise with the larger test dataset that is part of
    the original data. When downloaded, the data is organized into a `train` folder
    and a `test` folder. The `train` folder contains a folder called `pos` that has
    12,500 positive movie reviews, each inside a separate text file, and similarly,
    a folder called `neg` with 12,500 negative movie reviews.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示模型的工作原理，我们希望将模型的训练时间保持得尽可能低。因此，我们将原始训练集划分为一个新的训练集和测试集，但强烈建议读者使用原始数据集的一部分更大的测试数据集重复此练习。下载后，数据组织在`train`文件夹和`test`文件夹中。`train`文件夹包含一个名为`pos`的文件夹，其中包含12,500条正面电影评论，每个评论都在一个单独的文本文件中，同样，还有一个名为`neg`的文件夹，包含12,500条负面电影评论。
- en: Our first task is to load all this information into R and perform some necessary
    preprocessing. To do this, we are going to install and use the `tm` package, which
    is a specialized package for performing text-mining operations. This package is
    very useful when working with text data and we will use it again in a subsequent
    chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是将其所有信息加载到R中并进行一些必要的预处理。为此，我们将安装并使用`tm`包，这是一个专门用于执行文本挖掘操作的包。当处理文本数据时，这个包非常有用，我们将在下一章再次使用它。
- en: When working with the `tm` package, the first task is to organize the various
    sources of text into a **corpus**. In linguistics, this commonly refers to a collection
    of documents. In the `tm` package, it is just a collection of strings representing
    individual sources of text, along with some metadata that describes some information
    about them, such as the names of the files from which they were retrieved.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`tm`包工作时，首要任务是组织各种文本来源到一个**语料库**中。在语言学中，这通常指的是一系列文档的集合。在`tm`包中，它只是表示单个文本来源的字符串集合，以及一些描述这些信息的元数据，例如从哪些文件中检索到的文件名。
- en: With the `tm` package, we build a corpus using the `Corpus()` function, to which
    we must provide a source for the various documents we want to import. We could
    create a vector of strings and pass this as an argument to `Corpus()` using the
    `VectorSource()` function. Instead, as our data source is a series of text files
    in a directory, we will use the `DirSource()` function. First, we will create
    two string variables that will contain the absolute paths to the aforementioned
    `neg` and `pos` folders on our machine (this will depend on where the dataset
    is downloaded).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tm`包，我们通过`Corpus()`函数构建语料库，我们必须提供要导入的各种文档的来源。我们可以创建一个字符串向量，并将其作为参数传递给`Corpus()`函数使用`VectorSource()`函数。相反，由于我们的数据源是一个目录中的文本文件序列，我们将使用`DirSource()`函数。首先，我们将创建两个字符串变量，它们将包含我们机器上上述`neg`和`pos`文件夹的绝对路径（这取决于数据集下载的位置）。
- en: 'Then, we can use the `Corpus()` function twice to create two corpora for positive
    and negative reviews, which will then be merged into a single corpus:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`Corpus()`函数两次来创建两个语料库，分别用于正面和负面评论，然后合并成一个单一的语料库：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The second argument to the `Corpus()` function, `readerControl`, is a list of
    optional parameters. We used this to specify that the language of our text files
    is English. The `recursive` parameter in the `c()` function used to merge the
    two corpora is necessary to maintain the metadata information stored in the corpus
    objects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`Corpus()`函数的第二个参数`readerControl`是一个可选参数列表。我们使用它来指定我们的文本文件的语言是英语。在`c()`函数中用于合并两个语料库的`recursive`参数是必要的，以保持存储在语料库对象中的元数据信息。'
- en: 'Note that we can merge the two corpora without actually losing the sentiment
    label. Each text file representing a movie review is named using the format `<counter>_<score>.txt`,
    and this information is stored in the metadata portion of the corpus object created
    by the `Corpus()` function. We can see the metadata for the first review in our
    corpus using the `meta()` function:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以合并这两个语料库而不丢失情感标签。代表电影评论的每个文本文件都使用格式`<counter>_<score>.txt`命名，并且这些信息存储在由`Corpus()`函数创建的语料库对象的元数据部分中。我们可以使用`meta()`函数查看我们语料库中第一个评论的元数据：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `meta()` function thus retrieves a metadata object for each entry in our
    corpus. The `ID` attribute in this object contains the name of the file. The score
    part of the name is a number between 0 and 10, where higher numbers denote positive
    reviews, and low numbers denote negative reviews. In the training data, we only
    have polar reviews; that is, reviews that are in the ranges 0-4 and 7-10\. We
    can thus use this information to create a vector of document names:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`meta()`函数检索我们语料库中每个条目的元数据对象。该对象中的`ID`属性包含文件名。名称中的分数部分是一个介于0到10之间的数字，其中较大的数字表示正面评论，而较小的数字表示负面评论。在训练数据中，我们只有极性评论；也就是说，评论在0-4和7-10的范围内。因此，我们可以使用这些信息来创建一个文档名称向量：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From this list of document names, we''ll extract the score component using
    the `sub()` function with an appropriate regular expression. If the score of a
    movie review is less than or equal to 5, it is a negative review and if it is
    greater, it is a positive review:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个文档名称列表中，我们将使用`sub()`函数和适当的正则表达式提取分数组件。如果电影评论的分数小于或等于5，则它是负面评论；如果分数更高，则是正面评论：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `sub()` function is just one of R's functions that uses regular expressions.
    For readers unfamiliar with the concept, a regular expression is essentially a
    pattern language for describing strings. Online tutorials for regular expressions
    are easy to find. An excellent resource for learning about regular expressions'
    as well as text processing more generally, is *Speech and Language Processing
    Second Edition*, *Jurafsky and Martin*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`sub()` 函数只是 R 语言中众多使用正则表达式的函数之一。对于不熟悉这个概念的人来说，正则表达式本质上是一种用于描述字符串的模式语言。在线教程很容易找到。关于正则表达式以及更广泛的文本处理，一个极好的资源是
    *《语音与语言处理 第二版》*，作者为 *Jurafsky 和 Martin*。'
- en: The features of our model will be binary features that describe the presence
    or absence of specific words in the dictionary. Intuitively, we should expect
    that a movie review containing words such as *boring*, *cliché*, and *horrible*
    is likely to be a negative review. A movie review with words such as *inspiring*,
    *enjoyable*, *moving*, and *excellent* is likely to be a good review.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的特征将是描述字典中特定单词存在或不存在情况的二进制特征。直观地，我们应该预期包含像 *boring*、*cliché* 和 *horrible*
    这样的单词的电影评论很可能是负面评论。而包含像 *inspiring*、*enjoyable*、*moving* 和 *excellent* 这样的单词的电影评论很可能是好评。
- en: When working with text data, we almost always need to perform a series of preprocessing
    steps. For example, we tend to convert all the words to a lowercase format because
    we don't want to have two separate features for the words *Excellent* and *excellent*.
    We also want to remove anything from our text that will likely be uninformative
    as features. For this reason, we tend to remove punctuation, numbers, and **stop
    words**. Stop words are words such as *the*, *and*, *in*, and *he*, which are
    very frequently used in the English language and are bound to appear in nearly
    all of the movie reviews. Finally, because we are removing words from sentences
    and creating repeated spaces, we will want to remove these in order to assist
    the process of tokenization (the process of splitting up the text into words).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理文本数据时，我们几乎总是需要执行一系列预处理步骤。例如，我们倾向于将所有单词转换为小写格式，因为我们不希望对于单词 *Excellent* 和 *excellent*
    有两个不同的特征。我们还想从文本中移除任何可能作为特征不具信息量的内容。因此，我们倾向于移除标点符号、数字和**停用词**。停用词是一些在英语中非常常用且几乎会出现在所有电影评论中的词，如
    *the*、*and*、*in* 和 *he*。最后，因为我们从句子中移除了单词并创建了重复的空格，所以我们将想要移除这些内容，以协助分词（将文本分割成单词的过程）。
- en: 'The `tm` package has two functions, `tm_map()` and `content_transformer()`,
    which together can be used to apply text transformations to the content of every
    entry in our corpus:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm` 包有两个函数，`tm_map()` 和 `content_transformer()`，它们可以一起使用来将文本转换应用于语料库中每个条目的内容：'
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have preprocessed our corpus, we are ready to compute our features.
    Essentially, what we need is a data structure known as a **document term matrix**.
    The rows of the matrix are the documents. The columns of the matrix are the words
    in our dictionary. Each entry in the matrix is a binary value, with `1` representing
    the fact that the word represented by the column number was found inside the review
    represented by the row number. For example, if the first column corresponds to
    the word `action`, the fourth row corresponds to the fourth movie review, and
    the value of the matrix at position (4,1) is `1`, this signifies that the fourth
    movie review contains the word `action`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预处理了语料库，我们准备计算我们的特征。本质上，我们需要的是一个称为**文档-词矩阵**的数据结构。矩阵的行是文档。矩阵的列是我们字典中的单词。矩阵中的每个条目都是一个二进制值，其中
    `1` 表示列号所代表的单词在行号所代表的评论中出现过。例如，如果第一列对应于单词 `action`，第四行对应于第四篇电影评论，并且矩阵在位置 (4,1)
    的值是 `1`，这表示第四篇电影评论包含单词 `action`。
- en: 'The `tm` package provides us with the `DocumentTermMatrix()` function that
    takes in a corpus object and builds a document term matrix. The particular matrix
    built has numerical entries that represent the total number of times a particular
    word is seen inside a particular text, so we will have to convert these into a
    binary factor afterward:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm` 包为我们提供了一个 `DocumentTermMatrix()` 函数，它接受一个语料库对象并构建一个文档-词矩阵。构建的特定矩阵具有数值条目，表示特定单词在特定文本中出现的总次数，因此我们将在之后将这些转换为二进制因子：'
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our document term matrix in this case has 117,473 columns, indicating that
    we have found this number of different words in the corpus. This matrix is very
    sparse, meaning that most of the entries are 0\. This is a very typical scenario
    when building document term matrices for text documents, especially text documents
    that are as short as movie reviews. Any particular movie review will only feature
    a tiny fraction of the words in the vocabulary. Let''s examine our matrix to see
    just how sparse it is:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的文档词频矩阵有117,473列，这表明我们在语料库中找到了这么多不同的单词。这个矩阵非常稀疏，这意味着大多数条目都是0。这是构建文本文档的词频矩阵时的一个非常典型场景，尤其是对于像电影评论这样短的文本文档。任何特定的电影评论只会突出词汇表中的一小部分单词。让我们检查我们的矩阵，看看它有多稀疏：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From the ratio of non-sparse to sparse entries, we can see that of the 2,936,825,000
    entries in the matrix (25000 × 117473), only 2,493,414 are nonzero. At this point,
    we should reduce the number of columns of this matrix for two reasons. On the
    one hand, because the words in our vocabulary will become the features in our
    model, we don't want to build a model that uses 117,473 features. This would take
    a very long time to train and, at the same time, is unlikely to provide us with
    a decent fit using only 25,000 data points.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从非稀疏到稀疏条目的比率来看，我们可以看到矩阵中的2,936,825,000个条目（25,000 × 117,473）中，只有2,493,414个是非零的。在此阶段，我们应该减少这个矩阵的列数有两个原因。一方面，因为我们的词汇表中的单词将成为我们模型中的特征，我们不希望构建一个使用117,473个特征的模型。这将花费很长时间来训练，同时，仅使用25,000个数据点，也不太可能提供令人满意的拟合。
- en: Another significant reason for us to want to reduce the number of columns is
    that many words will appear only once or twice in the whole corpus, and will be
    as uninformative about the user's sentiment as words that occur in nearly all
    the documents. Given this, we have a natural way to reduce the dimensions of the
    document term matrix, namely by dropping the columns (that is, removing certain
    words from the feature set) that are the sparsest. We can remove all columns that
    have a certain percentage of sparse elements using the `removeSparseTerms()` function.
    The first argument that we must provide this with is a document term matrix, and
    the second is the maximum degree of column sparseness that we will allow. Choosing
    the degree of sparseness is tricky because we don't want to throw away too many
    of the columns that will become our features. We will proceed by running our experiments
    with 99 percent sparseness, but encourage the reader to repeat with different
    values to see the effect this has on the number of features and model performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要减少列数的另一个重要原因是，许多单词在整个语料库中只出现一次或两次，它们对用户情感的了解程度与在几乎所有文档中出现的单词一样。鉴于这一点，我们有一种自然的方法可以减少文档词频矩阵的维度，即通过删除最稀疏的列（即从特征集中删除某些单词）。我们可以使用`removeSparseTerms()`函数删除具有一定百分比稀疏元素的列。我们必须提供的第一个参数是文档词频矩阵，第二个是我们将允许的最大列稀疏度。选择稀疏度是一个棘手的问题，因为我们不希望丢弃太多的列，这些列将成为我们的特征。我们将通过进行99%稀疏度的实验来继续进行，同时鼓励读者尝试不同的值，以观察这对特征数量和模型性能的影响。
- en: 'We have 25,000 rows in the matrix corresponding to the total number of documents
    in our corpus. If we allow a maximum of 99 percent sparseness, we are effectively
    removing words that do not occur in at least 1 percent of those 25,000 documents;
    that is, in at least 250 documents:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们矩阵中有25,000行，对应于我们语料库中文档的总数。如果我们允许最大99%的稀疏度，我们实际上是在删除至少在25,000个文档中不出现至少1%的单词；也就是说，至少在250个文档中：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have now significantly reduced the number of columns down to 1,603\. This
    is a substantially more reasonable number of features for us to work with. Next,
    we convert all entries to binary, using another function of `tm`, `weightBin()`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已将列数显著减少至1,603列。这对于我们来说是一个更加合理的特征数量。接下来，我们使用`tm`库中的另一个函数`weightBin()`将所有条目转换为二进制：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As the document term matrix is, in general, a very sparse matrix, R uses a
    compact data structure to store the information. To peek inside this matrix and
    examine the first few terms, we will use the `inspect()` function on a small slice
    of this matrix:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文档词频矩阵通常是一个非常稀疏的矩阵，R使用紧凑的数据结构来存储信息。为了窥视这个矩阵并检查前几个术语，我们将在这个矩阵的一小部分上使用`inspect()`函数：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It looks like the word `ability` does not appear in the first six documents
    and the word `able` appears in the document `10004_8.txt`. We now have both our
    features and our output vector. The next step is to convert our document term
    matrix into a data frame. This is needed by the function that will train our Naïve
    Bayes model. Then, before we train the model, we will split our data into a training
    set with 80 percent of the documents and a test set with 20 percent of the documents,
    as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来单词`ability`在前六个文档中没有出现，而单词`able`出现在文档`10004_8.txt`中。我们现在既有特征也有输出向量。下一步是将我们的文档-词矩阵转换为数据框。这是训练朴素贝叶斯模型所需的功能所必需的。然后，在我们训练模型之前，我们将数据分为一个包含80%文档的训练集和一个包含20%文档的测试集，如下所示：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To train a Naïve Bayes model, we will use the `naiveBayes()` function in the
    `e1071` package that we saw earlier. The first argument we will provide it with
    is our feature data frame, and the second argument is our vector of output labels:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个朴素贝叶斯模型，我们将使用我们在前面看到的`e1071`包中的`naiveBayes()`函数。我们将提供的第一个参数是我们的特征数据框，第二个参数是我们的输出标签向量：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can use the `predict()` function to obtain predictions on our training data:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`predict()`函数对我们的训练数据进行预测：
- en: '[PRE13]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have hit over 83 percent training accuracy with our simple Naïve Bayes model,
    which, admittedly, is not bad for such a simple model with an independence assumption
    that we know is not realistic for our data. Let''s repeat the same on our test
    data:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用简单的朴素贝叶斯模型达到了超过83%的训练准确率，诚然，对于一个具有独立性假设的简单模型来说，这已经很不错了，尽管我们知道这个假设对于我们的数据来说并不现实。让我们在测试数据上重复同样的操作：
- en: '[PRE14]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The test accuracy of over 82 percent is comparable to what we saw on our training
    data. There are a number of potential avenues for improvement here. The first
    involves noticing that words such as *movie* and *movies* are treated differently,
    even though they are the same word but inflected. In linguistics, **inflection**
    is the process by which the base form or **lemma** of a word is modified to agree
    with another word on attributes such as tense, case, gender, and number. For example,
    in English, verbs must agree with their subject.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 超过82%的测试准确率与我们在训练数据上看到的结果相当。这里有许多潜在的改进途径。首先，要注意到像*movie*和*movies*这样的单词被不同地对待，尽管它们是同一个单词的不同变形。在语言学中，**变形**是指将单词的基本形式或**词元**修改为与另一个词在诸如时态、格、性别和数量等属性上保持一致的过程。例如，在英语中，动词必须与主语一致。
- en: The `tm` package supports **stemming**, a process of removing the inflected
    part of a word in order to keep just a stem or root word. This is not always the
    same as retrieving what is known as the **morphological lemma** of a word, which
    is what we look up in a dictionary, but is a rough approximation. The `tm` package
    uses the well-known **Porter Stemmer**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包支持**词干提取**，这是一个去除单词变形部分的过程，以保留一个词干或根词。这并不总是与检索所谓的**形态词元**相同，这是我们查字典时查找的内容，但这是一个粗略的近似。`tm`包使用著名的**Porter词干提取器**。'
- en: Note
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Martin Porter*, the author of the Porter Stemmer, maintains a website at [http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/),
    which is a great source of information on his famous algorithm.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*Martin Porter*，Porter词干提取器的作者，维护了一个网站[http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)，这是关于他著名算法的极好信息来源。'
- en: 'To apply stemming to our corpus, we need to add a final transformation to our
    corpus using `tm_map()` and then recompute our document term matrix anew, as the
    columns (the word features) are now word stems:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对我们的语料库应用词干提取，我们需要使用`tm_map()`向语料库添加一个最终的转换，然后重新计算我们的文档-词矩阵，因为现在列（即单词特征）现在是词干：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that we have fewer columns that match our criterion of 99 percent maximum
    sparsity. We can use this new document term matrix to train another Naïve Bayes
    classifier and then measure the accuracy on our test set:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们匹配99%最大稀疏度标准的列更少了。我们可以使用这个新的文档-词矩阵来训练另一个朴素贝叶斯分类器，然后在测试集上测量准确率：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The result, 80 percent, is slightly lower than what we observed without stemming,
    although we are using slightly fewer features than before. Stemming is not always
    guaranteed to be a good idea, as in some problems it may improve performance,
    whereas in others it will make no difference or even make things worse. It is,
    however, a common transformation that is worth trying when working with text data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，80%，略低于我们没有进行词干提取时观察到的结果，尽管我们使用的特征比以前少。词干提取并不总是保证是一个好主意，因为在某些问题中它可能会提高性能，而在其他问题中则可能没有区别，甚至可能变得更糟。然而，当处理文本数据时，这是一种常见的转换，值得尝试。
- en: A second possible improvement is to use **additive smoothing** (also known as
    **laplacian smoothing**) during the training of our Naïve Bayes model. This is
    actually a form of regularization and it works by adding a fixed number to all
    the counts of feature and class combinations during training. Using our original
    document term matrix, we can compute a Naïve Bayes model with additive smoothing
    by specifying the `laplace` parameter. For our particular dataset, however, we
    did not witness any improvements by doing this.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种可能的改进是在我们的朴素贝叶斯模型训练过程中使用**加性平滑**（也称为**拉普拉斯平滑**）。这实际上是一种正则化形式，它通过在训练过程中向所有特征和类组合的计数中添加一个固定数值来实现。使用我们的原始文档词频矩阵，我们可以通过指定`laplace`参数来计算具有加性平滑的朴素贝叶斯模型。然而，对于我们的特定数据集，我们没有观察到任何通过这种方式进行的改进。
- en: 'There are a few more avenues of approach that we might try with a Naïve Bayes
    model, and we will propose them here for the reader to experiment with. The first
    of these is that it is often worth manually curating the list of words used as
    features for the model. When we study the terms selected by our document term
    matrix, we may find that some words are frequent in our training data, but we
    do not expect them to be frequent in general, or representative of the overall
    population. Furthermore, we may only want to experiment with words that we know
    are suggestive of emotion and sentiment. This can be done by specifying a specific
    dictionary of terms to use when constructing our document term matrix. Here is
    an example:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用朴素贝叶斯模型的一些其他方法，并且我们将在这里提出这些方法供读者实验。首先，手动整理用于模型特征的字词列表通常是值得的。当我们研究我们的文档词频矩阵选择的术语时，我们可能会发现有些词在我们的训练数据中很常见，但我们不期望它们在一般情况下很常见，或者不能代表整体人群。此外，我们可能只想实验那些我们知道能暗示情感和情绪的字词。这可以通过指定在构建我们的文档词频矩阵时使用的特定术语字典来完成。以下是一个例子：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is relatively straightforward to find examples of such lists on the internet.
    Another common preprocessing step that is used with a Naïve Bayes model is to
    remove correlations between features. One way of doing this is to perform PCA,
    as we saw in [Chapter 1](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 1. Gearing Up for Predictive Modeling"), *Gearing Up for Predictive Modeling*.
    Furthermore, this method also allows us to begin with a slightly more sparse document
    term matrix with a larger number of terms, as we know we will be reducing the
    overall number of features with PCA.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上可以相对容易地找到这样的列表示例。与朴素贝叶斯模型一起使用的另一个常见预处理步骤是去除特征之间的相关性。一种实现方式是执行PCA，正如我们在[第1章](part0015_split_000.html#E9OE2-c6198d576bbb4f42b630392bd61137d7
    "第1章. 准备预测建模")中看到的，*准备预测建模*。此外，这种方法还允许我们从具有更多术语的稍微稀疏的文档词频矩阵开始，因为我们知道我们将通过PCA减少整体特征数量。
- en: Potential model improvements notwithstanding, it is important to be aware of
    the limitations that the Naïve Bayes model imposes that impede our ability to
    train a highly accurate sentiment analyzer. Assuming that all the words in a movie
    review are independent of each other, once we know the sentiment involved, is
    quite an unrealistic assumption. Our model completely disregards sentence structure
    and word order. For example, the phrase *not bad* in a review might indicate a
    positive sentiment, but because we look at words in isolation, we will tend to
    correlate the word *bad* with a negative sentiment.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有潜在的模型改进，但了解朴素贝叶斯模型强加的限制，这些限制阻碍了我们训练一个高度准确的情感分析器的能力，这一点很重要。假设电影评论中的所有词都是相互独立的，一旦我们知道涉及的
    sentiment，这是一个相当不切实际的假设。我们的模型完全忽略了句子结构和词序。例如，评论中的短语*not bad*可能表示积极的 sentiment，但由于我们孤立地看待单词，我们倾向于将单词*bad*与消极的
    sentiment 联系起来。
- en: Negation in general is one of the hardest problems to handle in text processing.
    Our model also cannot handle common patterns of language, such as sarcasm, irony,
    quoted passages that include other people's thoughts, and other such linguistic
    devices.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 否定在文本处理中通常是最难处理的问题之一。我们的模型也无法处理常见的语言模式，例如讽刺、反语、包含他人观点的引述段落以及其他此类语言手段。
- en: The next section will introduce a more powerful graphical model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍一个更强大的图形模型。
- en: Note
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Hidden Markov models**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐马尔可夫模型**'
- en: A good reference to study for the Naïve Bayes classifier is *An empirical study
    of the Naïve Bayes classifier*, *I. Rish*, presented in the 2001 IJCAI workshop
    on *Empirical Methods in AI*. For sentiment analysis, we recommend the slides
    from *Bing Liu*'s AAAI 2011 tutorial at (as of this writing) [https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf](https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 研究朴素贝叶斯分类器的一个好参考是*I. Rish*在2001年IJCAI关于*人工智能中的经验方法*研讨会上的论文*An empirical study
    of the Naïve Bayes classifier*。对于情感分析，我们推荐*Bing Liu*在2011年AAAI教程中的幻灯片（截至本文写作时）[https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf](https://www.researchgate.net/profile/Irina_Rish/publication/228845263_An_Empirical_Study_of_the_Naive_Bayes_Classifier/links/00b7d52dc3ccd8d692000000/An-Empirical-Study-of-the-Naive-Bayes-Classifier.pdf)。
- en: 'A **Hidden Markov model**, often abbreviated to **HMM**, which we will use
    here, is a Bayesian network with a repeating structure that is commonly used to
    model and predict sequences. In this section, we''ll see two applications of this
    model: one to model DNA gene sequences, and another to model the sequences of
    letters that make up English text. The basic diagram for an HMM is shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐马尔可夫模型**，通常缩写为**HMM**，我们在这里将使用它，是一种具有重复结构的贝叶斯网络，通常用于建模和预测序列。在本节中，我们将看到该模型的两个应用：一个用于建模DNA基因序列，另一个用于建模构成英语文本的字母序列。HMM的基本图示如下：'
- en: '![Predicting the sentiment of movie reviews](img/00181.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![预测电影评论的情感](img/00181.jpeg)'
- en: As we can see in the diagram, the sequence flows from left to right and we have
    a pair of nodes for every entry in the sequence that we are trying to model. Nodes
    labeled *Ci* are known as **latent states**, **hidden states**, or merely **states**,
    as they are typically nodes that are not observable. The nodes labeled *Oi* are
    **observed states** or **observations**. We will use the terms *states* and *observations*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，序列从左到右流动，并且对于我们要尝试建模的序列中的每个条目，我们都有一个节点对。标记为*Ci*的节点被称为**潜在状态**、**隐藏状态**或仅仅是**状态**，因为它们通常是不可观察的节点。标记为*Oi*的节点是**观察状态**或**观察结果**。我们将使用术语*状态*和*观察结果*。
- en: Now, as this is a Bayesian network, we can immediately identify some key properties.
    All the observations are independent of each other given their corresponding state.
    Also, every state is independent of every other state earlier on in the sequence
    history, given the state that preceded it (which is its parent in the network).
    The key idea behind an HMM, therefore, is that the model moves in a linear fashion
    from one state to the next state.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于这是一个贝叶斯网络，我们可以立即识别一些关键属性。所有观察结果在给定它们对应的状态时是相互独立的。此外，每个状态在给定其前面的状态（在网络上是其父节点）的情况下，与序列历史中的任何其他状态都是独立的。因此，隐马尔可夫模型背后的关键思想是模型以线性方式从一个状态移动到下一个状态。
- en: In each latent state, it produces an observation, which is also known as an
    **emitted symbol**. These symbols are the observed part of the sequence. Hidden
    Markov models are very common in natural language processing, and a good example
    is their application to **part of speech tagging**. The task of a part of speech
    tagger is to read a sentence and return the sequence of corresponding part of
    speech labels for the words in that sentence. For example, given the previous
    sentence, a part of speech tagger might return *determiner* for the word *The*,
    *singular noun* for the word *task*, and so on.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个潜在状态中，它产生一个观察结果，这也被称为**发射符号**。这些符号是序列中观察到的部分。隐马尔可夫模型在自然语言处理中非常常见，一个很好的例子是它们在**词性标注**中的应用。词性标注器的任务是读取一个句子，并返回该句子中单词对应的词性标签序列。例如，给定前面的句子，一个词性标注器可能会为单词“The”返回*限定词*，为单词“task”返回*单数名词*，等等。
- en: To model this using an HMM, we would have the words be the emitted symbols,
    and the part of speech tags be the latent states, as the former are observable
    and the latter are what we want to determine. There are many other sequence labeling
    tasks in natural language processing to which Hidden Markov models have been applied,
    such as **named entity recognition**, where the goal is to identify the words
    in a sentence that refer to names of individuals, locations, organizations, and
    other entities.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用HMM来建模，我们将单词作为发射符号，将词性标签作为潜在状态，因为前者是可观察的，而后者是我们想要确定的。在自然语言处理中，有许多其他序列标注任务已经应用了隐马尔可夫模型，例如**命名实体识别**，其目标是识别句子中指代个人、地点、组织和其他实体的单词。
- en: A Hidden Markov model is comprised of five core components. The first of these
    is the set of possible latent class labels. For the part of speech tagger example,
    this might be a list of all the part of speech tags that we will use. The second
    component is the set of all possible emitted symbols. For an English part of speech
    tagger, this is the dictionary of English words.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏马尔可夫模型由五个核心组件组成。第一个是可能的潜在类别标签集合。对于词性标注器的例子，这可能是我们将使用的所有词性标签的列表。第二个组件是所有可能的发射符号集合。对于一个英语词性标注器，这是英语单词的字典。
- en: The next three components involve probabilities. The **starting probability
    vector** is a vector of probabilities that tells us the probability of starting
    in each latent state. For part of speech tagging, we may, for example, have a
    high probability of starting with a determiner such as *the*. The **transition
    probability matrix** is a matrix that tells us the probability of going to state
    *C[j]* when the current state is *C[i]*. Thus, this contains the probability of
    moving from a determiner to a noun for our part of speech example. Finally, the
    **emission probability matrix** tells us the probability of emitting every symbol
    in our dictionary for every state that we can be in. Note that some words (such
    as *bank*, which is both a noun and a verb) can be labeled with more than one
    part of speech tag, and so will have nonzero probabilities of being emitted from
    more than one state.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的三个组件涉及概率。**起始概率向量**是一个概率向量，它告诉我们开始于每个潜在状态的概率。例如，在词性标注中，我们可能会有一个很高的概率以一个像*the*这样的限定词开始。**转移概率矩阵**是一个矩阵，它告诉我们当当前状态是*C[i]*时，转移到状态*C[j]*的概率。因此，这包含了从限定词到名词的转移概率，以我们的词性标注为例。最后，**发射概率矩阵**告诉我们我们字典中每个符号在我们可以处于的每个状态下的概率。请注意，一些单词（如*bank*，它既是名词也是动词）可以标记为多个词性标签，因此将会有从多个状态发射的非零概率。
- en: In circumstances such as part of speech tagging, we usually have a collection
    of labeled sequences so that our data contains both sequences of observations
    as well as their corresponding states. In this case, similar to the Naïve Bayes
    model, we use relative frequency counts to populate the probability components
    of our model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在词性标注等情况下，我们通常有一组标记序列，因此我们的数据包含观察序列及其相应的状态。在这种情况下，类似于朴素贝叶斯模型，我们使用相对频率计数来填充我们模型中的概率组件。
- en: For example, to find a suitable starting probability vector, we could tabulate
    the starting state for every sequence in our dataset and use this to get the relative
    frequency of beginning in each state. When all we have are unlabeled sequences,
    the task is significantly harder because we might not even know how many states
    we need to include in our model. One method to assign states to unlabeled observation
    sequences in training data is known as the **Baum-Welch algorithm**.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了找到一个合适的起始概率向量，我们可以列出我们数据集中每个序列的起始状态，并使用这个来获取每个状态的相对频率。当我们只有未标记的序列时，任务会显著更难，因为我们甚至可能不知道我们需要在我们的模型中包含多少个状态。一种在训练数据中将状态分配给未标记观察序列的方法被称为**Baum-Welch算法**。
- en: Once we know the parameters of our model, the question becomes how to predict
    the most likely sequence of states behind a sequence of observations. Given an
    unlabeled sentence in English, a part of speech tagger based on an HMM must predict
    the sequence of part of speech labels. The most commonly used algorithm for this
    is based on a programming technique known as **dynamic programming** and is known
    as the **Viterbi algorithm**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道了我们模型的参数，问题就变成了如何预测观察序列背后的最可能状态序列。给定一个未标记的英文句子，基于HMM的词性标注器必须预测词性标签的序列。用于此的最常用算法是基于称为**动态规划**的编程技术，被称为**维特比算法**。
- en: The algorithms we have discussed for the Hidden Markov model are beyond the
    scope of this book, but are quite intuitive and well worth studying. Given a basic
    understanding of the core components of the model and its assumptions, our next
    goal is to see how we can apply them to some real-world situations. We will first
    see an example with labeled sequences and later, an example with unlabeled sequences.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书中讨论的用于隐马尔可夫模型的算法超出了本书的范围，但它们相当直观，值得深入研究。在了解模型的核心组件及其假设的基本理解之后，我们的下一个目标是看看我们如何将它们应用于一些现实世界的情况。我们将首先看到一个带有标记序列的例子，稍后，我们将看到一个带有未标记序列的例子。
- en: Tip
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Perhaps the most definitive and thorough introduction to Hidden Markov models
    is the seminal paper titled *A Tutorial on Hidden Markov Models and Selected Applications
    in Speech Recognition*, *L. R. Rabiner*, published in the *Proceedings of the
    IEEE, 1989*. The *Jurafsky* and *Martin* textbook we mentioned earlier is also
    an ideal reference to learn more about HMMs, including details on the Baum-Welch
    and Viterbi algorithms, as well as applications such as part of speech tagging
    and named entity recognition.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 也许对隐马尔可夫模型最权威和最全面的介绍是L. R. Rabiner发表的题为*A Tutorial on Hidden Markov Models and
    Selected Applications in Speech Recognition*的开创性论文，发表于*IEEE Proceedings, 1989*。我们之前提到的*Jurafsky*和*Martin*教科书也是学习HMM的理想参考，包括Baum-Welch和Viterbi算法的细节，以及诸如词性标注和命名实体识别等应用。
- en: Predicting promoter gene sequences
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测启动子基因序列
- en: The first application we will study in detail comes from the field of biology.
    There, we learn that the basic building blocks of DNA molecules are actually four
    fundamental molecules known as **nucleotides**. These are called *Thymine*, *Cytosine*,
    *Adenine*, and *Guanine*, and it is the order in which these molecules appear
    in a DNA strand that encodes the genetic information carried by the DNA.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要详细研究的第一个应用来自生物学领域。在那里，我们了解到DNA分子的基本构建块实际上是四种被称为**核苷酸**的基本分子。这些被称为*胸腺嘧啶*、*胞嘧啶*、*腺嘌呤*和*鸟嘌呤*，DNA链中这些分子出现的顺序编码了DNA携带的遗传信息。
- en: An interesting problem in molecular biology is finding **promoter sequences**
    within a larger DNA strand. These are special sequences of nucleotides that play
    an important role in regulating a genetic process known as **gene transcription**.
    This is the first step in the mechanism by which information in the DNA is read.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分子生物学中的一个有趣问题是找到更大DNA链中的**启动子序列**。这些是起着重要作用的特殊核苷酸序列，它们在调节称为**基因转录**的遗传过程中扮演着重要角色。这是DNA中信息读取机制的第一步。
- en: The *molecular biology (promoter gene sequences) dataset*, hosted by the UCI
    Machine Learning repository at [https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)](https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)),
    contains a number of gene sequences from DNA belonging to the bacterium *E. Coli*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由UCI机器学习仓库托管在[https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences)](https://archive.ics.uci.edu/ml/datasets/Molecular+Biology+(Promoter+Gene+Sequences))的*molecular
    biology (promoter gene sequences)*数据集，包含来自细菌*E. Coli*的DNA中的许多基因序列。
- en: 'The predictive task at hand is to build a model that will discern promoter
    gene sequences from non-promoter gene sequences. We will approach this problem
    using HMMs. Specifically, we will build an HMM for promoters and an HMM for non-promoters,
    and we will pick the model that gives us the highest probability for a test sequence
    in order to label that sequence:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当前预测任务是构建一个模型，能够从非启动子基因序列中区分出启动子基因序列。我们将使用HMM来解决这个问题。具体来说，我们将为启动子构建一个HMM，为非启动子构建一个HMM，然后选择为我们提供测试序列最高概率的模型，以标记该序列：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that it is important to strip whitespace using the `strip.white = TRUE`
    parameter setting in the call to `read.csv()`, as some fields have leading tab
    characters. The first column in the data frame contains a `+` or `-` to denote
    promoters or non-promoters respectively. The second column is just an identifier
    for the particular sequence and the third column is the sequence of nucleotides
    itself. We''ll begin by separating the data into positive and negative observations
    of promoter sequences using the first column:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用`read.csv()`函数调用中的`strip.white = TRUE`参数设置来去除空白是很重要的，因为一些字段有前导制表符。数据框的第一列包含一个`+`或`-`来表示启动子或非启动子。第二列是特定序列的标识符，第三列是核苷酸序列本身。我们将首先使用第一列将数据分离为启动子序列的正负观察：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In order to train our HMMs, we want to concatenate all the observations from
    each class into a single observation. We do, however, want to store information
    about the start and end of each sequence. Consequently, we will prepend each sequence
    with the character `S` to denote the start of a sequence and append each sequence
    with the character `X` to denote the end of a sequence:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的HMM，我们想要将每个类别的所有观察结果连接成一个单个观察结果。然而，我们确实想要存储每个序列的开始和结束信息。因此，我们将每个序列前面加上字符`S`来表示序列的开始，并在每个序列后面加上字符`X`来表示序列的结束：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will split each observation from a string into a vector of characters
    using the `strsplit()` function, which takes a string to split as the first argument
    and the character to use as the split points (delimiter). Here, we use an empty
    character on which to split, so that the whole string is broken up into single
    characters:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`strsplit()`函数将每个观察结果从字符串分割成一个字符向量，该函数将用于分割的字符串作为第一个参数，以及用作分割点的字符（分隔符）。在这里，我们使用一个空字符进行分割，这样整个字符串就被分割成单个字符：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now we have to specify the probability matrices for the HMMs that we want to
    train. In this particular situation, the states have a one-to-one correspondence
    with the emitted symbols, so in fact this type of problem can be simplified to
    a visible Markov model, which in this case is just a Markov chain. Nonetheless,
    the process we will follow for modeling this problem as an HMM is the same that
    we would follow in the more general case of having multiple symbols assigned to
    each state. We are going to assume that both positive and negative HMMs involve
    four states corresponding to the four nucleotides. Although both models will emit
    the same symbols in each state, they will differ in their transition probabilities
    from one state to the next.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须指定我们想要训练的HMM的概率矩阵。在这种情况下，状态与发出的符号有一一对应的关系，因此实际上这类问题可以简化为可见马尔可夫模型，在这种情况下它只是一个马尔可夫链。尽管如此，我们将遵循与具有多个符号分配给每个状态的情况相同的过程来将这个问题建模为HMM。我们将假设正负HMM都涉及四个状态，对应于四种核苷酸。尽管这两个模型在每个状态都会发出相同的符号，但它们在从一个状态到下一个状态的转移概率上会有所不同。
- en: 'Apart from the four states we mentioned earlier, we created a special terminating
    state at the end of each sequence using the symbol *X*. We also created a special
    starting state, which we called *S*, so that the starting probability of all the
    other states is 0\. In addition, the emission probabilities are trivial to compute
    as only one symbol is emitted per state. Due to the one-to-one correspondence
    between states and symbols, we will use the same alphabet to represent the states
    and the symbols they emit:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前提到的四个状态之外，我们在每个序列的末尾创建了一个特殊的终止状态，使用符号*X*表示。我们还创建了一个特殊的起始状态，我们称之为*S*，这样所有其他状态的起始概率都是0。此外，发射概率很容易计算，因为每个状态只发出一个符号。由于状态与符号之间的一一对应关系，我们将使用相同的字母表来表示状态及其发出的符号：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Computing the transition probability matrix requires us to do a bit more work.
    Thus, we defined our own function for this: `calculateTransitionProbabilities()`.
    The input to this function is a single vector of training sequences concatenated
    with each other, along with a vector containing the names of the states.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 计算转移概率矩阵需要我们做更多的工作。因此，我们为这个定义了自己的函数：`calculateTransitionProbabilities()`。这个函数的输入是一个由训练序列连接而成的单个向量，以及一个包含状态名称的向量。
- en: 'The function first computes an empty transition probability matrix. By cycling
    over each consecutive pair of states, it tallies up counts of state transitions.
    After all the data has been traversed, we normalize the transition probability
    matrix by dividing each row of the matrix by the sum of the elements in that row.
    This is done because the rows of this matrix must sum to one. We use the `sweep()`
    function, which allows us to apply a function on every element of the matrix using
    a summary statistic. Here is `calculateTransitionProbabilities()`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 函数首先计算一个空的转换概率矩阵。通过遍历每个连续的状态对，它累计状态转换的计数。在遍历完所有数据后，我们通过将矩阵的每一行除以该行的元素总和来归一化转换概率矩阵。这样做是因为这个矩阵的行必须总和为
    1。我们使用 `sweep()` 函数，它允许我们使用汇总统计量对矩阵的每个元素应用一个函数。以下是 `calculateTransitionProbabilities()`：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we are ready to train our models. The key observation to make on this dataset
    is that we have very few observations, just 53 of each class in fact. This dataset
    is too small to set a portion aside for testing. Instead, we will implement leave-one-out
    cross validation to estimate the accuracy of our models. To do this, we will begin
    by leaving an observation out from the positive observations. This leaves all
    the negative observations available for computing the transition probability matrix
    for our negative HMM:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练我们的模型。在这个数据集上，关键观察结果是观察值非常少，实际上每个类只有 53 个。这个数据集太小，无法留出一部分用于测试。相反，我们将实现留一法交叉验证来估计我们模型的准确性。为此，我们将从正观察值中省略一个观察值。这将为我们的负
    HMM 计算转换概率矩阵留下所有负观察值：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When in the start state (`S`), we can randomly move to a nucleotide state, but
    have zero probability of moving to the stop state (`X`) or staying in the start
    state. When in the nucleotide states, we can randomly transition to any state
    except back to the start state. Finally, the only valid transition from a stop
    state is to the start state for a new sequence.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当处于起始状态（`S`）时，我们可以随机移动到核苷酸状态，但移动到停止状态（`X`）或保持在起始状态的几率为零。当处于核苷酸状态时，我们可以随机转换到任何状态，但不能转换回起始状态。最后，从停止状态到起始状态的唯一有效转换是为了新的序列。
- en: 'We now introduce the `HMM` package in R, which is for working with Hidden Markov
    models, as the name implies. We can initialize an HMM with a specific set of parameters
    using the `initHMM()` function. As expected, this takes five inputs corresponding
    to the five components of a Hidden Markov model, which we discussed earlier:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们介绍 R 中的 `HMM` 包，它用于处理隐马尔可夫模型，正如其名所示。我们可以使用 `initHMM()` 函数使用一组特定的参数初始化一个
    HMM。正如预期的那样，这需要五个输入，对应于我们之前讨论过的隐马尔可夫模型的五个组成部分：
- en: '[PRE25]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The next step is to build the positive HMM, but we will have to do this multiple
    times, leaving out one observation for testing. This test observation will then
    be processed by the negative HMM we trained earlier and the positive HMM that
    was trained without that observation. If the positive HMM predicts a higher probability
    for the test observation than the negative HMM, our model will correctly classify
    the test observation. The following block of code performs a loop of these calculations
    for every positive observation:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建正 HMM，但我们需要多次进行此操作，每次测试时省略一个观察值。这个测试观察值随后将由我们之前训练的负 HMM 和没有该观察值训练的正 HMM
    处理。如果正 HMM 对测试观察值的预测概率高于负 HMM，则我们的模型将正确分类测试观察值。以下代码块执行这些计算的循环，针对每个正观察值：
- en: '[PRE26]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We'll now walk through the previous code block. Firstly, we keep track of any
    mistakes we make using the `incorrect` variable. For every observation in our
    positive observations list, we'll train a positive HMM without this observation.
    This observation then becomes our test observation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将逐步分析之前的代码块。首先，我们使用 `incorrect` 变量跟踪我们犯的任何错误。对于我们的正观察值列表中的每个观察值，我们将训练一个没有此观察值的正
    HMM。然后，这个观察值成为我们的测试观察值。
- en: 'To find the probability of a particular sequence given a particular HMM, we
    used the `forward()` function, which computes a matrix containing the logarithm
    of all the forward probabilities for every step in the observation sequence. The
    final column in this matrix, whose numerical index is just the length of the sequence,
    contains the forward probability for the whole sequence. We compute the positive
    sequence probability using the positive HMM that we trained and use the `exp()`
    function to undo the logarithm operation (although not strictly necessary in this
    case, where we just need a comparison). We repeat this for the negative sequence
    probability using the negative HMM. As our test observation was one of the positive
    observations, we will misclassify only if the negative sequence probability is
    greater than the positive sequence probability. After our code block completes
    its execution, we can see how many mistakes we have made:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到给定特定HMM的特定序列的概率，我们使用了`forward()`函数，该函数计算一个包含观察序列中每一步所有前向概率的对数矩阵。这个矩阵的最后一列，其数值索引就是序列的长度，包含了整个序列的前向概率。我们使用我们训练的正HMM来计算正序列概率，并使用`exp()`函数来撤销对数运算（尽管在这个情况下不是严格必要的，我们只需要比较）。我们使用负HMM重复这个过程来计算负序列概率。由于我们的测试观察是正观察之一，只有当负序列概率大于正序列概率时，我们才会误分类。在代码块执行完成后，我们可以看到我们犯了多少错误：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This means that out of the 53 positive observations, we misclassified 13 and
    correctly classified 40\. We are not done yet, though, as we need to do a similar
    loop with the negative observations. This time, we will train a positive HMM once
    with all the positive observations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在53个正观察中，我们误分类了13个，正确分类了40个。尽管如此，我们还没有完成，因为我们还需要对负观察执行类似的循环。这次，我们将使用所有正观察训练一个正HMM：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, we are going to iterate over all the negative observations. We will train
    a negative model by leaving one observation out as the test observation. We will
    then process this observation with both the positive HMM we just trained and the
    negative HMM trained without this observation in its training data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将迭代所有负观察。我们将通过省略一个观察作为测试观察来训练一个负模型。然后，我们将使用我们刚刚训练的正HMM和没有这个观察的训练数据的负HMM来处理这个观察。
- en: 'Finally, we will compare the predicted sequence probability for this test observation
    produced by the two HMMs and classify the test observation according to which
    model produced the higher probability. In essence, we are doing exactly the same
    process as we did earlier when we were iterating over the positive observations.
    The following code block will continue to update our `incorrect` variable and
    should be self-explanatory:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将比较由两个HMMs产生的这个测试观察序列的预测概率，并根据哪个模型产生了更高的概率来对测试观察序列进行分类。本质上，我们正在做与我们之前迭代正观察时完全相同的过程。下面的代码块将继续更新我们的`incorrect`变量，并且应该是自解释的：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The overall number of misclassifications in the cross-validation is stored
    in the `incorrect` variable:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证中的误分类总数存储在`incorrect`变量中：
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Our overall cross-validation accuracy is roughly 76 percent. Given that we are
    using the leave-one-out approach, and that the overall size of the training data
    is so small, we expect this estimate to have a relatively high variance.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的整体交叉验证准确率大约是76%。鉴于我们正在使用留一法，并且训练数据的整体大小如此之小，我们预计这个估计将具有相对较高的方差。
- en: In our HMM, the Markov property essentially makes the assumption that only the
    previous nucleotide determines the choice of the next nucleotide in the sequence.
    We can reasonably expect that there are longer-range dependencies at work and,
    as a result, we are limited in accuracy by the assumptions of our model. For this
    reason, there are models, such as the **Trigram HMM**, that take into account
    additional states in the past other than the current state.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的HMM中，马尔可夫属性本质上假设只有前一个核苷酸决定了序列中下一个核苷酸的选择。我们可以合理地预期存在更长的范围依赖，因此，我们受到模型假设的限制。因此，存在一些模型，如**三元HMM**，它们考虑了除了当前状态之外过去的状态。
- en: In the next section, we will study an example where we train a Hidden Markov
    model using unlabeled data. We will manually define the number of hidden states
    and use the Baum-Welch algorithm to train an HMM while estimating both state transitions
    and emissions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将研究一个示例，其中我们使用未标记的数据来训练一个隐马尔可夫模型。我们将手动定义隐藏状态的数量，并使用Baum-Welch算法来训练一个HMM，同时估计状态转换和发射。
- en: Predicting letter patterns in English words
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测英语单词中的字母模式
- en: In this section, we will model the patterns of letters that form English words.
    Beyond having different words, and sometimes alphabets, languages differ from
    each other in the patterns of letters that are used to form words. English words
    have a characteristic distribution of letters and letter sequences and, in this
    section, we will try to model the process of word formation in a very simplistic
    way by using a Hidden Markov model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将模拟构成英语单词的字母模式。除了有不同单词和有时有字母表外，语言之间的不同之处在于构成单词所使用的字母模式。英语单词具有独特的字母和字母序列分布，在本节中，我们将尝试通过使用隐马尔可夫模型以非常简单的方式来模拟单词形成的过程。
- en: The emitted symbols of our model will be the letters themselves but this time,
    we don't know what the states could be as we are using unlabeled data. For this
    reason, we are going to provide just the number of states that we want our model
    to have, and then use the Baum-Welch algorithm to train the parameters of our
    HMM.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型发出的符号将是字母本身，但这次，由于我们使用的是未标记的数据，我们不知道状态可能是什么。因此，我们将只提供我们希望模型拥有的状态数量，然后使用Baum-Welch算法来训练我们的HMM的参数。
- en: 'All we need for this task is a corpus of text in English. Earlier in this chapter,
    we studied movie reviews with the Naïve Bayes classifier, so we will use these
    for convenience, although other sources of English text could be used as well.
    We shall begin by reloading our movie reviews and will use the `tm` package to
    transform them all to lowercase:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项任务，我们只需要一个英语文本语料库。在本章的早期，我们研究了使用朴素贝叶斯分类器的电影评论，因此我们将使用这些评论以方便起见，尽管也可以使用其他英语文本来源。我们将首先重新加载我们的电影评论，并使用`tm`包将它们全部转换为小写：
- en: '[PRE31]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will read the text from every review and collect these in a single
    vector:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从每个评论中读取文本，并将这些文本收集到一个单独的向量中：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To simplify our task, aside from the individual letters, we will consider a
    category with all the whitespace characters (spaces, tabs, and so on) and represent
    these with the uppercase letter `W`. We will do the same for numerical digits
    with the uppercase character `N`, all punctuation marks with the uppercase character
    `P`, and use the uppercase character `O` for anything that is left. We use regular
    expressions for this:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的任务，除了单个字母外，我们还将考虑一个包含所有空白字符（空格、制表符等）的类别，并用大写字母`W`来表示这些字符。对于数字，我们将使用大写字符`N`，对于所有标点符号，我们将使用大写字符`P`，而对于任何剩下的东西，我们将使用大写字符`O`。我们使用正则表达式来完成这项任务：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once we have transformed all our text, we''ll pick out a sample and split each
    review into characters. The sequences of characters from each review will then
    be concatenated with each other to create one long character sequence. This works
    quite well in this context as the corpus of reviews contains complete sentences
    and concatenating them amounts to joining up complete sentences. We''ve chosen
    to use a sample of 100 movie reviews. We can use more, but the time taken to train
    the model would be longer:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将所有文本转换完毕，我们将挑选一个样本，并将每个评论拆分为字符。然后，每个评论中的字符序列将相互连接，以创建一个长的字符序列。在这个上下文中，这工作得相当好，因为评论语料库包含完整的句子，将它们连接起来相当于连接完整的句子。我们选择了100篇电影评论的样本。我们可以使用更多，但训练模型所需的时间会更长：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we''ll want to initialize our HMM. In this example, we''ll consider a
    model with three states, which we''ll arbitrarily name `s1`, `s2`, and `s3`. For
    emitted symbols, we have the lowercase alphabet and the four uppercase characters
    that, as we saw earlier, are being used to represent four special character categories
    such as numbers. R holds a vector of lowercase letters in the variable `letters`,
    which is very convenient for us:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望初始化我们的HMM。在这个例子中，我们将考虑一个有三个状态的模式，我们将任意命名为`s1`、`s2`和`s3`。对于发射符号，我们有小写字母和前面提到的四个大写字符，这些字符被用来表示四个特殊字符类别，如数字。`R`变量中包含小写字母的向量`letters`，这对我们来说非常方便：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we''ll create random starting, emission, and transmission probability
    matrices. We''ll generate random entries in the [0,1] interval using the `runif()`
    function. We will need to normalize every row in these matrices in order to ensure
    that the entries correspond to probabilities. To achieve this, we''ll use the
    `sweep()` function as we did earlier:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建随机的起始、发射和传输概率矩阵。我们将使用`runif()`函数在[0,1]区间内生成随机条目。我们需要对这些矩阵的每一行进行归一化，以确保条目对应于概率。为此，我们将使用之前使用的`sweep()`函数：
- en: '[PRE36]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We now initialize and train the HMM using the large character sequence we obtained
    earlier. This will take several minutes to run depending on the computational
    resources available, and this is the main reason we drew only a sample of the
    text earlier on:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用之前获得的大字符序列初始化和训练HMM。这需要几分钟的时间来运行，具体取决于可用的计算资源，这也是我们之前只抽取文本样本的主要原因：
- en: '[PRE37]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We trained our model in a completely unsupervised way by simply providing it
    with character sequences. We don''t have a meaningful test dataset on which to
    assess the performance of our model; rather, this exercise is worthwhile in that
    it produces an HMM that has interesting properties. It is instructive to take
    a peek at the symbol emission probabilities for each state. These are accessible
    via the `hmm$emissionProbs` attribute on the `hmm_trained` object:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简单地提供字符序列，以完全无监督的方式训练我们的模型。我们没有有意义的测试数据集来评估我们模型的性能；相反，这项练习是值得的，因为它产生了一个具有有趣特性的HMM。查看每个状态的符号发射概率是有教育意义的。这些概率可以通过`hmm_trained`对象上的`hmm$emissionProbs`属性访问：
- en: '![Predicting letter patterns in English words](img/00182.jpeg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![预测英语单词中的字母模式](img/00182.jpeg)'
- en: Let's examine these states carefully. All states have a relatively high probability
    of emitting a whitespace character. State 3 is very interesting as, besides whitespace,
    it seems to have grouped together punctuation and vowels. The HMM has successfully
    managed to group together the letters *a*, *e*, *i*, *o*, and *u* in the same
    category without any prior information about the English language.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细检查这些状态。所有状态都有相对较高的发射空白字符的概率。状态3非常有趣，因为它除了空白字符外，似乎还把标点和元音分组在一起。HMM成功地成功地将字母*a*、*e*、*i*、*o*和*u*归入同一类别，而没有关于英语语言的任何先验信息。
- en: This state also emits two consonants with a noticeable probability. The consonant
    *y* is emitted, which we know occasionally does behave like a vowel in words such
    as *rhythm* and *phylum*, for example. The consonant *s* is also emitted, and
    because it is often used to form the plural of nouns, we find this at the end
    of words just like punctuation marks. So, we see that this state seems to have
    grouped two main themes.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此状态也以明显的概率发射了两个辅音。辅音*y*被发射，我们知道它在诸如*rhythm*和*phylum*等单词中偶尔表现得像元音。辅音*s*也被发射，因为它经常用来构成名词的复数形式，所以我们发现它在单词的末尾，就像标点符号一样。因此，我们看到这个状态似乎将两个主要主题分组在一起。
- en: 'By contrast, state 1 tends to emit consonants and not vowels. In fact, only
    the vowel *u* seems to have a small probability of being emitted from this state.
    State 2 has a mix of vowels and consonants, but it is the only state in which
    the consonant *h* has a high probability. This is very interesting, as *h* is
    another letter of the alphabet that has vowel-like properties in pronunciation
    (it is often silent or part of a diphthong). We can learn more by examining the
    transition probabilities between the states:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，状态1倾向于发射辅音而不是元音。事实上，只有元音*u*似乎有从该状态发射的小概率。状态2有元音和辅音的混合，但它是唯一一个辅音*h*有高概率的状态。这非常有趣，因为*h*是另一个在发音上具有元音特性的字母（它通常是沉默的或双元音的一部分）。我们可以通过检查状态之间的转移概率来了解更多信息：
- en: '[PRE38]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Again, we can discover a wealth of interesting properties. For example, when
    we are in state 3, the vowel state, we have a 95 percent chance of going to state
    1, the consonant state. This is quite intuitive, in that English rarely has consecutive
    vowels. When we are in state 1, we have a 36 percent chance of going to the vowel
    state and a 51 percent chance of going to state 2.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以发现许多有趣的特性。例如，当我们处于状态3，即元音状态时，我们有95%的概率会转到状态1，即辅音状态。这在直觉上是很明显的，因为英语很少出现连续的元音。当我们处于状态1时，我们有36%的概率转到元音状态，有51%的概率转到状态2。
- en: Now we can begin to understand what state 2 represents. It primarily represents
    the state that emits the second consonant when we have two consecutive consonants.
    This is why the letter *h* has such a high probability in this state, as it participates
    in very common diphthongs, such as *ch*, *sh*, and *th*, the latter of course
    being found in very frequent words such as *the*. From this state, the most common
    successor state, with 72 percent probability, is the vowel state, as expected
    after two consecutive consonants.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始理解状态2代表什么了。它主要代表当我们有两个连续辅音时，发出第二个辅音的状态。这就是为什么在这个状态下，字母*h*有如此高的概率，因为它参与了非常常见的双元音，如*ch*、*sh*和*th*，当然，*th*在非常常见的单词如*the*中也能找到。从这个状态出发，最常见的后续状态，概率为72%，是元音状态，正如连续两个辅音之后所预期的。
- en: This experiment is worth repeating with different conditions. If we use different
    seeds or sample a different number of movie reviews, we may see different results,
    as the Baum-Welch algorithm is sensitive to initial conditions and is unsupervised.
    Specifically, our Hidden Markov model might learn a completely different set of
    states.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验值得在不同条件下重复进行。如果我们使用不同的种子或者采样不同数量的电影评论，我们可能会看到不同的结果，因为Baum-Welch算法对初始条件敏感，并且是无监督的。具体来说，我们的隐马尔可夫模型可能会学习到一组完全不同的状态。
- en: For example, on some iterations, we noticed that all punctuation and numerical
    digits are grouped into one state, another state becomes the vowel state, and
    the third state is a pure consonant state. We can reproduce this behavior if,
    in the previous code, we sample 40 texts and use the numbers 1816, 1817, and 1818
    for the three seeds. There are many more possibilities—some of which are easier
    to interpret than others.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在某些迭代中，我们注意到所有的标点符号和数字都被归入一个状态，另一个状态变成了元音状态，第三个状态是纯辅音状态。如果我们之前在代码中采样了40个文本，并使用1816、1817和1818这三个数字作为三个种子，我们可以重现这种行为。还有许多其他可能性——其中一些比其他更容易解释。
- en: 'Another parameter that is worth varying here is the number of states. If we
    use two states, then the split tends to be between vowels and consonants. If we
    increase the number of states, we will often continue to find results that are
    interpretable for as many as 10 states. Hidden Markov models are often also referred
    to as **generative models** because we can use them to generate examples of states
    and observations once they have been trained. We can do this with the `simHMM()`
    function by providing our model and the length of the sequence we want to generate:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里值得调整的另一个参数是状态的数量。如果我们使用两个状态，那么分割往往是在元音和辅音之间。如果我们增加状态的数量，我们通常会继续找到对于多达10个状态可解释的结果。隐马尔可夫模型通常也被称为**生成模型**，因为一旦训练完成，我们可以使用它们来生成状态和观察的示例。我们可以通过提供我们的模型和想要生成的序列长度来使用`simHMM()`函数实现这一点：
- en: '[PRE39]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As a final point, we can download and use the `markovchain` package, take our
    learned transition probability matrix, and find out in the long run how much time
    our model spends in each state. This is done using a **steady state calculation**,
    the mathematics of which we will not explore in this book. Thankfully, the `markovchain`
    package has a simple way to initialize a Markov chain when we know the probabilities
    that are involved. It does this by using the `simpleMc()` function, and we can
    use the `steadyStates()` function on our Markov chain to find out the steady state
    distribution:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点，我们可以下载并使用`markovchain`包，取我们学到的转移概率矩阵，并找出在长期中我们的模型在每个状态上花费了多少时间。这是通过**稳态计算**来完成的，其数学原理我们将在本书中不进行探讨。幸运的是，`markovchain`包有一个简单的方法来初始化马尔可夫链，当我们知道涉及的概率时。它是通过使用`simpleMc()`函数来实现的，我们可以在我们的马尔可夫链上使用`steadyStates()`函数来找出稳态分布：
- en: '[PRE40]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the long term, we spend 38 percent of our time in state 1, the first consonant
    state; 27 percent in state 2, the second consonant state; and 35 percent of our
    time in state 3, the main vowel state.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从长远来看，我们在状态1（第一个辅音状态）上花费了38%的时间，在状态2（第二个辅音状态）上花费了27%的时间，在状态3（主要的元音状态）上花费了35%的时间。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced ourselves to one of the very active areas of
    research in machine learning, namely the field of probabilistic graphical models.
    These models involve using a graphical structure to encode conditional independence
    relations between random variables. We saw how Bayes' theorem, a very simple formula
    that essentially tells us how we can predicate cause by observing effect, can
    be used to build a simple classifier known as the Naïve Bayes classifier. This
    is a simple model where we are trying to predict an output class that best explains
    a set of observed features, all of which are assumed to be independent of each
    other given the output class.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习研究中的一个非常活跃的领域，即概率图模型领域。这些模型涉及使用图形结构来编码随机变量之间的条件独立性关系。我们看到了贝叶斯定理，一个非常简单的公式，它本质上告诉我们如何通过观察效应来推断原因，可以用来构建一个简单的分类器，即朴素贝叶斯分类器。这是一个简单的模型，我们试图预测一个输出类别，该类别最好地解释了一组观察到的特征，所有这些特征都被假定为在给定输出类别的情况下相互独立。
- en: We used this model to predict user sentiment on a set of movie reviews where
    the features were the words that were present in the reviews. Although we obtained
    reasonable accuracy, we found that the assumptions in our model are quite strict
    and prevent us from doing substantially better. Often, a Naïve Bayes model is
    built during the modeling process to provide us with a baseline performance that
    we know we should exceed with more sophisticated models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用这个模型来预测一组电影评论中的用户情感，其中特征是评论中存在的单词。虽然我们获得了合理的准确性，但我们发现我们模型中的假设相当严格，这阻碍了我们取得更好的效果。通常，在建模过程中构建一个朴素贝叶斯模型，以提供我们知道应该超过更复杂模型的基线性能。
- en: We also studied Hidden Markov models, which are models typically used to label
    and predict sequences. Every position in the sequence is comprised of a hidden
    state and an observation emitted from that state. The key assumption of the model
    is that every state is independent of the entire sequence history, given the state
    that immediately preceded it. In addition, all observations are independent of
    each other as well as all other states in the sequence, given the state from which
    they were emitted.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了隐马尔可夫模型，这些模型通常用于标记和预测序列。序列中的每个位置都由一个隐藏状态和从该状态发出的观察组成。模型的关键假设是，每个状态在给定紧邻的前一个状态的情况下独立于整个序列历史。此外，所有观察都是相互独立的，以及所有其他状态，给定它们发出的状态。
- en: When we have labeled sequences, we can train a Hidden Markov model by using
    state transition and symbol emission counts obtained from the data itself. It
    is also possible to train an unsupervised HMM using a very smart algorithm known
    as the Baum-Welch algorithm. Even though we did not dive into the algorithmic
    details, we saw an example of how this works in practice by training an HMM on
    sequences of characters in English words.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有标记序列时，我们可以通过使用从数据本身获得的状态转换和符号发射计数来训练一个隐马尔可夫模型。还可以使用一个非常聪明的算法，即Baum-Welch算法，来训练一个无监督的HMM。尽管我们没有深入算法的细节，但我们通过在一个英文单词的字符序列上训练HMM的例子，看到了这种实际操作是如何工作的。
- en: From this, we saw that the resulting model picked up on some interesting properties
    of language. Incidentally, even though we did not mention it, it is also possible
    to train a Naïve Bayes model with missing class labels, this time using the **EM
    algorithm**. Despite also having relatively strict independence assumptions, HMMs
    are quite powerful and have been successfully applied to a wide variety of applications
    from speech processing to molecular biology.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们看到了所得到的模型捕捉到了语言的一些有趣特性。顺便提一下，尽管我们没有提到，也可以使用**EM算法**训练一个带有缺失类标签的朴素贝叶斯模型。尽管HMMs也有相对严格的独立性假设，但它们相当强大，并且已经在从语音处理到分子生物学等广泛的应用中取得了成功。
- en: In the next chapter, we will look at analyzing and making predictions on time
    series. Many real-world applications involve taking measurements over a particular
    period of time and using them to make predictions about the future. For example,
    we might want to predict tomorrow's weather based on the weather today, or tomorrow's
    stock market index based on market fluctuations over the past few weeks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨对时间序列进行分析和预测。许多现实世界应用涉及在特定时间段内进行测量，并使用这些测量来预测未来。例如，我们可能想根据今天的天气预测明天的天气，或者根据过去几周的市场波动预测明天的股票市场指数。
