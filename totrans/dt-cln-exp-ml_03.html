<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer030">
<h1 id="_idParaDest-26"><em class="italic"><a id="_idTextAnchor025"/>Chapter 2</em>: Examining Bivariate and Multivariate Relationships between Features and Targets</h1>
<p>In this chapter, we'll look at the correlation between possible features and target variables. Bivariate exploratory analysis, using crosstabs (two-way frequencies), correlations, scatter plots, and grouped boxplots can uncover key issues for modeling. Common issues include high correlation between features and non-linear relationships between features and the target variable. We will use pandas methods for bivariate analysis and Matplotlib for visualizations in this chapter. We will also discuss the implications of what we find in terms of feature engineering and modeling.</p>
<p>We will also use multivariate techniques to understand the relationship between features. This includes leaning on some machine learning algorithms to identify possibly problematic observations. After, we will provide tentative recommendations for eliminating certain observations from our modeling, as well as for transforming key features.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Identifying outliers and extreme values in bivariate relationships</li>
<li>Using scatter plots to view bivariate relationships between continuous features</li>
<li>Using grouped boxplots to view bivariate relationships between continuous and categorical features</li>
<li>Using linear regression to identify data points with significant influence</li>
<li>Using K-nearest neighbors to find outliers</li>
<li>Using Isolation Forest to find outliers</li>
</ul>
<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Technical requirements</h1>
<p>This chapter will rely heavily on the pandas and Matplotlib libraries, but you don't require any prior knowledge of these. If you have installed Python from a scientific distribution, such as Anaconda or WinPython, then these libraries have probably already been installed. We will also be using Seaborn for some of our graphics and the statsmodels library for some summary statistics. If you need to install any of the packages, you can do so by running <strong class="source-inline">pip install [package name]</strong> from a terminal window or Windows PowerShell. The code for this chapter can be found in this book's GitHub repository at <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning</a>.</p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Identifying outliers and extreme values in bivariate relationships</h1>
<p>It is hard to develop<a id="_idIndexMarker136"/> a reliable model without having a good sense of the bivariate relationships in our data. We not only care about the relationship between particular features and target variables but also about how features <a id="_idIndexMarker137"/>move together. If features are highly <a id="_idIndexMarker138"/>correlated, then modeling their independent effect becomes tricky or unnecessary. This may be a challenge, even if the features are highly correlated over just a range of values.</p>
<p>Having a good understanding of <a id="_idIndexMarker139"/>bivariate relationships is also important for identifying outliers. A value might be unexpected, even if it is not an extreme value. This is because some values for a feature are unusual when a second feature has certain values. This is easy to illustrate when one feature is categorical and the other is continuous.</p>
<p>The following <a id="_idIndexMarker140"/>diagram illustrates the number of bird sightings per day over several years but shows different distributions for the two sites. One site has a (mean) sightings per day of 33, while the other has 52. (This is a fictional example that's been pulled from my <em class="italic">Python Data Cleaning Cookbook</em>.) The overall mean (not shown) is 42. What should we make of a value of 58 for daily sightings? Is it an outlier? This depends on which of the two sites was being observed. If there were 58 sightings in a day at site A, 58 would be an unusually high number. However, this wouldn't be true for site B, where 58 sightings would not be very different from the mean for that site:</p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 2.1 – Daily Bird Sightings " height="477" src="image/B17978_02_001.jpg" width="891"/>
</div>
</div>
<p class="figure-caption">Figure 2.1 – Daily Bird Sightings</p>
<p>This <a id="_idIndexMarker141"/>hints <a id="_idIndexMarker142"/>at a useful rule of thumb: whenever a feature of interest is correlated with another feature, we should take that relationship <a id="_idIndexMarker143"/>into account when we're trying to identify outliers (or any modeling with that feature, actually). It is helpful to state this a little more precisely <a id="_idIndexMarker144"/>and extend it to cases where both features are continuous. If we assume a linear relationship between feature <em class="italic">x</em> and feature <em class="italic">y</em>, we can describe that relationship with the familiar <em class="italic">y = mx + b</em> equation, where <em class="italic">m</em> is the slope and <em class="italic">b</em> is the <em class="italic">y</em>-intercept. Then, we can expect the value of <em class="italic">y</em> to be somewhere close to <em class="italic">x</em> times the estimated slope, plus the <em class="italic">y</em>-intercept. Unexpected values are those that deviate substantially from this relationship, where the value of <em class="italic">y</em> is much higher or lower than what would be predicted, given the value of <em class="italic">x</em>. This can be extended to multiple <em class="italic">x</em>, or predictor, variables.</p>
<p>In this section, we will learn how to identify outliers and unexpected values by examining the relationship a feature has with another feature. In subsequent sections of this chapter, we will use multivariate techniques to make additional improvements to our outlier detection.</p>
<p>We will <a id="_idIndexMarker145"/>work with data based on COVID-19 cases <a id="_idIndexMarker146"/>by country in this section. The dataset contains cases and deaths per million people in the population. We <a id="_idIndexMarker147"/>will treat both columns as <a id="_idIndexMarker148"/>possible targets. It also contains demographic data for each country, such as GDP per capita, median age, and diabetes prevalence. Let's get started:</p>
<p class="callout-heading">Note</p>
<p class="callout">Our World in Data provides COVID-19 public-use data at <a href="https://ourworldindata.org/coronavirus-source-data">https://ourworldindata.org/coronavirus-source-data</a>. The dataset that's being used in this section was downloaded on July 9, 2021. There are more columns in the data than I have included. I created the <strong class="source-inline">region</strong> column based on country.</p>
<ol>
<li>Let's start by loading the COVID-19 dataset and looking at how it is structured. We will also import the Matplotlib and Seaborn libraries since we will do a couple of visualizations:<p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p><p class="source-code">covidtotals.info()</p><p class="source-code"><strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong></p><p class="source-code"><strong class="bold">Index: 221 entries, AFG to ZWE</strong></p><p class="source-code"><strong class="bold">Data columns (total 16 columns):</strong></p><p class="source-code"><strong class="bold"> #  Column                  Non-Null Count     Dtype  </strong></p><p class="source-code"><strong class="bold">--  --------                ---------------    -------</strong></p><p class="source-code"><strong class="bold"> 0  lastdate                221    non-null    object </strong></p><p class="source-code"><strong class="bold"> 1  location                221    non-null    object </strong></p><p class="source-code"><strong class="bold"> 2  total_cases             192    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 3  total_deaths            185    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 4  total_cases_mill        192    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 5  total_deaths_mill       185    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 6  population              221    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 7  population_density      206    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 8  median_age              190    non-null    float64</strong></p><p class="source-code"><strong class="bold"> 9  gdp_per_capita          193    non-null    float64</strong></p><p class="source-code"><strong class="bold">10  aged_65_older           188    non-null    float64</strong></p><p class="source-code"><strong class="bold">11  total_tests_thous        13    non-null    float64</strong></p><p class="source-code"><strong class="bold">12  life_expectancy         217    non-null    float64</strong></p><p class="source-code"><strong class="bold">13  hospital_beds_thous     170    non-null    float64</strong></p><p class="source-code"><strong class="bold">14  diabetes_prevalence     200    non-null    float64</strong></p><p class="source-code"><strong class="bold">15  region                  221    non-null    object</strong></p><p class="source-code"><strong class="bold">dtypes: float64(13), object(3)</strong></p><p class="source-code"><strong class="bold">memory usage: 29.4+ KB</strong></p></li>
<li>A great <a id="_idIndexMarker149"/>place to start with our examination<a id="_idIndexMarker150"/> of bivariate relationships <a id="_idIndexMarker151"/>is with correlations. First, let's<a id="_idIndexMarker152"/> create a DataFrame that contains a few key features:<p class="source-code">totvars = ['location','total_cases_mill', </p><p class="source-code">  'total_deaths_mill']</p><p class="source-code">demovars = ['population_density','aged_65_older', </p><p class="source-code">  'gdp_per_capita','life_expectancy', </p><p class="source-code">  'diabetes_prevalence']</p><p class="source-code">covidkeys = covidtotals.loc[:, totvars + demovars]</p></li>
<li>Now, we <a id="_idIndexMarker153"/>can get the Pearson correlation matrix for these features. There is a strong positive correlation<a id="_idIndexMarker154"/> of 0.71 between <a id="_idIndexMarker155"/>cases and deaths per million. The <a id="_idIndexMarker156"/>percentage of the population that's aged 65 or older is positively correlated with cases and deaths, at 0.53 for both. Life expectancy is also highly correlated with cases per million. There seems to be at least some correlation of <strong class="bold">gross domestic product</strong> (<strong class="bold">GDP</strong>) per <a id="_idIndexMarker157"/>person with cases:<p class="source-code">corrmatrix = covidkeys.corr(method="pearson")</p><p class="source-code">corrmatrix</p><p class="source-code">                  <strong class="bold">total_cases_mill  total_deaths_mill\</strong></p><p class="source-code"><strong class="bold">total_cases_mill              1.00               0.71</strong></p><p class="source-code"><strong class="bold">total_deaths_mill             0.71               1.00</strong></p><p class="source-code"><strong class="bold">population_density            0.04              -0.03</strong></p><p class="source-code"><strong class="bold">aged_65_older                 0.53               0.53</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                0.46               0.22</strong></p><p class="source-code"><strong class="bold">life_expectancy               0.57               0.46</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence           0.02              -0.01</strong></p><p class="source-code">                <strong class="bold">population_density  aged_65_older  gdp_per_capita\</strong></p><p class="source-code"><strong class="bold">total_cases_mill         0.04          0.53       0.46</strong></p><p class="source-code"><strong class="bold">total_deaths_mill       -0.03          0.53       0.22</strong></p><p class="source-code"><strong class="bold">population_density       1.00          0.06       0.41</strong></p><p class="source-code"><strong class="bold">aged_65_older            0.06          1.00       0.49</strong></p><p class="source-code"><strong class="bold">gdp_per_capita           0.41          0.49       1.00</strong></p><p class="source-code"><strong class="bold">life_expectancy          0.23          0.73       0.68</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence      0.01         -0.06       0.12</strong></p><p class="source-code"><strong class="bold">                  life_expectancy  diabetes_prevalence</strong></p><p class="source-code"><strong class="bold">total_cases_mill        0.57           0.02  </strong></p><p class="source-code"><strong class="bold">total_deaths_mill       0.46          -0.01  </strong></p><p class="source-code"><strong class="bold">population_density      0.23           0.01  </strong></p><p class="source-code"><strong class="bold">aged_65_older           0.73          -0.06  </strong></p><p class="source-code"><strong class="bold">gdp_per_capita          0.68           0.12  </strong></p><p class="source-code"><strong class="bold">life_expectancy         1.00           0.19  </strong></p><p class="source-code"><strong class="bold">diabetes_prevalence     0.19           1.00</strong></p></li>
</ol>
<p>It is <a id="_idIndexMarker158"/>worth noting the correlation <a id="_idIndexMarker159"/>between possible features, such as between life expectancy and GDP per capita (0.68) and life expectancy and those aged 65 or older (0.73).</p>
<ol>
<li value="4">It<a id="_idIndexMarker160"/> can be helpful to see the correlation<a id="_idIndexMarker161"/> matrix as a heat map. This can be done by passing the correlation matrix to the Seaborn <strong class="source-inline">heatmap</strong> method:<p class="source-code">sns.heatmap(corrmatrix, xticklabels =</p><p class="source-code">  corrmatrix.columns, yticklabels=corrmatrix.columns, </p><p class="source-code">  cmap="coolwarm")</p><p class="source-code">plt.title('Heat Map of Correlation Matrix')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This creates the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 2.2 – Heat map of COVID data, with the strongest correlations in red and peach " height="272" src="image/B17978_02_002.jpg" width="398"/>
</div>
</div>
<p class="figure-caption">Figure 2.2 – Heat map of COVID data, with the strongest correlations in red and peach</p>
<p>We <a id="_idIndexMarker162"/>want to pay attention to the <a id="_idIndexMarker163"/>cells shown with <a id="_idIndexMarker164"/>warmer colors – in this case, mainly peach. I <a id="_idIndexMarker165"/>find that using a heat map helps me keep correlations in mind when modeling.</p>
<p class="callout-heading">Note </p>
<p class="callout">All the color images contained in this book can be downloaded. Check the <em class="italic">Preface</em> of this book for the respective link.</p>
<ol>
<li value="5">Let's take a closer look at the relationship between total cases per million and deaths per million. One way to get a better sense of this than with just a correlation coefficient is by comparing the high and low values for each and seeing <a id="_idIndexMarker166"/>how they move together. In the <a id="_idIndexMarker167"/>following code, we're using the <strong class="source-inline">qcut</strong> method to create a categorical feature with five values distributed relatively evenly, from very low to very high, for cases. We have done the same for deaths:<p class="source-code">covidkeys['total_cases_q'] = \</p><p class="source-code">  pd.qcut(covidkeys['total_cases_mill'],</p><p class="source-code">  labels=['very low','low','medium','high',</p><p class="source-code">  'very high'], q=5, precision=0)</p><p class="source-code">covidkeys['total_deaths_q'] = \</p><p class="source-code">  pd.qcut(covidkeys['total_deaths_mill'],</p><p class="source-code">  labels=['very low','low','medium','high',</p><p class="source-code">  'very high'], q=5, precision=0)</p></li>
<li>We can<a id="_idIndexMarker168"/> use the <strong class="source-inline">crosstab</strong> function to view the number of countries for each quintile of cases and <a id="_idIndexMarker169"/>quintile of deaths. As we would expect, most of the countries are along the diagonal. There are 27 countries with very low cases and very low deaths, and 25 countries with very high cases and very high deaths. The interesting counts are those not on the diagonal, such as the four countries with very high cases but only medium deaths, nor the one with medium cases and very high deaths. Let's also look at the means of our features so that we can reference them later:<p class="source-code">pd.crosstab(covidkeys.total_cases_q, </p><p class="source-code">  covidkeys.total_deaths_q)</p><p class="source-code"><strong class="bold">total_deaths_q  very low  low  medium  high  very high</strong></p><p class="source-code"><strong class="bold">total_cases_q</strong>                                         </p><p class="source-code"><strong class="bold">very low              27    7       0     0          0</strong></p><p class="source-code"><strong class="bold">low                    9   24       4     0          0</strong></p><p class="source-code"><strong class="bold">medium                 1    6      23     6          1</strong></p><p class="source-code"><strong class="bold">high                   0    0       6    21         11</strong></p><p class="source-code"><strong class="bold">very high              0    0       4    10         25</strong></p><p class="source-code">covidkeys.mean()</p><p class="source-code"><strong class="bold">total_cases_mill         36,649</strong></p><p class="source-code"><strong class="bold">total_deaths_mill           683</strong></p><p class="source-code"><strong class="bold">population_density          453</strong></p><p class="source-code"><strong class="bold">aged_65_older                 9</strong></p><p class="source-code"><strong class="bold">gdp_per_capita           19,141</strong></p><p class="source-code"><strong class="bold">life_expectancy              73</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence           8</strong></p></li>
<li>Let's<a id="_idIndexMarker170"/> take a closer look at the <a id="_idIndexMarker171"/>countries away from the <a id="_idIndexMarker172"/>diagonal. Four countries – Cyprus, Kuwait, Maldives, and Qatar – have fewer deaths per million than <a id="_idIndexMarker173"/>average but well above average cases per million. Interestingly, all four countries are very small in terms of population; three of the four have population densities far below the average of 453; again, three of the four have people aged 65 or older percentages that are much lower than average:<p class="source-code">covidtotals.loc[(covidkeys.total_cases_q=="very high")</p><p class="source-code">  &amp; (covidkeys.total_deaths_q=="medium")].T</p><p class="source-code"><strong class="bold">iso_code            CYP        KWT        MDV    QAT</strong></p><p class="source-code"><strong class="bold">lastdate            2021-07-07 2021-07-07 2021-07-07  2021-07-07</strong></p><p class="source-code"><strong class="bold">location            Cyprus    Kuwait   Maldives  Qatar</strong></p><p class="source-code"><strong class="bold">total_cases         80,588    369,227  74,724  222,918</strong></p><p class="source-code"><strong class="bold">total_deaths        380       2,059    213     596</strong></p><p class="source-code"><strong class="bold">total_cases_mill    90,752    86,459   138,239 77,374</strong></p><p class="source-code"><strong class="bold">total_deaths_mill   428       482      394     207</strong></p><p class="source-code"><strong class="bold">population         888,005 4,270,563 540,542 2,881,060</strong></p><p class="source-code"><strong class="bold">population_density  128       232      1,454    227</strong></p><p class="source-code"><strong class="bold">median_age          37        34       31       32</strong></p><p class="source-code"><strong class="bold">gdp_per_capita      32,415    65,531   15,184   116,936</strong></p><p class="source-code"><strong class="bold">aged_65_older       13        2        4        1</strong></p><p class="source-code"><strong class="bold">total_tests_thous   NaN       NaN      NaN     NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy     81        75       79      80</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous 3         2        NaN     1</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence 9         16       9       17</strong></p><p class="source-code"><strong class="bold">region              Eastern   West     South   West  </strong></p><p class="source-code"><strong class="bold">                    Europe    Asia     Asia    Asia</strong></p></li>
<li>Let's <a id="_idIndexMarker174"/>take a closer look at the <a id="_idIndexMarker175"/>country with more <a id="_idIndexMarker176"/>deaths than we would have <a id="_idIndexMarker177"/>expected based on cases. For Mexico, the number of cases per million are well below average, while the number of deaths per million are quite a bit above average:<p class="source-code">covidtotals.loc[(covidkeys. total_cases_q=="medium")</p><p class="source-code">  &amp; (covidkeys.total_deaths_q=="very high")].T</p><p class="source-code"><strong class="bold">iso_code                         MEX</strong></p><p class="source-code"><strong class="bold">lastdate                  2021-07-07</strong></p><p class="source-code"><strong class="bold">location                      Mexico</strong></p><p class="source-code"><strong class="bold">total_cases                2,558,369</strong></p><p class="source-code"><strong class="bold">total_deaths                 234,192</strong></p><p class="source-code"><strong class="bold">total_cases_mill              19,843</strong></p><p class="source-code"><strong class="bold">total_deaths_mill              1,816</strong></p><p class="source-code"><strong class="bold">population               128,932,753</strong></p><p class="source-code"><strong class="bold">population_density                66</strong></p><p class="source-code"><strong class="bold">median_age                        29</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                17,336</strong></p><p class="source-code"><strong class="bold">aged_65_older                      7</strong></p><p class="source-code"><strong class="bold">total_tests_thous                NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy                   75</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous                1</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence               13</strong></p><p class="source-code"><strong class="bold">region                 North America</strong></p></li>
</ol>
<p>Correlation<a id="_idIndexMarker178"/> coefficients and heat maps <a id="_idIndexMarker179"/>are a good place to start when we want to get a sense of the bivariate relationships in our dataset. However, it can be <a id="_idIndexMarker180"/>hard to visualize the relationship <a id="_idIndexMarker181"/>between continuous variables with just a correlation coefficient. This is particularly true when the relationship is not linear – that is, when it varies based on the ranges of a feature. We can often improve our understanding of the relationship between two features with a scatter plot. We will do that in the next section.</p>
<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Using scatter plots to view bivariate relationships between continuous features</h1>
<p>In this section, we'll learn how to get a scatter plot of our data.</p>
<p>We can <a id="_idIndexMarker182"/>use scatter plots to get a more complete picture of the relationship between two features <a id="_idIndexMarker183"/>than what can be detected by a correlation coefficient alone. This is particularly useful when that relationship changes across certain ranges of the data. In this section, we will create scatter plots of some of the same features we examined in the previous section. Let's get started:</p>
<ol>
<li value="1">It is helpful to plot a regression line through the data points. We can do this with Seaborn's <strong class="source-inline">regplot</strong> method. Let's load the COVID-19 data again, along with the Matplotlib and Seaborn libraries, and generate a scatter plot of <strong class="source-inline">total_cases_mill</strong> by <strong class="source-inline">total_deaths_mill</strong>:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p><p class="source-code">ax = sns.regplot(x="total_cases_mill",</p><p class="source-code">  y="total_deaths_mill", data=covidtotals)</p><p class="source-code">ax.set(xlabel="Cases Per Million", ylabel="Deaths Per </p><p class="source-code">  Million", title="Total COVID Cases and Deaths by </p><p class="source-code">  Country")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker184"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 2.3 – Total COVID Cases and Deaths by Country " height="436" src="image/B17978_02_003.jpg" width="562"/>
</div>
</div>
<p class="figure-caption">Figure 2.3 – Total COVID Cases and Deaths by Country</p>
<p>The<a id="_idIndexMarker185"/> regression line is an estimate of the relationship between cases per million and <a id="_idIndexMarker186"/>deaths per million. The slope of the line indicates how much we can expect deaths per million to increase with a 1-unit increase in cases per million. Those points on the scatter plot that are significantly above the regression line should be examined more closely.</p>
<ol>
<li value="2">The country with deaths per million near 6,000 and cases per million below 75,000 is clearly an outlier. Let's take a closer look:<p class="source-code">covidtotals.loc[(covidtotals.total_cases_mill&lt;75000) \</p><p class="source-code">  &amp; (covidtotals.total_deaths_mill&gt;5500)].T</p><p class="source-code"><strong class="bold">iso_code                           PER</strong></p><p class="source-code"><strong class="bold">lastdate                    2021-07-07</strong></p><p class="source-code"><strong class="bold">location                          Peru</strong></p><p class="source-code"><strong class="bold">total_cases                  2,071,637</strong></p><p class="source-code"><strong class="bold">total_deaths                   193,743</strong></p><p class="source-code"><strong class="bold">total_cases_mill                62,830</strong></p><p class="source-code"><strong class="bold">total_deaths_mill                5,876</strong></p><p class="source-code"><strong class="bold">population                  32,971,846</strong></p><p class="source-code"><strong class="bold">population_density                  25</strong></p><p class="source-code"><strong class="bold">median_age                          29</strong></p><p class="source-code"><strong class="bold">gdp_per_capita                  12,237</strong></p><p class="source-code"><strong class="bold">aged_65_older                        7</strong></p><p class="source-code"><strong class="bold">total_tests_thous                  NaN</strong></p><p class="source-code"><strong class="bold">life_expectancy                     77</strong></p><p class="source-code"><strong class="bold">hospital_beds_thous                  2</strong></p><p class="source-code"><strong class="bold">diabetes_prevalence                  6</strong></p><p class="source-code"><strong class="bold">region                   South America</strong></p></li>
</ol>
<p>Here, we can <a id="_idIndexMarker187"/>see that the outlier country is Peru. Peru does have above-average cases per million, but its number of deaths per million is still much greater than would be expected <a id="_idIndexMarker188"/>given the number of cases. If we draw a line that's perpendicular to the <em class="italic">x</em> axis at 62,830, we can see that it crosses the regression line at about 1,000 deaths per million, which is far fewer than the 5,876 for Peru. The only other values in the data for Peru that also stand out as very different from the dataset averages are population density and GDP per person, both of which are substantially lower than average. Here, none of our features may help us explain the high number of deaths in Peru. </p>
<p class="callout-heading">Note</p>
<p class="callout">When creating a scatter plot, it is common to put a feature or predictor variable on the <em class="italic">x</em> axis and a target variable on the <em class="italic">y</em> axis. If a regression line is drawn, then that represents the increase in the target that's been predicted by a 1-unit increase in the predictor. But scatter plots can also be used to examine the relationship between two predictors or two possible targets.</p>
<p>Looking back at how <a id="_idIndexMarker189"/>we defined an outlier in <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>, an argument can be made that Peru is an outlier. But we still have more<a id="_idIndexMarker190"/> work to do before we can come to that conclusion. Peru is not the only country with points on the scatter plot far above or below the regression line. It is generally a good idea to investigate many of these points. Let's take a look:</p>
<ol>
<li value="1">Creating scatter plots that contain most of the key continuous features can help us identify other possible outliers and better visualize the correlations we observed in the first section of this chapter. Let's create scatter plots of people who are aged 65 and older and GDP per capita with total cases per million:<p class="source-code">fig, axes = plt.subplots(1,2, sharey=True)</p><p class="source-code">sns.regplot(x=covidtotals.aged_65_older, </p><p class="source-code">  y=covidtotals.total_cases_mill, ax=axes[0])</p><p class="source-code">sns.regplot(x=covidtotals.gdp_per_capita, </p><p class="source-code">  y=covidtotals.total_cases_mill, ax=axes[1])</p><p class="source-code">axes[0].set_xlabel("Aged 65 or Older")</p><p class="source-code">axes[0].set_ylabel("Cases Per Million")</p><p class="source-code">axes[1].set_xlabel("GDP Per Capita")</p><p class="source-code">axes[1].set_ylabel("")</p><p class="source-code">plt.suptitle("Age 65 Plus and GDP with Cases Per </p><p class="source-code">  Million")</p><p class="source-code">plt.tight_layout()</p><p class="source-code">fig.subplots_adjust(top=0.92)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 2.4 – Age 65 Plus and GDP with Cases Per Million " height="455" src="image/B17978_02_004.jpg" width="608"/>
</div>
</div>
<p class="figure-caption">Figure 2.4 – Age 65 Plus and GDP with Cases Per Million</p>
<p>These scatter plots show that some countries that had very high cases per million had <a id="_idIndexMarker191"/>values close to what we would expect, given the age of the population or the GDP. These are extreme values, but not necessarily outliers as we have defined them. </p>
<p>It is possible to use scatter plots to illustrate the relationships between two features and a target, all in one graphic. Let's return to the land temperatures data that we worked with in the previous chapter to explore this.</p>
<p class="callout-heading">Data Note</p>
<p class="callout">The land temperature dataset contains the average temperature readings (in Celsius) in 2019 from over 12,000 stations across the world, though the majority of the stations are in the United States. The dataset was retrieved from the Global Historical Climatology Network integrated database. It has been made available for public use by the United States National Oceanic and Atmospheric Administration at <a href="https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4">https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4</a>.</p>
<ol>
<li value="2">We <a id="_idIndexMarker192"/>expect the average temperature at a weather station to be impacted by both latitude and elevation. Let's say that our previous analysis showed that elevation does not start having much of an impact on temperature until approximately the 1,000-meter mark. We can split the <strong class="source-inline">landtemps</strong> DataFrame into low- and high-elevation stations, with 1,000 meters as the threshold. In the following code, we can see that this gives us 9,538 low-elevation stations with an average temperature of 12.16 degrees Celsius, and 2,557 high-elevation stations with an average temperature of 7.58:<p class="source-code">landtemps = pd.read_csv("data/landtemps2019avgs.csv")</p><p class="source-code">low, high = landtemps.loc[landtemps.elevation&lt;=1000],</p><p class="source-code">  landtemps.loc[landtemps.elevation&gt;1000]</p><p class="source-code">low.shape[0], low.avgtemp.mean()</p><p class="source-code"><strong class="bold">(9538, 12.161417937651676)</strong></p><p class="source-code">high.shape[0], high.avgtemp.mean()</p><p class="source-code"><strong class="bold">(2557, 7.58321486951755)</strong></p></li>
<li>Now, we <a id="_idIndexMarker193"/>can visualize the relationship between elevation and latitude and temperature <a id="_idIndexMarker194"/>in one scatter plot:<p class="source-code">plt.scatter(x="latabs", y="avgtemp", c="blue",</p><p class="source-code">  data=low)</p><p class="source-code">plt.scatter(x="latabs", y="avgtemp", c="red", </p><p class="source-code">  data=high)</p><p class="source-code">plt.legend(('low elevation', 'high elevation'))</p><p class="source-code">plt.xlabel("Latitude (N or S)")</p><p class="source-code">plt.ylabel("Average Temperature (Celsius)")</p><p class="source-code">plt.title("Latitude and Average Temperature in 2019")</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following scatter plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 2.5 – Latitude and Average Temperature in 2019 " height="435" src="image/B17978_02_005.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 2.5 – Latitude and Average Temperature in 2019</p>
<p>Here, we <a id="_idIndexMarker195"/>can see that the temperatures gradually decrease as the distance from the equator (measured in latitude) increases. We can also see that high-elevation weather stations (those with red dots) are generally below low-elevation stations – that is, they have lower temperatures at similar latitudes.</p>
<ol>
<li value="4">There <a id="_idIndexMarker196"/>also seems to be at least some difference in slope between high- and low-elevation stations. Temperatures appear to decline more quickly as latitude increases with high-elevation stations. We can draw two regression lines through the scatter plot – one for high and one for low-elevation stations – to get a clearer picture of this. To simplify the code a bit, let's create a categorical <a id="_idIndexMarker197"/>feature, <strong class="source-inline">elevation_group</strong>, for low- and high-elevation stations:<p class="source-code">landtemps['elevation_group'] = </p><p class="source-code">  np.where(landtemps.elevation&lt;=1000,'low','high')</p><p class="source-code">sns.lmplot(x="latabs", y="avgtemp", </p><p class="source-code">  hue="elevation_group", palette=dict(low="blue", </p><p class="source-code">  high="red"), legend_out=False, data=landtemps)</p><p class="source-code">plt.xlabel("Latitude (N or S)")</p><p class="source-code">plt.ylabel("Average Temperature")</p><p class="source-code">plt.legend(('low elevation', 'high elevation'), </p><p class="source-code">  loc='lower left')</p><p class="source-code">plt.yticks(np.arange(-60, 40, step=20))</p><p class="source-code">plt.title("Latitude and Average Temperature in 2019")</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces<a id="_idIndexMarker198"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 2.6 – Latitude and Average Temperature in 2019 with regression lines " height="479" src="image/B17978_02_006.jpg" width="516"/>
</div>
</div>
<p class="figure-caption">Figure 2.6 – Latitude and Average Temperature in 2019 with regression lines</p>
<p>Here, we can <a id="_idIndexMarker199"/>see the steeper negative slope for high-elevation stations.</p>
<ol>
<li value="5">If we <a id="_idIndexMarker200"/>want to see a scatter plot with two continuous features and a continuous target, rather than forcing one of the features to be dichotomous, as we did in the previous example, we can take advantage of Matplotlib's 3D functionality:<p class="source-code">fig = plt.figure()</p><p class="source-code">plt.suptitle("Latitude, Temperature, and Elevation in </p><p class="source-code">  2019")</p><p class="source-code">ax = plt.axes(projection='3d')</p><p class="source-code">ax.set_xlabel("Elevation")</p><p class="source-code">ax.set_ylabel("Latitude")</p><p class="source-code">ax.set_zlabel("Avg Temp")</p><p class="source-code">ax.scatter3D(landtemps.elevation, landtemps.latabs, </p><p class="source-code">  landtemps.avgtemp)</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This<a id="_idIndexMarker201"/> produces<a id="_idIndexMarker202"/> the following three-dimensional scatter plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 2.7 – Latitude, Temperature, and Elevation in 2019 " height="383" src="image/B17978_02_007.jpg" width="425"/>
</div>
</div>
<p class="figure-caption">Figure 2.7 – Latitude, Temperature, and Elevation in 2019</p>
<p>Scatter plots <a id="_idIndexMarker203"/>are a go-to visualization for teasing out relationships between continuous features. We <a id="_idIndexMarker204"/>get a better sense of those relationships than correlation coefficients alone can reveal. However, we need a very different visualization if we are examining the relationship between a continuous feature and a categorical one. Grouped boxplots are useful in those cases. We will learn how to create grouped boxplots with Matplotlib in the next section.</p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Using grouped boxplots to view bivariate relationships between continuous and categorical features</h1>
<p>Grouped boxplots<a id="_idIndexMarker205"/> are an underappreciated visualization. They are helpful when we're examining the relationship between continuous and categorical features since they show how the distribution of a continuous feature can vary by the values of the categorical feature.</p>
<p>We can explore<a id="_idIndexMarker206"/> this by returning to the <strong class="bold">National Longitudinal Survey</strong> (<strong class="bold">NLS</strong>) data <a id="_idIndexMarker207"/>we worked with in the previous chapter. The NLS has one observation per survey respondent but collects annual data on<a id="_idIndexMarker208"/> education and employment (data for each year is captured in different columns).</p>
<p class="callout-heading">Data Note</p>
<p class="callout">As stated in <a href="B17978_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Examining the Distribution of Features and Targets</em>, the NLS of Youth is conducted by the United States Bureau of Labor Statistics. Separate files for SPSS, Stata, and SAS can be downloaded from the respective repository. The NLS data can be downloaded from <a href="https://www.nlsinfo.org/investigator/pages/search">https://www.nlsinfo.org/investigator/pages/search</a>.</p>
<p>Follow these steps to <a id="_idIndexMarker209"/>create grouped bloxplots:</p>
<ol>
<li value="1">Among the many columns in the NLS DataFrame, there's <strong class="source-inline">highestdegree</strong> and <strong class="source-inline">weeksworked17</strong>, which represent the highest degree the respondent earned and the number of weeks the person worked in 2017, respectively. Let's look at the distribution of weeks worked for each value of the degree that was earned. First, we must define a function, <strong class="source-inline">gettots</strong>, to get the descriptive statistics we want. Then, we <a id="_idIndexMarker210"/>must pass a <strong class="source-inline">groupby</strong> series object, <strong class="source-inline">groupby(['highestdegree'])['weeksworked17']</strong>, to that function using <strong class="source-inline">apply</strong>:<p class="callout-heading">Note</p><p class="callout">We will not go over how to use <strong class="source-inline">groupby</strong> or <strong class="source-inline">apply</strong> in this book. I have covered many examples of their use in my book, <em class="italic">Python Data Cleaning Cookbook</em>.</p><p class="source-code">nls97 = pd.read_csv("data/nls97.csv")</p><p class="source-code">nls97.set_index("personid", inplace=True)</p><p class="source-code">def gettots(x):</p><p class="source-code">  out = {}</p><p class="source-code">  out['min'] = x.min()</p><p class="source-code">  out['qr1'] = x.quantile(0.25)</p><p class="source-code">  out['med'] = x.median()</p><p class="source-code">  out['qr3'] = x.quantile(0.75)</p><p class="source-code">  out['max'] = x.max()</p><p class="source-code">  out['count'] = x.count()</p><p class="source-code">  return pd.Series(out)</p><p class="source-code">nls97.groupby(['highestdegree'])['weeksworked17'].\</p><p class="source-code">  apply(gettots).unstack()</p><p class="source-code">                   <strong class="bold">min   qr1   med   qr3   max   count</strong></p><p class="source-code"><strong class="bold">highestdegree</strong>                                  </p><p class="source-code"><strong class="bold">0. None              0     0    40    52    52     510</strong></p><p class="source-code"><strong class="bold">1. GED               0     8    47    52    52     848</strong></p><p class="source-code"><strong class="bold">2. High School       0    31    49    52    52   2,665</strong></p><p class="source-code"><strong class="bold">3. Associates        0    42    49    52    52     593</strong></p><p class="source-code"><strong class="bold">4. Bachelors         0    45    50    52    52   1,342</strong></p><p class="source-code"><strong class="bold">5. Masters           0    46    50    52    52     538</strong></p><p class="source-code"><strong class="bold">6. PhD               0    46    50    52    52      51</strong></p><p class="source-code"><strong class="bold">7. Professional      0    47    50    52    52      97</strong></p></li>
</ol>
<p>Here, we can see how different the distribution of weeks worked for people with less than a high school degree is from that distribution for people with a bachelor's degree or more. For those with no degree, more than 25% had 0 weeks worked. For those with a bachelor's degree, even those at the 25th percentile worked 45 weeks during the year. The interquartile range covers the whole distribution for individuals with no degree (0 to 52), but only a small part of the range for individuals with bachelor's degrees (45 to 52).</p>
<p>We should also make note of the class imbalance for <strong class="source-inline">highestdegree</strong>. The counts get quite small after master's degrees and the counts for high school degrees are nearly twice that of the next largest group. We will likely need to collapse some categories before we do any modeling with this data.</p>
<ol>
<li value="2">Grouped boxplots <a id="_idIndexMarker211"/>make the differences in distributions <a id="_idIndexMarker212"/>even clearer. Let's create some with the same data. We will use Seaborn for this plot:<p class="source-code">import seaborn as sns</p><p class="source-code">myplt = sns.boxplot(x='highestdegree', </p><p class="source-code">  y= 'weeksworked17' , data=nls97,</p><p class="source-code">  order=sorted(nls97.highestdegree.dropna().unique()))</p><p class="source-code">myplt.set_title("Boxplots of Weeks Worked by Highest </p><p class="source-code">  Degree")</p><p class="source-code">myplt.set_xlabel('Highest Degree Attained')</p><p class="source-code">myplt.set_ylabel('Weeks Worked 2017')</p><p class="source-code">myplt.set_xticklabels(myplt.get_xticklabels(),</p><p class="source-code">  rotation=60, horizontalalignment='right')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker213"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 2.8 – Boxplots of Weeks Worked by Highest Degree " height="451" src="image/B17978_02_008.jpg" width="613"/>
</div>
</div>
<p class="figure-caption">Figure 2.8 – Boxplots of Weeks Worked by Highest Degree</p>
<p>The grouped boxplots<a id="_idIndexMarker214"/> illustrate the dramatic difference in interquartile range for weeks worked by the degree earned. At the associate's degree level (a 2-year college degree in the United States) or above, there are values below the whiskers, represented by dots. Below the associate's degree level, the boxplots do not identify any outliers or extreme values. For example, a 0 weeks worked value is not an extreme value for someone with no degree, but it is for someone with an associate's degree or more.</p>
<ol>
<li value="3">We<a id="_idIndexMarker215"/> can also use grouped boxplots to illustrate how the <a id="_idIndexMarker216"/>distribution of COVID-19 cases varies by region. Let's also add a swarmplot to view the data points since there aren't too many of them:<p class="source-code">sns.boxplot(x='total_cases_mill', y='region',</p><p class="source-code">  data=covidtotals)</p><p class="source-code">sns.swarmplot(y="region", x="total_cases_mill",</p><p class="source-code">  data=covidtotals, size=1.5, color=".3", linewidth=0)</p><p class="source-code">plt.title("Boxplots of Total Cases Per Million by </p><p class="source-code">  Region")</p><p class="source-code">plt.xlabel("Cases Per Million")</p><p class="source-code">plt.ylabel("Region")</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 2.9 – Boxplots of Total Cases Per Million by Region " height="450" src="image/B17978_02_009.jpg" width="608"/>
</div>
</div>
<p class="figure-caption">Figure 2.9 – Boxplots of Total Cases Per Million by Region</p>
<p>These <a id="_idIndexMarker217"/>grouped boxplots show just how much the median cases per<a id="_idIndexMarker218"/> million varies by region, from East Africa and East Asia on the low end to Eastern Europe and Western Europe on the high end. Extremely high values for East Asia are <em class="italic">below</em> the first quartile for Western Europe. We should probably avoid drawing too many conclusions beyond that since the counts for most regions (the number of countries) are fairly small.</p>
<p>So far in this chapter, we have focused mainly on bivariate relationships between features, as well as those between a feature and a target. The statistics and visualizations we have generated will inform the modeling we will do. We are already getting a sense of likely features, their influence on targets, and how the distributions of some features change with the values of another feature.</p>
<p>We will explore multivariate relationships in the remaining sections of this chapter. We want to have some sense of how multiple features move together before we begin our modeling. Do some features no longer matter once other features are included? Which observations pull on our parameter estimates more than others, and what are the implications for model fitting? Similarly, which observations are not like the others, because they <a id="_idIndexMarker219"/>either have invalid values or because they seem to be capturing a completely different <a id="_idIndexMarker220"/>phenomenon than the other observations? We will begin to answer those questions in the next three sections. Although we will not get any definitive answers until we construct our models, we can start making difficult modeling decisions by anticipating them.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Using linear regression to identify data points with significant influence</h1>
<p>It is not<a id="_idIndexMarker221"/> unusual to find that a few observations have a surprisingly high degree of influence on our model, our parameter estimates, and our predictions. This may or may not be <a id="_idIndexMarker222"/>desirable. Observations with significant influence may be unhelpful if they reflect a different social or natural process than the rest of the data does. For example, let's say we have a dataset of flying animals that migrate a great distance, and this is almost exclusively bird species, except for data on monarch butterflies. If we are using the wing architecture as a predictor of migration distance, the monarch butterfly data should probably be removed.</p>
<p>We should return to the distinction we made in the first section between an extreme value and an outlier. We mentioned that an outlier can be thought of as an observation with feature values, or relationships between feature values, that are so unusual that they cannot help explain relationships in the rest of the data. An extreme value, on the other hand, may reflect a natural and explainable trend in a feature, or the same relationship between features that has been observed throughout the data.</p>
<p>Distinguishing between an outlier and an extreme value matters most with observations that have a high influence on our model. A standard measure of influence in regression analysis is <strong class="bold">Cook's Distance</strong> (<strong class="bold">Cook's D</strong>). This <a id="_idIndexMarker223"/>gives us a measure of how much our predictions would change if an observation were to be removed from the data. </p>
<p>Let's construct a relatively straightforward multivariate regression model in this section with the<a id="_idIndexMarker224"/> COVID-19 data we have been using, and then generate a Cook's D value for each<a id="_idIndexMarker225"/> observation:</p>
<ol>
<li value="1">Let's load the COVID-19 data and the Matplotlib and statsmodels libraries:<p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import statsmodels.api as sm</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p></li>
<li>Now, let's look at the distribution of total cases per million in population and some possible predictors:<p class="source-code">xvars = ['population_density','aged_65_older',</p><p class="source-code">  'gdp_per_capita','diabetes_prevalence']</p><p class="source-code">covidtotals[['total_cases_mill'] + xvars].\</p><p class="source-code">  quantile(np.arange(0.0,1.05,0.25)) </p><p class="source-code">  <strong class="bold">total_cases_mill  population_density  aged_65_older\</strong></p><p class="source-code"><strong class="bold">0.00             8.52                0.14         1.14</strong></p><p class="source-code"><strong class="bold">0.25         2,499.75               36.52         3.50 </strong></p><p class="source-code"><strong class="bold">0.50        19,525.73               87.25         6.22</strong></p><p class="source-code"><strong class="bold">0.75        64,834.62              213.54        13.92</strong></p><p class="source-code"><strong class="bold">1.00       181,466.38           20,546.77        27.05</strong></p><p class="source-code"><strong class="bold">         gdp_per_capita  diabetes_prevalence  </strong></p><p class="source-code"><strong class="bold">0.00             661.24                 0.99  </strong></p><p class="source-code"><strong class="bold">0.25           3,823.19                 5.34  </strong></p><p class="source-code"><strong class="bold">0.50          12,236.71                 7.20  </strong></p><p class="source-code"><strong class="bold">0.75          27,216.44                10.61  </strong></p><p class="source-code"><strong class="bold">1.00         116,935.60                30.53</strong></p></li>
<li>Next, let's define a function, <strong class="source-inline">getlm</strong>, that uses statsmodels to run a linear regression model and generate influence statistics, including Cook's D. This function <a id="_idIndexMarker226"/>takes a DataFrame, the name of the target column, and the column names for the features (it is customary to refer to a target as <em class="italic">y</em> and features as <em class="italic">X</em>).</li>
</ol>
<p>We<a id="_idIndexMarker227"/> will use <strong class="source-inline">dropna</strong> to drop any observations where one of the features has a missing value. The function returns the estimated coefficients (along with <strong class="source-inline">pvalues</strong>), the influence measures for each observation, and the full regression results (<strong class="source-inline">lm</strong>):</p>
<p class="source-code">def getlm(df, ycolname, xcolnames):</p>
<p class="source-code">  df = df[[ycolname] + xcolnames].dropna()</p>
<p class="source-code">  y = df[ycolname]</p>
<p class="source-code">  X = df[xcolnames]</p>
<p class="source-code">  X = sm.add_constant(X)</p>
<p class="source-code">  lm = sm.OLS(y, X).fit()</p>
<p class="source-code">  influence = lm.get_influence().summary_frame()</p>
<p class="source-code">  coefficients = pd.DataFrame(zip(['constant'] + </p>
<p class="source-code">    xcolnames, lm.params, lm.pvalues),</p>
<p class="source-code">    columns=['features','params','pvalues'])</p>
<p class="source-code">  return coefficients, influence, lm </p>
<ol>
<li value="4">Now, we can call the <strong class="source-inline">getlm</strong> function while specifying the total cases per million as the target and population density (people per square mile), age 65 plus the percentage, GDP per capita, and diabetes prevalence as predictors. Then, we can<a id="_idIndexMarker228"/> print the parameter estimates. Ordinarily, we would want to look at a full <a id="_idIndexMarker229"/>summary of the model, which can be generated with <strong class="source-inline">lm.summary()</strong>. We'll skip that here for ease of understanding:<p class="source-code">coefficients, influence, lm = getlm(covidtotals,</p><p class="source-code">  'total_cases_mill', xvars)</p><p class="source-code">coefficients</p><p class="source-code"><strong class="bold">features                        params       pvalues</strong></p><p class="source-code"><strong class="bold">0  constant                 -1,076.471         0.870</strong></p><p class="source-code"><strong class="bold">1  population_density           -6.906         0.030</strong></p><p class="source-code"><strong class="bold">2  aged_65_older             2,713.918         0.000</strong></p><p class="source-code"><strong class="bold">3  gdp_per_capita                0.532         0.001</strong></p><p class="source-code"><strong class="bold">4  diabetes_prevalence         736.809         0.241</strong></p></li>
</ol>
<p>The coefficients for population density, age 65 plus, and GDP are all significant at the 95% level (have p-values less than 0.05). The result for population density is interesting since our bivariate analysis did not reveal a relationship between population density and cases per million. The coefficient indicates a 6.9-point reduction in cases per million, with a 1-point increase in people per square mile. Put more broadly, more crowded countries have fewer cases per million people once we control for the percentage of people that are 65 or older and their GDP per capita. This could be spurious, or it could be a relationship that can only be detected with multivariate analysis. (It could also be that population density is highly correlated with a feature that has a greater effect on cases per million, but that feature has been left out of the model. This would give us a biased coefficient estimate for population density.)</p>
<ol>
<li value="5">We <a id="_idIndexMarker230"/>can use the influence DataFrame that we created in our call to <strong class="source-inline">getlm</strong> to take a closer look at those observations with a high Cook's D. One way of defining a high Cook's D is by using three times the mean value for Cook's D for all observations. Let's create a <strong class="source-inline">covidtotalsoutliers</strong> DataFrame with all the values above that threshold.</li>
</ol>
<p>There<a id="_idIndexMarker231"/> were 13 countries with Cook's D values above the threshold. Let's print out the first five in descending order of the Cook's D value. Bahrain and Maldives are in the top quarter of the distribution for cases (see the descriptives we printed earlier in this section). They also have high population densities and low percentages of age 65 or older. All else being equal, we would expect lower cases per million for those two countries, given what our model says about the relationship between population density and age to cases. Bahrain does have a very high GDP per capita, however, which our model tells us is associated with high case numbers.</p>
<p>Singapore and Hong Kong have extremely high population densities and below-average cases per million, particularly Hong Kong. These two locations, alone, may account for the direction of the population density coefficient. They both also have very high GDP per capita values, which might be a drag on that coefficient. It may just be that our model should not include locations that are city-states:</p>
<p class="source-code">influencethreshold = 3*influence.cooks_d.mean()</p>
<p class="source-code">covidtotals = covidtotals.join(influence[['cooks_d']])</p>
<p class="source-code">covidtotalsoutliers = \</p>
<p class="source-code">  covidtotals.loc[covidtotals.cooks_d &gt;</p>
<p class="source-code">  influencethreshold]</p>
<p class="source-code">covidtotalsoutliers.shape</p>
<p class="source-code">(13, 17)</p>
<p class="source-code">covidtotalsoutliers[['location','total_cases_mill', </p>
<p class="source-code">  'cooks_d'] + xvars].sort_values(['cooks_d'],</p>
<p class="source-code">  ascending=False).head()</p>
<p class="source-code">     <strong class="bold">location  total_cases_mill   cooks_d    population_density\</strong></p>
<p class="source-code"><strong class="bold">iso_code</strong>               </p>
<p class="source-code"><strong class="bold">BHR  Bahrain        156,793.409     0.230    1,935.907</strong></p>
<p class="source-code"><strong class="bold">SGP  Singapore       10,709.116     0.200    7,915.731</strong></p>
<p class="source-code"><strong class="bold">HKG  Hong Kong        1,593.307     0.181    7,039.714</strong></p>
<p class="source-code"><strong class="bold">JPN  Japan            6,420.871     0.095    347.778</strong></p>
<p class="source-code"><strong class="bold">MDV  Maldives       138,239.027     0.069    1,454.433</strong></p>
<p class="source-code"><strong class="bold">         aged_65_older  gdp_per_capita  diabetes_prevalence  </strong></p>
<p class="source-code"><strong class="bold">iso_code                                                      </strong></p>
<p class="source-code"><strong class="bold">BHR              2.372      43,290.705          16.520</strong></p>
<p class="source-code"><strong class="bold">SGP             12.922      85,535.383          10.990</strong></p>
<p class="source-code"><strong class="bold">HKG             16.303      56,054.920           8.330</strong></p>
<p class="source-code"><strong class="bold">JPN             27.049      39,002.223           5.720</strong></p>
<p class="source-code"><strong class="bold">MDV              4.120      15,183.616           9.190</strong></p>
<ol>
<li value="6">So, let's <a id="_idIndexMarker232"/>take a look at our regression model estimates if we remove Hong Kong <a id="_idIndexMarker233"/>
and Singapore:<p class="callout-heading">Note</p><p class="callout">I am not necessarily recommending this as an approach. We should look at each observation more carefully than we have done so far to determine whether it makes sense to exclude it from our analysis. We are removing the observations here just to demonstrate their effect on the model.</p><p class="source-code">coefficients, influence, lm2 = \</p><p class="source-code">  getlm(covidtotals.drop(['HKG','SGP']),</p><p class="source-code">  'total_cases_mill', xvars)</p><p class="source-code">coefficients</p><p class="source-code">   <strong class="bold">features                  params     pvalues</strong></p><p class="source-code"><strong class="bold">0  constant              -2,864.219       0.653</strong></p><p class="source-code"><strong class="bold">1  population_density        26.989       0.005</strong></p><p class="source-code"><strong class="bold">2  aged_65_older          2,669.281       0.000</strong></p><p class="source-code"><strong class="bold">3  gdp_per_capita             0.553       0.000</strong></p><p class="source-code"><strong class="bold">4  diabetes_prevalence      319.262       0.605</strong></p></li>
</ol>
<p>The <a id="_idIndexMarker234"/>big change in the model is that the population density coefficient has now changed direction. This demonstrates how sensitive the population density estimate is to outlier observations whose feature and target values may not be generalizable to the rest of the data. In this case, that might be true for city-states such as Hong Kong and Singapore.</p>
<p>Generating influence measures with linear regression is a very useful technique, and it has the advantage that it is fairly easy to interpret, as we have seen. However, it does have one important disadvantage: it assumes a linear relationship between features, and that features are normally distributed. This is often not the case. We also needed to understand the relationships in the data enough to create <em class="italic">labels</em>, to identify total cases per million as the target. This is not always possible either. In the next two sections, we'll look at machine learning algorithms for outlier detection that do not make these assumptions.</p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Using K-nearest neighbors to find outliers</h1>
<p>Machine learning tools can help us identify observations that are unlike others when we have unlabeled data – that is, when there is no target or dependent variable. Even when selecting targets and features is relatively straightforward, it might be helpful to identify outliers <a id="_idIndexMarker235"/>without making any assumptions about relationships between features, or the distribution of features. </p>
<p>Although<a id="_idIndexMarker236"/> we typically use <strong class="bold">K-nearest neighbors</strong> (<strong class="bold">KNN</strong>) with<a id="_idIndexMarker237"/> labeled data, for classification or regression problems, we can use it to identify anomalous observations. These are observations where there is the greatest difference between their values and their nearest neighbors' values. KNN is a very popular algorithm because it is intuitive, makes few assumptions about the structure of the data, and is quite flexible. The main disadvantage of KNN is that it is not as efficient as many other approaches, particularly parametric techniques such as linear regression. We will discuss these advantages in much greater detail in <a href="B17978_09_ePub.xhtml#_idTextAnchor113"><em class="italic">Chapter 9</em></a>, <em class="italic">K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient Boosted Regression</em>, and <a href="B17978_12_ePub.xhtml#_idTextAnchor144"><em class="italic">Chapter 12</em></a>, <em class="italic">K-Nearest Neighbors for Classification</em>.</p>
<p>We will use <strong class="bold">PyOD</strong>, short for <strong class="bold">Python outlier detection</strong>, to identify countries in the COVID-19 data that<a id="_idIndexMarker238"/> are significantly different from others. PyOD can use several algorithms to identify outliers, including KNN. Let's get started:</p>
<ol>
<li value="1">First, we need to import the KNN module from PyOD and <strong class="source-inline">StandardScaler</strong> from the <strong class="source-inline">sklearn</strong> preprocessing utility functions. We also load the COVID-19 data:<p class="source-code">import pandas as pd</p><p class="source-code">from pyod.models.knn import KNN</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p></li>
</ol>
<p>Next, we standardize the data, which is important when we have features with very different ranges, from over 100,000 for total cases per million and GDP<a id="_idIndexMarker239"/> per capita to less than 20 for <a id="_idIndexMarker240"/>diabetes prevalence and age 65 and older. We can use scikit-learn's standard scaler, which converts each feature value into a z-score, as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="" height="70" src="image/B17978_02_0011.jpg" width="474"/>
</div>
</div>
<p>Here, <img alt="" height="40" src="image/B17978_02_002.png" width="50"/> is the value for the <em class="italic">i</em>th observation of the <em class="italic">j</em>th feature, <img alt="" height="38" src="image/B17978_02_003.png" width="36"/> is the mean for feature <img alt="" height="42" src="image/B17978_02_004.png" width="21"/>, and <img alt="" height="40" src="image/B17978_02_005.png" width="30"/> is the standard deviation for that feature.</p>
<ol>
<li value="2">We can use the scaler for just the features we will be including in our model, and then drop all observations that are missing values for one or more features:<p class="source-code">standardizer = StandardScaler()</p><p class="source-code">analysisvars =['location', 'total_cases_mill', </p><p class="source-code">  'total_deaths_mill','population_density',</p><p class="source-code">  'diabetes_prevalence', 'aged_65_older', </p><p class="source-code">  'gdp_per_capita']</p><p class="source-code">covidanalysis = </p><p class="source-code">  covidtotals.loc[:,analysisvars].dropna()</p><p class="source-code">covidanalysisstand = </p><p class="source-code">  standardizer.fit_transform(covidanalysis.iloc[:,1:])</p></li>
<li>Now, we can run the model and generate predictions and anomaly scores. First, we must set <strong class="source-inline">contamination</strong> to <strong class="source-inline">0.1</strong> to indicate that we want 10% of observations to be identified as outliers. This is pretty arbitrary but not a bad starting point. After using the <strong class="source-inline">fit</strong> method to run the KNN algorithm, we get predictions (1 if an outlier, 0 if an inlier) and an anomaly score, which is the basis of <a id="_idIndexMarker241"/>the prediction (in this case, the top 10% of anomaly scores will get a prediction of 1):<p class="source-code">clf_name = 'KNN'</p><p class="source-code">clf = KNN(contamination=0.1)</p><p class="source-code">clf.fit(covidanalysisstand)</p><p class="source-code">y_pred = clf.labels_</p><p class="source-code">y_scores = clf.decision_scores_</p></li>
<li>We can <a id="_idIndexMarker242"/>combine the two NumPy arrays with the predictions and anomaly scores – <strong class="source-inline">y_pred</strong> and <strong class="source-inline">y_scores</strong>, respectively – and convert them into the columns of a DataFrame. This makes it easier to view the range of anomaly scores and their associated predictions. 18 countries have been identified as outliers (this is a result of setting <strong class="source-inline">contamination</strong> to <strong class="source-inline">0.1</strong>). Outliers have anomaly scores of 1.77 to 9.34, while inliers have scores of 0.11 to 1.74:<p class="source-code">pred = pd.DataFrame(zip(y_pred, y_scores), </p><p class="source-code">  columns=['outlier','scores'], </p><p class="source-code">  index=covidanalysis.index)</p><p class="source-code">pred.outlier.value_counts()</p><p class="source-code"><strong class="bold">0    156</strong></p><p class="source-code"><strong class="bold">1     18</strong></p><p class="source-code">pred.groupby(['outlier'])[['scores']].\</p><p class="source-code">  agg(['min','median','max'])</p><p class="source-code">               <strong class="bold">scores</strong>            </p><p class="source-code">           <strong class="bold">min      median    max</strong></p><p class="source-code"><strong class="bold">outlier                   </strong></p><p class="source-code"><strong class="bold">0         0.11        0.84   1.74</strong></p><p class="source-code"><strong class="bold">1         1.77        2.48   9.34</strong></p></li>
<li>Let's <a id="_idIndexMarker243"/>take a closer look at<a id="_idIndexMarker244"/> the countries with the highest anomaly scores:<p class="source-code">covidanalysis = covidanalysis.join(pred).\</p><p class="source-code">  loc[:,analysisvars + ['scores']].\</p><p class="source-code">  sort_values(['scores'], ascending=False)</p><p class="source-code">covidanalysis.head(10)</p><p class="source-code">      <strong class="bold">location           total_cases_mill   total_deaths_mill\</strong></p><p class="source-code"><strong class="bold">iso_code</strong>                                                     …  </p><p class="source-code"><strong class="bold">SGP   Singapore                 10,709.12         6.15</strong></p><p class="source-code"><strong class="bold">HKG   Hong Kong                  1,593.31        28.28</strong></p><p class="source-code"><strong class="bold">PER   Peru                      62,830.48     5,876.01</strong></p><p class="source-code"><strong class="bold">QAT   Qatar                     77,373.61       206.87</strong></p><p class="source-code"><strong class="bold">BHR   Bahrain                  156,793.41       803.37</strong></p><p class="source-code"><strong class="bold">LUX   Luxembourg               114,617.81     1,308.36</strong></p><p class="source-code"><strong class="bold">BRN   Brunei                       608.02         6.86</strong></p><p class="source-code"><strong class="bold">KWT   Kuwait                    86,458.62       482.14</strong></p><p class="source-code"><strong class="bold">MDV   Maldives                 138,239.03       394.05</strong></p><p class="source-code"><strong class="bold">ARE   United Arab Emirates      65,125.17       186.75</strong></p><p class="source-code"><strong class="bold">          aged_65_older    gdp_per_capita    scores</strong></p><p class="source-code"><strong class="bold">iso_code                                         </strong></p><p class="source-code"><strong class="bold">SGP               12.92         85,535.38      9.34</strong></p><p class="source-code"><strong class="bold">HKG               16.30         56,054.92      8.03</strong></p><p class="source-code"><strong class="bold">PER                7.15         12,236.71      4.37</strong></p><p class="source-code"><strong class="bold">QAT                1.31        116,935.60      4.23</strong></p><p class="source-code"><strong class="bold">BHR                2.37         43,290.71      3.51</strong></p><p class="source-code"><strong class="bold">LUX               14.31         94,277.96      2.73</strong></p><p class="source-code"><strong class="bold">BRN                4.59         71,809.25      2.60</strong></p><p class="source-code"><strong class="bold">KWT                2.35         65,530.54      2.52</strong></p><p class="source-code"><strong class="bold">MDV                4.12         15,183.62      2.51</strong></p><p class="source-code"><strong class="bold">ARE                1.14         67,293.48      2.45</strong></p></li>
</ol>
<p>Several <a id="_idIndexMarker245"/>of the locations we identified as having high influence in the previous section have high <a id="_idIndexMarker246"/>anomaly scores, including Singapore, Hong Kong, Bahrain, and Maldives. This is more evidence that we need to take a closer look at the data for these countries. Perhaps there is invalid data or there are theoretical reasons why they are very different than the rest of the data.</p>
<p>Unlike the linear model in the previous section, there is no defined target. We include both total cases per million and total deaths per million in this case. Peru has been identified as an outlier here, though it was not with the linear model. This is partly because of Peru's very high deaths per million, which is the highest in the dataset (we did not use deaths per million in our linear regression model).</p>
<ol>
<li value="6">Notice<a id="_idIndexMarker247"/> that Japan is not on this list of outliers. Let's take a look at its anomaly score:<p class="source-code">covidanalysis.loc['JPN','scores']</p><p class="source-code">2.03</p></li>
</ol>
<p>The anomaly score is the 15th highest in the dataset. Compare this with the 4th highest Cook's D score for Japan from the previous section. </p>
<p>It is <a id="_idIndexMarker248"/>interesting to compare these results with a similar analysis we could conduct with Isolation Forest. We will do that in the next section.</p>
<p class="callout-heading">Note</p>
<p class="callout">This has been a very simplified example of the approach we would take with a typical machine learning project. The most important omission here is that we are conducting our analysis on the full dataset. For reasons we will discuss at the beginning of <a href="B17978_04_ePub.xhtml#_idTextAnchor043"><em class="italic">Chapter 4</em></a>, <em class="italic">Encoding, Transforming, and Scaling Features</em>, we want to split our data into training and testing datasets very early in the process. We will learn how to incorporate outlier detection in a machine learning pipeline in the remaining chapters of this book.</p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Using Isolation Forest to find outliers</h1>
<p><strong class="bold">Isolation Forest</strong> is<a id="_idIndexMarker249"/> a relatively new machine learning technique for identifying anomalies. It has quickly become popular, partly because its algorithm is optimized to<a id="_idIndexMarker250"/> find outliers, rather than normal values. It finds outliers by successively partitioning the data until a data point has been isolated. Points that require fewer partitions to be isolated receive higher anomaly scores. This process turns out to be fairly easy on system resources. In this section, we will learn how to use it to detect outlier COVID-19 cases and deaths.</p>
<p>Isolation Forest is <a id="_idIndexMarker251"/>a good alternative to KNN, particularly when we're working with large datasets. The efficiency of the algorithm allows it to handle <a id="_idIndexMarker252"/>large samples and a high number <a id="_idIndexMarker253"/>of features. Let's get started:</p>
<ol>
<li value="1">We can do an analysis similar to the one in the previous section with Isolation Forest rather than KNN. Let's start by loading scikit-learn's <strong class="source-inline">StandardScaler</strong> and <strong class="source-inline">IsolationForest</strong> modules, as well as the COVID-19 data:<p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.ensemble import IsolationForest</p><p class="source-code">covidtotals = pd.read_csv("data/covidtotals.csv")</p><p class="source-code">covidtotals.set_index("iso_code", inplace=True)</p></li>
<li>Next, we must standardize the data:<p class="source-code">analysisvars = ['location','total_cases_mill','total_deaths_mill',</p><p class="source-code">  'population_density','aged_65_older','gdp_per_capita']</p><p class="source-code">standardizer = StandardScaler()</p><p class="source-code">covidanalysis = covidtotals.loc[:, analysisvars].dropna()</p><p class="source-code">covidanalysisstand =</p><p class="source-code">  standardizer.fit_transform(covidanalysis.iloc[:, 1:])</p></li>
<li>Now, we are ready to run our anomaly detection model. The <strong class="source-inline">n_estimators</strong> parameter indicates how many trees to build. Setting <strong class="source-inline">max_features</strong> to <strong class="source-inline">1.0</strong> will use all of our features. The <strong class="source-inline">predict</strong> method gives us the anomaly prediction, which<a id="_idIndexMarker254"/> is <strong class="source-inline">-1</strong> for an anomaly. This is based on the anomaly score, which we<a id="_idIndexMarker255"/> can get using <strong class="source-inline">decision_function</strong>:<p class="source-code">clf=IsolationForest(n_estimators=50, </p><p class="source-code">  max_samples='auto', contamination=.1, </p><p class="source-code">  max_features=1.0)</p><p class="source-code">clf.fit(covidanalysisstand)</p><p class="source-code">covidanalysis['anomaly'] = </p><p class="source-code">  clf.predict(covidanalysisstand)</p><p class="source-code">covidanalysis['scores'] = </p><p class="source-code">  clf.decision_function(covidanalysisstand)</p><p class="source-code">covidanalysis.anomaly.value_counts()</p><p class="source-code"> <strong class="bold">1    156</strong></p><p class="source-code"><strong class="bold">-1     18</strong></p><p class="source-code"><strong class="bold">Name: anomaly, dtype: int6</strong>4</p></li>
<li>Let's take a closer look at the outliers (we will also create a DataFrame of the inliers to use in a later step). We sort by anomaly score and show the countries with the highest (most negative) score. Singapore, Hong Kong, Bahrain, Qatar, and Peru are, again, the most anomalous:<p class="source-code">inlier, outlier = </p><p class="source-code">  covidanalysis.loc[covidanalysis.anomaly==1],\</p><p class="source-code">  covidanalysis.loc[covidanalysis.anomaly==-1]</p><p class="source-code">outlier[['location','total_cases_mill',</p><p class="source-code">  'total_deaths_mill',</p><p class="source-code">  'scores']].sort_values(['scores']).head(10)</p><p class="source-code">     <strong class="bold">location   total_cases_mill  total_deaths_mill   scores</strong></p><p class="source-code"><strong class="bold">iso_code</strong>                                              </p><p class="source-code"><strong class="bold">SGP  Singapore      10,709.12          6.15     -0.20</strong></p><p class="source-code"><strong class="bold">HKG  Hong Kong       1,593.31         28.28     -0.16</strong></p><p class="source-code"><strong class="bold">BHR  Bahrain       156,793.41        803.37     -0.14</strong></p><p class="source-code"><strong class="bold">QAT  Qatar          77,373.61        206.87     -0.13</strong></p><p class="source-code"><strong class="bold">PER  Peru           62,830.48      5,876.01     -0.12</strong></p><p class="source-code"><strong class="bold">LUX  Luxembourg    114,617.81      1,308.36     -0.09</strong></p><p class="source-code"><strong class="bold">JPN  Japan           6,420.87        117.40     -0.08</strong></p><p class="source-code"><strong class="bold">MDV  Maldives      138,239.03        394.05     -0.07</strong></p><p class="source-code"><strong class="bold">CZE  Czechia       155,782.97      2,830.43     -0.06</strong></p><p class="source-code"><strong class="bold">MNE  Montenegro    159,844.09      2,577.77     -0.03</strong></p></li>
<li>It's <a id="_idIndexMarker256"/>helpful to look at a visualization <a id="_idIndexMarker257"/>of the outliers and inliers:<p class="source-code">fig = plt.figure()</p><p class="source-code">ax = plt.axes(projection='3d')</p><p class="source-code">ax.set_title('Isolation Forest Anomaly Detection')</p><p class="source-code">ax.set_zlabel("Cases Per Million (thous.)")</p><p class="source-code">ax.set_xlabel("GDP Per Capita (thous.)")</p><p class="source-code">ax.set_ylabel("Aged 65 Plus %")</p><p class="source-code">ax.scatter3D(inlier.gdp_per_capita/1000,</p><p class="source-code">  inlier.aged_65_older, inlier.total_cases_mill/1000, </p><p class="source-code">  label="inliers", c="blue")</p><p class="source-code">ax.scatter3D(outlier.gdp_per_capita/1000,</p><p class="source-code">  outlier.aged_65_older, </p><p class="source-code">  outlier.total_cases_mill/1000, label="outliers", </p><p class="source-code">  c="red")</p><p class="source-code">ax.legend()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This <a id="_idIndexMarker258"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 2.10 – Isolation Forest Anomaly Detection – GDP Per Capita and Cases Per Million " height="417" src="image/B17978_02_010.jpg" width="404"/>
</div>
</div>
<p class="figure-caption">Figure 2.10 – Isolation Forest Anomaly Detection – GDP Per Capita and Cases Per Million</p>
<p>Although we <a id="_idIndexMarker259"/>are only able to see three dimensions with this visualization, the plot does illustrate some of what makes an outlier an outlier. We expect cases to increase as the GDP per capita and the age 65 plus percentage increase. We can see that the outliers deviate from the expected pattern, having cases per million noticeably above or below countries with similar GDPs and age 65 plus values.</p>
<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Summary</h1>
<p>In this chapter, we used bivariate and multivariate statistical techniques and visualizations to get a better sense of bivariate relationships among features. We looked at common statistics, such as the Pearson correlation. We also examined bivariate relationships through visualizations, with scatter plots when both features are continuous, and with grouped boxplots when one feature is categorical. The last three sections of this chapter explored multivariate techniques for examining relationships and identifying outliers, including machine learning algorithms such as KNN and Isolation Forest.</p>
<p>Now that we have a good sense of the distribution of our data, we are ready to start engineering our features, including imputing missing values and encoding, transforming, and scaling our variables. This will be our focus for the next two chapters.</p>
</div>
</div>
</body></html>