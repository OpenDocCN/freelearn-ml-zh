<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Emotion Detection with CNNs</h1>
                </header>
            
            <article>
                
<p class="mce-root">Up until recently, interacting with a computer was not too dissimilar from interacting with, say, a power tool; we pick it up, turn it on, manually control it, and then put it down until the next time we require it for that specific task. But recently, we are seeing signs that this is about to change; computers allow natural forms of interaction and are becoming more ubiquitous, more capable, and more ingrained in our daily lives. They are becoming less like heartless dumb tools and more like friends, able to entertain us, look out for us, and assist us with our work.</p>
<p class="mce-root">With this shift comes a need for computers to be able to understand our emotional state. For example, you don't want your social robot cracking a joke after you arrive back from work having lost your job (to an AI bot!). This is a field of computer science known as <strong>affective computing</strong> (also referred to as <strong>artificial emotional intelligence</strong> or <strong>emotional AI</strong>), a field that studies systems that can recognize, interpret, process, and simulate human emotions. The first stage of this is being able to recognize emotional state, which is the topic of this chapter. We will first introduce the data and model we will be using, and then walk through how we approach the problem of expression recognition on the iPhone and how to appropriately preprocess the data for inference.</p>
<p class="mce-root">By the end of of this chapter, you will have achieved the following:</p>
<ul>
<li>Built a simple application that will infer your mood in real time using the front camera feed</li>
<li>Gained hands-on experience using the <kbd>Vision</kbd> framework</li>
<li>Developed a deeper understanding and intuition of how <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) work and how they can be applied at the edge</li>
</ul>
<p class="mce-root">Let's start by introducing the data and model we will be using. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Facial expressions</h1>
                </header>
            
            <article>
                
<p>Our face is one of the strongest indicators of emotions; as we laugh or cry, we put our emotions on display, allowing others to glimpse into our minds. It's a form of nonverbal communication that, apparently, accounts for over 50% of our communication with others. Forty independently controlled muscles make the face one of the most complex systems we possess, which could be the reason we use it as a medium for communicating something so important as our current emotional state. But can we classify it?</p>
<p>In 2013, the <strong>International Conference on Machine Learning</strong> (<strong>ICML</strong>) ran a competition inviting contestants to build a facial expression classifier using a training dataset of over 28,000 grayscale images. They were labeled as either anger, disgust, fear, happiness, sadness, surprise, or neutral. The following are a few samples of this training data (available at <a href="https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge">https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge</a>):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b20a9c86-91b5-48b3-bde6-93ab67ba7fed.png"/></div>
<p>As previously mentioned, the training dataset consists of 28,709 grayscale images of faces in 48 x 48 pixels, where each face is centered and associated with a label defining the assigned emotion. This emotion can be one of the following labels (textual description was added for legibility):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/a4ac4f1b-3cd4-480c-890e-24af9138d668.png" style="width:48.67em;height:27.92em;"/></div>
<p>Neural networks (or any other machine learning algorithm) can't really do anything by themselves. All a <span>neural network does is find a direct or indirect correlation between two datasets (inputs and their corresponding outputs). In order for a neural network to learn, we need to present it with two meaningful datasets where some true correlation exists between the inputs and outputs. A good practice when tackling any new data problem is to come up with a predictive theory of how you might approach it or search for correlation using techniques such as data visualization or some other explorational data analysis technique. In doing so, we also better understand how we need to prepare our data to align it with the training data.</span></p>
<p><span>Let's look at the results of a data visualization technique that can be performed on the training data; here, it's our assumption that some pattern exists between each expression (happy, sad, angry, and so on). One way of visually inspecting this is by averaging each expression and the associated variance. This can be achieved simply by finding the mean and standard deviation across all images for their respective class (expression example, happy, angry, and so on). The results of some of the expressions can be seen in the following image: </span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/34542a2e-b0a6-41d2-8f6c-f909ea917e36.png"/></div>
<p>After you get over the creepiness of the images, you get a sense that a pattern does exist, and you understand what our model needs to learn to be able to recognize facial expressions. Some other notable, and fairly visible, takeaways from this exercise include the amount of variance with the disgust expression; this hints that our model might find it difficult to effectively learn to recognize this expression. The other <span>observation - </span>and the one more applicable to our task in this chapter - is that the training data consists of forward-facing faces with little padding beyond the face, therefore highlighting what the model expects for its input. Now that we have a better sense of our data; let's move on and introduce the model we will be using in this chapter. </p>
<p>In <a href="5cf26de5-5f92-4d1d-8b83-3e28368df233.xhtml" target="_blank">chapter 3</a>, <em>Recognising Objects in the World,</em> we presented the intuition behind CNNs or ConvNets. So, given that we won't be introducing any new concepts in this chapter, we will omit any discussion on the details of the model and just present it here for reference, with some commentary about its architecture and the format of the data it is expecting for its input:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/3ca0c020-6418-4e82-9f76-bce205f70ae9.png"/></div>
<p>The preceding figure is a visualization of the architecture of the model; it's your typical CNN, with a stack of convolutional and pooling layers before being flattened and fed into a series of fully connected layers. Finally, it is fed into a softmax activation layer for multi-class classification. As mentioned earlier, the model is expecting a 3D tensor with the dimensions 48 x 48 x 1 (width, height, channels). To avoid feeding our model with large numbers (0 - 255), the input has been normalized (dividing each pixel by 255, which gives us a range of 0.0 - 1.0). The model outputs the probability of a given input with respect to each class, that is, seven outputs with each class representing the probability of how likely it is correlated for the given input. To make a prediction, we simply take the class with the largest probability. </p>
<p>This model was trained on 22,967 samples, reversing the other 5,742 samples for validation. After 15 epochs, the model achieved approximately 59% accuracy on the validation set, managing to squeeze into the 13<sup>th</sup> place of the Kaggle competition (at the time of writing this chapter). The following graphs show the training accuracy and loss during training:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/4cf6c2bf-9e4e-43e8-8474-86c3fd69e450.png"/></div>
<p>This concludes our brief introduction of the data and model we will be using for this chapter. The two main takeaways are an appreciation of what data the model has been fed during training, and the fact that our model achieved just 59% accuracy.</p>
<p>The former dictates how we approach obtaining and process the data before feeding it into the model. The latter poses an opportunity for further investigation to better understand what is pulling the accuracy down and how to improve it; it also can be seen as a design challenge—a design to be made around this constraint.</p>
<p>In this chapter, we are mainly concerned with the former so, in the next section, we will explore how to obtain and preprocess the data before feeding it to the model. Let's get started. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input data and preprocessing </h1>
                </header>
            
            <article>
                
<p>In this section, we will implement the preprocessing functionality required to transform images into something the model is expecting. We will build up this functionality in a playground project before migrating it across to our project in the next section.</p>
<p>If you haven't done so already, pull down the latest code from the accompanying repository:<span> <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a></span>. Once downloaded, navigate to the directory <kbd>Chapter4/Start/</kbd> and open the Playground project <kbd>ExploringExpressionRecognition.playground</kbd>. Once loaded, you will see the playground for this chapter, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bdcff650-0176-4b84-910f-8f1e8913e55a.png"/></div>
<p>Before starting, to avoid looking at images of me, please replace the test images with either personal photos of your own or royalty free images from the internet, ideally a set expressing a range of emotions.</p>
<p>Along with the test images, this playground includes a compiled Core ML model (we introduced it in the previous image) with its generated set of wrappers for inputs, outputs, and the model itself. Also included are some extensions for <kbd>UIImage</kbd>, <kbd>UIImageView</kbd>, <kbd>CGImagePropertyOrientation</kbd>, and an empty <kbd>CIImage</kbd> extension, to which we will return later in the chapter. The others provide utility functions to help us visualize the images as we work through this playground. </p>
<p>Before jumping into the code, let's quickly discuss the approach we will take in order to determine what we actually <span>need to </span>implement.</p>
<p>Up to this point, our process of performing machine learning has been fairly straightforward; apart from some formatting of input data, our model didn't require too much work. This is not the case here. A typical photo of someone doesn't normally have just a face, nor is their face nicely aligned to the frame unless you're processing passport photos. When developing machine learning applications, you have two broad paths.</p>
<p>The first, which is becoming increasingly popular, is to use an end-to-end machine learning model capable of just being fed the raw input and producing adequate results. One particular field that has had great success with end-to-end models is speech recognition. Prior to end-to-end deep learning, speech recognition systems were made up of many smaller modules, each one focusing on extracting specific pieces of data to feed into the next module, which was typically manually engineered. Modern speech recognition systems use end-to-end models that take the raw input and output the result. Both of the described approaches can been seen in the following diagram: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/11db396a-0e52-4e50-85c4-0f1d067eb6e1.png"/></div>
<p>Obviously, this approach is not constrained to speech recognition and we have seen it applied to image recognition tasks, too, along with many others. But there are two things that make this particular case different; the first is that we can simplify the problem by first extracting the face. This means our model has less features to learn and offers a smaller, more specialized model that we can tune. The second thing, which is no doubt obvious, is that our training data consisted of <span>only </span>faces and not natural images. So, we have no other choice but to run our data through two models, the first to extract faces and the second to perform expression recognition on the extracted faces, as shown in this diagram: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c2d9524b-d2e3-40d2-a2e0-323d9fca879f.png" style="width:41.17em;height:18.92em;"/></div>
<p>Luckily for us, Apple has mostly taken care of our first task of detecting faces through the <kbd>Vision</kbd> framework it released with iOS 11. The <kbd>Vision</kbd> framework provides performant image analysis and computer vision tools, exposing them through a simple API. This allows for face detection, feature detection and tracking, and classification of scenes in images and video. The latter (expression recognition) is something we will take care of using the Core ML model introduced earlier.</p>
<div class="packt_infobox">Prior to the introduction of the <kbd>Vision</kbd> framework, face detection would typically be performed using the Core Image filter. Going back further, you had to use something like OpenCV. You can learn more about Core Image here: <a href="https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html">https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html</a>.</div>
<p>Now that we have got a bird's-eye view of the work that needs to be done, let's turn our attention to the editor and start putting all of this together. Start by loading the images; add the following snippet to your playground:</p>
<pre>var images = [UIImage]()<br/>for i in 1...3{<br/>    guard let image = UIImage(named:"images/joshua_newnham_\(i).jpg")<br/>        else{ fatalError("Failed to extract features") }<br/>    <br/>    images.append(image)<br/>}<br/><br/>let faceIdx = 0 <br/>let imageView = UIImageView(image: images[faceIdx])<br/>imageView.contentMode = .scaleAspectFit</pre>
<p>In the preceding snippet, we are simply loading each of the images we have included in our resources' <kbd>Images</kbd> folder and adding them to an array we can access conveniently throughout the playground. Once all the images are loaded, we set the constant <kbd>faceIdx</kbd>, which will ensure that we access the same images throughout our experiments. Finally, we create an <kbd>ImageView</kbd> to easily preview it. Once it has finished running, click on the eye icon in the right-hand panel to preview the loaded image, as shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/b30cea31-9392-46f5-a018-527fa1fdb3b5.png"/></div>
<p><span>Next,</span><span> w</span>e will take advantage of the functionality available in the <kbd>Vision</kbd> framework to detect faces. The typical flow when working with the <kbd>Vision</kbd> framework is <strong>defining a request</strong>, which determines what analysis you want to perform, and <strong>defining the handler</strong>, which will be responsible for executing the request and providing means of obtaining the results (either through delegation or explicitly queried). The result of the analysis is a collection of observations that you need to cast into the appropriate observation type; concrete examples of each of these can be seen here: </p>
<p>As illustrated in the preceding diagram, the request determines what type of image analysis will be performed; the handler, using a request or multiple requests and an image, performs the actual analysis and generates the results (also known as <strong>observations</strong>). These are accessible via a property or delegate if one has been assigned. The type of observation is dependent on the request performed; it's worth highlighting that the <kbd>Vision</kbd> framework is tightly integrated into Core ML and provides another layer of abstraction and uniformity between you and the data and process. For example, using a classification Core ML model would return an observation of type <kbd>VNClassificationObservation</kbd>. This layer of abstraction not only simplifies things but also provides a consistent way of working with machine learning models.</p>
<p>In the previous figure, we showed a request handler specifically for static images. <kbd>Vision</kbd> also provides a specialized request handler for handling sequences of images, which is more appropriate when dealing with requests such as tracking. The following diagram illustrates some concrete examples of the types of requests and observations applicable to this use case:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/baf94581-ad26-4ebd-9bf2-0c2575426b4a.png"/></div>
<p>So, when do you use <kbd>VNImageRequestHandler</kbd> and <kbd>VNSequenceRequestHandler</kbd>? Though the names provide clues as to when one should be used over the other, it's worth outlining some differences. </p>
<p>The image request handler is for interactive exploration of an image; it holds a reference to the image for its life cycle and allows optimizations of various request types. The sequence request handler is more appropriate for performing tasks such as tracking and does not optimize for multiple requests on an image.</p>
<p>Let's see how this all looks in code; add the following snippet to your playground:</p>
<pre>let faceDetectionRequest = VNDetectFaceRectanglesRequest()<br/>let faceDetectionRequestHandler = VNSequenceRequestHandler()</pre>
<p>Here, we are simply creating the request and handler; as discussed in the preceding code, the request encapsulates the type of image analysis while the handler is responsible for executing the request. Next, we will get <kbd>faceDetectionRequestHandler</kbd> to run <kbd>faceDetectionRequest</kbd>; add the following code:</p>
<pre>try? faceDetectionRequestHandler.perform(<br/>    [faceDetectionRequest],<br/>    on: images[faceIdx].cgImage!,<br/>    orientation: CGImagePropertyOrientation(images[faceIdx].imageOrientation)) </pre>
<p>The <kbd>perform</kbd> function of the handler can throw an error if it fails; for this reason, we wrap the call with <kbd>try?</kbd> at the beginning of the statement and can interrogate the <kbd>error</kbd> property of the handler to identify the reason for failing. We pass the handler a list of requests (in this case, only our <kbd>faceDetectionRequest</kbd>), the image we want to perform the analysis on, and, finally, the orientation of the image that can be used by the request during analysis.</p>
<p>Once the analysis is done, we can inspect the observation obtained through the <kbd>results</kbd> property of the request itself, as shown in the following code: </p>
<pre>if let faceDetectionResults = faceDetectionRequest.results as? [VNFaceObservation]{<br/>    for face in faceDetectionResults{<br/><strong>        // ADD THE NEXT SNIPPET OF CODE HERE</strong><br/>    }<br/>}</pre>
<p>The type of observation is dependent on the analysis; in this case, we're expecting a <kbd>VNFaceObservation</kbd>. Hence, we cast it to the appropriate type and then iterate through all the observations.</p>
<p>Next, we will take each recognized face and extract the bounding box. Then, we'll proceed to draw it in the image (using an extension method of <kbd>UIImageView</kbd> found within the <kbd>UIImageViewExtension.swift</kbd> file). Add the following block within the <kbd>for</kbd> loop shown in the preceding code:</p>
<pre>if let currentImage = imageView.image{<br/>    let bbox = face.boundingBox<br/>    <br/>    let imageSize = CGSize(<br/>        width:currentImage.size.width,<br/>        height: currentImage.size.height)<br/>    <br/>    let w = bbox.width * imageSize.width<br/>    let h = bbox.height * imageSize.height<br/>    let x = bbox.origin.x * imageSize.width<br/>    let y = bbox.origin.y * imageSize.height<br/>    <br/>    let faceRect = CGRect(<br/>        x: x,<br/>        y: y,<br/>        width: w,<br/>        height: h)<br/>    <br/>    let invertedY = imageSize.height - (faceRect.origin.y + faceRect.height)<br/>    let invertedFaceRect = CGRect(<br/>        x: x,<br/>        y: invertedY,<br/>        width: w,<br/>        height: h)<br/>    <br/>    imageView.drawRect(rect: invertedFaceRect)<br/>}</pre>
<p>We can obtain the bounding box of each face via the let <kbd>boundingBox</kbd> property; the result is normalized, so we then need to scale this based on the dimensions of the image. For example, you can obtain the width by multiplying <kbd>boundingBox</kbd> with the width of the image: <kbd>bbox.width * imageSize.width</kbd>.</p>
<p>Next, we invert the <em>y </em>axis as the coordinate system of Quartz 2D is inverted with respect to that of UIKit's coordinate system, as shown in this diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/0d810f17-d915-44c5-98f3-557955090048.png" style="width:42.42em;height:20.75em;"/> </div>
<p>We invert our coordinates by subtracting the bounding box's origin and height from height of the image and then passing this to our <kbd>UIImageView</kbd> to render the rectangle. Click on the eye icon in the right<span>-hand </span>panel in line with the statement <kbd>imageView.drawRect(rect: invertedFaceRect)</kbd> to preview the results; if successful, you should see something like the following:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/4e9221cd-d50f-44f4-89d7-1b6bfad5379b.png" style="width:20.42em;height:20.83em;"/></div>
<div class="packt_infobox">An alternative to inverting the face rectangle would be to use an <kbd>AfflineTransform</kbd>, such as:<br/>
<kbd>var transform = CGAffineTransform(scaleX: 1, y: -1)</kbd><br/>
<kbd>transform = transform.translatedBy(x: 0, y: -imageSize.height)</kbd><br/>
<kbd>let invertedFaceRect = faceRect.apply(transform)<br/></kbd><br/>
This approach leads to less code and therefore less chances of errors. So, it is the recommended approach. The long approach was taken previously to help illuminate the details.</div>
<p>Let's now take a quick detour and experiment with another type of request; this time, we will analyze our image using <kbd>VNDetectFaceLandmarksRequest</kbd>. It is similar to <kbd>VNDetectFaceRectanglesRequest</kbd> in that this request will detect faces and expose their bounding boxes; but, unlike <kbd>VNDetectFaceRectanglesRequest</kbd>, <kbd>VNDetectFaceLandmarksRequest</kbd> also provides detected facial landmarks. A landmark is a prominent facial feature such as your eyes, nose, eyebrow, face contour, or any other feature that can be detected and describes a significant attribute of a face. Each detected facial landmark consists of a set of points that describe its contour (outline). Let's see how this looks; add a new request as shown in the following code:</p>
<pre><strong>imageView.image = images[faceIdx]</strong><br/><br/><strong>let faceLandmarksRequest = VNDetectFaceLandmarksRequest()</strong><br/><br/>try? faceDetectionRequestHandler.perform(<br/>    [<strong>faceLandmarksRequest</strong>],<br/>    on: images[faceIdx].cgImage!,<br/>    orientation: CGImagePropertyOrientation(images[faceIdx].imageOrientation))</pre>
<p>The preceding snippet should look familiar to you; it's almost the same as what we did previously, but this time replacing <kbd>VNDetectFaceRectanglesRequest</kbd> with <kbd>VNDetectFaceLandmarksRequets</kbd>. We have also refreshed the image in our image view with the statement <kbd>imageView.image = images[faceIdx]</kbd>. As we did before, let's iterate through each of the detected observations and extract some of the common landmarks. Start off by creating the outer loop, as shown in this code:</p>
<pre>if let faceLandmarkDetectionResults = faceLandmarksRequest.results as? [VNFaceObservation]{<br/>    for face in faceLandmarkDetectionResults{<br/>        if let currentImage = imageView.image{<br/>            let bbox = face.boundingBox<br/>            <br/>            let imageSize = CGSize(width:currentImage.size.width,<br/>                                   height: currentImage.size.height)<br/>            <br/>            let w = bbox.width * imageSize.width<br/>            let h = bbox.height * imageSize.height<br/>            let x = bbox.origin.x * imageSize.width<br/>            let y = bbox.origin.y * imageSize.height<br/>            <br/>            let faceRect = CGRect(x: x,<br/>                                  y: y,<br/>                                  width: w,<br/>                                  height: h)<br/>                                    <br/>        }<br/>    }<br/>}</pre>
<p>Up to this point, the code will look familiar; next, we will look at each of the landmarks. But first, let's create a function to handle the transformation of our points from the Quartz 2D coordinate system to UIKit's coordinate system. We add the following function but within the same block as our <kbd>faceRect</kbd> declaration:</p>
<pre>func getTransformedPoints(<br/>    landmark:VNFaceLandmarkRegion2D,<br/>    faceRect:CGRect,<br/>    imageSize:CGSize) -&gt; [CGPoint]{<br/>    <br/>    return landmark.normalizedPoints.map({ (np) -&gt; CGPoint in<br/>        return CGPoint(<br/>            x: faceRect.origin.x + np.x * faceRect.size.width,<br/>            y: imageSize.height - (np.y * faceRect.size.height + faceRect.origin.y))<br/>    })<br/>} </pre>
<p>As mentioned before, each landmark consists of a set of points that describe the contour of that particular landmark, and, like our previous feature, the points are normalized between 0.0 - 1.0. Therefore, we need to scale them based on the associated face rectangle, which is exactly what we did in the preceding example. For each point, we are scaling and transforming it into the appropriate coordinate space, and then returning the mapped array to the caller. </p>
<p>Let's now define some constants that we will use to visualize each landmark; we add the following two constants in the function we implemented just now, <kbd>getTransformedPoints</kbd>:</p>
<pre>let landmarkWidth : CGFloat = 1.5<br/>let landmarkColor : UIColor = UIColor.red </pre>
<p>We will now step through a few of the landmarks, showing how we extract the features and occasionally showing the result. Let's start with the left eye and right eye; continue adding the following code just after the constants you just defined:</p>
<pre>if let landmarks = face.landmarks?.leftEye {<br/>    let transformedPoints = getTransformedPoints(<br/>        landmark: landmarks,<br/>        faceRect: faceRect,<br/>        imageSize: imageSize)<br/>    <br/>    imageView.drawPath(pathPoints: transformedPoints,<br/>                       closePath: true,<br/>                       color: landmarkColor,<br/>                       lineWidth: landmarkWidth,<br/>                       vFlip: false)<br/>    <br/>    var center = transformedPoints<br/>        .reduce(CGPoint.zero, { (result, point) -&gt; CGPoint in<br/>        return CGPoint(<br/>            x:result.x + point.x,<br/>            y:result.y + point.y)<br/>    })<br/>    <br/>    center.x /= CGFloat(transformedPoints.count)<br/>    center.y /= CGFloat(transformedPoints.count)<br/>    imageView.drawCircle(center: center,<br/>                         radius: 2,<br/>                         color: landmarkColor,<br/>                         lineWidth: landmarkWidth,<br/>                         vFlip: false)<br/>}<br/><br/>if let landmarks = face.landmarks?.rightEye {<br/>    let transformedPoints = getTransformedPoints(<br/>        landmark: landmarks,<br/>        faceRect: faceRect,<br/>        imageSize: imageSize)<br/>    <br/>    imageView.drawPath(pathPoints: transformedPoints,<br/>                       closePath: true,<br/>                       color: landmarkColor,<br/>                       lineWidth: landmarkWidth,<br/>                       vFlip: false)<br/>    <br/>    var center = transformedPoints.reduce(CGPoint.zero, { (result, point) -&gt; CGPoint in<br/>        return CGPoint(<br/>            x:result.x + point.x,<br/>            y:result.y + point.y)<br/>    })<br/>    <br/>    center.x /= CGFloat(transformedPoints.count)<br/>    center.y /= CGFloat(transformedPoints.count)<br/>    imageView.drawCircle(center: center,<br/>                         radius: 2,<br/>                         color: landmarkColor,<br/>                         lineWidth: landmarkWidth,<br/>                         vFlip: false)<br/>} </pre>
<p>Hopefully, as is apparent from the preceding code snippet, we get a reference to each of the landmarks by interrogating the face observations landmark property, which itself references the appropriate landmark. In the preceding code, we get reference to the landmarks <kbd>leftEye</kbd> and <kbd>rightEye</kbd>. And for each, we first render the contour of the eye, as shown in this screenshot: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/11105948-31bf-4caa-8a12-e6371e1fbeac.png" style="width:15.58em;height:15.83em;"/></div>
<p>Next, we iterate through each of the points to find the center of the eye and render a circle using the following code:</p>
<pre>var center = transformedPoints<br/>    .reduce(CGPoint.zero, { (result, point) -&gt; CGPoint in<br/>    return CGPoint(<br/>        x:result.x + point.x,<br/>        y:result.y + point.y)<br/>})<br/><br/>center.x /= CGFloat(transformedPoints.count)<br/>center.y /= CGFloat(transformedPoints.count)<br/>imageView.drawCircle(center: center,<br/>                     radius: 2,<br/>                     color: landmarkColor,<br/>                     lineWidth: landmarkWidth,<br/>                     vFlip: false)</pre>
<p>This is slightly unnecessary as one of the landmarks available is <kbd>leftPupil</kbd>, but I wanted to use this instance to highlight the importance of inspecting the available landmarks. The next half of the block is concerned with performing the same tasks for the right eye; by the end of it, you should have an image resembling something like the following, with both eyes drawn:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/626871f2-e4cf-4623-844e-a85119d87c41.png" style="width:18.25em;height:18.67em;"/> </div>
<p>Let's continue highlighting some of the landmarks available. Next, we will inspect the face contour and nose; add the following code:</p>
<pre>if let landmarks = face.landmarks?.faceContour {<br/>    let transformedPoints = getTransformedPoints(<br/>        landmark: landmarks,<br/>        faceRect: faceRect,<br/>        imageSize: imageSize)<br/>    <br/>    imageView.drawPath(pathPoints: transformedPoints,<br/>                       closePath: false,<br/>                       color: landmarkColor,<br/>                       lineWidth: landmarkWidth,<br/>                       vFlip: false)<br/>}<br/><br/>if let landmarks = face.landmarks?.nose {<br/>    let transformedPoints = getTransformedPoints(<br/>        landmark: landmarks,<br/>        faceRect: faceRect,<br/>        imageSize: imageSize)<br/>    <br/>    imageView.drawPath(pathPoints: transformedPoints,<br/>                       closePath: false,<br/>                       color: landmarkColor,<br/>                       lineWidth: landmarkWidth,<br/>                       vFlip: false)<br/>}<br/><br/>if let landmarks = face.landmarks?.noseCrest {<br/>    let transformedPoints = getTransformedPoints(<br/>        landmark: landmarks,<br/>        faceRect: faceRect,<br/>        imageSize: imageSize)<br/>    <br/>    imageView.drawPath(pathPoints: transformedPoints,<br/>                       closePath: false,<br/>                       color: landmarkColor,<br/>                       lineWidth: landmarkWidth,<br/>                       vFlip: false)<br/>}</pre>
<p>The patterns should be obvious now; here we can draw the landmarks <kbd>faceContour</kbd>, <kbd>nose</kbd>, and <kbd>noseCrest</kbd>; with that done, your image should look something like the following:</p>
<p>As an exercise, draw the lips (and any other facial landmark) using the landmarks <kbd>innerLips</kbd> and <kbd>outerLips</kbd>. With that implemented, you should end up with something like this:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/7c95fd6d-891b-4152-a42d-3de1698b3c12.png" style="width:20.42em;height:20.42em;"/></div>
<p>Before returning to our task of classifying facial expressions, let's quickly finish our detour with some practical uses for landmark detection (other than drawing <span>or placing glasses </span>on a face).</p>
<p>As highlighted earlier, our training set consists of images that are predominantly forward-facing and orientated fairly straight. With this in mind, one practical use of knowing the position of each eye is being able to qualify an image; that is, is the face sufficiently in view and orientated correctly? Another use would be to slightly realign the face so that it fits in better with your training set (keeping in mind that our images are reduced to 28 x 28, so some detriment to quality can be ignored).</p>
<p>For now, I'll leave the implementation of these to you but, by using the angle between the two eyes, you can apply an affine transformation to correct the orientation, that is, rotate the image.</p>
<p>Let's now return to our main task of classification; as we did before, we will create a <kbd>VNDetectFaceRectanglesRequest</kbd> request to handle identifying each face within a given image and, for each face, we will perform some preprocessing before feeding it into our model. If you recall our discussion on the model, our model is expecting a single-channel (grayscale) image of a face with the size 48 x 48 and its values normalized between 0.0 and 1.0. Let's walk through each part of the task piece by piece, starting with creating the request, as we did previously:</p>
<pre>imageView.image = images[faceIdx]<br/><strong>let model = ExpressionRecognitionModelRaw()</strong><br/><br/>if let faceDetectionResults = faceDetectionRequest.results as? [VNFaceObservation]{<br/>    for face in faceDetectionResults{<br/>        if let currentImage = imageView.image{<br/>            let bbox = face.boundingBox<br/>            <br/>            let imageSize = CGSize(width:currentImage.size.width,<br/>                                   height: currentImage.size.height)<br/>            <br/>            let w = bbox.width * imageSize.width<br/>            let h = bbox.height * imageSize.height<br/>            let x = bbox.origin.x * imageSize.width<br/>            let y = bbox.origin.y * imageSize.height<br/>            <br/>            let faceRect = CGRect(x: x,<br/>                                  y: y,<br/>                                  width: w,<br/>                                  height: h)                        <br/>        }<br/>    }<br/>}</pre>
<p>The preceding code should look familiar to you now, with the only difference <span>being</span><span> </span><span>the instantiation of our model (<span>the bold statement</span>): </span><kbd>let model = ExpressionRecognitionModelRaw()</kbd><span>. Next, we want to crop out the face from the image; in order to do this, we will need to write a utility function that will implement this. Since we want to carry this over to our application, let's write it as an extension of the</span> <kbd>CIImage</kbd> <span>class. Click on the</span> <kbd>CIImageExtension.swift</kbd> <span>file within the</span> <kbd>Sources</kbd> <span>folder in the left</span><span>-hand </span><span>panel to open up the relevant file; currently, this file is just an empty extension body, as shown in the following code: </span></p>
<pre>extension CIImage{<br/>}</pre>
<p>Go ahead and add the following snippet of code within the body of <kbd>CIImage</kbd> to implement the functionality of cropping:</p>
<pre>public func crop(rect:CGRect) -&gt; CIImage?{<br/>    let context = CIContext()<br/>    guard let img = context.createCGImage(self, from: rect) else{<br/>        return nil<br/>    }<br/>    return CIImage(cgImage: img)<br/>}</pre>
<p>In the preceding code, we are simply creating a new image of itself constrained to the region passed in; this method, <kbd>context.createCGImage</kbd>, returns a <kbd>CGImage</kbd>, which we then wrap in a <kbd>CIImage</kbd> before returning to the caller. With our crop method taken care of, we return to our main playground source and add the following snippet after the face rectangle declared previously to crop a face from our image:</p>
<pre>let ciImage = CIImage(cgImage:images[faceIdx].cgImage!)<br/><br/>let cropRect = CGRect(<br/>    x: max(x - (faceRect.width * 0.15), 0),<br/>    y: max(y - (faceRect.height * 0.1), 0),<br/>    width: min(w + (faceRect.width * 0.3), imageSize.width),<br/>    height: min(h + (faceRect.height * 0.6), imageSize.height))<br/><br/>guard let croppedCIImage = ciImage.crop(rect: cropRect) else{<br/>    fatalError("Failed to cropped image")<br/>} </pre>
<p>We first create an instance of <kbd>CIImage</kbd> from <kbd>CGImage</kbd> (referenced by the <kbd>UIImage</kbd> instance); we then pad out our face rectangle. The reason for doing this is to better match it with our training data; if you refer to our previous experiments, the detected bounds fit tightly around the eyes and chin while our training data encompasses a more holistic view of the face. The numbers selected were through trial and error, but I imagine there is some statistically relevant ratio between the distance between the eyes and height of the face—maybe. We finally crop our image using the <kbd>crop</kbd> method we implemented earlier.</p>
<p>Next, we will resize the image (to the size the model is expecting) but, once again, this functionality is not yet available. So, our next task! Jump back into the <kbd>CIImageExtension.swift</kbd> file and add the following method to handle resizing:</p>
<pre>public func resize(size: CGSize) -&gt; CIImage {<br/>    let scale = min(size.width,size.height) / min(self.extent.size.width, self.extent.size.height)<br/>    <br/>    let resizedImage = self.transformed(<br/>        by: CGAffineTransform(<br/>            scaleX: scale,<br/>            y: scale))<br/>    <br/>    let width = resizedImage.extent.width<br/>    let height = resizedImage.extent.height<br/>    let xOffset = (CGFloat(width) - size.width) / 2.0<br/>    let yOffset = (CGFloat(height) - size.height) / 2.0<br/>    let rect = CGRect(x: xOffset,<br/>                      y: yOffset,<br/>                      width: size.width,<br/>                      height: size.height)<br/>    <br/>    return resizedImage<br/>        .clamped(to: rect)<br/>        .cropped(to: CGRect(<br/>            x: 0, y: 0,<br/>            width: size.width,<br/>            height: size.height))<br/>}</pre>
<div class="packt_infobox"><span>You may notice that we are not inverting the face rectangle here as we did before; the reason is that we were only required to do this to transform from the Quartz 2D coordinate system to UIKit's coordinate system, which we are not doing here.</span></div>
<p>Despite the number of lines, the majority of the code is concerned with calculating the scale and translation required to center it. Once we have calculated these, we simply pass in a <kbd>CGAffineTransform</kbd>, with our scale, to the <kbd>transformed</kbd> method and then our centrally aligned rectangle to the <kbd>clamped</kbd> method. With this now implemented, let's return to our main playground code and make use of it by resizing our cropped image, as shown in the following lines:</p>
<pre>let resizedCroppedCIImage = croppedCIImage.resize(<br/>    size: CGSize(width:48, height:48))</pre>
<p>Three more steps are required before we can pass our data to our model for inference. The first is to convert it to a single channel, the second is to rescale the pixels so that they are between the values of 0.0 and 1.0, and finally we wrap it in a <kbd>MLMultiArray</kbd>, which we can then feed into our model's <kbd>predict</kbd> method. To achieve the previous, we will add another extension to our <kbd>CIImage</kbd> class. It will render out the image using a single channel, along with extracting the pixel data and returning it in an array, which we can then easily access for rescaling. Jump back into the <kbd>CIImageExtension.swift</kbd> file and add the following method:</p>
<pre>public func getGrayscalePixelData() -&gt; [UInt8]?{<br/>    var pixelData : [UInt8]?<br/>    <br/>    let context = CIContext()<br/>    <br/>    let attributes = [<br/>        kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,<br/>        kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue<br/>        ] as CFDictionary<br/>    <br/>    var nullablePixelBuffer: CVPixelBuffer? = nil<br/>    let status = CVPixelBufferCreate(<br/>        kCFAllocatorDefault,<br/>        Int(self.extent.size.width),<br/>        Int(self.extent.size.height),<br/>        kCVPixelFormatType_OneComponent8,<br/>        attributes,<br/>        &amp;nullablePixelBuffer)<br/>    <br/>    guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer<br/>        else { return nil }<br/>    <br/>    CVPixelBufferLockBaseAddress(<br/>        pixelBuffer,<br/>        CVPixelBufferLockFlags(rawValue: 0))<br/>    <br/>    context.render(<br/>        self,<br/>        to: pixelBuffer,<br/>        bounds: CGRect(x: 0,<br/>                       y: 0,<br/>                       width: self.extent.size.width,<br/>                       height: self.extent.size.height),<br/>        colorSpace:CGColorSpaceCreateDeviceGray())<br/>    <br/>    <br/>    let width = CVPixelBufferGetWidth(pixelBuffer)<br/>    let height = CVPixelBufferGetHeight(pixelBuffer);<br/>    <br/>    if let baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) {<br/>        pixelData = Array&lt;UInt8&gt;(repeating: 0, count: width * height)<br/>        let buf = baseAddress.assumingMemoryBound(to: UInt8.self)<br/>        for i in 0..&lt;width*height{<br/>            pixelData![i] = buf[i]<br/>        }<br/>    }<br/>    <br/>    CVPixelBufferUnlockBaseAddress(<br/>        pixelBuffer,<br/>        CVPixelBufferLockFlags(rawValue: 0))<br/>    <br/>    return pixelData<br/>}</pre>
<p>Once again, don't be intimidated by the amount of code; there are two main tasks this method does. The first is rendering out the image to a <kbd>CVPixelBuffer</kbd> using a single channel, grayscale. To highlight this, the code responsible is shown in the following block: </p>
<pre>public func getGrayscalePixelData() -&gt; [UInt8]?{<br/>    let context = CIContext()<br/><br/>    let attributes = [<br/>        kCVPixelBufferCGImageCompatibilityKey:kCFBooleanTrue,<br/>        kCVPixelBufferCGBitmapContextCompatibilityKey:kCFBooleanTrue<br/>        ] as CFDictionary<br/>    <br/>    var nullablePixelBuffer: CVPixelBuffer? = nil<br/>    let status = CVPixelBufferCreate(<br/>        kCFAllocatorDefault,<br/>        Int(self.extent.size.width),<br/>        Int(self.extent.size.height),<br/>        kCVPixelFormatType_OneComponent8,<br/>        attributes,<br/>        &amp;nullablePixelBuffer)<br/>    <br/>    guard status == kCVReturnSuccess, let pixelBuffer = nullablePixelBuffer<br/>        else { return nil }<br/>    <br/>    // Render the CIImage to our CVPixelBuffer and return it<br/>    CVPixelBufferLockBaseAddress(<br/>        pixelBuffer,<br/>        CVPixelBufferLockFlags(rawValue: 0))<br/>    <br/>    context.render(<br/>        self,<br/>        to: pixelBuffer,<br/>        bounds: CGRect(x: 0,<br/>                       y: 0,<br/>                       width: self.extent.size.width,<br/>                       height: self.extent.size.height),<br/>        colorSpace:CGColorSpaceCreateDeviceGray())        <br/>    <br/>    CVPixelBufferUnlockBaseAddress(<br/>        pixelBuffer,<br/>        CVPixelBufferLockFlags(rawValue: 0))<br/>}</pre>
<p>We render the image to a <kbd>CVPixelBuffer</kbd> to provide a convenient way for us to access the raw pixels that we can then use to populate our array. We then return this to the caller. The main chunk of code that is responsible for this is shown here: </p>
<pre>let width = CVPixelBufferGetWidth(pixelBuffer)<br/>let height = CVPixelBufferGetHeight(pixelBuffer);<br/><br/>if let baseAddress = CVPixelBufferGetBaseAddress(pixelBuffer) {<br/>    pixelData = Array&lt;UInt8&gt;(repeating: 0, count: width * height)<br/>    let buf = baseAddress.assumingMemoryBound(to: UInt8.self)<br/>    for i in 0..&lt;width*height{<br/>        pixelData![i] = buf[i]<br/>    }<br/>}</pre>
<p>Here, we first determine the dimensions by obtaining the width and height of our image, using <kbd>CVPixelBufferGetWidth</kbd> and <kbd>CVPixelBufferGetHeight</kbd> respectively. Then we use these to create an appropriately sized array to hold the pixel data. We then obtain the base address of our <kbd>CVPixelBuffer</kbd> and call its <kbd>assumingMemoryBound</kbd> method to give us a typed pointer. We can use this to access each pixel, which we do to populate our <kbd>pixelData</kbd> array before returning it.</p>
<p>With your <kbd>getGrayscalePixelData</kbd> method now implemented, return to the main source of the playground and resume where you left off by adding the following code: </p>
<pre>guard let resizedCroppedCIImageData =<br/>    resizedCroppedCIImage.getGrayscalePixelData() else{<br/>        fatalError("Failed to get (grayscale) pixel data from image")<br/>}<br/><br/>let scaledImageData = resizedCroppedCIImageData.map({ (pixel) -&gt; Double in<br/>    return Double(pixel)/255.0<br/>})</pre>
<p>In the preceding snippet, we are obtaining the raw pixels of our cropped image using our <kbd>getGrayscalePixelData</kbd> method, before rescaling them by dividing each pixel by 255.0 (the maximum value). Our final task of preparation is putting our data into a data structure that our model will accept, a <kbd>MLMultiArray</kbd>. Add the following code to do just this:</p>
<pre>guard let array = try? MLMultiArray(shape: [1, 48, 48], dataType: .double) else {<br/>    fatalError("Unable to create MLMultiArray")<br/>}<br/><br/>for (index, element) in scaledImageData.enumerated() {<br/>    array[index] = NSNumber(value: element)<br/>}</pre>
<p>We start by creating an instance of <kbd>MLMultiArray</kbd> with the shape of our input data and then proceed to copy across our standardized pixel data. </p>
<p>With our model instantiated and data prepared, we can now perform inference using the following code:</p>
<pre>DispatchQueue.global(qos: .background).async {<br/>    let prediction = try? model.prediction(<br/>        image: array)<br/>    <br/>    if let classPredictions = prediction?.classLabelProbs{<br/>        DispatchQueue.main.sync {<br/>            for (k, v) in classPredictions{<br/>                print("\(k) \(v)")<br/>            }<br/>        }<br/>    }<br/>} </pre>
<p>Previously, we dispatched inference on a background thread then printed out all probabilities of each class to the console. With that now complete, run your playground, and if everything is working fine, you should get something like the following:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>Angry </td>
<td><span>0.0341557003557682</span></td>
</tr>
<tr>
<td>
<p class="mce-root">Happy</p>
</td>
<td>0.594196200370789</td>
</tr>
<tr>
<td>Disgust</td>
<td>2.19011440094619e-06</td>
</tr>
<tr>
<td>Sad</td>
<td>0.260873317718506</td>
</tr>
<tr>
<td>Fear</td>
<td>0.013140731491148</td>
</tr>
<tr>
<td>Surprise</td>
<td>0.000694742717314512</td>
</tr>
<tr>
<td>Neutral</td>
<td>0.0969370529055595</td>
</tr>
</tbody>
</table>
<p>As a designer and builder of intelligent systems, it is your task to interpret these results and present them to the user. Some questions you'll want to ask yourself are as follows:</p>
<ul>
<li>What is an acceptable threshold of a probability before setting the class as true?</li>
<li>Can this threshold be dependent on probabilities of other classes to remove ambiguity? That is, if <strong>Sad</strong> and <strong>Happy</strong> have a probability of 0.3, you can infer that the prediction is inaccurate, or at least not useful.</li>
<li>Is there a way to accept multiple probabilities?</li>
<li>Is it useful to expose the threshold to the user and have it manually set and/or tune it?</li>
</ul>
<p>These are only a few questions you should ask. The specific questions, and their answers, will depend on your use case and users. At this point, we have everything we need to preprocess and perform inference; let's now turn our attention to the application for this chapter. </p>
<div class="packt_infobox">If you find that you are not getting any output, it could be that you need to flag the playground as running indefinitely so that it doesn't exit before running the background thread. You can do this by adding the following statement in your playground: <kbd>PlaygroundPage.current.needsIndefiniteExecution = true</kbd><br/>
When this is set to <kbd>true</kbd>, you will need to explicitly stop the playground.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bringing it all together</h1>
                </header>
            
            <article>
                
<p>If you haven't done already, pull down the latest code from the accompanying repository: <a href="https://github.com/packtpublishing/machine-learning-with-core-ml">https://github.com/packtpublishing/machine-learning-with-core-ml</a>. Once downloaded, navigate to the directory <kbd>Chapter4/Start/FacialEmotionDetection</kbd> and open the project <kbd>FacialEmotionDetection.xcodeproj</kbd>. Once loaded, you will hopefully recognize the project structure as it closely resembles our first example. For this reason, we will just concentrate on the main components that are unique for this project, and I suggest reviewing previous chapters for clarification on anything that is unclear. </p>
<p>Let's start by reviewing our project and its main components; your project should look similar to what is shown in the following screenshot: </p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/d3332a21-5276-4a59-9ba0-9780f0ca27b5.png"/></div>
<p>As shown in the preceding screenshot, the project looks a lot like our previous projects. I am going to make an assumption that the classes <kbd>VideoCapture</kbd>, <kbd>CaptureVideoPreviewView</kbd>, and <kbd>UIColorExtension</kbd> look familiar and you are comfortable with their contents. <kbd>CIImageExtension</kbd> is what we just implemented in the previous section, and therefore we won't be covering it here. The <kbd>EmotionVisualizerView</kbd> class is a custom view that visualizes the outputs from our model. And, finally, we have the bundled <kbd>ExpressionRecognitionModelRaw.mlmodel</kbd>. Our main focus in this section will be on wrapping the functionality we implemented in the previous section to handle preprocessing and hooking it up within the <kbd>ViewController</kbd> class. Before we start, let's quickly review what we are doing and consider some real-life applications for expression/emotion recognition. </p>
<p>In this section, we are building a simple visualization of the detected faces; we will pass in our camera feed to our preprocessor, then hand it over to our model to perform inference, and finally feed the results to our <kbd>EmotionVisualizerView</kbd> to render the output as an overlay on the screen. It's a simple example but sufficiently implements the mechanics required to embed in your own creations. So, what are some of its practical uses?</p>
<p>In a broad sense, there are three main uses: <strong>analytical</strong>, <strong>reactive</strong>, and <strong>anticipatory</strong>. Analytical is generally what you are likely to hear. These applications typically observe reactions by the user in relation to the content being presented; for example, you might measure arousal from content observed by the user, which is then used to drive future decisions.</p>
<p>While analytical experiences remain mostly passive, reactive applications proactively adjust the experience based on live feedback. One example that illustrates this well is <strong>DragonBot</strong>, a research project from the <em>Social Robotics Group</em> at MIT exploring intelligent tutoring systems.</p>
<p>DragonBot uses emotional awareness to adapt to the student; for example, one of its applications is a reading game that adapts the words based on the recognized emotion. That is, the system can adjust the difficulty of the task (words in this case) based on the user's ability, determined by the recognized emotion.</p>
<p>Finally, we have anticipatory applications. Anticipatory applications are semi-autonomous. They proactively try to infer the user's context and predict a likely action, therefore adjusting their state or triggering an action. An fictional example could be an email client that delays sending messages if the user had composed the message when angry. </p>
<p>Hopefully, this highlights some of the opportunities, but for now, let's return to our example and start building out the class that will be responsible for handling the preprocessing. Start off by creating a new swift file called <kbd>ImageProcess.swift</kbd>; and, within the file, add the following code:</p>
<pre>import UIKit<br/>import Vision<br/><br/>protocol ImageProcessorDelegate : class{<br/>    func onImageProcessorCompleted(status: Int, faces:[MLMultiArray]?)<br/>}<br/><br/>class ImageProcessor{<br/>    <br/>    weak var delegate : ImageProcessorDelegate?<br/>    <br/>    init(){<br/>        <br/>    }<br/>    <br/>    public func getFaces(pixelBuffer:CVPixelBuffer){<br/>        DispatchQueue.global(qos: .background).async {  <br/><br/>    }<br/>}</pre>
<p>Here, we have defined the protocol for the delegate to handle the result once the preprocessing has completed, as well as the main class that exposes the method for initiating the task. Most of the code we will be using is what we have written in the playground; start off by declaring the request and request handler at the class level: </p>
<pre>let faceDetection = VNDetectFaceRectanglesRequest()<br/><br/>let faceDetectionRequest = VNSequenceRequestHandler()</pre>
<p>Let's now make use of the request by having our handler execute it within the body of the <kbd>getFaces</kbd> method's background queue dispatch block:</p>
<pre>let ciImage = CIImage(cvPixelBuffer: pixelBuffer)<br/>let width = ciImage.extent.width<br/>let height = ciImage.extent.height<br/><br/>// Perform face detection<br/>try? self.faceDetectionRequest.perform(<br/>    [self.faceDetection],<br/>    on: ciImage) <br/><br/>var facesData = [MLMultiArray]()<br/><br/>if let faceDetectionResults = self.faceDetection.results as? [VNFaceObservation]{<br/>    for face in faceDetectionResults{<br/><br/>    }<br/>}</pre>
<p>This should all look familiar to you. We pass in our request and image to the image handler. Then, we instantiate an array to hold the data for each face detected in the image. Finally, we obtain the observations and start iterating through each of them. It's within this block that we will perform the preprocessing and populate our <kbd>facesData</kbd> array as we had done in the playground. Add the following code within the loop:</p>
<pre>let bbox = face.boundingBox<br/><br/>let imageSize = CGSize(width:width,<br/>                       height:height)<br/><br/>let w = bbox.width * imageSize.width<br/>let h = bbox.height * imageSize.height<br/>let x = bbox.origin.x * imageSize.width<br/>let y = bbox.origin.y * imageSize.height<br/><br/>let paddingTop = h * 0.2<br/>let paddingBottom = h * 0.55<br/>let paddingWidth = w * 0.15<br/><br/>let faceRect = CGRect(x: max(x - paddingWidth, 0),<br/>                      y: max(0, y - paddingTop),<br/>                      width: min(w + (paddingWidth * 2), imageSize.width),<br/>                      height: min(h + paddingBottom, imageSize.height))</pre>
<p>In the preceding block, we obtain the detected face's bounding box and create the cropping bounds, including padding. Our next task will be to crop the face from the image, resize it to our target size of 48 x 48, extract the raw pixel data along with normalizing the values, and finally populate an <kbd>MLMultiArray</kbd>. This is then added to our <kbd>facesData</kbd> array to be returned to the delegate; appending the following code to your script does just that:</p>
<pre>if let pixelData = ciImage.crop(rect: faceRect)?<br/>    .resize(size: CGSize(width:48, height:48))<br/>    .getGrayscalePixelData()?.map({ (pixel) -&gt; Double in<br/>        return Double(pixel)/255.0 <br/>    }){<br/>    if let array = try? MLMultiArray(shape: [1, 48, 48], dataType: .double)     {<br/>        for (index, element) in pixelData.enumerated() {<br/>            array[index] = NSNumber(value: element)<br/>        }<br/>        facesData.append(array)<br/>    }<br/>}</pre>
<p>Nothing new has been introduced here apart from chaining the methods to make it more legible (at least for me). Our final task is to notify the delegate once we have finished; add the following just outside the observations loop block: </p>
<pre>DispatchQueue.main.async {<br/>    self.delegate?.onImageProcessorCompleted(status: 1, faces: facesData)<br/>}</pre>
<p>Now, with that complete, our <kbd>ImageProcessor</kbd> is ready to be used. Let's hook everything up. Jump into the <kbd>ViewController</kbd> class, where we will hook our <kbd>ImageProcessor</kbd>. We will pass its results to our model and finally pass the output from our model to <kbd>EmotionVisualizerView</kbd> to present the results to the user. Let's start by reviewing what currently exists:</p>
<pre>import UIKit<br/>import Vision<br/>import AVFoundation<br/><br/>class ViewController: UIViewController {<br/><br/>    @IBOutlet weak var previewView: CapturePreviewView!<br/>    <br/>    @IBOutlet weak var viewVisualizer: EmotionVisualizerView!<br/>    <br/>    @IBOutlet weak var statusLabel: UILabel!<br/>    <br/>    let videoCapture : VideoCapture = VideoCapture() <br/>    <br/>    override func viewDidLoad() {<br/>        super.viewDidLoad()<br/>        <br/>        videoCapture.delegate = self<br/>        <br/>        videoCapture.asyncInit { (success) in<br/>            if success{<br/><br/>                (self.previewView.layer as! AVCaptureVideoPreviewLayer).session = self.videoCapture.captureSession<br/><br/>                (self.previewView.layer as! AVCaptureVideoPreviewLayer).videoGravity = AVLayerVideoGravity.resizeAspectFill<br/>                <br/>                self.videoCapture.startCapturing()<br/>            } else{<br/>                fatalError("Failed to init VideoCapture")<br/>            }<br/>        }<br/>        <br/>        imageProcessor.delegate = self<br/>    }<br/>}<br/><br/>extension ViewController : VideoCaptureDelegate{<br/>    <br/>    func onFrameCaptured(<br/>        videoCapture: VideoCapture,<br/>        pixelBuffer:CVPixelBuffer?,<br/>        timestamp:CMTime){<br/>        // Unwrap the parameter pixxelBuffer; exit early if nil<br/>        guard let pixelBuffer = pixelBuffer else{<br/>            print("WARNING: onFrameCaptured; null pixelBuffer")<br/>            return<br/>        }<br/>    }<br/>}</pre>
<p>Our <kbd>ViewController</kbd> has references to its IB counterpart, most notably the <kbd>previewView</kbd> and <kbd>viewVisualizer</kbd>. The former will render the captured camera frames and <kbd>viewVisualizer</kbd> will be responsible for visualizing the output of our model. We then have <kbd>videoCapture</kbd>, which is a utility class that encapsulates setting up, capturing, and tearing down the camera. We get access to the captured frames by assigning ourselves as the delegate and implement the appropriate protocol as we have done as an extension at the bottom.</p>
<p>Let's begin by declaring the model and <kbd>ImageProcessor</kbd> variables required for our task; add the following at the class level of your <kbd>ViewController</kbd>:</p>
<pre>let imageProcessor : ImageProcessor = ImageProcessor()<br/><br/>let model = ExpressionRecognitionModelRaw()</pre>
<p>Next, we need to assign ourselves as the delegate of <kbd>ImageProcessor</kbd> in order to receive the results once the processing has completed. Add the following statement to the bottom of your <kbd>viewDidLoad</kbd> method:</p>
<pre>imageProcessor.delegate = self</pre>
<p>We will return shortly to implement the required protocol; for now, let's make use of our <kbd>ImageProcessor</kbd> by passing in the frame we receive from the camera. Within the <kbd>onFrameCaptured</kbd> method, we add the following statement, which will pass each frame to our <kbd>ImageProcessor</kbd> instance. It's shown in bold in the following code block: </p>
<pre>extension ViewController : VideoCaptureDelegate{<br/>    <br/>    func onFrameCaptured(<br/>        videoCapture: VideoCapture,<br/>        pixelBuffer:CVPixelBuffer?,<br/>        timestamp:CMTime){<br/><br/>        guard let pixelBuffer = pixelBuffer else{<br/>            print("WARNING: onFrameCaptured; null pixelBuffer")<br/>            return<br/>        }<br/>        <br/>        <strong>self.imageProcessor.getFaces(</strong><br/><strong>            pixelBuffer: pixelBuffer)</strong><br/>    }<br/>} </pre>
<p>Our final task will be to implement the <kbd>ImageProcessorDelegate</kbd> protocol; this will be called when our <kbd>ImageProcessor</kbd> has completed identifying and extracting each face for a given camera frame along with performing the preprocessing necessary for our model. Once completed, we will pass the data to our model to perform inference, and finally pass these onto our <kbd>EmotionVisualizerView</kbd>. Because nothing new is being introduced here, let's go ahead and add the block in its entirety:</p>
<pre>extension ViewController : ImageProcessorDelegate{<br/>    <br/>    func onImageProcessorCompleted(<br/>        status: Int,<br/>        faces:[MLMultiArray]?){<br/>        guard let faces = faces else{ return }<br/>        <br/>        self.statusLabel.isHidden = faces.count &gt; 0<br/>        <br/>        guard faces.count &gt; 0 else{<br/>            return<br/>        }<br/>        <br/>        DispatchQueue.global(qos: .background).async {<br/>            for faceData in faces{<br/>                <br/>                let prediction = try? self.model<br/>                    .prediction(image: faceData)<br/>                <br/>                if let classPredictions =<br/>                    prediction?.classLabelProbs{<br/>                    DispatchQueue.main.sync {<br/>                        self.viewVisualizer.update(<br/>                            labelConference: classPredictions<br/>                        )<br/>                    }<br/>                }<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>The only notable thing to point out is that our model needs to perform inference in the background thread and our <kbd>ImageProcessor</kbd> calls its delegate on the main thread. For this reason, we dispatch inference to the background and then return the results on the main thread—this is necessary whenever you want to update the user interface. </p>
<p>With that complete, we are now in a good place to build and deploy to test; if all goes well, you should see something like the following:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c5daed44-e9db-4000-b067-b433927216f6.png" style="width:36.50em;height:34.08em;"/></div>
<p>Let's wrap up the chapter by reviewing what we have covered and point out some interesting areas to explore before moving on to the next chapter.</p>
<div class="packt_infobox">In this chapter, we have taken a naive approach with respect to processing the captured frames; in a commercial application you would want to optimize this process such as utilizing <strong>object tracking</strong> from the <kbd>Vision</kbd> framework to replace explicit face detection, which is computationally cheaper.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p>In this chapter, we applied a CNN for the task of recognizing facial expressions. Using this, we could infer the emotional state of a given face. As usual, we again spent the majority of our time understanding the required input for the model and implementing the functionality to facilitate this. But, in doing so, we uncovered some important considerations when developing intelligent applications; the first is the explicit awareness of using either an end-to-end solution or a multi-step approach, with the multi-step approach being the most common one you will use.</p>
<p>This essentially means you, the designer and builder of intelligent applications, will be building data pipelines consisting of many models, each transforming the data in preparation for the next. This is similar to how deep networks work but provides greater flexibility. The second <span>consideration </span>is highlighting the availability of complementary frameworks available on iOS, in particular the <kbd>Vision</kbd> framework. It was used as one of the steps in our pipeline but offers a lot of convenience for common tasks, as well as a consistent workflow.</p>
<p>In this example, our pipeline consisted of only two steps, face detection and then emotion <span>recognition.</span> But we also briefly played with a feature of the <kbd>Vision</kbd> framework that can be used to identify facial landmarks. So, it is plausible to consider facial landmarks to train the emotional classifier rather than the raw pixels, in which case our pipeline would consist of three steps: face detection, landmark detection, and, finally, emotion recognition.</p>
<p>Finally, we briefly explored some use cases showing how emotion recognition could be applied; as our computers shift away from being pure tools towards being companions, being able to detect and react to the emotional state of the user will <span>become </span>increasingly more important. So, it's an area well worth further exploring. </p>
<p>In the next chapter, we will introduce the concept of transfer learning and how we can use it to transfer styles from one image onto another.</p>


            </article>

            
        </section>
    </body></html>