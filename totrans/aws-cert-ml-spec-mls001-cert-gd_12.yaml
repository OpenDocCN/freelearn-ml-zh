- en: '*Chapter 9*: Amazon SageMaker Modeling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned several methods of model optimization and
    evaluation techniques. We also learned various ways of storing data, processing
    data, and applying different statistical approaches to data. So, how can we now
    build a pipeline for this? Well, we can read data, process data, and build machine
    learning models on the processed data. But what if my first machine learning model
    does not perform well? Can I fine-tune my model? The answer is *Yes*; you can
    perform nearly everything using Amazon SageMaker. In this chapter, we will walk
    you through the following topics using Amazon SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different instances of Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and preparing data in Jupyter Notebook in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training in Amazon SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SageMaker's built-in machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing custom training and inference code in SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the data used in this chapter's examples from GitHub at [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-9](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-9).
  prefs: []
  type: TYPE_NORMAL
- en: Creating notebooks in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're working with machine learning, then you need to perform actions such
    as storing data, processing data, preparing data for model training, model training,
    and deploying the model for inference. They are not easy, and each of these stages
    requires a machine to perform the task. With Amazon SageMaker, life becomes much
    easier when carrying out these steps.
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon SageMaker?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker provides training instances to train a model using the data and provides
    endpoint instances to infer by using the model. It also provides notebook instances,
    running Jupyter Notebooks, to clean and understand the data. If you're happy with
    your cleaning process, then you should store them in S3 as part of the staging
    for training. You can launch training instances to consume this training data
    and produce a machine learning model. The machine learning model can be stored
    in S3, and endpoint instances can consume the model to produce results for end
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you draw this in a block diagram, then it will look similar to *Figure 9.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A pictorial representation of the different layers of the Amazon
    SageMaker instances'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – A pictorial representation of the different layers of the Amazon
    SageMaker instances
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the Amazon SageMaker console and get a better feel
    for it. Once you log in to your AWS account and go to Amazon SageMaker, you will
    see something similar to *Figure 9.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – A quick look at the SageMaker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – A quick look at the SageMaker console
  prefs: []
  type: TYPE_NORMAL
- en: There are three different sections, including **Notebook**, **Training**, and
    **Inference**, which have been expanded in *Figure 9.2* so that we can dive in
    and understand them better.
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook** has three different options that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Notebook instances**: This helps us to create, open, start, and stop notebook
    instances. These instances are responsible for running Jupyter Notebooks. They
    allow us to choose the instance type based on the workload of the use case. The
    best practice is to use a notebook instance to orchestrate the data pipeline for
    processing a large dataset. For example, making a call from a notebook instance
    to AWS Glue for ETL services or Amazon EMR to run Spark applications. If you''re
    asked to create a secured notebook instance outside AWS, then you need to take
    care of endpoint security, network security, launching the machine, managing storage
    on it, and managing Jupyter Notebook applications running on the instance. The
    user does not need to manage any of these with SageMaker.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip install` or a `conda install`. However, as soon as the notebook instance
    is terminated, the customization will be lost. To avoid such a scenario, you can
    customize your notebook instance through a script provided through `/home/ec2-user/anaconda3/envs/`
    and customize the specific environment as required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Git repositories**: AWS CodeCommit, GitHub, or any other Git server can be
    associated with the notebook instance for the persistence of your notebooks. If
    access is given, then the same notebook can be used by other developers to collaborate
    and save in a source control fashion. Git repositories can either be added separately
    by using this option or they can be associated with a notebook instance during
    the creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 9.2*, **Training** offers **Algorithms**, **Training
    jobs**, and **Hyperparameter tuning jobs**. Let''s understand their usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms**: This is the first step toward deciding on an algorithm that
    we are going to run on our cleaned data. You can either choose a custom algorithm
    or create a custom algorithm based on the use case. Otherwise, you can run SageMaker
    algorithms on the cleaned data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training jobs**: You can create training jobs from a notebook instance via
    API calls. You can set the number of instances, input the data source details,
    perform checkpoint configuration, and output data configuration. Amazon SageMaker
    manages the training instances and stores the model artifacts as output in the
    specified location. Both incremental training (that is, to train the model from
    time to time for better results) and managed spot training (that is, to reduce
    costs) can also be achieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning jobs**: Usually, hyperparameters are set for an algorithm
    prior to the training process. During the training process, we let the algorithm
    figure out the best values for these parameters. With hyperparameter tuning, we
    obtain the best model that has the best value of hyperparameters. This can be
    done through a console or via API calls. The same can be orchestrated from a notebook
    instance too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference** has many offerings and is evolving every day:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compilation jobs**: If your model is trained using a machine learning framework
    such as Keras, MXNet, ONNX, PyTorch, TFLite, TensorFlow, or XGBoost, and your
    model artifacts are available on a S3 bucket, then you can choose either **Target
    device** or **Target platform**. Target device is used to deploy your model, such
    as an AWS SageMaker machine learning instance or an AWS IoT Greengrass device.
    Target platform is used to decide the operating system, architecture, and accelerator
    on which you want your model to run. You can also store the compiled module in
    your S3 bucket for future use. This essentially helps you in cross-platform model
    deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model packages**: These are used to create deployable SageMaker models. You
    can create your own algorithm, package it using the model package APIs, and publish
    it to AWS Marketplace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models**: Models are created using model artifacts. They are similar to mathematical
    equations with variables; that is, you input the values for the variables and
    get an output. These models are stored in S3 and will be used for inference by
    the endpoints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VariantWeight` API to make the endpoints serve 80% of the request with the
    older model and 20% of the request with the new model. This is the most common
    production scenario where the data changes rapidly and the model needs to be trained
    and tuned periodically. Another possible use case is to test the model results
    with live data, then a certain percentage of the requests can be routed to the
    new model, and the results can be monitored to testify the accuracy of the model
    on real-time unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoints**: These are used to create an URL to which the model is exposed
    and can be requested to give the model results as a response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InputFilter`, `JoinSource`, `OutputFilter` APIs can be used to associate input
    records with output results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we have got an overview of Amazon SageMaker. Let's put our knowledge to
    work in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Amazon SageMaker console keeps changing. There's a possibility that when
    you're reading this book, the console might look different.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon SageMaker notebook instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The very first step, in this section, is to create a Jupyter Notebook, and
    this requires a notebook instance. Let''s start by creating a notebook instance,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to your AWS account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to **Services** > **Amazon SageMaker**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the left navigation pane, click on **Notebook instances** and then click
    on the **Create notebook instance** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide a `notebookinstance` and leave the `ml.t2.medium setting`. In the `Create
    a new role` in **IAM role**. You will be asked to specify the bucket name. For
    the purpose of this example, it's chosen as any bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Following the successful creation of a role, you should see something similar
    to *Figure 9.3*:![Figure 9.3 – Amazon SageMaker role creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.3 – Amazon SageMaker role creation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Leave everything else in its default setting and click on the **Create notebook
    instance** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the instance is in the `InService` state, select the instance. Click on
    the **Actions** drop-down menu and choose **Open Jupyter**. This opens your Jupyter
    Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we are all set to run our Jupyter Notebook on the newly created instance.
    We will perform **Exploratory Data Analysis** (**EDA**) and plot different types
    of graphs to visualize the data. Once we are familiar with the Jupyter Notebook,
    we will build some models to predict house prices in Boston. We will apply the
    algorithms that we have learned in previous chapters and compare them to find
    the best model that offers the best prediction according to our data. Let's dive
    in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the Jupyter Notebook, click on **New** and select **Terminal**. Run the
    following commands in Command Prompt to download the codes to the instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the Git repository is cloned to the SageMaker notebook instance, type `exit`
    into Command Prompt to quit. Now, your code is ready to execute.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to `Chapter-9` in the Jupyter Notebook's **Files** section, as shown
    in *Figure 9.4*:![Figure 9.4 – Jupyter Notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.4 – Jupyter Notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Click on the first notebook in *1.*`Boston-House-Price-SageMaker-Notebook-Instance-Example.ipynb`.
    It will prompt you to choose the kernel for the notebook. Please select `conda_python3`,
    as shown in *Figure 9.5*:![Figure 9.5 – Jupyter Notebook kernel selection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.5 – Jupyter Notebook kernel selection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the notebook, navigate to **Kernel** > **Restart & Clear Output**. Click
    on the play icon to run the cells one after another. Please ensure you have run
    each individual cell and inspect the output from each execution/run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can experiment by adding cells and deleting cells to familiarize yourself
    with the Jupyter Notebook operations. In one of the paragraphs, there is a bash
    command that allows you to install the `xgboost` libraries from the notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final cell explains how we have compared the different scores of various
    modeling techniques to draw a conclusion mathematically. *Figure 9.6* clearly
    shows that the best model to predict house prices in Boston is XGBoost:![Figure
    9.6 – Comparing the models
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.6 – Comparing the models
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once you've completed the execution of this notebook, please feel free to shut
    down the kernel and stop your notebook instance from the SageMaker console. This
    is the best practice to save on cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next hands-on section, we will familiarize ourselves with Amazon SageMaker's
    training and inference instances. We will also use the Amazon SageMaker API to
    make this process easier. We will use the same notebook instance as we did in
    the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with Amazon SageMaker's training and inference instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn about training a model and hosting the model
    to generate predicted results. Let''s dive in by using the notebook instance from
    the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to your AWS account at [https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/notebook-instances).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on `InService`, open it in a new tab, as shown in *Figure 9.7*:![Figure
    9.7 – The InService instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.7 – The InService instance
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the tab named SageMaker Examples in the Jupyter Notebook home page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the `k_nearest_neighbors_covtype.ipynb` notebook. Click on **Use** and
    create a copy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you run the following paragraph, as shown n *Figure 9.8*, you can also
    check a training job in **Training** > **Training jobs** of the SageMaker home
    page:![Figure 9.8 – The SageMaker fit API call
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.8 – The SageMaker fit API call
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The training job looks similar to *Figure 9.9*. It launches an ECS container
    in the backend and uses the IAM execution role created in the previous example
    to run the training job for this request:![Figure 9.9 – Training obs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.9 – Training obs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you go inside and check the logs in CloudWatch, it gives you more details
    about the containers and the steps they performed. As a machine learning engineer,
    it's worth going in and checking the CloudWatch metrics for Algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, if you run the following paragraph, as shown in *Figure 9.10*, in the notebook,
    then it will create an endpoint configuration and an endpoint where the model
    from the earlier training job is deployed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I have changed the instance type to save on cost. It is the instance or the
    machine that will host your model. Please choose your instance wisely. We will
    learn about choosing instance types in the next section. I have also changed `endpoint_name`
    so that it can be recognized easily:![Figure 9.10 – Creating the predictor object
    with endpoint details
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16735_09_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.10 – Creating the predictor object with endpoint details
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to **Inference** > **Endpoints**. This will show you the endpoint that
    was created as a result of the previous paragraph execution. This endpoint has
    a configuration and can be navigated and traced through **Inference** > **Endpoint
    Configurations**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you view the **Inference** section in the notebook, you will notice that
    it uses the test data to predict results. It uses the predictor object from the
    SageMaker API to make predictions. The predictor object contains the endpoint
    details, model name, and instance type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API call to the endpoint occurs in the **Inference** section, and it is
    authenticated via the IAM role with which the notebook instance is created. The
    same API calls can be traced through CloudWatch invocation metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, running the `delete_endpoint` method in the notebook will delete the
    endpoint. To delete the endpoint configurations, navigate to **Inference** > **Endpoint
    Configurations** and select the configuration on the screen. Click on **Actions**
    > **Delete** > **Delete**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, please feel free to shut down the kernel and stop your notebook instance
    from the SageMaker console. This is the best practice to save on cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this section, we learned about using the notebook instance, training instances,
    inference endpoints, and endpoint configurations to clean our data, train models,
    and generate predicted results from them. In the next section, we will learn about
    model tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B16735_08_Final_VK_ePub.xhtml#_idTextAnchor162), *Evaluating
    and Optimizing Models*, you learned many important concepts about model tuning.
    Let's now explore this topic from a practical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to tune a model on SageMaker, we have to call `create_hyper_parameter_tuning_job`
    and pass the following main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`HyperParameterTuningJobName`: This is the name of the tuning job. It is useful
    to track the training jobs that have been started on behalf of your tuning job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HyperParameterTuningJobConfig`: Here, you can configure your tuning options.
    For example, which parameters you want to tune, the range of values for them,
    the type of optimization (such as random search or Bayesian search), the maximum
    number of training jobs you want to spin up, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainingJobDefinition`: Here, you can configure your training job. For example,
    the data channels, the output location, the resource configurations, the evaluation
    metrics, and the stop conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In SageMaker, the main metric that we want to use to evaluate the models to
    select the best one is known as an **objective metric**.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we are configuring `HyperParameterTuningJobConfig`
    for a decision tree-based algorithm. We want to check the best configuration for
    a **max_depth** hyperparameter, which is responsible for controlling the depth
    of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `IntegerParameterRanges`, we have to specify the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameter name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum value that we want to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum value that we want to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each type of hyperparameter must fit in one of the parameter ranges sections,
    such as categorical, continuous, or integer parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In `ResourceLimits`, we are specifying the number of training jobs along with
    the number of parallel jobs that we want to run. Remember that the goal of the
    tuning process is to execute many training jobs with different hyperparameter
    settings. This is so that the best one will be selected for the final model. That's
    why we have to specify these training job execution rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then set up our search strategy in `Strategy` and, finally, set up the objective
    function in `HyperParameterTuningJobObjective`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The second important configuration we need to set is `TrainingJobDefinition`.
    Here, we have to specify all the details regarding the training jobs that will
    be executed. One of the most important settings is the `TrainingImage` setting,
    which refers to the container that will be started to execute the training processes.
    This container, as expected, must have your training algorithm implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we present an example of a built-in algorithm, **eXtreme Gradient Boosting**,
    so that you can set the training image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can go ahead and set your training definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have to specify the data input configuration, which is also known
    as the data channels. In the following section of code, we are setting up two
    data channels – train and validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to specify where the results will be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set the resource configurations, roles, static parameters, and
    stopping conditions. In the following section of code, we want to use two instances
    of type `ml.c4.2xlarge` with 10 GB of storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we are using other variables in this configuration file, `bucket`
    and `prefix`, which should be replaced by your bucket name and prefix key (if
    needed), respectively. We are also referring to `s3_input_train` and `s3_input_validation`,
    which are two variables that point to the train and validation datasets in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have set your configurations, you can spin up the tuning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's find out how to track the execution of this process.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking your training jobs and selecting the best model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have started the tuning process, there are two additional steps that
    you might want to check: tracking the process of tuning and selecting the winner
    model (that is, the one with the best set of hyperparameters).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find your training jobs, you should go to the SageMaker console
    and navigate to **Hyperparameter training jobs**. You will then find a list of
    executed tuning jobs, including yours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Finding your tuning job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Finding your tuning job
  prefs: []
  type: TYPE_NORMAL
- en: 'If you access your tuning job, by clicking under its name, you will find a
    summary page, which includes the most relevant information regarding the tuning
    process. Under the **Training jobs** tab, you will see all of the training jobs
    that have been executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Summary of the training jobs in the tuning process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Summary of the training jobs in the tuning process
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you click on the **Best training job** tab, you will find the best
    set of hyperparameters for your model, including a handy button to create a new
    model based on those best hyperparameters that have just been found:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Finding the best set of hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Finding the best set of hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker is very intuitive, and once you know the main concepts
    behind model optimization, playing with SageMaker should be easier. By now, we
    have understood how to use SageMaker for our specific needs. In the next section,
    we will explore how to select the instance type for various use cases and the
    security of our notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing instance types in Amazon SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker is a pay-for-usage model. There is no minimum fee for it.
  prefs: []
  type: TYPE_NORMAL
- en: When we think about instances on SageMaker, it all starts with an EC2 instance.
    This instance is responsible for all your processing. It's a managed EC2 instance.
    These instances won't show up in the EC2 console and cannot be SSHed either. The
    instance type starts with `ml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker offers instances of the following families:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **t** family: This is a burstable CPU family. With this family, you get
    a normal ratio of CPU and memory. This means that if you have a long-running training
    job, then you lose performance over time as you spend the CPU credits. If you
    have very small jobs, then they are cost-effective. For example, if you want a
    notebook instance to launch training jobs, then this family is the most relevant
    and cost-effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **m** family: In the previous family, we saw that CPU credits are consumed
    faster due to their burstable nature. If you have a long-running machine learning
    job that requires constant throughput, then this is the right family. It comes
    with a similar CPU and memory ratio as the **t** family.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **r** family: This is a memory-optimized family. *When do we need this?*
    Well, imagine a use case where you have to load the data in memory and do some
    data engineering on the data. In this scenario, you will require more memory and
    your job will be memory-optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **c** family: **c** family instances are compute-optimized. This is a requirement
    for jobs that need higher compute power and less memory to store the data. If
    you refer to the following table, C5.2x large has 8 vCPU and 16 GiB memory, which
    makes it compute-optimized with less memory. For example, a use case needs to
    be tested on a fewer number of records and it is compute savvy, then this instance
    family is the to-go option to get some sample records from a huge dataframe and
    test your algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **p** family: This is a GPU family that supports accelerated computing
    jobs such as training and inference. Notably, **p** family instances are ideal
    for handling large, distributed training jobs, and this leads to less time required
    to train. As a result, this becomes cost-effective. The P3/P3dn GPU compute instance
    can go up to 1 petaFLOP per second compute with up to 256 GB of GPU memory and
    100 Gbps (gigabits) of networking with 8x NVIDIA v100 GPUs. They are highly optimized
    for training and are not fully utilized for inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **g** family: For cost-effective, small-scale training jobs, **g** family
    GPU instances are ideal. G4 has the lowest cost per inference for GPU instances.
    It uses T4 NVIDIA GPUs. The G4 GPU compute instance goes up to 520 TeraFLOPs of
    compute time with 8x NVIDIA T4 GPUs. This instance family is the best for simple
    networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following table, 2x large instance types have been taken from each family
    for a visual comparison between the CPU and memory ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 9.1 – A table showing the CPU and memory ratio of different instance
    types'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 – A table showing the CPU and memory ratio of different instance types
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: To remember this easily, you can think of T for Tiny, M for Medium, C for Compute,
    and P and G for GPU. CPU family instance types are T, M, R, and C. GPU family
    instance types are P and G.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right instance type for a training job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is no rule of thumb to decide the instance type that you require. It changes
    based on the size of the data, the complexity of the network, the machine learning
    algorithm, and several other factors such as time and cost. Asking the right question
    will save money and make it cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: If the deciding factor is *Instance Size*, then classifying the problem for
    CPU or GPU is the right step. Once that is done, then it is good to consider whether
    it can be multi-GPU or multi-CPU. That would solve your question about distributed
    training. This also solves your *Instance Count* factor. If it's compute-intensive,
    then it would be wise to check the memory requirements too.
  prefs: []
  type: TYPE_NORMAL
- en: The next deciding factor is *Instance Family*. The right questions here would
    be, *Is it optimized for time and cost?* In the previous step, we have already
    figured out the problem to be solved in either the CPU or GPU, and this narrows
    down the selection process. Now, let's learn about inference jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right instance type for an inference job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The majority of the cost and complexity of machine learning in production is
    inference. Usually, inference runs on a single input in real time. They are usually
    less compute/memory-intensive. They have to be highly available as they run all
    the time and serve the end user requests or are integrated as part of an application.
  prefs: []
  type: TYPE_NORMAL
- en: You can choose any of the instance types that we learned about recently based
    on the workload. Other than that, AWS has **Inf1** and **Elastic Inference** type
    instances for inference. Elastic inference allows you to attach a fraction of
    a GPU instance to any CPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example where an application is integrated with inference jobs,
    then the requirement of CPU and memory for the application is different from the
    inference jobs. For those use cases, you need to choose the right instance type
    and size. In such scenarios, it is good to have a separation between your application
    fleets and inference fleets. This might require some management. If such management
    is a problem for your requirement, then choose elastic inference, where both the
    application and inference jobs can be colocated. This means that you can host
    multiple models on the same fleet, and you can load all of these different models
    on different accelerators in memory and concurrent requests can be served.
  prefs: []
  type: TYPE_NORMAL
- en: It's always recommended that you run some examples in a lower environment before
    deciding on your instance types and family in the production environment. In the
    next section, we will dive into and understand the different ways of securing
    our Amazon SageMaker notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Securing SageMaker notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are reading this section of the chapter, then you have already learned
    how to use notebook instances, which type of training instances should be chosen,
    and how to configure and use endpoints. Now, let''s learn about securing those
    instances. The following aspects will help to secure the instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encryption**: When we say or think about securing via encryption, then it
    is all about the data. But what does this mean? It means protecting data at rest
    using encryption, protecting data in transit with encryption, and using KMS for
    better role separation and internet traffic privacy through TLS 1.2 encryption.
    SageMaker instances can be launched with encrypted volumes by using an AWS-managed
    KMS key. This helps you to secure the Jupyter Notebook server by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RootAccess` field to `Disabled` when you call `CreateNotebookInstance` or
    `UpdateNotebookInstance`. The data scientist will have access to their user space
    and can install Python packages. However, they cannot sudo into the root user
    and make changes to the operating system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IAM role**: During the launch of a notebook instance, it is necessary to
    create an IAM role for execution or to use an existing role for execution. This
    is used to launch the service-managed EC2 instance with an instance profile associated
    with the role. This role will restrict the API calls based on the policies attached
    to this role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPC connection**: When you launch a SageMaker notebook instance, by default,
    it gets created within the SageMaker Service Account, which has a service-managed
    VPC, and it will, by default, have access to the internet via an internet gateway,
    and that gateway is managed by the service. If you are only dealing with AWS-related
    services, then it is recommended that you launch a SageMaker notebook instance
    in your VPC within a private subnet and with a well-customized security group.
    The AWS services can be invoked or used from this notebook instance via VPC endpoints
    attached to that VPC. The best practice is to control them via endpoint policies
    for better API controls. This ensures the restriction on data egress outside your
    VPC and secured environment. In order to capture all the network traffic, you
    can turn on the VPC flow logs, which can be monitored and tracked via CloudWatch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EnableNetworkIsolation` parameter to `True` when you are calling `CreateTrainingJob`,
    `CreateHyperParameterTuningJob`, or `CreateModel`. Network isolation can be used
    along with the VPC, which ensures that containers cannot make any outbound network
    calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connecting a private network to your VPC**: You can launch your SageMaker
    notebook instance inside the private subnet of your VPC. This can access data
    from your private network by communicating to the private network, which can be
    done by connecting your private network to your VPC by using Amazon VPN or AWS
    Direct Connect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we learned about several ways in which we can secure our SageMaker
    notebooks. In the next section, we will learn about creating SageMaker pipelines
    with Lambda Functions.
  prefs: []
  type: TYPE_NORMAL
- en: Creating alternative pipelines with Lambda Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indeed, SageMaker is an awesome platform that you can use to create training
    and inference pipelines. However, we can always work with different services to
    come up with similar solutions. One of these services that we will learn about
    next is known as **Lambda Functions**.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Lambda is a serverless compute service where you can literally run a function
    as a service. In other words, you can concentrate your efforts on just writing
    your function. Then, you just need to tell AWS how to run it (that is, the environment
    and resource configurations), so all the necessary resources will be provisioned
    to run your code and then discontinued once it is completed.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout [*Chapter 6*](B16735_06_Final_VK_ePub.xhtml#_idTextAnchor115), *AWS
    Services for Data Processing*, you explored how Lambda Functions integrate with
    many different services, such as Kinesis and AWS Batch. Indeed, AWS did a very
    good job of integrating Lambda with 140 services (and the list is constantly increasing).
    That means that when you are working with a specific AWS service, you will remember
    that it is likely to integrate with Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to bear this in mind because Lambda Functions can really expand
    your possibilities to create scalable and integrated architectures. For example,
    you can trigger a Lambda Function when a file is uploaded to S3 in order to preprocess
    your data before loading it to Redshift. Alternatively, you can create an API
    that triggers a Lambda Function at each endpoint execution. Again, the possibilities
    are endless with this powerful service.
  prefs: []
  type: TYPE_NORMAL
- en: It is also useful to know that you can write your function in different programming
    languages, such as Node.js, Python, Go, Java, and more. Your function does not
    necessarily have to be triggered by another AWS service, that is, you can trigger
    it manually for your web or mobile application, for example.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to deployment, you can upload your function as a ZIP file or as
    a container image. Although this is not ideal for an automated deployment process,
    coding directly into the AWS Lambda console is also possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with any other service, this one also has some downsides that you should
    be aware of:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory allocation for your function: This is from 128 MB to 10,240 MB (AWS
    has recently increased this limit from 3 GB to 10 GB, as stated previously).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Function timeout: This is a maximum of 900 seconds (15 minutes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Function layer: This is a maximum of 5 layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burst concurrency: This is from 500 to 3,000, depending on the region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deployment package size: This is 250 MB unzipped, including layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Container image code package size: This is 10 GB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available space in the `/tmp` directory: This is 512 MB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before going for Lambda Functions, make sure these restrictions fit your use
    case. Bringing Lambda Functions closer to your scope of alternative pipelines
    for SageMaker, one potential use of Lambda is to create inference pipelines for
    our models.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, SageMaker has a very handy `.deploy()` method that will create
    endpoints for model inference. This is so that you can call to pass the input
    data in order to receive predictions back. Here, we can create this inference
    endpoint by using the API gateway and Lambda Functions.
  prefs: []
  type: TYPE_NORMAL
- en: In case you don't need an inference endpoint and you just want to make predictions
    and store results somewhere (in a batch fashion), then all we need is a Lambda
    Function, which is able to fetch the input data, instantiate the model object,
    make predictions, and store the results in the appropriated location. Of course,
    it does this by considering all the limitations that we have discussed earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Alright, now that we have a good background about Lambda and some use cases,
    let's take a look at the most important configurations that we should be aware
    of for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and configuring a Lambda Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, you should know that you can create a Lambda Function in different
    ways, such as via the AWS CLI (Lambda API reference), the AWS Lambda console,
    or even deployment frameworks (for example, *the serverless framework*).
  prefs: []
  type: TYPE_NORMAL
- en: Serverless frameworks are usually provider and programming language-independent.
    In other words, they usually allow you to choose where you want to deploy a serverless
    infrastructure from a variate list of cloud providers and programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of serverless architecture is not specific to AWS. In fact, many
    cloud providers offer other services that are similar to AWS Lambda Functions.
    That''s why these serverless frameworks have been built: to help developers and
    engineers to deploy their services, wherever they want, including AWS. This is
    unlikely to come up in your exam, but it is definitely something that you should
    know so that you are aware of different ways in which to solve your challenges
    as a data scientist or data engineer.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to pass the AWS Machine Learning Specialty exam, here, we have
    taken the approach to walk through the AWS Lambda console. This is so that you
    will become more familiar with their interface and the most important configuration
    options.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you navigate to the Lambda console and request a new Lambda Function,
    AWS will provide you with some starting options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Author from scratch: This is if you want to create your function from scratch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use a blueprint: This is if you want to create your function from a sample
    code and configuration preset for common use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Container image: This is if you want to select a container image to deploy
    your function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Browse serverless app repository: This is if you want to deploy a sample Lambda
    application from the AWS Serverless Application Repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Starting from scratch, the next step is to set up your Lambda configurations.
    AWS splits these configurations between basic and advanced settings. In the basic
    configuration, you will set your function name, runtime environment, and permissions.
    *Figure 9.14* shows these configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Creating a new Lambda Function from the AWS Lambda console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.14 – Creating a new Lambda Function from the AWS Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we have a very important configuration that you should remember during
    your exam: the **execution role**. Your Lambda Function might need permissions
    to access other AWS resources, such as S3, Redshift, and more. The execution role
    grants permissions to your Lambda Function so that it can access resources as
    needed.'
  prefs: []
  type: TYPE_NORMAL
- en: You have to remember that your VPC and security group configurations will also
    interfere with how your Lambda Function runs. For example, if you want to create
    a function that needs internet access to download something, then you have to
    deploy this function in a VPC with internet access. The same logic applies to
    other resources, such as access to relational databases, Kinesis, Redshift, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in order to properly configure a Lambda Function, we have to, at
    least, write its code, set the execution role, and make sure the VPC and security
    group configurations match our needs. Next, let's take a look at other configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Completing your configurations and deploying a Lambda Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once your Lambda is created in the AWS console, you can set additional configurations
    before deploying the function. One of these configurations is the event trigger.
    As we mentioned earlier, your Lambda Function can be triggered from a variety
    of services or even manually.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A very common example of a trigger is **Event Bridge**. This is an AWS service
    where you can schedule the execution of your function.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the event trigger you choose, your function will have access to
    different event metadata. For example, if your function is triggered by a **PUT**
    event on S3 (for example, someone uploads a file to a particular S3 bucket), then
    your function will receive the metadata associated with this event, for example,
    bucket name and object key. Other types of triggers will give you different types
    of event metadata!
  prefs: []
  type: TYPE_NORMAL
- en: 'You have access to those metadata through the event parameter that belongs
    to the signature of the entry point of your function. Not clear enough? OK, let''s
    see how your function code should be declared, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, `lambda_handler` is the method that represents the entry point of your
    function. When it is triggered, this method will be called, and it will receive
    the event metadata associated with the event trigger (through the `event` parameter).
    That's how you have access to the information associated with the underlying event
    that has triggered your function! The `event` parameter is a JSON-like object.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to test your function, but you don't want to trigger it directly
    from the underlying event, that is no problem; you can use **test events**. They
    simulate the underlying event by preparing a JSON object that will be passed to
    your function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.15* shows a very intuitive example. Let''s suppose you have created
    a function that is triggered when a user uploads a file to S3 and now you want
    to test your function. You can either upload a file to S3 (which forces the trigger)
    or create a test event.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By creating a test event, you can prepare a JSON object that simulates the
    S3-put event and then pass this object to your function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Creating a test event from the Lambda console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.15 – Creating a test event from the Lambda console
  prefs: []
  type: TYPE_NORMAL
- en: 'Another type of configuration you can set is an **environment variable**, which
    will be available on your function. *Figure 9.16* shows how to add environment
    variables in a Lambda Function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Adding environment variables to a Lambda Function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.16 – Adding environment variables to a Lambda Function
  prefs: []
  type: TYPE_NORMAL
- en: 'You can always come back to these basic configurations to make adjustments
    as necessary. *Figure 9.17* shows what you will find in the basic configurations
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Changing the basic configurations of a Lambda Function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.17 – Changing the basic configurations of a Lambda Function
  prefs: []
  type: TYPE_NORMAL
- en: In terms of monitoring, by default, Lambda Functions produce a **CloudWatch**
    Logs stream and standard metrics. You can access log information by navigating
    through your Lambda Function monitoring section and clicking on *View logs in
    CloudWatch*.
  prefs: []
  type: TYPE_NORMAL
- en: In CloudWatch, each Lambda Function will have a **Log group** and, inside that
    Log group, many **Log streams**. Log streams store the execution logs of the associated
    function. In other words, a log stream is a sequence of logs that share the same
    source, which, in this case, is your Lambda Function. A log group is a group of
    log streams that share the same retention, monitoring, and access control settings.
  prefs: []
  type: TYPE_NORMAL
- en: We are reaching the end of this section, but not the end of this topic on Lambda
    Functions. As we mentioned earlier, this AWS service has a lot of use cases and
    integrates with many other services. In the next section, we will take a look
    at another AWS service that will help us to orchestrate executions of Lambda Functions.
    These are known as **AWS Step Functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Step Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Step Functions is an AWS service that allows you to create workflows in order
    to orchestrate the execution of Lambda Functions. This is so that you can connect
    them in a sort of event sequence, known as **steps**. These steps are grouped
    in a **state machine**.
  prefs: []
  type: TYPE_NORMAL
- en: Step Functions incorporates retry functionality so that you can configure your
    pipeline to proceed only after a particular step has succeeded. The way you set
    these retry configurations is by creating a **retry policy**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Just like the majority of AWS services, AWS Step Functions also integrates with
    other services, not only AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a state machine is relatively simple. All you have to do is navigate
    to the AWS Step Functions console, then create a new state machine. On the *Create
    state machine* page, you can specify whether you want to create your state machine
    from scratch, from a template, or whether you just want to run a sample project.
  prefs: []
  type: TYPE_NORMAL
- en: AWS will help you with this state machine creation, so even if you choose to
    create it from scratch, you will find code snippets for a variate list of tasks,
    such as AWS Lambda invocation, SNS topic publication, running Athena queries,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of demonstration, we will create a very simple, but still helpful,
    example of how to use Step Functions to execute a Lambda Function with the retry
    option activated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we created a state machine with two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Invoke Lambda function: This will start the execution of your underlying Lambda.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: This is a simple pass task just to show you how to connect a second
    step in the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the first step, we have also set up a retry policy, which will try to re-execute
    this task if there are any failures. We are setting up the interval (in seconds)
    to try again and to show here the number of attempts. *Figure 9.18* shows the
    state machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – The state machine'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16735_09_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.18 – The state machine
  prefs: []
  type: TYPE_NORMAL
- en: We have now reached the end of this section and the end of this chapter. Next,
    let's summarize what we have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the usage of SageMaker for creating notebook
    instances and training instances. As we went through we learned how to use SageMaker
    for hyperparameter tuning jobs. As the security of our assets in AWS is an essential
    part, we learned about the various ways to secure SageMaker instances. With hands-on
    practices, we created Step Functions and orchestrated our pipeline using AWS Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: AWS products are evolving every day to help us solve our IT problems. It's not
    easy to remember all the product names. The only way to learn is through practice.
    When you're solving a problem or building a product, then focus on the different
    technological areas of your product. Those areas can be an AWS service, for example,
    scheduling jobs, logging, tracing, monitoring metrics, autoscaling, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Compute time, storage, and networking are the baselines. It is recommended that
    you practice some examples for each of these services. Referring to the AWS documentation
    for clarifying any doubts is also the best option. It is always important to design
    your solutions in a cost-effective way, so exploring the cost-effective way to
    use these services is equally important as building the solution. I wish you all
    the best!
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Which of the following models are supervised algorithms? Select two options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Clustering
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Classification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Association rule mining
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Regression
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You would like to turn your Amazon SageMaker machine learning models and endpoints
    into customer-facing applications. You decide to put these on a single web server
    that can be accessed by customers via a browser. However, you realize that the
    web server is not inherently scalable; if it receives a lot of traffic, it could
    run out of CPU or memory. How can you make this approach more scalable and secure?
    Select three answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Create an IAM role, so the webserver can access the SageMaker endpoints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Deploy a load balancer and set up autoscaling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Make all customers IAM users so that they can access SageMaker.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Keep the operating system and language runtimes for the web server patch
    secured.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the preceding situation, what would be a better AWS service to automate
    server and operating system maintenance, capacity provisioning, and automatic
    scaling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. AWS Lambda
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. AWS Fargate
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. AWS ELB
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Amazon SageMaker is a fully managed service that enables you to quickly and
    easily integrate machine learning-based models into your applications. It also
    provides services such as notebook, training, and endpoint instances to help you
    get the job done.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. TRUE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. FALSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Chose three correct statements from the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Notebook instances clean and understand data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Training instances use data to train the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Endpoint instances use models to produce inferences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Notebook instances clean, understand, and build models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: E. Training instances are used to predict results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the first step of creating a notebook?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Give it a name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Choose a kernel.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Starting developing code in paragraph format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Linear learner and XGBoost algorithms can be used in supervised learning models
    such as regression and classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. TRUE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. FALSE
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which of these statements about hyperparameter tuning is true?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A. Hyperparameter tuning is a guaranteed way to improve your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: B. Hyperparameter tuning does not require any input values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C. Hyperparameter tuning uses regression to choose the best value to test.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: D. Hyperparameter tuning is an unsupervised machine learning regression problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. B and D
  prefs: []
  type: TYPE_NORMAL
- en: 2\. A, B, and D
  prefs: []
  type: TYPE_NORMAL
- en: 3\. A
  prefs: []
  type: TYPE_NORMAL
- en: 4\. A
  prefs: []
  type: TYPE_NORMAL
- en: 5\. A, B, and C
  prefs: []
  type: TYPE_NORMAL
- en: 6\. B
  prefs: []
  type: TYPE_NORMAL
- en: 7\. A
  prefs: []
  type: TYPE_NORMAL
- en: 8\. C
  prefs: []
  type: TYPE_NORMAL
