- en: Selecting the Right Model with Hyperparameter Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have explored a wide variety of machine learning algorithms, I am
    sure you have realized that most of them come with a great number of settings
    to choose from. These settings or tuning knobs, the so-called **hyperparameters**,
    help us to control the behavior of the algorithm when we try to maximize performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: For example, we might want to choose the depth or split criterion in a decision
    tree or tune the number of neurons in a neural network. Finding the values of
    important parameters of a model is a tricky task but necessary for almost all
    models and datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive deeper into **model evaluation** and **hyperparameter
    tuning**. Assume that we have two different ...
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any operating system—macOS, Windows, and Linux-based OSes along
    with this book. We recommend you have at least 4 GB RAM in your system.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a model
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model evaluation strategies come in many different forms and shapes. In the
    following sections, we will, therefore, highlight three of the most commonly used
    techniques to compare models against each other:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: k-fold cross-validation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrapping
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McNemar's test
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In principle, model evaluation is simple: after training a model on some data,
    we can estimate its effectiveness by comparing model predictions to some ground
    truth values. We learned early on that we should split the data into training
    and test sets, and we tried to follow this instruction whenever possible. But
    why exactly did we do that again?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model the wrong way
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reason we never evaluate a model on the training set is that, in principle,
    any dataset can be learned if we throw a strong enough model at it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick demonstration of this can be given with the help of the Iris dataset,
    which we talked about extensively in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*. There, the goal was to classify species
    of iris flowers based on their physical dimensions. We can load the Iris dataset
    using scikit-learn:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'An innocent approach to this problem would be to store all data points in the
    matrix, `X`, and all class labels in the vector, `y`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we choose a model and its hyperparameters. For example, let''s use the
    algorithm from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*, which provides only a single hyperparameter: the
    number of neighbors, *k*. With *k=1*, we get a very simple model that classifies
    the label of an unknown point as belonging to the same class as its closest neighbor.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择一个模型及其超参数。例如，让我们使用[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)中的算法，*监督学习的第一步*，它只提供了一个超参数：邻居的数量，*k*。当*k=1*时，我们得到一个非常简单的模型，它将未知点的标签分类为其最近邻所属的同一类别。
- en: 'In the following steps, you will learn how to build a **k-nearest neighbour**
    (**k-NN**) and compute its accuracy:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习如何构建一个**k-最近邻**（**k-NN**）模型并计算其准确率：
- en: 'In OpenCV, kNN instantiates as follows:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在OpenCV中，kNN实例化如下：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we train the model and use it to predict labels for the data that we
    already know:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们训练模型并使用它来预测已知数据的标签：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we compute the fraction of the correctly labeled points:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们计算正确标记点的比例：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the accuracy score is `1.0`, it indicates that 100% of points
    were correctly labeled by our model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，准确率分数是`1.0`，这表明我们的模型正确地标记了100%的点。
- en: If a model gets 100% accuracy on the training set, we say the model memorized
    the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型在训练集上达到100%的准确率，我们说该模型已经记住了数据。
- en: But is the expected accuracy truly being measured? Have we come up with a model
    that we expect to be correct 100% of the time?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但预期的准确率真的被测量了吗？我们是否提出了一个我们期望100%正确率的模型？
- en: As you may have gathered, the answer is no. This example shows that even a simple
    algorithm is capable of memorizing a real-world dataset. Imagine how easy this
    task would have been for a deep neural network! Usually, the more parameters a
    model has, the more powerful it is. We will come back to this shortly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所可能推测的，答案是否定的。这个例子表明，即使是简单的算法也能够记住现实世界的数据集。想象一下，对于深度神经网络来说，这项任务是多么容易！通常，模型具有的参数越多，它的能力就越强。我们很快就会回到这一点。
- en: Evaluating a model in the right way
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正确评估模型
- en: A better sense of a model's performance can be found using what's known as a
    test set, but you already knew this. When presented with data held out from the
    training procedure, we can check whether a model has learned some dependencies
    in the data that hold across the board or whether it just memorized the training
    set.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用所谓的测试集来更好地了解模型的表现，但你已经知道了这一点。当面对从训练过程中分离出来的数据时，我们可以检查模型是否已经学会了数据中的一些跨领域的依赖关系，或者它只是记住了训练集。
- en: 'We can split the data into training and test sets using the familiar `train_test_split`
    from scikit-learn''s `model_selection` module:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用来自scikit-learn的`model_selection`模块中的熟悉函数`train_test_split`将数据分为训练集和测试集：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: But how do we choose the right train-test ratio? Is there even such a thing
    as a right ratio? Or is this considered another hyperparameter of the model?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何选择正确的训练-测试比例？是否存在这样一个正确的比例？或者这是模型的一个超参数？
- en: 'There are two competing concerns here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里存在两个相互竞争的考虑因素：
- en: If our ...
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们的 ...
- en: Selecting the best model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳模型
- en: When a model is under-performing, it is often not clear how to make it better.
    Throughout this book, I have declared a rule of thumbs, for example, how to select
    the number of layers in a neural network. Even worse, the answer is often counter-intuitive!
    For example, adding another layer to the network might make the results worse,
    and adding more training data might not change performance at all.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型表现不佳时，通常不清楚如何使其变得更好。在这本书中，我提出了一条经验法则，例如，如何选择神经网络中的层数。更糟糕的是，答案往往是反直觉的！例如，向网络中添加另一个层可能会使结果变得更差，而添加更多的训练数据可能根本不会改变性能。
- en: You can see why these issues are some of the most important aspects of machine
    learning. At the end of the day, the ability to determine what steps will or will
    not improve our model is what separates the successful machine learning practitioner
    from all others.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以理解为什么这些问题是机器学习最重要的方面之一。最终，确定哪些步骤会或不会提高我们的模型的能力，这是区分成功的机器学习实践者和其他人的关键。
- en: 'Let''s have a look at a specific example. Remember [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml),
    *Using Decision Trees to Make a Medical Diagnosis*, where we used decision trees
    in a regression task? We were fitting two different trees to a sin function—one
    with depth 2 and one with depth 5\. As a reminder, the regression result looked
    like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的例子。记得[第5章](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml)，*使用决策树进行医疗诊断*，在那里我们使用了决策树进行回归任务？我们拟合了两个不同的树到正弦函数上——一个深度为2，一个深度为5。作为提醒，回归结果看起来是这样的：
- en: '![](img/e4b66301-dbb4-4153-b592-ef5624085515.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e4b66301-dbb4-4153-b592-ef5624085515.png)'
- en: It should be clear that neither of these fits are particularly good. However,
    the two decision trees fail in two different ways!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 应该很明显，这两种拟合都不是特别好。然而，两个决策树以两种不同的方式失败了！
- en: The decision tree with depth 2 (thick line in the preceding screenshot) attempts
    to fit four straight lines through the data. Because the data is intrinsically
    more complicated than a few straight lines, this model fails. We could train it
    as much as we wanted, on as many training samples as we could generate—it would
    never be able to describe this dataset well. Such a model is said to underfit
    the data. In other words, the model does not have enough complexity to account
    for all the features in the data. Hence, the model has a high bias.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度为2的决策树（前一张截图中的粗线）试图通过数据拟合四条直线。因为数据本质上比几条直线更复杂，所以这个模型失败了。我们可以尽可能多地训练它，在尽可能多的训练样本上训练它——它永远无法很好地描述这个数据集。这样的模型被称为欠拟合数据。换句话说，模型没有足够的复杂性来解释数据中的所有特征。因此，这个模型具有很高的偏差。
- en: The other decision tree (thin line, depth 5) makes a different mistake. This
    model has enough flexibility to nearly perfectly account for the fine structures
    in the data. However, at some points, the model seems to follow the particular
    pattern of the noise; we added to the sin function rather than the sin function
    itself. You can see that on the right-hand side of the graph, where the blue curve
    (thin line) would jitter a lot. Such a model is said to overfit the data. In other
    words, the model is so complex that it ends up accounting for random errors in
    the data. Hence, the model has a high variance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个决策树（细线，深度5）犯了一个不同的错误。这个模型有足够的灵活性，几乎完美地解释了数据中的细微结构。然而，在某些点上，模型似乎遵循了噪声的特定模式；我们添加到正弦函数中，而不是正弦函数本身。您可以在图表的右侧看到这一点，那里蓝色曲线（细线）会有很多波动。这样的模型被称为过度拟合数据。换句话说，模型过于复杂，最终解释了数据中的随机误差。因此，这个模型具有很高的方差。
- en: 'Long story short—here''s the secret sauce: fundamentally, selecting the right
    model comes down to finding a sweet spot in the trade-off between bias and variance.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之——这是秘密配方：从根本上说，选择正确的模型归结为在偏差和方差之间找到一个甜蜜点。
- en: The amount of flexibility a model has (also known as the **model complexity**)
    is mostly dictated by its hyperparameters. That is why it is so important to tune
    them!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的灵活性（也称为**模型复杂性**）主要是由其超参数决定的。这就是为什么调整它们如此重要的原因！
- en: 'Let''s return to the kNN algorithm and the Iris dataset. If we repeated the
    procedure of fitting the model to the Iris data for all possible values of *k*
    and calculated both training and test scores, we would expect the result to look
    something like the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到kNN算法和鸢尾花数据集。如果我们对所有可能的*k*值重复拟合模型的程序，并计算训练和测试分数，我们预计结果将类似于以下内容：
- en: '![](img/67d69189-ef9f-41e5-baca-de06a01932b6.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/67d69189-ef9f-41e5-baca-de06a01932b6.png)'
- en: The preceding image shows model score as a function of model complexity. If
    there is one thing I would want you to remember from this chapter, it would be
    this diagram. Let's unpack it.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图片显示了模型分数作为模型复杂度的函数。如果我想让您从这一章记住一件事，那将是这张图。让我们来分析一下。
- en: The diagram describes the model score (either training or test scores) as a
    function of model complexity. As mentioned in the preceding diagram, the model
    complexity of a neural network roughly grows with the number of neurons in the
    network. In the case of kNN, the opposite logic applies—the larger the value for
    *k*, the smoother the decision boundary, and hence, the lower the complexity.
    In other words, kNN with *k=1* would be all of the way to the right in the preceding
    diagram, where the training score is perfect. No wonder we got 100% accuracy on
    the training set!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图表描述了模型得分（无论是训练分数还是测试分数）作为模型复杂度的函数。正如前面图表中提到的，神经网络的模型复杂度大致与网络中的神经元数量成正比。在kNN的情况下，逻辑相反——*k*的值越大，决策边界越平滑，因此复杂度越低。换句话说，*k=1*的kNN在前面图表中会完全位于最右侧，即训练分数完美。难怪我们在训练集上得到了100%的准确率！
- en: 'From the preceding diagram, we can gather that there are three regimes in the
    model complexity landscape:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面图表中，我们可以得出结论，模型复杂度景观中有三个阶段：
- en: Very low model complexity (a high-bias model) underfits the training data. In
    this regime, the model achieves only low scores on both the training and test
    set, no matter how long we trained it for.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常低的模型复杂度（高偏差模型）会欠拟合训练数据。在这个阶段，无论我们训练多长时间，模型在训练集和测试集上的得分都很低。
- en: A model with very high complexity (or a high-variance) overfits the training
    data, which indicates that the model can predict on the training data very well
    but fails on the unseen data. In this regime, the model has started to learn intricacies
    or peculiarities that only appear in the training data. Since these peculiarities
    do not apply to unseen data, the training score gets lower and lower.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复杂度非常高的模型（或高方差）会过度拟合训练数据，这表明模型可以很好地预测训练数据，但在未见过的数据上失败。在这个阶段，模型已经开始学习只出现在训练数据中的复杂细节或特殊性。由于这些特殊性不适用于未见过的数据，训练分数会越来越低。
- en: For some intermediate value, the test score is maximal. It is this intermediate
    regime, where the test score is maximal, that we are trying to find. This is the
    sweet spot in the trade-off between bias and variance!
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于某个中间值，测试分数达到最大。我们正在寻找的就是这个中间阶段，即测试分数达到最大值。这是偏差和方差权衡中的最佳点！
- en: 'This means that we can find the best algorithm for the task at hand by mapping
    out the model complexity landscape. Specifically, we can use the following indicators
    to know which regime we are currently in:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以通过绘制模型复杂度景观图来找到当前任务的最佳算法。具体来说，我们可以使用以下指标来了解我们目前处于哪个阶段：
- en: If both training and test scores are below our expectations, we are probably
    in the leftmost regime in the preceding diagram, where the model is underfitting
    the data. In this case, a good idea might be to increase the model complexity
    and try again.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练分数和测试分数都低于我们的预期，我们可能处于前面图表中最左侧的阶段，即模型欠拟合数据。在这种情况下，一个好的想法可能是增加模型复杂度并再次尝试。
- en: If the training score is much higher than the test score, we are probably in
    the rightmost regime in the preceding diagram, where the model is overfitting
    the data. In this case, a good idea might be to decrease the model complexity
    and try again.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果训练分数远高于测试分数，我们可能处于前面图表中最右侧的阶段，即模型过度拟合数据。在这种情况下，一个好的想法可能是降低模型复杂度并再次尝试。
- en: Although this procedure works in general, there are more sophisticated strategies
    for model evaluation that proved to be more thorough than a simple train-test
    split, which we will talk about in the following sections.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个程序在一般情况下是有效的，但还有更复杂的模型评估策略，这些策略比简单的训练-测试分割更全面，我们将在接下来的章节中讨论。
- en: Understanding cross-validation
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解交叉验证
- en: Cross-validation is a method of evaluating the generalization performance of
    a model that is generally more stable and thorough than splitting the dataset
    into training and test sets.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种评估模型泛化性能的方法，通常比将数据集分为训练集和测试集的方法更稳定、更全面。
- en: The most commonly used version of cross-validation is **k-fold cross-validation**,
    where *k* is a number specified by the user (usually five or ten). Here, the dataset
    is partitioned into *k* parts of more or less equal size, called **folds**. For
    a dataset that contains *N* data points, each fold should hence have approximately
    *N / k* samples. Then, a series of models are trained on the data, using *k -
    1* folds for training and one remaining fold for testing. The procedure is repeated
    for *k* iterations, each time choosing a different fold for ...
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Manually implementing cross-validation in OpenCV
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to perform cross-validation in OpenCV is to do the data splits
    by hand.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to implement two-fold cross-validation, we would perform the following
    procedure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Split the data into two equally sized parts:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instantiate the classifier:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Train the classifier on the first fold, then predict the labels of the second
    fold:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Train the classifier on the second fold, then predict the labels of the first
    fold:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Compute accuracy scores for both folds:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This procedure will yield two accuracy scores, one for the first fold (92% accuracy)
    and one for the second fold (88% accuracy). On average, our classifier hence achieved
    90% accuracy on unseen data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn for k-fold cross-validation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In scikit-learn, cross-validation can be performed in three steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset. Since we already did this earlier, we don't have to do it
    again.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instantiate the classifier:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Perform cross-validation with the `cross_val_score` function. This function
    takes as input a model, the full dataset (`X`), the target labels (`y`), and an
    integer value for the number of folds (`cv`). It is not necessary to split the
    data by hand—the function will do that automatically depending on the number of
    folds. After the cross-validation is completed, the function returns the test
    scores:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Implementing leave-one-out cross-validation
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular way to implement cross-validation is to choose the number of
    folds equal to the number of data points in the dataset. In other words, if there
    are *N* data points, we set *k=N*. This means that we will end up having to do
    *N* iterations of cross-validation, but in every iteration, the training set will
    consist of only a single data point. The advantage of this procedure is that we
    get to use all-but-one data points for training. Hence, this procedure is also
    known as **leave-one-out** cross-validation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, this functionality is provided by the `LeaveOneOut` method
    from the `model_selection` module:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This object can be passed directly to the `cross_val_score` function in the
    following way:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Because every test set now contains a single data point, we would expect the
    scorer to return 150 values—one for each data point in the dataset. Each of these
    points we get could be either right or wrong. Hence, we expect `scores` to be
    a list of ones (`1`) and zeros (`0`), which corresponds to correct and incorrect
    classifications, respectively:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个测试集现在都包含一个数据点，我们预计评分器将返回150个值——对应于数据集中的每个数据点。我们得到的每个点可能是正确的或错误的。因此，我们预计`scores`将是一个包含`1`和`0`的列表，分别对应于正确的和错误的分类：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If we want to know the average performance of the classifier, we would still
    compute the mean and standard deviation of the scores:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想了解分类器的平均性能，我们仍然会计算分数的平均值和标准差：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can see this scoring scheme returns very similar results to five-fold cross-validation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这种评分方案返回的结果与五折交叉验证非常相似。
- en: You can learn more about other useful cross-validation procedures at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html)了解更多关于其他有用的交叉验证过程。
- en: Estimating robustness using bootstrapping
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自助法估计鲁棒性
- en: An alternative procedure to k-fold cross-validation is **bootstrapping**.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与k折交叉验证的另一种方法是**自助法**。
- en: Instead of splitting the data into folds, bootstrapping builds a training set
    by drawing samples randomly from the dataset. Typically, a bootstrap is formed
    by drawing samples with replacement. Imagine putting all of the data points into
    a bag and then drawing randomly from the bag. After drawing a sample, we would
    put it back in the bag. This allows for some samples to show up multiple times
    in the training set, which is something cross-validation does not allow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与将数据分割成折不同，自助法通过从数据集中随机抽取样本来构建训练集。通常，自助法是通过替换抽取样本形成的。想象一下将所有数据点放入一个袋子，然后从袋子中随机抽取。抽取一个样本后，我们会将其放回袋子中。这允许某些样本在训练集中出现多次，这是交叉验证不允许的。
- en: The classifier is then tested on all samples that are not part of the bootstrap
    (the so-called **out-of-bag** examples), and the procedure is repeated a large
    number of times ...
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将分类器测试在所有不属于自助法的样本上（所谓的**袋外**示例），并重复进行大量次 ...
- en: Manually implementing bootstrapping in OpenCV
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中手动实现自助法
- en: 'Bootstrapping can be implemented with the following procedure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法可以通过以下步骤实现：
- en: Load the dataset. Since we already did this earlier, we don't have to do it
    again.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集。由于我们之前已经这样做过了，所以我们不需要再次进行。
- en: 'Instantiate the classifier:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化分类器：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From our dataset with *N* samples, randomly choose *N* samples with replacement
    to form a bootstrap. This can be done most easily with the `choice` function from
    NumPy''s `random` module. We tell the function to draw `len(X)` samples in the
    `[0, len(X)-1]` range with replacement (`replace=True`). The function then returns
    a list of indices, from which we form our bootstrap:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从包含*N*个样本的数据集中，随机选择*N*个样本进行替换以形成自助法。这可以通过NumPy的`random`模块中的`choice`函数最简单地完成。我们告诉函数从`[0,
    len(X)-1]`范围内抽取`len(X)`个样本，并替换（`replace=True`）。然后函数返回一个索引列表，我们据此形成我们的自助法：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Put all samples that do not show in the bootstrap in the out-of-bag set:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有未出现在自助法中的样本放入袋外集：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Train the classifier on the bootstrap samples:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在自助法样本上训练分类器：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Test the classifier on the out-of-bag samples:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在袋外样本上测试分类器：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Repeat *steps 3-6* for a specific number of iterations.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对特定次数的迭代重复*步骤3-6*。
- en: Iteration of the bootstrap. Repeat these steps up to 10,000 times to get 10,000
    accuracy scores, then average the scores to get an idea of the classifier's mean
    performance.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自助法的迭代。重复这些步骤多达10,000次以获得10,000个准确度分数，然后平均这些分数以了解分类器的平均性能。
- en: 'For our convenience, we can build a function from *steps 3*-*6* so that it
    is easy to run the procedure for some `n_iter` number of times. We also pass a
    model (our kNN classifier, `model`), the feature matrix (`X`), and the vector
    with all class labels (`y`):'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们可以从*步骤3*-*6*构建一个函数，以便于运行`n_iter`次数的迭代过程。我们还传递一个模型（我们的kNN分类器，`model`）、特征矩阵（`X`）和包含所有类别标签的向量（`y`）：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The steps within the `for` loop are essentially *steps 3*-*6* from the code
    mentioned earlier. This involved training the classifier on the bootstrap and
    testing it on the out-of-bag examples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`for`循环内的步骤本质上是从前面提到的代码中的*步骤3*-*6*。这涉及到在自助法上训练分类器并在袋外示例上测试它：'
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we need to return the accuracy score. You might expect a `return` statement
    here. However, a more elegant way is to use the `yield` statement, which turns
    the function automatically into a generator. This means we don''t have to initialize
    an empty list (`acc = []`) and then append the new accuracy score at each iteration
    (`acc.append(accuracy_score(...))`). The bookkeeping is done automatically:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要返回准确率分数。你可能在这里期望一个`return`语句。然而，更优雅的方法是使用`yield`语句，这会自动将函数转换为生成器。这意味着我们不需要初始化一个空列表（`acc
    = []`），然后在每次迭代中追加新的准确率分数（`acc.append(accuracy_score(...))`）。记录工作会自动完成：
- en: '[PRE25]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To make sure we all get the same result, let''s fix the seed of the random
    number generator:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们得到相同的结果，让我们固定随机数生成器的种子：
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let''s run the procedure for `n_iter=10` times by converting the function
    output into a list:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过将函数输出转换为列表来运行`n_iter=10`次的程序：
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As you can see, for this small sample we get accuracy scores anywhere between
    92% and 98%. To get a more reliable estimate of the model''s performance, we repeat
    the procedure 1,000 times and calculate both mean and standard deviation of the
    resulting scores:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于这个小样本，我们得到的准确率分数在92%到98%之间。为了更可靠地估计模型性能，我们重复该程序1,000次，并计算结果的均值和标准差：
- en: '[PRE28]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You are always welcome to increase the number of repetitions. But once `n_iter`
    is large enough, the procedure should be robust to the randomness of the sampling
    procedure. In this case, we do not expect to see any more changes to the distribution
    of score values as we keep increasing `n_iter` to, for example, 10,000 iterations:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你随时可以增加重复次数。但一旦`n_iter`足够大，该程序应该对采样过程的随机性具有鲁棒性。在这种情况下，我们预计随着我们将`n_iter`增加到例如10,000次迭代，分数值的分布不会发生更多变化：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Typically, the scores obtained with bootstrapping would be used in a *statistical
    test* to assess the *significance* of our result. Let's have a look at how that
    is done.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用自助法得到的分数会在**统计检验**中用来评估我们结果的**显著性**。让我们看看这是如何操作的。
- en: Assessing the significance of our results
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们结果的显著性
- en: Assume for a moment that we implemented the cross-validation procedure for two
    versions of our kNN classifier. The resulting test scores are—92.34% for Model
    A and 92.73% for Model B. How do we know which model is better?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为我们的kNN分类器的两个版本实现了交叉验证程序。得到的测试分数是——模型A为92.34%，模型B为92.73%。我们如何知道哪个模型更好？
- en: 'Following our logic introduced here, we might argue for Model B because it
    has a better test score. But what if the two models are not significantly different?
    These could have two underlying causes, which are both a consequence of the randomness
    of our testing procedure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这里介绍的逻辑，我们可能会支持模型B，因为它有更好的测试分数。但如果两个模型没有显著差异呢？这可能有两个潜在原因，这两个原因都是我们测试程序随机性的结果：
- en: For all we know, Model B just got lucky. Perhaps we chose a really low k for
    our cross-validation procedure. Perhaps Model B ended up with a beneficial train-test
    split so that the model had no problem classifying ...
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就我们所知，模型B可能只是运气好。也许我们在交叉验证程序中选择了非常低的k值。也许模型B最终得到了一个有益的训练-测试分割，使得模型在分类时没有问题...
- en: Implementing Student's t-test
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Student的t检验
- en: 'One of the most famous statistical tests is **Student''s t-test**. You might
    have heard of it before: it allows us to determine whether two sets of data are
    significantly different from one another. This was a really important test for
    William Sealy Gosset, the inventor of the test, who worked at the Guinness brewery
    and wanted to know whether two batches of stout differed in quality.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的统计检验之一是**Student的t检验**。你可能之前听说过它：它允许我们确定两组数据是否显著不同。这对威廉·西利·高斯来说非常重要，他是这项测试的发明者，他在Guinness酿酒厂工作，想知道两批黑啤的质量是否有所不同。
- en: Note that "Student" here is capitalized. Although Gosset wasn't allowed to publish
    his test due to company policy, he did so anyway under his pen name, Student.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，“Student”在这里是大写的。尽管高斯因公司政策不允许发表他的测试，但他还是以笔名Student发表了。
- en: In practice, the t-test allows us to determine whether two data samples come
    from underlying distributions with the same mean or *expected value*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，t检验允许我们确定两个数据样本是否来自具有相同均值或**期望值**的潜在分布。
- en: For our purposes, this means that we can use the t-test to determine whether
    the test scores of two independent classifiers have the same mean value. We start
    by hypothesizing that the two sets of test scores are identical. We call this
    the *null hypothesis* because this is the hypothesis we want to nullify, that
    is, we are looking for evidence to *reject* the hypothesis because we want to
    ensure that one classifier is significantly better than the other.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We accept or reject a null hypothesis based on a parameter known as the **p-value**
    that the t-test returns. The p-value takes on values between `0` and `1`. A p-value
    of `0.05` would mean that the null hypothesis is right only 5 out of 100 times.
    A small p-value hence indicates strong evidence that the hypothesis can be safely
    rejected. It is customary to use *p=0.05* as a cut-off value below which we reject
    the null hypothesis.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'If this is all too confusing, think of it this way: when we run a t-test to
    compare classifier test scores, we are looking to obtain a small p-value because
    that means that the two classifiers give significantly different results.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement Student''s t-test with SciPy''s `ttest_ind` function from
    the `stats` module:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s start with a simple example. Assume we ran five-fold cross-validation
    on two classifiers and obtained the following scores:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This means that Model A achieved 100% accuracy in all five folds, whereas Model
    B got 0% accuracy. In this case, it is clear that the two results are significantly
    different. If we run the t-test on this data, we should hence find a really small
    p-value:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And we do! We actually get the smallest possible p-value, *p=0.0*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, what if the two classifiers got exactly the same numbers,
    except during different folds? In this case, we would expect the two classifiers
    to be equivalent, which is indicated by a really large p-value:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Analogous to the aforementioned, we get the largest possible p-value, *p=1.0*.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what happens in a more realistic example, let''s return to our kNN classifier
    from an earlier example. Using the test scores obtained from the ten-fold cross-validation
    procedure, we can compare two different kNN classifiers with the following procedure:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain a set of test scores for Model A. We choose Model A to be the kNN classifier
    from earlier (*k=1*):'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Obtain a set of test scores for Model B. Let''s choose Model B to be a kNN
    classifier with *k=3*:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Apply the t-test to both sets of scores:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: As you can see, this is a good example of two classifiers giving different cross-validation
    scores (96.0% and 96.7%) that turn out to be not significantly different! Because
    we get a large p-value (*p=0.777*), we expect the two classifiers to be equivalent
    77 out of 100 times.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Implementing McNemar's test
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A more advanced statistical technique is **McNemar's test**. This test can be
    used on paired data to determine whether there are any differences between the
    two samples. As in the case of the t-test, we can use McNemar's test to determine
    whether two models give significantly different classification results.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: McNemar's test operates on pairs of data points. This means that we need to
    know, for both classifiers, how they classified each data point. Based on the
    number of data points that the first classifier got right but the second got wrong
    and vice versa, we can determine whether the two classifiers are equivalent.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume the preceding Model A and Model B were applied to the same five
    data points. Whereas Model ...
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters with grid search
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most commonly used tool for hyperparameter tuning is *grid search*, which
    is basically a fancy term for saying we will try all possible parameter combinations
    with a `for` loop.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at how that is done in practice.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple grid search
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Returning to our kNN classifier, we find that we have only one hyperparameter
    to tune: *k*. Typically, you would have a much larger number of open parameters
    to mess with, but the kNN algorithm is simple enough for us to manually implement
    a grid search.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get started, we need to split the dataset as we have done before
    into training and test sets:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we choose a 75-25 split:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Then, the goal is to loop over all possible values of *k*. As we do this, we
    want to keep ...
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the value of a validation set
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following our best practice of splitting the data into training and test sets,
    we might be tempted to tell people that we have found a model that performs with
    97.4% accuracy on the dataset. However, our result might not necessarily generalize
    to new data. The argument is the same as earlier on in this book when we warranted
    the train-test split that we need an independent dataset for evaluation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: However, when we implemented a grid search in the last section, we used the
    test set to evaluate the outcome of the grid search and update the hyperparameter, *k*.
    This means we can no longer use the test set to evaluate the final data! Any model
    choices made based on the test set accuracy would leak information from the test
    set into the model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: One way to resolve this data is to split the data again and introduce what is
    known as a **validation set**. The validation set is different from the training
    and test set and is used exclusively for selecting the best parameters of the
    model. It is a good practice to do all exploratory analysis and model selection
    on this validation set and keep a separate test set, which is only used for the
    final evaluation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we should end up splitting the data into three different sets:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: A **training set**, which is used to build the model
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **validation set**, which is used to select the parameters of the model
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **test set**, which is used to evaluate the performance of the final model
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Such a three-way split is illustrated in the following diagram:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55d53de3-6073-4095-a210-4d7bf70204d1.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows an example of how to split a dataset into training,
    validation, and test sets. In practice, the three-way split is achieved in two
    steps:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into two chunks: one that contains training and validation sets
    and another that contains the test set:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Split `X_trainval` again into proper training and validation sets:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, we repeat the manual grid search from the preceding code, but this time,
    we will use the validation set to find the best *k* (see code highlights):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We now find that a 100% validation score (`best_acc`) can be achieved with *k=7*
    (`best_k`)! However, recall that this score might be overly optimistic. To find
    out how well the model really performs, we need to test it on held-out data from
    the test set.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: To arrive at our final model, we can use the value for *k* we found during grid
    search and re-train the model on both the training and validation data. This way,
    we used as much data as possible to build the model while still honoring the train-test
    split principle.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'This means we should retrain the model on `X_trainval`, which contains both
    the training and validation sets and score it on the test set:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: With this procedure, we find a formidable score of 94.7% accuracy on the test
    set. Because we honored the train-test split principle, we can now be sure that
    this is the performance we can expect from the classifier when applied to novel
    data. It is not as high as the 100% accuracy reported during validation, but it
    is still a very good score!
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Combining grid search with cross-validation
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One potential danger of the grid search we just implemented is that the outcome
    might be relatively sensitive to how exactly we split the data. After all, we
    might have accidentally chosen a split that put most of the easy-to-classify data
    points in the test set, resulting in an overly optimistic score. Although we would
    be happy at first, as soon as we tried the model on some new held-out data, we
    would find that the actual performance of the classifier is much lower than expected.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can combine grid search with cross-validation. This way, the data
    is split multiple times into training and validation sets, and cross-validation
    is performed at every step of the grid search to evaluate ...
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Combining grid search with nested cross-validation
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although grid search with cross-validation makes for a much more robust model
    selection procedure, you might have noticed that we performed the split into training
    and validation sets still only once. As a result, our results might still depend
    too much on the exact training-validation split of the data.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of splitting the data into training and validation sets once, we can
    go a step further and use multiple splits for cross-validation. This will result
    in what is known as **nested cross-validation**, and the process is illustrated
    in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步将数据一次分为训练集和验证集，并使用多个分割进行交叉验证。这将导致所谓的**嵌套交叉验证**，其过程在下图中说明：
- en: '![](img/5e469a29-e08e-478e-8c5e-fea9a6a789f7.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5e469a29-e08e-478e-8c5e-fea9a6a789f7.png)'
- en: In nested cross-validation, there is an outer loop over the grid search box
    that repeatedly splits the data into training and validation sets. For each of
    these splits, a grid search is run, which will report back a set of best parameter
    values. Then, for each outer split, we get a test score using the best settings.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌套交叉验证中，有一个外层循环遍历网格搜索框，反复将数据分为训练集和验证集。对于这些分割中的每一个，都会运行一次网格搜索，并返回一组最佳参数值。然后，对于每个外部分割，我们使用最佳设置得到一个测试分数。
- en: Running a grid search over many parameters and on large datasets can be computationally
    intensive. A particular parameter setting on a particular cross-validation split
    can be done completely independently from the other parameter settings and models.
    Hence, parallelization over multiple CPU cores or a cluster is very important
    for grid search and cross-validation.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多参数和大数据集上运行网格搜索可能会非常计算密集。在特定的交叉验证分割上的特定参数设置可以完全独立于其他参数设置和模型进行。因此，对于网格搜索和交叉验证，跨多个CPU核心或集群的并行化非常重要。
- en: Now that we know how to find the best parameters of a model, let's take a closer
    look at the different evaluation metrics that we can use to score a model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何找到模型的最佳参数，让我们更详细地看看我们可以用来评分模型的不同评估指标。
- en: Scoring models using different evaluation metrics
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同的评估指标评分模型
- en: So far, we have evaluated classification performance using accuracy (the fraction
    of correctly classified samples) and regression performance using R². However,
    these are only two of the many possible ways to summarize how well a supervised
    model performs on a given dataset. In practice, these evaluation metrics might
    not be appropriate for our application, and it is important to choose the right
    metric when selecting between models and adjusting parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用准确率（正确分类样本的分数）评估分类性能，使用R²评估回归性能。然而，这些只是许多可能的总结监督模型在给定数据集上表现良好的方法之一。在实践中，这些评估指标可能不适合我们的应用，因此在选择模型和调整参数时选择正确的指标非常重要。
- en: When selecting a metric, we should always have the end goal of the machine learning
    application in mind. In practice, we are usually interested not just in making
    accurate predictions but also in using these predictions as part of a larger ...
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择指标时，我们应始终牢记机器学习应用的最终目标。在实践中，我们通常不仅对做出准确的预测感兴趣，而且希望将这些预测作为更大系统的一部分 ...
- en: Choosing the right classification metric
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择正确的分类指标
- en: 'We talked about several essential scoring functions in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*. Among the most fundamental metrics for classification
    were the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)，“监督学习的第一步”中讨论了几个重要的评分函数。在分类的最基本指标中包括以下内容：
- en: '**Accuracy**: This counts the number of data points in the test set that have
    been predicted correctly and returns that number as a fraction of the test set
    size (`sklearn.metrics.accuracy_score`). This is the most basic scoring function
    for classifiers, and we have made extensive use of it throughout this book.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：这计算测试集中被正确预测的数据点的数量，并将其作为测试集大小的分数返回（`sklearn.metrics.accuracy_score`）。这是分类器最基本的成绩函数，我们在整本书中广泛使用了它。'
- en: '**Precision**: This describes the ability of a classifier not to label a positive
    sample as a negative (`sklearn.metrics.precision_score`).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**：这描述了分类器不将正样本标记为负样本的能力（`sklearn.metrics.precision_score`）。'
- en: '**Recall** **(or sensitivity)**: This describes the ability of a classifier
    to retrieve all of the positive samples (`sklearn.metrics.recall_score`).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率** **（或灵敏度）**：这描述了分类器检索所有正样本的能力（`sklearn.metrics.recall_score`）。'
- en: Although precision and recall are important measures, looking at only one of
    them will not give us a good idea of the big picture. One way to summarize the
    two measures is known as the **f-score** or **f-measure** (`sklearn.metrics.f1_score`),
    which computes the harmonic mean of precision and recall as *2(precision x recall)
    / (precision + recall).*
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然精确度和召回率是重要的度量，但只看其中一个并不能给我们一个对整体情况的良好理解。总结这两个度量的一种方法被称为**f分数**或**f度量**（`sklearn.metrics.f1_score`），它计算精确度和召回率的调和平均值，即*2(精确度
    x 召回率) / (精确度 + 召回率)*。
- en: Sometimes we need to do more than maximize accuracy. For example, if we are
    using machine learning in a commercial application, then the decision-making should
    be driven by the business goals. One of these goals might be to guarantee at least
    90% recall. The challenge then becomes to develop a model that still has reasonable
    accuracy while satisfying all secondary requirements. Setting goals like this
    is often called **setting the operating point**.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候我们需要做的不仅仅是最大化准确性。例如，如果我们在一个商业应用中使用机器学习，那么决策应该由业务目标驱动。这些目标之一可能保证至少90%的召回率。那么挑战就变成了开发一个模型，它仍然具有合理的准确性，同时满足所有次要要求。设定这样的目标通常被称为**设定操作点**。
- en: However, it is often not clear what the operating point should be when developing
    a new system. To understand the problem better, it is important to investigate
    all possible trade-offs of precision and recall them at once. This is possible
    using a tool called the **precision-recall curve** (`sklearn.metrics.precision_recall_curve`).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在开发新系统时，往往不清楚操作点应该是什么。为了更好地理解问题，重要的是要调查所有可能的精确度和召回率的权衡，并一次性记录下来。这可以通过一个名为**精确度-召回率曲线**（`sklearn.metrics.precision_recall_curve`）的工具来实现。
- en: Another commonly used tool to analyze the behavior of classifiers is the **Receiver
    Operating Characteristic** (**ROC**) curve.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 分析分类器行为的另一个常用工具是**接收者操作特征**（**ROC**）曲线。
- en: The ROC curve considers all possible thresholds for a given classifier similar
    to the precision-recall curve, but it shows the *false positive rate* against
    the *true positive rate* instead of reporting precision and recall.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线考虑了给定分类器的所有可能阈值，类似于精确度-召回率曲线，但它显示的是*假阳性率*与*真阳性率*之间的关系，而不是报告精确度和召回率。
- en: Choosing the right regression metric
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的回归度量
- en: 'Evaluation for regression can be done in similar detail as we did for classification.
    In [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First Steps in Supervised
    Learning*, we also talked about some fundamental metrics for regression:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 回归的评估可以像我们在分类中做的那样进行详细的评估。在[第3章](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml)，*监督学习的第一步*中，我们也讨论了一些回归的基本度量：
- en: '**Mean squared error**: The most commonly used error metric for regression
    problems is to measure the squared error between the predicted and true target
    value for every data point in the training set, averaged across all data points
    (`sklearn.metrics.mean_squared_error`).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**：回归问题中最常用的误差度量是测量训练集中每个数据点的预测值和真实目标值之间的平方误差，并对所有数据点进行平均（`sklearn.metrics.mean_squared_error`）。'
- en: '**Explained variance**: A more sophisticated metric is to measure to what degree
    a model can explain the variation or dispersion of the test data (`sklearn.metrics.explained_variance_score`).
    Often, the amount of explained variance is measured using ...'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释方差**：一个更复杂的度量是测量模型可以解释测试数据变异或分散的程度（`sklearn.metrics.explained_variance_score`）。通常，解释方差的量是通过...来测量的。'
- en: Chaining algorithms together to form a pipeline
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将算法链接起来形成管道
- en: Most machine learning problems we have discussed so far consist of at least
    a preprocessing step and a classification step. The more complicated the problem,
    the longer this *processing chain* might get. One convenient way to glue multiple
    processing steps together and even use them in grid search is by using the `Pipeline`
    class from scikit-learn.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止讨论的大多数机器学习问题至少包括一个预处理步骤和一个分类步骤。问题越复杂，这个*处理链*可能就越长。一个方便的方法是将多个处理步骤粘合在一起，甚至使用`Pipeline`类从scikit-learn中进行网格搜索。
- en: Implementing pipelines in scikit-learn
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现管道
- en: The `Pipeline` class itself has a `fit`, a `predict`, and a `score` method,
    which all behave just like any other estimator in scikit-learn. The most common
    use case of the `Pipeline` class is to chain different preprocessing steps together
    with a supervised model like a classifier.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to the breast cancer dataset from [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml),
    *Using Decision Trees to Make a Medical Diagnosis*. Using scikit-learn, we import
    the dataset and split it into training and test sets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Using pipelines in grid searches
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using a pipeline in a grid search works the same way as using any other estimator.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: We define a parameter grid to search over and construct `GridSearchCV` from
    the pipeline and the parameter grid. When specifying the parameter grid, there
    is, however, a slight change. We need to specify for each parameter which step
    of the pipeline it belongs to. Both parameters that we want to adjust, `C` and
    `gamma`, are parameters of `SVC`. In the preceding section, we gave this step
    the name `"svm"`. The syntax to define a parameter grid for a pipeline is to specify
    for each parameter the step name, followed by `__` (a double underscore), followed
    by the parameter name.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we would construct the parameter grid as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With this parameter grid, we can use `GridSearchCV` as usual:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The best score in the grid is stored in `best_score_`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Similarly, the best parameters are stored in `best_params_`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'But recall that the cross-validation score might be overly optimistic. To know
    the true performance of the classifier, we need to score it on the test set:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In contrast to the grid search we did before, now, for each split in the cross-validation,
    `MinMaxScaler` is refit with only the training splits, and no information is leaked
    from the test split into the parameter search.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: This makes it easy to build a pipeline to chain together a whole variety of
    steps! You can mix and match estimators in the pipeline at will, you just need
    to make sure that every step in the pipeline provides a `transform` method (except
    for the last step). This allows an estimator in the pipeline to produce a new
    representation of the data, which, in turn, can be used as input to the next step.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The `Pipeline` class is not restricted to preprocessing and classification but
    can, in fact, join any number of estimators together. For example, we could build
    a pipeline containing feature extraction, feature selection, scaling, and classification,
    for a total of four steps. Similarly, the last step could be regression or clustering
    instead of classification.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we tried to complement our existing machine learning skills
    by discussing best practices in model selection and hyperparameter tuning. You
    learned how to tweak the hyperparameters of a model using grid search and cross-validation
    in both OpenCV and scikit-learn. We also talked about a wide variety of evaluation
    metrics and how to chain algorithms into a pipeline. Now, you are almost ready
    to start working on some real-world problems on your own.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们试图通过讨论模型选择和超参数调整的最佳实践来补充我们现有的机器学习技能。你学习了如何使用网格搜索和交叉验证在OpenCV和scikit-learn中调整模型的超参数。我们还讨论了广泛的评估指标以及如何将算法链接到一个管道中。现在，你几乎准备好开始独立解决一些实际问题了。
- en: In the next chapter, you will be introduced to an exciting and a new topic,
    that is, OpenVINO toolkit, which was one of the key releases in OpenCV 4.0.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将接触到一项既令人兴奋又全新的主题，那就是OpenVINO工具包，它是OpenCV 4.0中的关键发布之一。
