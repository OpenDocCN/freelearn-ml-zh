- en: Selecting the Right Model with Hyperparameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have explored a wide variety of machine learning algorithms, I am
    sure you have realized that most of them come with a great number of settings
    to choose from. These settings or tuning knobs, the so-called **hyperparameters**,
    help us to control the behavior of the algorithm when we try to maximize performance.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we might want to choose the depth or split criterion in a decision
    tree or tune the number of neurons in a neural network. Finding the values of
    important parameters of a model is a tricky task but necessary for almost all
    models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive deeper into **model evaluation** and **hyperparameter
    tuning**. Assume that we have two different ...
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer to the code for this chapter from the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter11).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: You will need OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any operating system—macOS, Windows, and Linux-based OSes along
    with this book. We recommend you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided along with this book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Model evaluation strategies come in many different forms and shapes. In the
    following sections, we will, therefore, highlight three of the most commonly used
    techniques to compare models against each other:'
  prefs: []
  type: TYPE_NORMAL
- en: k-fold cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bootstrapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McNemar's test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In principle, model evaluation is simple: after training a model on some data,
    we can estimate its effectiveness by comparing model predictions to some ground
    truth values. We learned early on that we should split the data into training
    and test sets, and we tried to follow this instruction whenever possible. But
    why exactly did we do that again?'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model the wrong way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reason we never evaluate a model on the training set is that, in principle,
    any dataset can be learned if we throw a strong enough model at it.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick demonstration of this can be given with the help of the Iris dataset,
    which we talked about extensively in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*. There, the goal was to classify species
    of iris flowers based on their physical dimensions. We can load the Iris dataset
    using scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'An innocent approach to this problem would be to store all data points in the
    matrix, `X`, and all class labels in the vector, `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we choose a model and its hyperparameters. For example, let''s use the
    algorithm from [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First
    Steps in Supervised Learning*, which provides only a single hyperparameter: the
    number of neighbors, *k*. With *k=1*, we get a very simple model that classifies
    the label of an unknown point as belonging to the same class as its closest neighbor.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn how to build a **k-nearest neighbour**
    (**k-NN**) and compute its accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In OpenCV, kNN instantiates as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train the model and use it to predict labels for the data that we
    already know:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we compute the fraction of the correctly labeled points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the accuracy score is `1.0`, it indicates that 100% of points
    were correctly labeled by our model.
  prefs: []
  type: TYPE_NORMAL
- en: If a model gets 100% accuracy on the training set, we say the model memorized
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: But is the expected accuracy truly being measured? Have we come up with a model
    that we expect to be correct 100% of the time?
  prefs: []
  type: TYPE_NORMAL
- en: As you may have gathered, the answer is no. This example shows that even a simple
    algorithm is capable of memorizing a real-world dataset. Imagine how easy this
    task would have been for a deep neural network! Usually, the more parameters a
    model has, the more powerful it is. We will come back to this shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model in the right way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A better sense of a model's performance can be found using what's known as a
    test set, but you already knew this. When presented with data held out from the
    training procedure, we can check whether a model has learned some dependencies
    in the data that hold across the board or whether it just memorized the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can split the data into training and test sets using the familiar `train_test_split`
    from scikit-learn''s `model_selection` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: But how do we choose the right train-test ratio? Is there even such a thing
    as a right ratio? Or is this considered another hyperparameter of the model?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two competing concerns here:'
  prefs: []
  type: TYPE_NORMAL
- en: If our ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the best model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a model is under-performing, it is often not clear how to make it better.
    Throughout this book, I have declared a rule of thumbs, for example, how to select
    the number of layers in a neural network. Even worse, the answer is often counter-intuitive!
    For example, adding another layer to the network might make the results worse,
    and adding more training data might not change performance at all.
  prefs: []
  type: TYPE_NORMAL
- en: You can see why these issues are some of the most important aspects of machine
    learning. At the end of the day, the ability to determine what steps will or will
    not improve our model is what separates the successful machine learning practitioner
    from all others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a specific example. Remember [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml),
    *Using Decision Trees to Make a Medical Diagnosis*, where we used decision trees
    in a regression task? We were fitting two different trees to a sin function—one
    with depth 2 and one with depth 5\. As a reminder, the regression result looked
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e4b66301-dbb4-4153-b592-ef5624085515.png)'
  prefs: []
  type: TYPE_IMG
- en: It should be clear that neither of these fits are particularly good. However,
    the two decision trees fail in two different ways!
  prefs: []
  type: TYPE_NORMAL
- en: The decision tree with depth 2 (thick line in the preceding screenshot) attempts
    to fit four straight lines through the data. Because the data is intrinsically
    more complicated than a few straight lines, this model fails. We could train it
    as much as we wanted, on as many training samples as we could generate—it would
    never be able to describe this dataset well. Such a model is said to underfit
    the data. In other words, the model does not have enough complexity to account
    for all the features in the data. Hence, the model has a high bias.
  prefs: []
  type: TYPE_NORMAL
- en: The other decision tree (thin line, depth 5) makes a different mistake. This
    model has enough flexibility to nearly perfectly account for the fine structures
    in the data. However, at some points, the model seems to follow the particular
    pattern of the noise; we added to the sin function rather than the sin function
    itself. You can see that on the right-hand side of the graph, where the blue curve
    (thin line) would jitter a lot. Such a model is said to overfit the data. In other
    words, the model is so complex that it ends up accounting for random errors in
    the data. Hence, the model has a high variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Long story short—here''s the secret sauce: fundamentally, selecting the right
    model comes down to finding a sweet spot in the trade-off between bias and variance.'
  prefs: []
  type: TYPE_NORMAL
- en: The amount of flexibility a model has (also known as the **model complexity**)
    is mostly dictated by its hyperparameters. That is why it is so important to tune
    them!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to the kNN algorithm and the Iris dataset. If we repeated the
    procedure of fitting the model to the Iris data for all possible values of *k*
    and calculated both training and test scores, we would expect the result to look
    something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/67d69189-ef9f-41e5-baca-de06a01932b6.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows model score as a function of model complexity. If
    there is one thing I would want you to remember from this chapter, it would be
    this diagram. Let's unpack it.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram describes the model score (either training or test scores) as a
    function of model complexity. As mentioned in the preceding diagram, the model
    complexity of a neural network roughly grows with the number of neurons in the
    network. In the case of kNN, the opposite logic applies—the larger the value for
    *k*, the smoother the decision boundary, and hence, the lower the complexity.
    In other words, kNN with *k=1* would be all of the way to the right in the preceding
    diagram, where the training score is perfect. No wonder we got 100% accuracy on
    the training set!
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding diagram, we can gather that there are three regimes in the
    model complexity landscape:'
  prefs: []
  type: TYPE_NORMAL
- en: Very low model complexity (a high-bias model) underfits the training data. In
    this regime, the model achieves only low scores on both the training and test
    set, no matter how long we trained it for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A model with very high complexity (or a high-variance) overfits the training
    data, which indicates that the model can predict on the training data very well
    but fails on the unseen data. In this regime, the model has started to learn intricacies
    or peculiarities that only appear in the training data. Since these peculiarities
    do not apply to unseen data, the training score gets lower and lower.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some intermediate value, the test score is maximal. It is this intermediate
    regime, where the test score is maximal, that we are trying to find. This is the
    sweet spot in the trade-off between bias and variance!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means that we can find the best algorithm for the task at hand by mapping
    out the model complexity landscape. Specifically, we can use the following indicators
    to know which regime we are currently in:'
  prefs: []
  type: TYPE_NORMAL
- en: If both training and test scores are below our expectations, we are probably
    in the leftmost regime in the preceding diagram, where the model is underfitting
    the data. In this case, a good idea might be to increase the model complexity
    and try again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the training score is much higher than the test score, we are probably in
    the rightmost regime in the preceding diagram, where the model is overfitting
    the data. In this case, a good idea might be to decrease the model complexity
    and try again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this procedure works in general, there are more sophisticated strategies
    for model evaluation that proved to be more thorough than a simple train-test
    split, which we will talk about in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation is a method of evaluating the generalization performance of
    a model that is generally more stable and thorough than splitting the dataset
    into training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used version of cross-validation is **k-fold cross-validation**,
    where *k* is a number specified by the user (usually five or ten). Here, the dataset
    is partitioned into *k* parts of more or less equal size, called **folds**. For
    a dataset that contains *N* data points, each fold should hence have approximately
    *N / k* samples. Then, a series of models are trained on the data, using *k -
    1* folds for training and one remaining fold for testing. The procedure is repeated
    for *k* iterations, each time choosing a different fold for ...
  prefs: []
  type: TYPE_NORMAL
- en: Manually implementing cross-validation in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to perform cross-validation in OpenCV is to do the data splits
    by hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to implement two-fold cross-validation, we would perform the following
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into two equally sized parts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the classifier on the first fold, then predict the labels of the second
    fold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the classifier on the second fold, then predict the labels of the first
    fold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute accuracy scores for both folds:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This procedure will yield two accuracy scores, one for the first fold (92% accuracy)
    and one for the second fold (88% accuracy). On average, our classifier hence achieved
    90% accuracy on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Using scikit-learn for k-fold cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In scikit-learn, cross-validation can be performed in three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset. Since we already did this earlier, we don't have to do it
    again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instantiate the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform cross-validation with the `cross_val_score` function. This function
    takes as input a model, the full dataset (`X`), the target labels (`y`), and an
    integer value for the number of folds (`cv`). It is not necessary to split the
    data by hand—the function will do that automatically depending on the number of
    folds. After the cross-validation is completed, the function returns the test
    scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Implementing leave-one-out cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another popular way to implement cross-validation is to choose the number of
    folds equal to the number of data points in the dataset. In other words, if there
    are *N* data points, we set *k=N*. This means that we will end up having to do
    *N* iterations of cross-validation, but in every iteration, the training set will
    consist of only a single data point. The advantage of this procedure is that we
    get to use all-but-one data points for training. Hence, this procedure is also
    known as **leave-one-out** cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, this functionality is provided by the `LeaveOneOut` method
    from the `model_selection` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This object can be passed directly to the `cross_val_score` function in the
    following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Because every test set now contains a single data point, we would expect the
    scorer to return 150 values—one for each data point in the dataset. Each of these
    points we get could be either right or wrong. Hence, we expect `scores` to be
    a list of ones (`1`) and zeros (`0`), which corresponds to correct and incorrect
    classifications, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to know the average performance of the classifier, we would still
    compute the mean and standard deviation of the scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can see this scoring scheme returns very similar results to five-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about other useful cross-validation procedures at [http://scikit-learn.org/stable/modules/cross_validation.html](http://scikit-learn.org/stable/modules/cross_validation.html).
  prefs: []
  type: TYPE_NORMAL
- en: Estimating robustness using bootstrapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An alternative procedure to k-fold cross-validation is **bootstrapping**.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of splitting the data into folds, bootstrapping builds a training set
    by drawing samples randomly from the dataset. Typically, a bootstrap is formed
    by drawing samples with replacement. Imagine putting all of the data points into
    a bag and then drawing randomly from the bag. After drawing a sample, we would
    put it back in the bag. This allows for some samples to show up multiple times
    in the training set, which is something cross-validation does not allow.
  prefs: []
  type: TYPE_NORMAL
- en: The classifier is then tested on all samples that are not part of the bootstrap
    (the so-called **out-of-bag** examples), and the procedure is repeated a large
    number of times ...
  prefs: []
  type: TYPE_NORMAL
- en: Manually implementing bootstrapping in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bootstrapping can be implemented with the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset. Since we already did this earlier, we don't have to do it
    again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Instantiate the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From our dataset with *N* samples, randomly choose *N* samples with replacement
    to form a bootstrap. This can be done most easily with the `choice` function from
    NumPy''s `random` module. We tell the function to draw `len(X)` samples in the
    `[0, len(X)-1]` range with replacement (`replace=True`). The function then returns
    a list of indices, from which we form our bootstrap:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Put all samples that do not show in the bootstrap in the out-of-bag set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the classifier on the bootstrap samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Test the classifier on the out-of-bag samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Repeat *steps 3-6* for a specific number of iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iteration of the bootstrap. Repeat these steps up to 10,000 times to get 10,000
    accuracy scores, then average the scores to get an idea of the classifier's mean
    performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For our convenience, we can build a function from *steps 3*-*6* so that it
    is easy to run the procedure for some `n_iter` number of times. We also pass a
    model (our kNN classifier, `model`), the feature matrix (`X`), and the vector
    with all class labels (`y`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps within the `for` loop are essentially *steps 3*-*6* from the code
    mentioned earlier. This involved training the classifier on the bootstrap and
    testing it on the out-of-bag examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to return the accuracy score. You might expect a `return` statement
    here. However, a more elegant way is to use the `yield` statement, which turns
    the function automatically into a generator. This means we don''t have to initialize
    an empty list (`acc = []`) and then append the new accuracy score at each iteration
    (`acc.append(accuracy_score(...))`). The bookkeeping is done automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To make sure we all get the same result, let''s fix the seed of the random
    number generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s run the procedure for `n_iter=10` times by converting the function
    output into a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, for this small sample we get accuracy scores anywhere between
    92% and 98%. To get a more reliable estimate of the model''s performance, we repeat
    the procedure 1,000 times and calculate both mean and standard deviation of the
    resulting scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You are always welcome to increase the number of repetitions. But once `n_iter`
    is large enough, the procedure should be robust to the randomness of the sampling
    procedure. In this case, we do not expect to see any more changes to the distribution
    of score values as we keep increasing `n_iter` to, for example, 10,000 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Typically, the scores obtained with bootstrapping would be used in a *statistical
    test* to assess the *significance* of our result. Let's have a look at how that
    is done.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing the significance of our results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Assume for a moment that we implemented the cross-validation procedure for two
    versions of our kNN classifier. The resulting test scores are—92.34% for Model
    A and 92.73% for Model B. How do we know which model is better?
  prefs: []
  type: TYPE_NORMAL
- en: 'Following our logic introduced here, we might argue for Model B because it
    has a better test score. But what if the two models are not significantly different?
    These could have two underlying causes, which are both a consequence of the randomness
    of our testing procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: For all we know, Model B just got lucky. Perhaps we chose a really low k for
    our cross-validation procedure. Perhaps Model B ended up with a beneficial train-test
    split so that the model had no problem classifying ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Student's t-test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most famous statistical tests is **Student''s t-test**. You might
    have heard of it before: it allows us to determine whether two sets of data are
    significantly different from one another. This was a really important test for
    William Sealy Gosset, the inventor of the test, who worked at the Guinness brewery
    and wanted to know whether two batches of stout differed in quality.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that "Student" here is capitalized. Although Gosset wasn't allowed to publish
    his test due to company policy, he did so anyway under his pen name, Student.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the t-test allows us to determine whether two data samples come
    from underlying distributions with the same mean or *expected value*.
  prefs: []
  type: TYPE_NORMAL
- en: For our purposes, this means that we can use the t-test to determine whether
    the test scores of two independent classifiers have the same mean value. We start
    by hypothesizing that the two sets of test scores are identical. We call this
    the *null hypothesis* because this is the hypothesis we want to nullify, that
    is, we are looking for evidence to *reject* the hypothesis because we want to
    ensure that one classifier is significantly better than the other.
  prefs: []
  type: TYPE_NORMAL
- en: We accept or reject a null hypothesis based on a parameter known as the **p-value**
    that the t-test returns. The p-value takes on values between `0` and `1`. A p-value
    of `0.05` would mean that the null hypothesis is right only 5 out of 100 times.
    A small p-value hence indicates strong evidence that the hypothesis can be safely
    rejected. It is customary to use *p=0.05* as a cut-off value below which we reject
    the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this is all too confusing, think of it this way: when we run a t-test to
    compare classifier test scores, we are looking to obtain a small p-value because
    that means that the two classifiers give significantly different results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement Student''s t-test with SciPy''s `ttest_ind` function from
    the `stats` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with a simple example. Assume we ran five-fold cross-validation
    on two classifiers and obtained the following scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that Model A achieved 100% accuracy in all five folds, whereas Model
    B got 0% accuracy. In this case, it is clear that the two results are significantly
    different. If we run the t-test on this data, we should hence find a really small
    p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: And we do! We actually get the smallest possible p-value, *p=0.0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, what if the two classifiers got exactly the same numbers,
    except during different folds? In this case, we would expect the two classifiers
    to be equivalent, which is indicated by a really large p-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Analogous to the aforementioned, we get the largest possible p-value, *p=1.0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what happens in a more realistic example, let''s return to our kNN classifier
    from an earlier example. Using the test scores obtained from the ten-fold cross-validation
    procedure, we can compare two different kNN classifiers with the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain a set of test scores for Model A. We choose Model A to be the kNN classifier
    from earlier (*k=1*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Obtain a set of test scores for Model B. Let''s choose Model B to be a kNN
    classifier with *k=3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the t-test to both sets of scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, this is a good example of two classifiers giving different cross-validation
    scores (96.0% and 96.7%) that turn out to be not significantly different! Because
    we get a large p-value (*p=0.777*), we expect the two classifiers to be equivalent
    77 out of 100 times.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing McNemar's test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A more advanced statistical technique is **McNemar's test**. This test can be
    used on paired data to determine whether there are any differences between the
    two samples. As in the case of the t-test, we can use McNemar's test to determine
    whether two models give significantly different classification results.
  prefs: []
  type: TYPE_NORMAL
- en: McNemar's test operates on pairs of data points. This means that we need to
    know, for both classifiers, how they classified each data point. Based on the
    number of data points that the first classifier got right but the second got wrong
    and vice versa, we can determine whether the two classifiers are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume the preceding Model A and Model B were applied to the same five
    data points. Whereas Model ...
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters with grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most commonly used tool for hyperparameter tuning is *grid search*, which
    is basically a fancy term for saying we will try all possible parameter combinations
    with a `for` loop.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at how that is done in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Returning to our kNN classifier, we find that we have only one hyperparameter
    to tune: *k*. Typically, you would have a much larger number of open parameters
    to mess with, but the kNN algorithm is simple enough for us to manually implement
    a grid search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we get started, we need to split the dataset as we have done before
    into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we choose a 75-25 split:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Then, the goal is to loop over all possible values of *k*. As we do this, we
    want to keep ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the value of a validation set
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following our best practice of splitting the data into training and test sets,
    we might be tempted to tell people that we have found a model that performs with
    97.4% accuracy on the dataset. However, our result might not necessarily generalize
    to new data. The argument is the same as earlier on in this book when we warranted
    the train-test split that we need an independent dataset for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: However, when we implemented a grid search in the last section, we used the
    test set to evaluate the outcome of the grid search and update the hyperparameter, *k*.
    This means we can no longer use the test set to evaluate the final data! Any model
    choices made based on the test set accuracy would leak information from the test
    set into the model.
  prefs: []
  type: TYPE_NORMAL
- en: One way to resolve this data is to split the data again and introduce what is
    known as a **validation set**. The validation set is different from the training
    and test set and is used exclusively for selecting the best parameters of the
    model. It is a good practice to do all exploratory analysis and model selection
    on this validation set and keep a separate test set, which is only used for the
    final evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, we should end up splitting the data into three different sets:'
  prefs: []
  type: TYPE_NORMAL
- en: A **training set**, which is used to build the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **validation set**, which is used to select the parameters of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **test set**, which is used to evaluate the performance of the final model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Such a three-way split is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55d53de3-6073-4095-a210-4d7bf70204d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding diagram shows an example of how to split a dataset into training,
    validation, and test sets. In practice, the three-way split is achieved in two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the data into two chunks: one that contains training and validation sets
    and another that contains the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Split `X_trainval` again into proper training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we repeat the manual grid search from the preceding code, but this time,
    we will use the validation set to find the best *k* (see code highlights):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We now find that a 100% validation score (`best_acc`) can be achieved with *k=7*
    (`best_k`)! However, recall that this score might be overly optimistic. To find
    out how well the model really performs, we need to test it on held-out data from
    the test set.
  prefs: []
  type: TYPE_NORMAL
- en: To arrive at our final model, we can use the value for *k* we found during grid
    search and re-train the model on both the training and validation data. This way,
    we used as much data as possible to build the model while still honoring the train-test
    split principle.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means we should retrain the model on `X_trainval`, which contains both
    the training and validation sets and score it on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: With this procedure, we find a formidable score of 94.7% accuracy on the test
    set. Because we honored the train-test split principle, we can now be sure that
    this is the performance we can expect from the classifier when applied to novel
    data. It is not as high as the 100% accuracy reported during validation, but it
    is still a very good score!
  prefs: []
  type: TYPE_NORMAL
- en: Combining grid search with cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One potential danger of the grid search we just implemented is that the outcome
    might be relatively sensitive to how exactly we split the data. After all, we
    might have accidentally chosen a split that put most of the easy-to-classify data
    points in the test set, resulting in an overly optimistic score. Although we would
    be happy at first, as soon as we tried the model on some new held-out data, we
    would find that the actual performance of the classifier is much lower than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we can combine grid search with cross-validation. This way, the data
    is split multiple times into training and validation sets, and cross-validation
    is performed at every step of the grid search to evaluate ...
  prefs: []
  type: TYPE_NORMAL
- en: Combining grid search with nested cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although grid search with cross-validation makes for a much more robust model
    selection procedure, you might have noticed that we performed the split into training
    and validation sets still only once. As a result, our results might still depend
    too much on the exact training-validation split of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of splitting the data into training and validation sets once, we can
    go a step further and use multiple splits for cross-validation. This will result
    in what is known as **nested cross-validation**, and the process is illustrated
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e469a29-e08e-478e-8c5e-fea9a6a789f7.png)'
  prefs: []
  type: TYPE_IMG
- en: In nested cross-validation, there is an outer loop over the grid search box
    that repeatedly splits the data into training and validation sets. For each of
    these splits, a grid search is run, which will report back a set of best parameter
    values. Then, for each outer split, we get a test score using the best settings.
  prefs: []
  type: TYPE_NORMAL
- en: Running a grid search over many parameters and on large datasets can be computationally
    intensive. A particular parameter setting on a particular cross-validation split
    can be done completely independently from the other parameter settings and models.
    Hence, parallelization over multiple CPU cores or a cluster is very important
    for grid search and cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to find the best parameters of a model, let's take a closer
    look at the different evaluation metrics that we can use to score a model.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring models using different evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have evaluated classification performance using accuracy (the fraction
    of correctly classified samples) and regression performance using R². However,
    these are only two of the many possible ways to summarize how well a supervised
    model performs on a given dataset. In practice, these evaluation metrics might
    not be appropriate for our application, and it is important to choose the right
    metric when selecting between models and adjusting parameters.
  prefs: []
  type: TYPE_NORMAL
- en: When selecting a metric, we should always have the end goal of the machine learning
    application in mind. In practice, we are usually interested not just in making
    accurate predictions but also in using these predictions as part of a larger ...
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right classification metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We talked about several essential scoring functions in [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml),
    *First Steps in Supervised Learning*. Among the most fundamental metrics for classification
    were the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This counts the number of data points in the test set that have
    been predicted correctly and returns that number as a fraction of the test set
    size (`sklearn.metrics.accuracy_score`). This is the most basic scoring function
    for classifiers, and we have made extensive use of it throughout this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: This describes the ability of a classifier not to label a positive
    sample as a negative (`sklearn.metrics.precision_score`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** **(or sensitivity)**: This describes the ability of a classifier
    to retrieve all of the positive samples (`sklearn.metrics.recall_score`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although precision and recall are important measures, looking at only one of
    them will not give us a good idea of the big picture. One way to summarize the
    two measures is known as the **f-score** or **f-measure** (`sklearn.metrics.f1_score`),
    which computes the harmonic mean of precision and recall as *2(precision x recall)
    / (precision + recall).*
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes we need to do more than maximize accuracy. For example, if we are
    using machine learning in a commercial application, then the decision-making should
    be driven by the business goals. One of these goals might be to guarantee at least
    90% recall. The challenge then becomes to develop a model that still has reasonable
    accuracy while satisfying all secondary requirements. Setting goals like this
    is often called **setting the operating point**.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is often not clear what the operating point should be when developing
    a new system. To understand the problem better, it is important to investigate
    all possible trade-offs of precision and recall them at once. This is possible
    using a tool called the **precision-recall curve** (`sklearn.metrics.precision_recall_curve`).
  prefs: []
  type: TYPE_NORMAL
- en: Another commonly used tool to analyze the behavior of classifiers is the **Receiver
    Operating Characteristic** (**ROC**) curve.
  prefs: []
  type: TYPE_NORMAL
- en: The ROC curve considers all possible thresholds for a given classifier similar
    to the precision-recall curve, but it shows the *false positive rate* against
    the *true positive rate* instead of reporting precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right regression metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluation for regression can be done in similar detail as we did for classification.
    In [Chapter 3](323dbb44-1e2b-4eaa-8cd1-2575e6766ffc.xhtml), *First Steps in Supervised
    Learning*, we also talked about some fundamental metrics for regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error**: The most commonly used error metric for regression
    problems is to measure the squared error between the predicted and true target
    value for every data point in the training set, averaged across all data points
    (`sklearn.metrics.mean_squared_error`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explained variance**: A more sophisticated metric is to measure to what degree
    a model can explain the variation or dispersion of the test data (`sklearn.metrics.explained_variance_score`).
    Often, the amount of explained variance is measured using ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaining algorithms together to form a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most machine learning problems we have discussed so far consist of at least
    a preprocessing step and a classification step. The more complicated the problem,
    the longer this *processing chain* might get. One convenient way to glue multiple
    processing steps together and even use them in grid search is by using the `Pipeline`
    class from scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing pipelines in scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Pipeline` class itself has a `fit`, a `predict`, and a `score` method,
    which all behave just like any other estimator in scikit-learn. The most common
    use case of the `Pipeline` class is to chain different preprocessing steps together
    with a supervised model like a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s return to the breast cancer dataset from [Chapter 5](5e1a6c2e-f10d-4599-993c-16e772b10a50.xhtml),
    *Using Decision Trees to Make a Medical Diagnosis*. Using scikit-learn, we import
    the dataset and split it into training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Using pipelines in grid searches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using a pipeline in a grid search works the same way as using any other estimator.
  prefs: []
  type: TYPE_NORMAL
- en: We define a parameter grid to search over and construct `GridSearchCV` from
    the pipeline and the parameter grid. When specifying the parameter grid, there
    is, however, a slight change. We need to specify for each parameter which step
    of the pipeline it belongs to. Both parameters that we want to adjust, `C` and
    `gamma`, are parameters of `SVC`. In the preceding section, we gave this step
    the name `"svm"`. The syntax to define a parameter grid for a pipeline is to specify
    for each parameter the step name, followed by `__` (a double underscore), followed
    by the parameter name.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we would construct the parameter grid as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With this parameter grid, we can use `GridSearchCV` as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The best score in the grid is stored in `best_score_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the best parameters are stored in `best_params_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'But recall that the cross-validation score might be overly optimistic. To know
    the true performance of the classifier, we need to score it on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: In contrast to the grid search we did before, now, for each split in the cross-validation,
    `MinMaxScaler` is refit with only the training splits, and no information is leaked
    from the test split into the parameter search.
  prefs: []
  type: TYPE_NORMAL
- en: This makes it easy to build a pipeline to chain together a whole variety of
    steps! You can mix and match estimators in the pipeline at will, you just need
    to make sure that every step in the pipeline provides a `transform` method (except
    for the last step). This allows an estimator in the pipeline to produce a new
    representation of the data, which, in turn, can be used as input to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: The `Pipeline` class is not restricted to preprocessing and classification but
    can, in fact, join any number of estimators together. For example, we could build
    a pipeline containing feature extraction, feature selection, scaling, and classification,
    for a total of four steps. Similarly, the last step could be regression or clustering
    instead of classification.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we tried to complement our existing machine learning skills
    by discussing best practices in model selection and hyperparameter tuning. You
    learned how to tweak the hyperparameters of a model using grid search and cross-validation
    in both OpenCV and scikit-learn. We also talked about a wide variety of evaluation
    metrics and how to chain algorithms into a pipeline. Now, you are almost ready
    to start working on some real-world problems on your own.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will be introduced to an exciting and a new topic,
    that is, OpenVINO toolkit, which was one of the key releases in OpenCV 4.0.
  prefs: []
  type: TYPE_NORMAL
