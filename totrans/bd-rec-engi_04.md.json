["```py\nx1 <- rnorm(30) \nx2 <- rnorm(30) \nEuc_dist = dist(rbind(x1,x2) ,method=\"euclidean\") \n\n```", "```py\nvec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ) \nvec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 ) \nlibrary(lsa) \ncosine(vec1,vec2) \n\n```", "```py\nvec1 = c( 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 ) \nvec2 = c( 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0 ) \nlibrary('clusteval') \ncluster_similarity(vec1, vec2, similarity = \"jaccard\") \n\n```", "```py\nCoef = cor(mtcars, method=\"pearson\") \n\n```", "```py\n#MF \nlibrary(recommenderlab) \ndata(\"MovieLense\") \ndim(MovieLense) \n\n#applying MF using NMF \nmat  = as(MovieLense,\"matrix\") \nmat[is.na(mat)] = 0 \nres = nmf(mat,10) \nres \n\n#fitted values \nr.hat <- fitted(res) \ndim(r.hat) \n\np <- basis(res) \ndim(p) \nq <- coef(res) \ndim(q) \n\n```", "```py\nsampleMat <- function(n) { i <- 1:n; 1 / outer(i - 1, i, \"+\") } \noriginal.mat <- sampleMat(9)[, 1:6] \n(s <- svd(original.mat)) \nD <- diag(s$d) \n#  X = U D V' \ns$u %*% D %*% t(s$v) \n\n```", "```py\nlibrary(MASS) \ndata(\"Boston\") \nset.seed(0) \nwhich_train <- sample(x = c(TRUE, FALSE), size = nrow(Boston), \n                      replace = TRUE, prob = c(0.8, 0.2)) \ntrain <- Boston[which_train, ] \ntest <- Boston[!which_train, ] \nlm.fit =lm(medv~. ,data=train ) \nsummary(lm.fit) \n\nCall: \nlm(formula = medv ~ ., data = train) \n\nResiduals: \n     Min       1Q   Median       3Q      Max  \n-15.2631  -2.7614  -0.5243   1.7867  24.6306  \n\nCoefficients: \n              Estimate Std. Error t value Pr(>|t|)     \n(Intercept)  39.549376   5.814446   6.802 3.82e-11 *** \ncrim         -0.090720   0.040872  -2.220  0.02701 *   \nzn            0.050080   0.015307   3.272  0.00116 **  \nindus         0.032339   0.070343   0.460  0.64596     \nchas          2.451235   0.992848   2.469  0.01397 *   \nnox         -18.517205   4.407645  -4.201 3.28e-05 *** \nrm            3.480574   0.469970   7.406 7.91e-13 *** \nage           0.012625   0.015786   0.800  0.42434     \ndis          -1.470081   0.223349  -6.582 1.48e-10 *** \nrad           0.322494   0.077050   4.186 3.51e-05 *** \ntax          -0.012839   0.004339  -2.959  0.00327 **  \nptratio      -0.972700   0.148454  -6.552 1.77e-10 *** \nblack         0.008399   0.003153   2.663  0.00805 **  \nlstat        -0.592906   0.058214 -10.185  < 2e-16 *** \n--- \nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nResidual standard error: 4.92 on 396 degrees of freedom \nMultiple R-squared:  0.7321,     Adjusted R-squared:  0.7233  \nF-statistic: 83.26 on 13 and 396 DF,  p-value: < 2.2e-16 \n\n#predict new values \npred = predict(lm.fit,test[,-14]) \n\n```", "```py\nset.seed(1) \nx1 = rnorm(1000)           # sample continuous variables  \nx2 = rnorm(1000) \nz = 1 + 4*x1 + 3*x2        # data creation \npr = 1/(1+exp(-z))         # applying logit function \ny = rbinom(1000,1,pr)      # bernoulli response variable \n\n  #now feed it to glm: \ndf = data.frame(y=y,x1=x1,x2=x2)   \nglm( y~x1+x2,data=df,family=\"binomial\") \n\n```", "```py\ndata(\"iris\") \nlibrary(dplyr) \niris2 = sample_n(iris, 150) \ntrain = iris2[1:120,] \ntest = iris2[121:150,] \ncl = train$Species \nlibrary(caret) \nfit <- knn3(Species~., data=train, k=3) \npredictions <- predict(fit, test[,-5], type=\"class\") \ntable(predictions, test$Species) \n\n```", "```py\nlibrary(e1071) \ndata(iris) \nsample = iris[sample(nrow(iris)),] \ntrain = sample[1:105,] \ntest = sample[106:150,] \ntune =tune(svm,Species~.,data=train,kernel =\"radial\",scale=FALSE,ranges =list(cost=c(0.001,0.01,0.1,1,5,10,100))) \ntune$best.model \n\nCall: \nbest.tune(method = svm, train.x = Species ~ ., data = train, ranges = list(cost = c(0.001,  \n    0.01, 0.1, 1, 5, 10, 100)), kernel = \"radial\", scale = FALSE) \n\nParameters: \n   SVM-Type:  C-classification  \n SVM-Kernel:  radial  \n       cost:  10  \n      gamma:  0.25  \n\nNumber of Support Vectors:  25 \n\nsummary(tune) \n\nParameter tuning of 'svm': \n- sampling method: 10-fold cross validation  \n- best parameters: \n cost \n   10 \n- best performance: 0.02909091  \n- Detailed performance results: \n   cost      error dispersion \n1 1e-03 0.72909091 0.20358585 \n2 1e-02 0.72909091 0.20358585 \n3 1e-01 0.04636364 0.08891242 \n4 1e+00 0.04818182 0.06653568 \n5 5e+00 0.03818182 0.06538717 \n6 1e+01 0.02909091 0.04690612 \n7 1e+02 0.07636364 0.08679584 \n\ncost =10 is chosen from summary result of tune variable \nmodel =svm(Species~.,data=train,kernel =\"radial\",cost=10,scale=FALSE) \n\n```", "```py\npred = predict(model,test) \n\n```", "```py\nlibrary(tree) \ndata(iris) \nsample = iris[sample(nrow(iris)),] \ntrain = sample[1:105,] \ntest = sample[106:150,] \nmodel = tree(Species~.,train) \nsummary(model) \n\n```", "```py\nplot(model) #plot trees \ntext(model) #apply text \n\n```", "```py\npred = predict(model,test[,-5],type=\"class\") \n\n```", "```py\nlibrary(randomForest) \ndata(iris) \nsample = iris[sample(nrow(iris)),] \ntrain = sample[1:105,] \ntest = sample[106:150,] \nmodel =randomForest(Species~.,data=train,mtry=2,importance =TRUE,proximity=TRUE) \n\n```", "```py\npred = predict(model,newdata=test[,-5]) \n\n```", "```py\nlibrary(gbm) \ndata(iris) \nsample = iris[sample(nrow(iris)),] \ntrain = sample[1:105,] \ntest = sample[106:150,] \nmodel = gbm(Species~.,data=train,distribution=\"multinomial\",n.trees=5000,interaction.depth=4) \nsummary(model) \n\n```", "```py\npred = predict(model,newdata=test[,-5],n.trees=5000) \n\n```", "```py\np.pred <- apply(pred,1,which.max) \n\n```", "```py\nlibrary(cluster) \ndata(iris) \niris$Species = as.numeric(iris$Species) \nkmeans<- kmeans(x=iris, centers=5) \nclusplot(iris,kmeans$cluster, color=TRUE, shade=TRUE,labels=13, lines=0) \n\n```", "```py\nlibrary(cluster) \nlibrary(ggplot2) \ndata(iris) \niris$Species = as.numeric(iris$Species) \ncost_df <- data.frame() \nfor(i in 1:100){ \nkmeans<- kmeans(x=iris, centers=i, iter.max=50) \ncost_df<- rbind(cost_df, cbind(i, kmeans$tot.withinss)) \n} \nnames(cost_df) <- c(\"cluster\", \"cost\") \n#Elbow method to identify the idle number of Cluster \n#Cost plot \nggplot(data=cost_df, aes(x=cluster, y=cost, group=1)) + \ntheme_bw(base_family=\"Garamond\") + \ngeom_line(colour = \"darkgreen\") + \ntheme(text = element_text(size=20)) + \nggtitle(\"Reduction In Cost For Values of 'k'\\n\") + \nxlab(\"\\nClusters\") + ylab(\"Within-Cluster Sum of Squares\\n\") \n\n```", "```py\ndata(USArrests) \nhead(states) \n[1] \"Alabama\"    \"Alaska\"     \"Arizona\"    \"Arkansas\"   \"California\" \"Colorado\"  \n\nnames(USArrests) \n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"  \n\n```", "```py\napply(USArrests , 2, var) \n\nMurder    Assault   UrbanPop       Rape  \n  18.97047 6945.16571  209.51878   87.72916  \n\n```", "```py\npca =prcomp(USArrests , scale =TRUE) \n\npca \nStandard deviations: \n[1] 1.5748783 0.9948694 0.5971291 0.4164494 \n\nRotation: \n                PC1        PC2        PC3         PC4 \nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780 \nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748 \nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773 \nRape     -0.5434321 -0.1673186  0.8177779  0.08902432 \n\n```", "```py\nnames(pca) \n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\" \n\n```", "```py\npca$rotation=-pca$rotation \npca$x=-pca$x \nbiplot (pca , scale =0) \n\n```", "```py\nlibrary(tm) \ndata(crude) \ntdm <- TermDocumentMatrix(crude,control=list(weighting =   function(x) weightTfIdf(x, normalize =TRUE), stopwords = TRUE)) \ninspect(tdm) \n\n```"]