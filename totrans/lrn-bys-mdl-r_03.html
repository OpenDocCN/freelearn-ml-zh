<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Introducing Bayesian Inference"><div class="titlepage" id="aid-SJGS2"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Introducing Bayesian Inference</h1></div></div></div><p>In <a class="link" title="Chapter 1. Introducing the Probability Theory" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Introducing the Probability Theory</em></span>, we learned about the Bayes theorem as the relation between conditional probabilities of two random variables such as <span class="emphasis"><em>A</em></span> and <span class="emphasis"><em>B</em></span>. This theorem is <a id="id112" class="indexterm"/>the basis for updating beliefs or model parameter values in Bayesian inference, given the observations. In this chapter, a more formal treatment of Bayesian inference will be given. To begin with, let us try to understand how uncertainties in a real-world problem are treated in Bayesian approach.</p><div class="section" title="Bayesian view of uncertainty"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Bayesian view of uncertainty</h1></div></div></div><p>The<a id="id113" class="indexterm"/> classical or frequentist statistics typically take<a id="id114" class="indexterm"/> the view that any physical process-generating data containing noise can be modeled by a stochastic model with fixed values of parameters. The parameter values are learned from the observed data through procedures such as <span class="strong"><strong>maximum likelihood estimate</strong></span>. The<a id="id115" class="indexterm"/> essential idea is to search in the parameter space to find the parameter values that maximize the probability of observing the data seen so far. Neither the uncertainty in the estimation of model parameters from data, nor the uncertainty in the model itself that explains the phenomena under study, is dealt with in a formal way. <span class="emphasis"><em>The Bayesian approach, on the other hand, treats all sources of uncertainty using probabilities</em></span>. Therefore, neither the model to explain an observed dataset nor its parameters are fixed, but they are treated as uncertain variables. Bayesian inference provides a framework to learn the entire distribution of model parameters, not just the values, which maximize the probability of observing the given data. The learning can come from both the evidence provided by observed data and domain knowledge from experts. There is also a framework to select the best model among the family of models suited to explain a given dataset. </p><p>Once we have the distribution of model parameters, we can eliminate the effect of uncertainty of parameter estimation in the future values of a random variable predicted using the learned model. This is done by averaging over the model parameter values through marginalization of joint probability distribution, as explained in <a class="link" title="Chapter 1. Introducing the Probability Theory" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Introducing the Probability Theory</em></span>.</p><p>Consider the joint probability distribution of <span class="emphasis"><em>N</em></span> random variables again, as discussed in <a class="link" title="Chapter 1. Introducing the Probability Theory" href="part0014.xhtml#aid-DB7S2">Chapter 1</a>, <span class="emphasis"><em>Introducing the Probability Theory</em></span>:</p><div class="mediaobject"><img src="../Images/image00252.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>This time, we <a id="id116" class="indexterm"/>have added one more term, <span class="emphasis"><em>m</em></span>, to <a id="id117" class="indexterm"/>the argument of the probability distribution, in order to indicate explicitly that the parameters <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="Bayesian view of uncertainty"/></span> are generated by the model <span class="emphasis"><em>m</em></span>. Then, according to Bayes theorem, the probability distribution of model parameters conditioned on the observed data <span class="inlinemediaobject"><img src="../Images/image00254.jpeg" alt="Bayesian view of uncertainty"/></span> and model <span class="emphasis"><em>m</em></span> is given by:</p><div class="mediaobject"><img src="../Images/image00255.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>Formally, the term on the LHS of the equation <span class="inlinemediaobject"><img src="../Images/image00256.jpeg" alt="Bayesian view of uncertainty"/></span> is called <a id="id118" class="indexterm"/>
<span class="strong"><strong>posterior probability distribution</strong></span>. The second term appearing in the numerator of RHS, <span class="inlinemediaobject"><img src="../Images/image00257.jpeg" alt="Bayesian view of uncertainty"/></span>, is called the <a id="id119" class="indexterm"/>
<span class="strong"><strong>prior probability distribution</strong></span>. It represents the prior belief about the model parameters, before observing any data, say, from the domain knowledge. Prior distributions can also have parameters and they are called hyperparameters. The term <span class="inlinemediaobject"><img src="../Images/image00252.jpeg" alt="Bayesian view of uncertainty"/></span> is the likelihood of model <span class="emphasis"><em>m</em></span> explaining the observed data. Since <span class="inlinemediaobject"><img src="../Images/image00258.jpeg" alt="Bayesian view of uncertainty"/></span>, it can be considered as a normalization constant <span class="inlinemediaobject"><img src="../Images/image00259.jpeg" alt="Bayesian view of uncertainty"/></span>. The preceding equation can be rewritten in an iterative form as follows:</p><div class="mediaobject"><img src="../Images/image00260.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00261.jpeg" alt="Bayesian view of uncertainty"/></span> represents <a id="id120" class="indexterm"/>values of observations that are obtained at time step <span class="emphasis"><em>n</em></span>, <span class="inlinemediaobject"><img src="../Images/image00262.jpeg" alt="Bayesian view of uncertainty"/></span> is the marginal parameter distribution updated until time step <span class="emphasis"><em>n - 1</em></span>, and <span class="inlinemediaobject"><img src="../Images/image00263.jpeg" alt="Bayesian view of uncertainty"/></span> is the model <a id="id121" class="indexterm"/>parameter distribution updated after seeing the observations <span class="inlinemediaobject"><img src="../Images/image00264.jpeg" alt="Bayesian view of uncertainty"/></span> at time step <span class="emphasis"><em>n</em></span>. </p><p>Casting Bayes theorem in this iterative form is useful for online learning and it suggests the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Model parameters can be learned in an iterative way as more and more data or evidence is obtained</li><li class="listitem">The posterior distribution estimated using the data seen so far can be treated as a prior model when the next set of observations is obtained</li><li class="listitem">Even if no data is available, one could make predictions based on prior distribution created using the domain knowledge alone</li></ul></div><p>To make these points clear, let's take a simple illustrative example. Consider the case where one is trying to estimate the distribution of the height of males in a given region. The data used for this example is the height measurement in centimeters obtained from <span class="emphasis"><em>M</em></span> volunteers sampled randomly from the population. We assume that the heights are distributed according to a normal distribution with the mean <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Bayesian view of uncertainty"/></span> and variance <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Bayesian view of uncertainty"/></span>:</p><div class="mediaobject"><img src="../Images/image00267.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>As mentioned earlier, in classical statistics, one tries to estimate the values of <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Bayesian view of uncertainty"/></span> and <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Bayesian view of uncertainty"/></span> from observed data. Apart from the best estimate value for each parameter, one could also determine an <a id="id122" class="indexterm"/>error term of the estimate. In the Bayesian <a id="id123" class="indexterm"/>approach, on the other hand, <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Bayesian view of uncertainty"/></span> and <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Bayesian view of uncertainty"/></span> are also treated as random variables. Let's, for simplicity, assume <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Bayesian view of uncertainty"/></span> is a known constant. Also, let's assume that the prior distribution for <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Bayesian view of uncertainty"/></span> is a normal distribution with (hyper) parameters <span class="inlinemediaobject"><img src="../Images/image00268.jpeg" alt="Bayesian view of uncertainty"/></span> and <span class="inlinemediaobject"><img src="../Images/image00269.jpeg" alt="Bayesian view of uncertainty"/></span>. In this case, the expression for posterior distribution of <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Bayesian view of uncertainty"/></span> is given by:</p><div class="mediaobject"><img src="../Images/image00270.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>Here, for convenience, we have used the notation <span class="inlinemediaobject"><img src="../Images/image00271.jpeg" alt="Bayesian view of uncertainty"/></span> for <span class="inlinemediaobject"><img src="../Images/image00272.jpeg" alt="Bayesian view of uncertainty"/></span>. It is a simple exercise to expand the terms in the product and complete the squares in the exponential. This is given as an exercise at the end of the chapter. The resulting expression for the posterior distribution <span class="inlinemediaobject"><img src="../Images/image00273.jpeg" alt="Bayesian view of uncertainty"/></span> is given by:</p><div class="mediaobject"><img src="../Images/image00274.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00275.jpeg" alt="Bayesian view of uncertainty"/></span> represents<a id="id124" class="indexterm"/> the sample mean. Though<a id="id125" class="indexterm"/> the preceding expression looks complex, it has a very simple interpretation. The posterior distribution is also a normal distribution with the following mean:</p><div class="mediaobject"><img src="../Images/image00276.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>The variance is as follows:</p><div class="mediaobject"><img src="../Images/image00277.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>
<span class="emphasis"><em>The posterior mean is a weighted sum of prior mean </em></span>
<span class="inlinemediaobject"><img src="../Images/image00268.jpeg" alt="Bayesian view of uncertainty"/></span> <span class="emphasis"><em>and sample mean </em></span>
<span class="inlinemediaobject"><img src="../Images/image00278.jpeg" alt="Bayesian view of uncertainty"/></span>. As the sample size <span class="emphasis"><em>M</em></span> increases, the weight of the sample mean increases and that of the prior decreases. Similarly, posterior precision (inverse of the variance) is the sum of the prior precision <span class="inlinemediaobject"><img src="../Images/image00279.jpeg" alt="Bayesian view of uncertainty"/></span> and precision of the sample mean <span class="inlinemediaobject"><img src="../Images/image00280.jpeg" alt="Bayesian view of uncertainty"/></span>:</p><div class="mediaobject"><img src="../Images/image00281.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>As <span class="emphasis"><em>M</em></span> increases, the <a id="id126" class="indexterm"/>contribution of precision from observations (evidence) outweighs that from the prior knowledge.</p><p>Let's take <a id="id127" class="indexterm"/>a concrete example where we consider age distribution with the population mean 5.5 and population standard deviation 0.5. We sample 100 people from this population by using the following R script:</p><div class="informalexample"><pre class="programlisting">&gt;set.seed(100)
&gt;age_samples &lt;- rnorm(10000,mean = 5.5,sd=0.5)</pre></div><p>We can calculate the posterior distribution using the following R function:</p><div class="informalexample"><pre class="programlisting">&gt;age_mean &lt;- function(n){
  mu0 &lt;- 5
  sd0 &lt;- 1
  mus &lt;- mean(age_samples[1:n])
  sds &lt;- sd(age_samples[1:n])
  mu_n &lt;- (sd0^2/(sd0^2 + sds^2/n)) * mus + (sds^2/n/(sd0^2 + sds^2/n)) * mu0
  mu_n
}
&gt;samp &lt;- c(25,50,100,200,400,500,1000,2000,5000,10000)
&gt;mu &lt;- sapply(samp,age_mean,simplify = "array")
&gt;plot(samp,mu,type="b",col="blue",ylim=c(5.3,5.7),xlab="no of samples",ylab="estimate of mean")
&gt;abline(5.5,0)</pre></div><div class="mediaobject"><img src="../Images/image00282.jpeg" alt="Bayesian view of uncertainty"/></div><p style="clear:both; height: 1em;"> </p><p>One can see <a id="id128" class="indexterm"/>that as the number of samples increases, the <a id="id129" class="indexterm"/>estimated mean asymptotically approaches the population mean. The initial low value is due to the influence of the prior, which is, in this case, 5.0.</p><p>This simple and intuitive picture of how the prior knowledge and evidence from observations contribute to the overall model parameter estimate holds in any Bayesian inference. The precise mathematical expression for how they combine would be different. Therefore, one could start using a model for prediction with just prior information, either from the domain knowledge or the data collected in the past. Also, as new observations arrive, the model can be updated using the Bayesian scheme.</p><div class="section" title="Choosing the right prior distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec28"/>Choosing the right prior distribution</h2></div></div></div><p>In the preceding<a id="id130" class="indexterm"/> simple example, we <a id="id131" class="indexterm"/>saw that if the likelihood function has the form of a normal distribution, and when the prior distribution is chosen as normal, the posterior also turns out to be a normal distribution. Also, we could get a closed-form analytical expression for the posterior mean. Since the posterior is obtained by multiplying the prior and likelihood functions and normalizing by integration over the parameter variables, the form of the prior distribution has a significant influence on the posterior. This section gives some more details about the different types of prior distributions and guidelines as to which ones to use in a given context.</p><p>There are different ways of classifying prior distributions in a formal way. One of the approaches is based on how much information a prior provides. In this scheme, the prior distributions are classified as <span class="emphasis"><em>Informative</em></span>, <span class="emphasis"><em>Weakly Informative</em></span>, <span class="emphasis"><em>Least Informative</em></span>, and <span class="emphasis"><em>Non-informative</em></span>. A detailed discussion of each of these classes is beyond the scope of this book, and interested readers should consult relevant books (references 1 and 2 in the <span class="emphasis"><em>References</em></span> section of this chapter). Here, we take more of a practitioner's approach and illustrate some of the important classes of the prior distributions commonly used in practice.</p><div class="section" title="Non-informative priors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec06"/>Non-informative priors</h3></div></div></div><p>Let's <a id="id132" class="indexterm"/>start with the case where we do not have any prior knowledge about the model parameters. In this case, we want to express complete ignorance about model parameters through a mathematical expression. This is achieved through what are called non-informative priors. For example, in the case of a single random variable <span class="emphasis"><em>x</em></span> that can take any value between <span class="inlinemediaobject"><img src="../Images/image00283.jpeg" alt="Non-informative priors"/></span> and <span class="inlinemediaobject"><img src="../Images/image00284.jpeg" alt="Non-informative priors"/></span>, the non-informative prior for its mean <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Non-informative priors"/></span> would be the following:</p><div class="mediaobject"><img src="../Images/image00285.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>Here, the complete ignorance of the parameter value is captured through a uniform distribution function in the parameter space. Note that a uniform distribution is not a proper distribution function since its integral over the domain is not equal to 1; therefore, it is not normalizable. However, one can use an improper distribution function for the prior as long as it is multiplied by the likelihood function; the resulting posterior can be normalized.</p><p>If the parameter<a id="id133" class="indexterm"/> of interest is variance <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Non-informative priors"/></span>, then by definition it can only take non-negative values. In this case, we transform the variable so that the transformed variable has a uniform probability in the range from <span class="inlinemediaobject"><img src="../Images/image00283.jpeg" alt="Non-informative priors"/></span> to <span class="inlinemediaobject"><img src="../Images/image00284.jpeg" alt="Non-informative priors"/></span>:</p><div class="mediaobject"><img src="../Images/image00286.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00287.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>It is easy to show, using simple differential calculus, that the corresponding non-informative distribution function in the original variable <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Non-informative priors"/></span> would be as follows:</p><div class="mediaobject"><img src="../Images/image00288.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>Another well-known non-informative prior used in practical applications is the Jeffreys prior, which is named after the British statistician Harold Jeffreys. This prior is invariant under reparametrization of <span class="inlinemediaobject"><img src="../Images/image00289.jpeg" alt="Non-informative priors"/></span> and is defined as proportional to the square root of the determinant of the Fisher information matrix:</p><div class="mediaobject"><img src="../Images/image00290.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>Here, it is <a id="id134" class="indexterm"/>worth discussing the Fisher information matrix a little bit. If <span class="emphasis"><em>X</em></span> is a random variable distributed according to <span class="inlinemediaobject"><img src="../Images/image00291.jpeg" alt="Non-informative priors"/></span>, we may like to know how much information observations of <span class="emphasis"><em>X</em></span> carry about the unknown parameter <span class="inlinemediaobject"><img src="../Images/image00289.jpeg" alt="Non-informative priors"/></span>. This is what the Fisher Information Matrix provides. It is defined as the second moment of the score (first derivative of the logarithm of the likelihood function):</p><div class="mediaobject"><img src="../Images/image00292.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>Let's take a simple two-dimensional problem to understand the Fisher information matrix and Jeffreys prior. This example is given by Prof. D. Wittman of the University of California (reference 3 in the <span class="emphasis"><em>References</em></span> section of this chapter). Let's consider two types of food item: buns and hot dogs. </p><p>Let's assume that generally they are produced in pairs (a hot dog and bun pair), but occasionally hot dogs are also produced independently in a separate process. There are two observables such as the number of hot dogs (<span class="inlinemediaobject"><img src="../Images/image00293.jpeg" alt="Non-informative priors"/></span>) and the number of buns (<span class="inlinemediaobject"><img src="../Images/image00294.jpeg" alt="Non-informative priors"/></span>), and two model parameters such as the production rate of pairs (<span class="inlinemediaobject"><img src="../Images/image00295.jpeg" alt="Non-informative priors"/></span>) and the production rate of hot dogs alone (<span class="inlinemediaobject"><img src="../Images/image00296.jpeg" alt="Non-informative priors"/></span>). We assume that the uncertainty in the measurements of the counts of these two food products is distributed according to the normal distribution, with variance <span class="inlinemediaobject"><img src="../Images/image00297.jpeg" alt="Non-informative priors"/></span> and <span class="inlinemediaobject"><img src="../Images/image00298.jpeg" alt="Non-informative priors"/></span>, respectively. In this case, the Fisher Information matrix for this problem would be as follows:</p><div class="mediaobject"><img src="../Images/image00299.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>In this case, the<a id="id135" class="indexterm"/> inverse of the Fisher information matrix would correspond to the covariance matrix:</p><div class="mediaobject"><img src="../Images/image00300.jpeg" alt="Non-informative priors"/></div><p style="clear:both; height: 1em;"> </p><p>We have included one problem in the <span class="emphasis"><em>Exercises</em></span> section of this chapter to compute the Fisher information matrix and Jeffrey's prior. Readers are requested to attempt this in order to get a feeling of how to compute Jeffrey's prior from observations.</p></div><div class="section" title="Subjective priors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec07"/>Subjective priors</h3></div></div></div><p>One of<a id="id136" class="indexterm"/> the key strengths of Bayesian statistics compared to classical (frequentist) statistics is that the framework allows one to capture subjective beliefs about any random variables. Usually, people will have intuitive feelings about minimum, maximum, mean, and most probable or peak values of a random variable. For example, if one is interested in the distribution of hourly temperatures in winter in a tropical country, then the people who are familiar with tropical climates or climatology experts will have a belief that, in winter, the temperature can go as low as 15°C and as high as 27°C with the most probable temperature value being 23°C. This can be captured as a prior distribution through the Triangle distribution as shown here.</p><p>The Triangle distribution has three parameters corresponding to a minimum value (<span class="emphasis"><em>a</em></span>), the most probable value (<span class="emphasis"><em>b</em></span>), and a maximum value (<span class="emphasis"><em>c</em></span>). The mean and variance of this distribution are given by:</p><div class="mediaobject"><img src="../Images/image00301.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00302.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><p>One can <a id="id137" class="indexterm"/>also use a PERT distribution to represent a subjective belief about the minimum, maximum, and most probable value of a random variable. The PERT distribution is a reparametrized Beta distribution, as follows:</p><div class="mediaobject"><img src="../Images/image00303.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><p>Here:</p><div class="mediaobject"><img src="../Images/image00304.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00305.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><div class="mediaobject"><img src="../Images/image00306.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><p>The PERT distribution is commonly used for project completion time analysis, and the name originates from project evaluation and review techniques. Another area where Triangle and PERT distributions are commonly used is in <a id="id138" class="indexterm"/>
<span class="strong"><strong>risk modeling</strong></span>.</p><p>Often, people also have a belief about the relative probabilities of values of a random variable. For<a id="id139" class="indexterm"/> example, when studying the distribution of ages in a population such as Japan or some European countries, where there are more old people than young, an expert could give relative weights for the probability of different ages in the populations. This can be captured through a relative distribution containing the following details:</p><div class="mediaobject"><img src="../Images/image00307.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>min</em></span> and <span class="emphasis"><em>max</em></span> represent the minimum and maximum values, <span class="emphasis"><em>{values}</em></span> represents the set of possible observed values, and <span class="emphasis"><em>{weights}</em></span> represents their relative weights. For example, in the population age distribution problem, these could be the following:</p><div class="mediaobject"><img src="../Images/image00308.jpeg" alt="Subjective priors"/></div><p style="clear:both; height: 1em;"> </p><p>The weights need not have a sum of 1.</p></div><div class="section" title="Conjugate priors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec08"/>Conjugate priors</h3></div></div></div><p>If both the <a id="id140" class="indexterm"/>prior and posterior distributions are in the same family of distributions, then they are called <a id="id141" class="indexterm"/>
<span class="strong"><strong>conjugate distributions</strong></span> and the corresponding prior is called a <a id="id142" class="indexterm"/>
<span class="strong"><strong>conjugate prior for the likelihood function</strong></span>. Conjugate priors are very helpful for getting get analytical closed-form expressions for the posterior distribution. In the simple example we considered, we saw that when the noise is distributed according to the normal distribution, choosing a normal prior for the mean resulted in a normal posterior. The following table gives examples of some well-known conjugate pairs that we will use in the later chapters of this book:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>Likelihood function</p>
</th><th valign="bottom">
<p>Model parameters</p>
</th><th valign="bottom">
<p>Conjugate prior</p>
</th><th valign="bottom">
<p>Hyperparameters</p>
</th></tr></thead><tbody><tr><td valign="top">
<p>Binomial</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00309.jpeg" alt="Conjugate priors"/></span>
<p>(probability)</p>
</td><td valign="top">
<p>Beta</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00310.jpeg" alt="Conjugate priors"/></span>
</td></tr><tr><td valign="top">
<p>Poisson</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00311.jpeg" alt="Conjugate priors"/></span>
<p>(rate)</p>
</td><td valign="top">
<p>Gamma</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00312.jpeg" alt="Conjugate priors"/></span>
</td></tr><tr><td valign="top">
<p>Categorical</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00313.jpeg" alt="Conjugate priors"/></span>
<p>(probability, number of categories)</p>
</td><td valign="top">
<p>Dirichlet</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00295.jpeg" alt="Conjugate priors"/></span>
</td></tr><tr><td valign="top">
<p>Univariate normal (known variance <span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Conjugate priors"/></span>)</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Conjugate priors"/></span>
<p>(mean)</p>
</td><td valign="top">
<p>Normal</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00314.jpeg" alt="Conjugate priors"/></span>
</td></tr><tr><td valign="top">
<p>Univariate normal (known mean <span class="inlinemediaobject"><img src="../Images/image00265.jpeg" alt="Conjugate priors"/></span>)</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00266.jpeg" alt="Conjugate priors"/></span>
<p>(variance)</p>
</td><td valign="top">
<p>Inverse Gamma</p>
</td><td valign="top">
<span class="inlinemediaobject"><img src="../Images/image00310.jpeg" alt="Conjugate priors"/></span>
</td></tr></tbody></table></div></div><div class="section" title="Hierarchical priors"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec09"/>Hierarchical priors</h3></div></div></div><p>Sometimes, it is<a id="id143" class="indexterm"/> useful to define prior distributions for the hyperparameters itself. This is consistent with the Bayesian view that all parameters should be treated as uncertain by using probabilities. These distributions are called hyper-prior distributions. In theory, one can continue this into many levels as a hierarchical model. This is one way of eliciting the optimal prior distributions. For example:</p><div class="mediaobject"><img src="../Images/image00315.jpeg" alt="Hierarchical priors"/></div><p style="clear:both; height: 1em;"> </p><p><span class="inlinemediaobject"><img src="../Images/image00316.jpeg" alt="Hierarchical priors"/></span> is the prior distribution with a hyperparameter <span class="inlinemediaobject"><img src="../Images/image00295.jpeg" alt="Hierarchical priors"/></span>. We could define a prior distribution for <span class="inlinemediaobject"><img src="../Images/image00295.jpeg" alt="Hierarchical priors"/></span> through a second set of equations, as follows:</p><div class="mediaobject"><img src="../Images/image00317.jpeg" alt="Hierarchical priors"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00318.jpeg" alt="Hierarchical priors"/></span> is the hyper-prior distribution for the hyperparameter <span class="inlinemediaobject"><img src="../Images/image00295.jpeg" alt="Hierarchical priors"/></span>, parametrized by the hyper-hyper-parameter <span class="inlinemediaobject"><img src="../Images/image00298.jpeg" alt="Hierarchical priors"/></span>. One can define a prior distribution for <span class="inlinemediaobject"><img src="../Images/image00298.jpeg" alt="Hierarchical priors"/></span> in the same way and continue the <a id="id144" class="indexterm"/>process forever. The practical reason for formalizing such models is that, at some level of hierarchy, one can define a uniform prior for the hyper parameters, reflecting complete ignorance about the parameter distribution, and effectively truncate the hierarchy. In practical situations, typically, this is done at the second level. This corresponds to, in the preceding example, using a uniform distribution for <span class="inlinemediaobject"><img src="../Images/image00318.jpeg" alt="Hierarchical priors"/></span>.</p><p>I want to conclude this section by stressing one important point. Though prior distribution has a significant role in Bayesian inference, one need not worry about it too much, as long as the prior chosen is reasonable and consistent with the domain knowledge and evidence seen so far. The reasons are is that, first of all, as we have more evidence, the significance of the prior gets washed out. Secondly, when we use Bayesian models for prediction, we will average over the uncertainty in the estimation of the parameters using the posterior distribution. <span class="emphasis"><em>This averaging is the key ingredient of Bayesian inference and it removes many of the ambiguities in the selection of the right prior</em></span>.</p></div></div><div class="section" title="Estimation of posterior distribution"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec29"/>Estimation of posterior distribution</h2></div></div></div><p>So far, we <a id="id145" class="indexterm"/>discussed the essential concept behind Bayesian inference and also how to choose a prior distribution. Since one needs to compute the posterior distribution of model parameters <a id="id146" class="indexterm"/>before one can use the models for prediction, we discuss this task in this section. Though the Bayesian rule has a very simple-looking form, the<a id="id147" class="indexterm"/> computation of posterior distribution in a practically usable way is often very challenging. This is primarily because computation of the normalization constant <span class="inlinemediaobject"><img src="../Images/image00259.jpeg" alt="Estimation of posterior distribution"/></span> involves <span class="emphasis"><em>N</em></span>-dimensional integrals, when there are <span class="emphasis"><em>N</em></span> parameters. Even when one uses a conjugate prior, this computation can be very difficult to track analytically or numerically. This was one of the main reasons for not using Bayesian inference for multivariate modeling until recent decades. In this section, we will look at various approximate ways of computing posterior distributions that are used in practice.</p><div class="section" title="Maximum a posteriori estimation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec10"/>Maximum a posteriori estimation</h3></div></div></div><p>
<span class="strong"><strong>Maximum a posteriori</strong></span> (<span class="strong"><strong>MAP</strong></span>) estimation<a id="id148" class="indexterm"/> is a point estimation that corresponds to taking the <a id="id149" class="indexterm"/>maximum value or mode of the posterior distribution. Though taking a point estimation does not capture the variability in the parameter estimation, it does take into account the effect of prior distribution to some extent when compared to maximum likelihood estimation. MAP estimation is also called poor man's Bayesian inference.</p><p>From the Bayes rule, we have:</p><div class="mediaobject"><img src="../Images/image00319.jpeg" alt="Maximum a posteriori estimation"/></div><p style="clear:both; height: 1em;"> </p><p>Here, for convenience, we have used the notation <span class="emphasis"><em>X</em></span> for the <span class="emphasis"><em>N</em></span>-dimensional vector <span class="inlinemediaobject"><img src="../Images/image00320.jpeg" alt="Maximum a posteriori estimation"/></span>. The last relation follows because the denominator of RHS of Bayes rule is independent of <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="Maximum a posteriori estimation"/></span>. Compare this with the following maximum likelihood estimate:</p><div class="mediaobject"><img src="../Images/image00321.jpeg" alt="Maximum a posteriori estimation"/></div><p style="clear:both; height: 1em;"> </p><p>The difference <a id="id150" class="indexterm"/>between the MAP and ML estimate is that, whereas ML finds the mode of the likelihood function, MAP finds the mode of the product of the likelihood function and prior.</p></div><div class="section" title="Laplace approximation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec11"/>Laplace approximation</h3></div></div></div><p>We saw <a id="id151" class="indexterm"/>that the MAP estimate just <a id="id152" class="indexterm"/>finds the maximum value of the posterior distribution. Laplace approximation goes one step further and also computes the local curvature around the maximum up to quadratic terms. This is equivalent to assuming that the posterior distribution is approximately Gaussian (normal) around the maximum. This would be the case if the amount of data were large compared to the number of parameters: <span class="emphasis"><em>M &gt;&gt; N</em></span>.</p><div class="mediaobject"><img src="../Images/image00322.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="emphasis"><em>A</em></span> is an <span class="emphasis"><em>N x N</em></span> Hessian matrix obtained by taking the derivative of the log of the posterior distribution:</p><div class="mediaobject"><img src="../Images/image00323.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>It is straightforward to evaluate the previous expressions at <span class="inlinemediaobject"><img src="../Images/image00324.jpeg" alt="Laplace approximation"/></span>, using the following definition of conditional probability:</p><div class="mediaobject"><img src="../Images/image00325.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>We can get an expression for <span class="emphasis"><em>P(X|m)</em></span> from Laplace approximation that looks like the following:</p><div class="mediaobject"><img src="../Images/image00326.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>In the limit <a id="id153" class="indexterm"/>of a large number of <a id="id154" class="indexterm"/>samples, one can show that this expression simplifies to the following:</p><div class="mediaobject"><img src="../Images/image00327.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>The term <span class="inlinemediaobject"><img src="../Images/image00328.jpeg" alt="Laplace approximation"/></span> is called <a id="id155" class="indexterm"/>
<span class="strong"><strong>Bayesian information criterion</strong></span> (<span class="strong"><strong>BIC</strong></span>) and can be used for model selections or model comparison. This is one of the <span class="strong"><strong>goodness of fit</strong></span> terms for a statistical model. Another similar criterion that is commonly used is <a id="id156" class="indexterm"/>
<span class="strong"><strong>Akaike information criterion</strong></span> (<span class="strong"><strong>AIC</strong></span>), which is defined by <span class="inlinemediaobject"><img src="../Images/image00329.jpeg" alt="Laplace approximation"/></span>.</p><p>Now we will discuss how BIC can be used to compare different models for model selection. In the Bayesian framework, two models such as <span class="inlinemediaobject"><img src="../Images/image00330.jpeg" alt="Laplace approximation"/></span> and <span class="inlinemediaobject"><img src="../Images/image00331.jpeg" alt="Laplace approximation"/></span> are compared using the Bayes factor. The definition of the Bayes factor <span class="inlinemediaobject"><img src="../Images/image00332.jpeg" alt="Laplace approximation"/></span> is the ratio of posterior odds to prior odds that is given by:</p><div class="mediaobject"><img src="../Images/image00333.jpeg" alt="Laplace approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Here, posterior odds is the ratio of posterior probabilities of the two models of the given data and prior <a id="id157" class="indexterm"/>odds is the ratio of prior probabilities of the two models, as given in the preceding equation. If <span class="inlinemediaobject"><img src="../Images/image00334.jpeg" alt="Laplace approximation"/></span>, model <span class="inlinemediaobject"><img src="../Images/image00330.jpeg" alt="Laplace approximation"/></span> is preferred by the data and if <span class="inlinemediaobject"><img src="../Images/image00335.jpeg" alt="Laplace approximation"/></span>, model <span class="inlinemediaobject"><img src="../Images/image00331.jpeg" alt="Laplace approximation"/></span> is preferred by the data.</p><p>In reality, it is difficult to compute the Bayes factor because it is difficult to get the precise prior probabilities. It can be shown that, in the large <span class="emphasis"><em>N</em></span> limit, <span class="inlinemediaobject"><img src="../Images/image00336.jpeg" alt="Laplace approximation"/></span> can be viewed as a rough approximation to <span class="inlinemediaobject"><img src="../Images/image00337.jpeg" alt="Laplace approximation"/></span>.</p></div><div class="section" title="Monte Carlo simulations"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec12"/>Monte Carlo simulations</h3></div></div></div><p>The two<a id="id158" class="indexterm"/> approximations that we have discussed so far, the MAP and Laplace approximations, are useful when<a id="id159" class="indexterm"/> the posterior is a very sharply peaked function about the maximum value. Often, in real-life situations, the posterior will have long tails. This is, for example, the case in e-commerce where the probability of the purchasing of a product by a user has a long tail in the space of all products. So, in many practical situations, both MAP and Laplace approximations fail to give good results. Another approach is to directly sample from the posterior distribution. Monte Carlo simulation is a technique used for sampling from the posterior distribution and is one of the workhorses of Bayesian inference in practical applications. In this section, we will introduce the reader to <a id="id160" class="indexterm"/>
<span class="strong"><strong>Markov Chain Monte Carlo</strong></span> (<span class="strong"><strong>MCMC</strong></span>) simulations and also discuss two common MCMC methods used in practice.</p><p>As discussed <a id="id161" class="indexterm"/>earlier, let <span class="inlinemediaobject"><img src="../Images/image00338.jpeg" alt="Monte Carlo simulations"/></span> be the set of parameters that we are interested in estimating from the data through posterior distribution. Consider the case of the parameters being discrete, where each parameter has <span class="emphasis"><em>K</em></span> possible values, that is, <span class="inlinemediaobject"><img src="../Images/image00339.jpeg" alt="Monte Carlo simulations"/></span>. Set <a id="id162" class="indexterm"/>up a Markov process with states <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="Monte Carlo simulations"/></span> and transition probability matrix <span class="inlinemediaobject"><img src="../Images/image00340.jpeg" alt="Monte Carlo simulations"/></span>. The essential idea behind MCMC simulations is that one can choose the transition probabilities in such a way that the steady state distribution of the Markov chain would correspond to the posterior distribution we are interested in. Once this is done, sampling from the Markov chain output, after it has reached a steady state, will give samples of <span class="inlinemediaobject"><img src="../Images/image00341.jpeg" alt="Monte Carlo simulations"/></span> distributed according to the posterior distribution.</p><p>Now, the question is how to set up the Markov process in such a way that its steady state distribution corresponds to the posterior of interest. There are two well-known methods for this. One is the Metropolis-Hastings algorithm and the second is Gibbs sampling. We will discuss both in some detail here.</p><div class="section" title="The Metropolis-Hasting algorithm"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec01"/>The Metropolis-Hasting algorithm</h4></div></div></div><p>The<a id="id163" class="indexterm"/> Metropolis-Hasting algorithm was one of<a id="id164" class="indexterm"/> the first major algorithms proposed for MCMC (reference 4 in the <span class="emphasis"><em>References</em></span> section of this chapter). It has a very simple concept—something similar to a hill-climbing algorithm in optimization:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Let <span class="inlinemediaobject"><img src="../Images/image00342.jpeg" alt="The Metropolis-Hasting algorithm"/></span> be the state of the system at time step <span class="emphasis"><em>t</em></span>.
</li><li class="listitem">
To move the system to another state at time step <span class="emphasis"><em>t + 1</em></span>, generate a candidate state <span class="inlinemediaobject"><img src="../Images/image00343.jpeg" alt="The Metropolis-Hasting algorithm"/></span> by <a id="id165" class="indexterm"/>sampling <a id="id166" class="indexterm"/>from a proposal distribution <span class="inlinemediaobject"><img src="../Images/image00344.jpeg" alt="The Metropolis-Hasting algorithm"/></span>. The proposal distribution is chosen in such a way that it is easy to sample from it.
</li><li class="listitem">Accept the proposal move with the following probability:<div class="mediaobject"><img src="../Images/image00345.jpeg" alt="The Metropolis-Hasting algorithm"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">
If it is accepted, <span class="inlinemediaobject"><img src="../Images/image00346.jpeg" alt="The Metropolis-Hasting algorithm"/></span> = <span class="inlinemediaobject"><img src="../Images/image00343.jpeg" alt="The Metropolis-Hasting algorithm"/></span>; if not, <span class="inlinemediaobject"><img src="../Images/image00347.jpeg" alt="The Metropolis-Hasting algorithm"/></span>.
</li><li class="listitem">Continue the process until the distribution converges to the steady state.</li></ol><div style="height:10px; width: 1px"/></div><p>Here, <span class="inlinemediaobject"><img src="../Images/image00348.jpeg" alt="The Metropolis-Hasting algorithm"/></span> is the posterior distribution that we want to simulate. Under certain conditions, the preceding update rule will guarantee that, in the large time limit, the Markov process will approach a steady state distributed according to <span class="inlinemediaobject"><img src="../Images/image00348.jpeg" alt="The Metropolis-Hasting algorithm"/></span>.</p><p>The intuition behind the Metropolis-Hasting algorithm is simple. The proposal distribution <span class="inlinemediaobject"><img src="../Images/image00344.jpeg" alt="The Metropolis-Hasting algorithm"/></span> gives the conditional probability of proposing state <span class="inlinemediaobject"><img src="../Images/image00343.jpeg" alt="The Metropolis-Hasting algorithm"/></span> to make a transition in the next time step from the current state <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="The Metropolis-Hasting algorithm"/></span>. Therefore, <span class="inlinemediaobject"><img src="../Images/image00349.jpeg" alt="The Metropolis-Hasting algorithm"/></span> is the probability that the system is currently in state <span class="inlinemediaobject"><img src="../Images/image00343.jpeg" alt="The Metropolis-Hasting algorithm"/></span> and would make a transition to state <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="The Metropolis-Hasting algorithm"/></span> in the next time step. Similarly, <span class="inlinemediaobject"><img src="../Images/image00350.jpeg" alt="The Metropolis-Hasting algorithm"/></span> is the probability that the system is currently in state <span class="inlinemediaobject"><img src="../Images/image00253.jpeg" alt="The Metropolis-Hasting algorithm"/></span> and would make a transition to state <span class="inlinemediaobject"><img src="../Images/image00343.jpeg" alt="The Metropolis-Hasting algorithm"/></span> in the next time step. If the ratio of these two probabilities is more than 1, accept the move. Alternatively, accept the move only with the probability given by the ratio. Therefore, the Metropolis-Hasting algorithm is like a<a id="id167" class="indexterm"/> hill-climbing algorithm where one accepts all the moves that are in the upward direction and <a id="id168" class="indexterm"/>accepts moves in the downward direction once in a while with a smaller probability. The downward moves help the system not to get stuck in local minima.</p><p>Let's revisit the example of estimating the posterior distribution of the mean and variance of the height of people in a population discussed in the introductory section. This time we will estimate the posterior distribution by using the Metropolis-Hasting algorithm. The following lines of R code do this job:</p><div class="informalexample"><pre class="programlisting">&gt;set.seed(100)
&gt;mu_t &lt;- 5.5
&gt;sd_t &lt;- 0.5
&gt;age_samples &lt;- rnorm(10000,mean = mu_t,sd = sd_t)

&gt;#function to compute log likelihood
&gt;loglikelihood &lt;- function(x,mu,sigma){
    singlell &lt;- dnorm(x,mean = mu,sd = sigma,log = T)
    sumll &lt;- sum(singlell)
    sumll
    }

&gt;#function to compute prior distribution for mean on log scale
&gt;d_prior_mu &lt;- function(mu){
  dnorm(mu,0,10,log=T)
  }

&gt;#function to compute prior distribution for std dev on log scale
&gt;d_prior_sigma &lt;- function(sigma){
  dunif(sigma,0,5,log=T)
  }

&gt;#function to compute posterior distribution on log scale
&gt;d_posterior &lt;- function(x,mu,sigma){
  loglikelihood(x,mu,sigma) + d_prior_mu(mu) + d_prior_sigma(sigma)
   }

&gt;#function to make transition moves
  tran_move &lt;- function(x,dist = .1){
  x + rnorm(1,0,dist)
  }

&gt;num_iter &lt;- 10000
&gt;posterior &lt;- array(dim = c(2,num_iter))
&gt;accepted &lt;- array(dim=num_iter - 1)
&gt;theta_posterior &lt;-array(dim=c(2,num_iter))

&gt;values_initial &lt;- list(mu = runif(1,4,8),sigma = runif(1,1,5))
&gt;theta_posterior[1,1] &lt;- values_initial$mu
&gt;theta_posterior[2,1] &lt;- values_initial$sigma

&gt;for (t in 2:num_iter){
   #proposed next values for parameters
    theta_proposed &lt;- c(tran_move(theta_posterior[1,t-1]),tran_move(theta_posterior[2,t-1]))
    p_proposed &lt;- d_posterior(age_samples,mu = theta_proposed[1],sigma = theta_proposed[2])
    p_prev &lt;-d_posterior(age_samples,mu = theta_posterior[1,t-1],sigma = theta_posterior[2,t-1])
    eps &lt;- exp(p_proposed - p_prev)

    # proposal is accepted if posterior density is higher w/ theta_proposed
    # if posterior density is not higher, it is accepted with probability eps
    accept &lt;- rbinom(1,1,prob = min(eps,1))
    accepted[t - 1] &lt;- accept
    if (accept == 1){
      theta_posterior[,t] &lt;- theta_proposed
    } else {
      theta_posterior[,t] &lt;- theta_posterior[,t-1]
    }
}</pre></div><p>To plot <a id="id169" class="indexterm"/>the resulting <a id="id170" class="indexterm"/>posterior distribution, we use the sm package in R:</p><div class="informalexample"><pre class="programlisting">&gt;library(sm)
x &lt;- cbind(c(theta_posterior[1,1:num_iter]),c(theta_posterior[2,1:num_iter]))
xlim &lt;- c(min(x[,1]),max(x[,1]))
ylim &lt;- c(min(x[,2]),max(x[,2]))
zlim &lt;- c(0,max(1))

sm.density(x,
           xlab = "mu",ylab="sigma",
           zlab = " ",zlim = zlim,
           xlim = xlim ,ylim = ylim,col="white")
title("Posterior density")</pre></div><p>The resulting posterior distribution will look like the following figure:</p><div class="mediaobject"><img src="../Images/image00351.jpeg" alt="The Metropolis-Hasting algorithm"/></div><p style="clear:both; height: 1em;"> </p><p>Though the <a id="id171" class="indexterm"/>Metropolis-Hasting algorithm is simple to implement for any Bayesian inference problem, in practice it may not be very efficient<a id="id172" class="indexterm"/> in many cases. The main reason for this is that, unless one carefully chooses a proposal distribution <span class="inlinemediaobject"><img src="../Images/image00344.jpeg" alt="The Metropolis-Hasting algorithm"/></span>, there would be too many rejections and it would take a large number of updates to reach the steady state. This is particularly the case when the number of parameters are high. There are various modifications of the basic Metropolis-Hasting algorithms that try to overcome these difficulties. We will briefly describe these when we discuss various R packages for the Metropolis-Hasting algorithm in the following section.</p><div class="section" title="R packages for the Metropolis-Hasting algorithm"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec01"/>R packages for the Metropolis-Hasting algorithm</h5></div></div></div><p>There are several contributed<a id="id173" class="indexterm"/> packages in R for MCMC simulation using the Metropolis-Hasting algorithm, and here we describe some popular ones.</p><p>The <span class="strong"><strong>mcmc</strong></span> package<a id="id174" class="indexterm"/> contributed by Charles J. Geyer and Leif T. Johnson is one of the popular packages in R for MCMC simulations. It has the <code class="literal">metrop</code> function for running the basic Metropolis-Hasting algorithm. The <code class="literal">metrop</code> function uses a multivariate normal distribution as the proposal distribution.</p><p>Sometimes, it is useful to make a variable transformation to improve the speed of convergence in MCMC. The mcmc package has a function named <code class="literal">morph</code> for doing this. Combining these two, the function <code class="literal">morph.metrop</code> first transforms the variable, does a Metropolis on the transformed density, and converts the results back to the original variable.</p><p>Apart from the mcmc package, two other useful packages in R are <a id="id175" class="indexterm"/>
<span class="strong"><strong>MHadaptive</strong></span> contributed by Corey Chivers and the <a id="id176" class="indexterm"/>
<span class="strong"><strong>Evolutionary Monte Carlo</strong></span> (<span class="strong"><strong>EMC</strong></span>) <span class="strong"><strong>algorithm</strong></span> package by Gopi Goswami. Due to lack of space, we will not be discussing these two packages in this book. Interested readers are requested to download these from the C-RAN project's site and experiment with them.</p></div></div><div class="section" title="Gibbs sampling"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec02"/>Gibbs sampling</h4></div></div></div><p>As <a id="id177" class="indexterm"/>mentioned before, the <a id="id178" class="indexterm"/>Metropolis-Hasting algorithm suffers from the drawback of poor convergence, due to too many rejections, if one does not choose a good proposal distribution. To avoid this problem, two physicists Stuart Geman and Donald Geman proposed a new algorithm (reference 5 in the <span class="emphasis"><em>References</em></span> section of this chapter). This algorithm is called Gibbs sampling and it is named after the famous physicist J W Gibbs. Currently, Gibbs sampling is the workhorse of MCMC for Bayesian inference.</p><p>Let <span class="inlinemediaobject"><img src="../Images/image00352.jpeg" alt="Gibbs sampling"/></span> be the set of parameters of the model that we wish to estimate:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Start with an initial state <span class="inlinemediaobject"><img src="../Images/image00353.jpeg" alt="Gibbs sampling"/></span>.
</li><li class="listitem">At each time step, update the components one by one, by drawing from a distribution conditional on the most recent value of rest of the components:<div class="mediaobject"><img src="../Images/image00354.jpeg" alt="Gibbs sampling"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">After <span class="emphasis"><em>N</em></span> steps, all <a id="id179" class="indexterm"/>components of the parameter will be updated.</li><li class="listitem">Continue with step 2 until the Markov process converges to a steady state.</li></ol><div style="height:10px; width: 1px"/></div><p>Gibbs sampling <a id="id180" class="indexterm"/>is a very efficient algorithm since there are no rejections. However, to be able to use Gibbs sampling, the form of the conditional distributions of the posterior distribution should be known.</p><div class="section" title="R packages for Gibbs sampling"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec02"/>R packages for Gibbs sampling</h5></div></div></div><p>Unfortunately, there <a id="id181" class="indexterm"/>are not many contributed general purpose Gibbs sampling packages in R. The <span class="strong"><strong>gibbs.met</strong></span> package<a id="id182" class="indexterm"/> provides two generic functions for performing MCMC in a Naïve way for user-defined target distribution. The first function is <code class="literal">gibbs_met</code>. This performs Gibbs sampling with each 1-dimensional distribution sampled by using the Metropolis algorithm, with normal distribution as the proposal distribution. The second function, <code class="literal">met_gaussian</code>, updates the whole state with independent normal distribution centered around the previous state. The gibbs.met package is useful for general purpose MCMC on moderate dimensional problems.</p><p>In the <span class="emphasis"><em>Exercises</em></span> section of this chapter, we will discuss one problem that involves sampling from the two-dimensional normal distribution by using both the Metropolis-Hasting algorithm and Gibbs sampling to make these concepts more clear. Readers can use these mentioned packages for solving this exercise.</p><p>Apart from the <a id="id183" class="indexterm"/>general purpose MCMC packages, there are several packages in R designed to solve a particular type of machine-learning problems. The <span class="strong"><strong>GibbsACOV</strong></span> package<a id="id184" class="indexterm"/> can be used for one-way mixed-effects ANOVA and ANCOVA models. The <span class="strong"><strong>lda</strong></span> package<a id="id185" class="indexterm"/> performs collapsed Gibbs sampling methods for topic (LDA) models. The <a id="id186" class="indexterm"/>
<span class="strong"><strong>stocc</strong></span> package fits a spatial occupancy model via Gibbs sampling. The <span class="strong"><strong>binomlogit</strong></span> package<a id="id187" class="indexterm"/> implements an efficient MCMC for Binomial Logit models. <span class="strong"><strong>Bmk</strong></span> <a id="id188" class="indexterm"/>is a package for doing diagnostics of MCMC output. <span class="strong"><strong>Bayesian Output Analysis Program</strong></span> (<span class="strong"><strong>BOA</strong></span>) is <a id="id189" class="indexterm"/>another similar package. <span class="strong"><strong>RBugs</strong></span> is <a id="id190" class="indexterm"/>an interface of the well-known <a id="id191" class="indexterm"/>
<span class="strong"><strong>OpenBUGS</strong></span> MCMC package. The <span class="strong"><strong>ggmcmc</strong></span> package<a id="id192" class="indexterm"/> is a graphical tool for analyzing MCMC simulation. <span class="strong"><strong>MCMCglm</strong></span> is<a id="id193" class="indexterm"/> a package for generalized linear mixed models and <span class="strong"><strong>BoomSpikeSlab</strong></span> is a <a id="id194" class="indexterm"/>package for doing MCMC for Spike and Slab regression. Finally, <span class="strong"><strong>SamplerCompare</strong></span> is<a id="id195" class="indexterm"/> a package (more of a framework) for comparing the performance of various MCMC packages.</p></div></div></div><div class="section" title="Variational approximation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec13"/>Variational approximation</h3></div></div></div><p>In the <a id="id196" class="indexterm"/>variational approximation scheme, one <a id="id197" class="indexterm"/>assumes that the posterior distribution <span class="inlinemediaobject"><img src="../Images/image00355.jpeg" alt="Variational approximation"/></span> can be approximated to a factorized form:</p><div class="mediaobject"><img src="../Images/image00356.jpeg" alt="Variational approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Note that the factorized form is also a conditional distribution, so each <span class="inlinemediaobject"><img src="../Images/image00357.jpeg" alt="Variational approximation"/></span> can have dependence on other <span class="inlinemediaobject"><img src="../Images/image00358.jpeg" alt="Variational approximation"/></span>s through the conditioned variable <span class="emphasis"><em>X</em></span>. In other words, this is not a trivial factorization making each parameter independent. The advantage of this factorization is that one can choose more analytically tractable forms of distribution functions <span class="inlinemediaobject"><img src="../Images/image00359.jpeg" alt="Variational approximation"/></span>. In fact, one can vary the functions <span class="inlinemediaobject"><img src="../Images/image00360.jpeg" alt="Variational approximation"/></span> in such a way that it is as close to the <a id="id198" class="indexterm"/>true posterior <span class="inlinemediaobject"><img src="../Images/image00355.jpeg" alt="Variational approximation"/></span> as possible. This is mathematically formulated as a <a id="id199" class="indexterm"/>
<span class="strong"><strong>variational calculus</strong></span> problem, as explained here.</p><p>Let's use some<a id="id200" class="indexterm"/> measures to compute the distance between the two probability distributions, such as <span class="inlinemediaobject"><img src="../Images/image00361.jpeg" alt="Variational approximation"/></span> and <span class="inlinemediaobject"><img src="../Images/image00362.jpeg" alt="Variational approximation"/></span>, where <span class="inlinemediaobject"><img src="../Images/image00352.jpeg" alt="Variational approximation"/></span>. One of the standard measures of distance between probability distributions is the Kullback-Leibler divergence, or KL-divergence for short. It is defined as follows:</p><div class="mediaobject"><img src="../Images/image00363.jpeg" alt="Variational approximation"/></div><p style="clear:both; height: 1em;"> </p><p>The reason why it is called a<a id="id201" class="indexterm"/> divergence and not distance is that <span class="inlinemediaobject"><img src="../Images/image00364.jpeg" alt="Variational approximation"/></span> is not symmetric with respect to <span class="emphasis"><em>Q</em></span> and <span class="emphasis"><em>P</em></span>. One can use the relation <span class="inlinemediaobject"><img src="../Images/image00365.jpeg" alt="Variational approximation"/></span> and rewrite the preceding expression as an equation for <span class="emphasis"><em>log P(X)</em></span>:</p><div class="mediaobject"><img src="../Images/image00366.jpeg" alt="Variational approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Here:</p><div class="mediaobject"><img src="../Images/image00367.jpeg" alt="Variational approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Note that, in<a id="id202" class="indexterm"/> the equation for <span class="emphasis"><em>ln P(X)</em></span>, there is no dependence on <span class="emphasis"><em>Q</em></span> on the LHS. Therefore, maximizing <span class="inlinemediaobject"><img src="../Images/image00368.jpeg" alt="Variational approximation"/></span> with respect to <span class="emphasis"><em>Q</em></span> will <a id="id203" class="indexterm"/>minimize <span class="inlinemediaobject"><img src="../Images/image00364.jpeg" alt="Variational approximation"/></span>, since their sum is a term independent of <span class="emphasis"><em>Q</em></span>. By choosing analytically tractable functions for <span class="emphasis"><em>Q</em></span>, one can do this maximization in practice. It will result in both an approximation for the posterior and a lower bound for <span class="emphasis"><em>ln P(X)</em></span> that is the logarithm of evidence or marginal likelihood, since <span class="inlinemediaobject"><img src="../Images/image00369.jpeg" alt="Variational approximation"/></span>.</p><p>Therefore, variational approximation gives us two quantities in one shot. A posterior distribution can be used to make predictions about future observations (as explained in the next section) and a lower bound for evidence can be used for model selection.</p><p>How does one implement this minimization of KL-divergence in practice? Without going into mathematical details, here we write a final expression for the solution:</p><div class="mediaobject"><img src="../Images/image00370.jpeg" alt="Variational approximation"/></div><p style="clear:both; height: 1em;"> </p><p>Here, <span class="inlinemediaobject"><img src="../Images/image00371.jpeg" alt="Variational approximation"/></span> implies that the expectation of the logarithm of the joint distribution <span class="inlinemediaobject"><img src="../Images/image00372.jpeg" alt="Variational approximation"/></span> is taken over all the parameters <span class="inlinemediaobject"><img src="../Images/image00373.jpeg" alt="Variational approximation"/></span> except for <span class="inlinemediaobject"><img src="../Images/image00358.jpeg" alt="Variational approximation"/></span>. Therefore, the minimization of KL-divergence <a id="id204" class="indexterm"/>leads to a set of coupled equations; one for each <span class="inlinemediaobject"><img src="../Images/image00374.jpeg" alt="Variational approximation"/></span> needs to be solved self-consistently to obtain the final solution. Though the variational approximation looks very complex mathematically, it has a very simple, intuitive explanation. The posterior distribution of each parameter <span class="inlinemediaobject"><img src="../Images/image00357.jpeg" alt="Variational approximation"/></span> is obtained by averaging the log of the joint distribution over all the other variables. This is analogous to the Mean Field<a id="id205" class="indexterm"/> theory in physics where, if there are <span class="emphasis"><em>N</em></span> interacting charged particles, the system can be approximated by saying that each particle is in a constant external field, which is the average of fields produced by all the other particles.</p><p>We will end this section by mentioning a few R packages for variational approximation. The <span class="strong"><strong>VBmix</strong></span> package<a id="id206" class="indexterm"/> can be used for variational approximation in Bayesian mixture models. A similar package is <a id="id207" class="indexterm"/>
<span class="strong"><strong>vbdm</strong></span> used for Bayesian discrete mixture models. The package <span class="strong"><strong>vbsr</strong></span> is <a id="id208" class="indexterm"/>used for variational inference in Spike Regression Regularized Linear Models.</p></div></div><div class="section" title="Prediction of future observations"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/>Prediction of future observations</h2></div></div></div><p>Once <a id="id209" class="indexterm"/>we have the posterior distribution inferred from data using some of the methods described already, it can be used to predict future observations. The probability of observing a value <span class="emphasis"><em>Y</em></span>, given observed data <span class="emphasis"><em>X</em></span>, and posterior distribution of parameters <span class="inlinemediaobject"><img src="../Images/image00375.jpeg" alt="Prediction of future observations"/></span> is given by:</p><div class="mediaobject"><img src="../Images/image00376.jpeg" alt="Prediction of future observations"/></div><p style="clear:both; height: 1em;"> </p><p>Note that, in this expression, the likelihood function <span class="inlinemediaobject"><img src="../Images/image00377.jpeg" alt="Prediction of future observations"/></span> is averaged by using the distribution <a id="id210" class="indexterm"/>of the parameter given by the posterior <span class="inlinemediaobject"><img src="../Images/image00375.jpeg" alt="Prediction of future observations"/></span>. This is, in fact, the core strength of the Bayesian inference. This Bayesian averaging eliminates the uncertainty in estimating the parameter values and makes the prediction more robust.</p></div></div></div>
<div class="section" title="Exercises" id="aid-TI1E1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Exercises</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Derive the <a id="id211" class="indexterm"/>equation for the posterior mean by expanding the square in the exponential <span class="inlinemediaobject"><img src="../Images/image00378.jpeg" alt="Exercises"/></span> for each <span class="emphasis"><em>i</em></span>, collecting all similar power terms, and making a perfect square again. Note that the product of exponentials can be written as the exponential of a sum of terms.
</li><li class="listitem">For this<a id="id212" class="indexterm"/> exercise, we use the dataset corresponding to Smartphone-Based Recognition of Human Activities and Postural Transitions, from the UCI Machine Learning repository (<a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions">https://archive.ics.uci.edu/ml/datasets/Smartphone-Based+Recognition+of+Human+Activities+and+Postural+Transitions</a>). It contains values of acceleration taken from an accelerometer on a smartphone. The original dataset contains <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>, and <span class="emphasis"><em>z</em></span> components of the acceleration and the corresponding timestamp values. For this exercise, we have used only the two horizontal components of the acceleration <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>. In this exercise, let's assume that the acceleration follows a normal distribution. Let's also assume a normal prior distribution for the mean values of acceleration with a hyperparameter for a mean that is uniformly distributed in the interval (-0.5, 0.5) and a known variance equal to 1. Find the posterior mean value by using the expression given in the equation.</li><li class="listitem">Write an R function to compute the Fisher information matrix. Obtain the Fisher information matrix for this problem by using the dataset mentioned in exercise 1 of this section.</li><li class="listitem">Set up an MCMC simulation for this problem by using the <span class="strong"><strong>mcmc</strong></span> package in R. Plot a histogram of the simulated data.</li><li class="listitem">Set up an MCMC simulation using Gibbs sampling. Compare the results with that of the Metropolis algorithm.</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="References" id="aid-UGI01"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Berger J.O. <span class="emphasis"><em>Statistical Decision Theory and Bayesian Analysis</em></span>. Springer Series in Statistics. 1993. ISBN-10: 0387960988</li><li class="listitem">Jayes E.T. <span class="emphasis"><em>Probability Theory: The Logic of Science</em></span>. Cambridge University Press. 2003. ISBN-10: 052159271</li><li class="listitem">Wittman D. "Fisher Matrix for Beginners". Physics Department, University of California at Davis (<a class="ulink" href="http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf">http://www.physics.ucdavis.edu/~dwittman/Fisher-matrix-guide.pdf</a>)</li><li class="listitem">Metropolis N, Rosenbluth A.W., Rosenbluth M.N., Teller A.H., Teller E. "Equations of State Calculations by Fast Computing Machines". Journal of Chemical Physics 21 (6): 1087–1092. 1953</li><li class="listitem">Geman S., Geman D. "Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images". IEEE Transactions on Pattern Analysis and Machine Intelligence 6 (6): 721-741. 1984</li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="Summary" id="aid-VF2I1"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Summary</h1></div></div></div><p>In this chapter, we covered basic principles of Bayesian inference. Starting with how uncertainty is treated differently in Bayesian statistics compared to classical statistics, we discussed deeply various components of Bayes' rule. Firstly, we learned the different types of prior distributions and how to choose the right one for your problem. Then we learned the estimation of posterior distribution using techniques such as MAP estimation, Laplace approximation, and MCMC simulations. Once the readers have comprehended this chapter, they will be in a position to apply Bayesian principles in their data analytics problems. Before we start discussing specific Bayesian machine learning problems, in the next chapter, we will review machine learning in general.</p></div></body></html>