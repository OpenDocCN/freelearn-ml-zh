- en: Creating Art with Style Transfer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore what was one of the most popular mainstream
    applications of deep learning in 2017—style transfer. We begin by introducing
    the concepts of style transfer and then its faster alternative, appropriately
    named fast neural style transfer. Similar to other chapters, we will provide the
    intuition behind the models (rather than granular details) and, in doing so, you
    will gain a deeper understanding and appreciation for the potential of deep learning
    algorithms. Unlike previous chapters, this chapter will focus more on the steps
    involved in getting the model working on iOS rather than building up the application,
    in order to keep it concise.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter you will have achieved the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Gained an intuitive understanding of how style transfer works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gained hands-on experience of working with the Core ML Tools Python package
    and custom layers to get Keras models working in Core ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started by introducing style transfer and building our understanding
    of how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Transferring style from one image to another
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine being able to have one of the greatest painters in history, such as Vincent
    van Gogh or Pablo Picasso, recreate a photo of your liking using their own unique
    style. In a nutshell, this is what style transfer allows us to do. Quite simply,
    it''s the process of generating a photo using the style of one with the content
    of another, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c79f7dd-e436-4a7f-8dbe-0a8917fdd108.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we will describe, albeit at a high level, how this works and
    then move on to an alternative that allows us to perform a similar process in
    significantly less time.
  prefs: []
  type: TYPE_NORMAL
- en: I encourage you to read the original paper, *A Neural Algorithm of Artistic
    Style*, by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, for a more
    comprehensive overview. This paper is available at [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576).
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, we have learned that neural networks learn by iteratively reducing
    a loss, calculated using some specified cost function that is to indicate how
    well the neural network did with respect to the expected output. The difference
    between the **predicted output** and **expected output** is then used to adjust
    the model's weights, through a process known as **backpropagation**, such to minimize
    this loss.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding description (intentionally) skips the details of this process
    as our goal here is to provide an intuitive understanding, rather than the granular
    details. I recommend reading Andrew Trask's *Grokking Deep Learning* for a gentle
    introduction to the underlying details of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the classification models we have worked with thus far, where the output
    is a probability distribution across some set of labels, we are instead interested
    in the model's generative abilities. That is, instead of adjusting the model's
    weights, we want to adjust the generated image's pixel values so as to reduce
    some defined cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So if we were to define a cost function that could measure the loss between
    the generated image and content image, and another to measure the loss between
    the generated image and style image, we could then simply combine them. Thus we
    obtain the overall loss and use this to adjust the generated image pixels values,
    to create something that has the targets content in the style of our targets style
    as illustrated in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17c518be-eba6-4893-9bad-489861742bae.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we have a general idea of the required process; what is left
    is building some intuition behind these cost functions. That is, how do you determine
    how well your generated image is, with respect to some content of the content
    image and with respect to a style of the style image? For this, we will backtrack
    a little and review what other layers of a CNN learn by inspecting each of their
    activations.
  prefs: []
  type: TYPE_NORMAL
- en: The details and images demonstrating what **convolutional neural networks**
    (**CNNs**) learn have been taken from the paper *Visualizing and Understanding
    Convolutional Networks*, by Matthew D. Zeiler and Rob Fergus, which is available
    at [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901).
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical architecture of a CNN consists of a series of convolutional and pooling
    layers, which is then fed into a fully connected network (for case of classification),
    as illustrated in this image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68df6574-ed16-4453-9739-0762baa74183.png)'
  prefs: []
  type: TYPE_IMG
- en: This flat representation misses an important property of a CNN, which is how,
    after each subsequent pair of convolution and pooling layers, the input's width
    and height reduce in size. The consequence of this is that the receptive field
    increases depth into the network; that is, deeper layers have a larger receptive
    field and thus capture higher level features than shallower layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better illustrate what each layer learns, we will reference the paper *Visualizing
    and Understanding Convolutional Networks*, by Matthew D. Zeiler and Rob Fergus.
    In their paper (previously referenced), they pass through images from their training
    set to identify the image patches that maximize each layer''s activations; by
    visualizing these patches, we get a sense of what each neuron (hidden unit) at
    each of the layers learns. Here is an screenshot showing some of these patches
    across a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b02ef6da-b1d9-4d10-8df6-40fdae4fd5b8.png)'
  prefs: []
  type: TYPE_IMG
- en: Source: Visualizing and Understanding Convolutional Networks; Matthew D Zeiler,
    Rob Fergus
  prefs: []
  type: TYPE_NORMAL
- en: What you can see in the preceding figure are nine image patches that maximize
    an individual hidden unit at each of the layers of this particular network. What
    has been omitted from the preceding figure is the variance in size; that is, the
    deeper you go, the larger the image patch will be.
  prefs: []
  type: TYPE_NORMAL
- en: What is hopefully obvious from the preceding image is that the shallower layers
    extract simple features. For example, we can see that a single hidden unit at
    **Layer 1** is activated by a diagonal edge and a single hidden unit at **Layer
    2** is activated with a vertically striped patch. While the deeper layers extract
    higher-level features, or more complex features, again, in the preceding figure,
    we can see that a single hidden unit at **Layer 4** is activated by patches of
    dog faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'We return to our task of defining a cost function for content and style, starting
    with the cost function for content. Given a content image and a generated image,
    we want to measure how close we are so as to minimize this difference, so that
    we retain the content. We can achieve this by selecting one of the deeper layers
    from our CNN, which we saw before have a large receptive field, and capture complex
    features. We pass through both the content images and the generated image and
    measure the distance between outputted activations (on this layer). This will
    hopefully seem logical given that the deeper layers learn complex features, such
    as a dog''s face or a car, but decouple them from lower-level features such as
    edges, color, and textures. The following figure depicts this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6116a977-45e8-465f-9eb2-281c004ab46b.png)'
  prefs: []
  type: TYPE_IMG
- en: This takes care of our cost function for the content which can be easily tested
    by running a network that implements this. If implemented correctly, it should
    result in a generated image that looks similar to that of the input (content image).
    Let's now turn our attention to measuring style.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in the preceding figure that shallower layers of a network learn simple
    features such as edges, textures, and color combinations. This gives us a clue
    as to which layers would be useful when trying to measure style, but we still
    need a way of extracting and measuring style. However, before we start, what exactly
    is style?
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick search on [http://www.dictionary.com/](http://www.dictionary.com/)
    reveals style being defined as *a distinctive appearance, typically determined
    by the principles according to which something is designed*. Let''s take Katsushika
    Hokusai''s *The Great Wave off Kanagawa* as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed306f8b-c601-42bb-b2a2-f5aa88ea612c.png)'
  prefs: []
  type: TYPE_IMG
- en: '*The Great Wave off Kanagawa* is an output of a process known as **woodblock
    printing**; this is where an artist''s sketch is broken down into layers (carved
    wooden blocks), with each layer (usually one for each color) used to reproduce
    the art piece. It''s similar to a manual printing press; this process produces
    a distinctive flat and simplistic style. Another dominate style (and possibly
    side-effect) that can be seen in the preceding image is that a limited range of
    colors is being used; for example, the water consists of no more than four colors.'
  prefs: []
  type: TYPE_NORMAL
- en: The way we can capture style is as defined in the paper *A Neural Algorithm
    of Artistic Style*, by L. Gatys, A. Ecker, and M. Bethge. This way is to use a
    style matrix (also known as **gram matrix**) to find the correlation between the
    activations across different channels for a given layer. It is these correlations
    that define the style and something we can then use to measure the difference
    between our style image and generated image to influence the style of the generated
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this more concrete, borrowing from an example used by Andrew Ng in
    his Coursera course on deep learning, let''s take **Layer 2** from the earlier
    example. What the style matrix calculates is the correlation across all channels
    for a given layer. If we use the following illustration, showing nine activations
    from two channels, we can see that a correlation exists between vertical textures
    from the first channel with orange patches from the second channel. That is, when
    we see a vertical texture in the first channel, we would expect the image patches
    that maximize the second channel''s activations to have an orange tint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f0c959f-2afa-49f8-a2eb-e100a3fa9196.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This style matrix is calculated for both the style image and generated image,
    with our optimization forcing our generated image to adopt these correlations.
    With both style matrices calculated, we can then calculate the loss by simply
    finding the sum of the square difference between the two matrices. The following
    figure illustrates this process, as we have previously done when describing the
    content loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0245269-6f46-4dbd-888c-e71fa9a0f5cc.png)'
  prefs: []
  type: TYPE_IMG
- en: With that, we have now concluded our introduction to style transfer, and hopefully
    given you some intuition of how we can use the network's perceptual understanding
    of images to extract content and style. This approach works well, but there is
    one drawback that we will address in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: A faster way to transfer style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you may have inferred from the title of this section, the big drawback of
    the approach introduced in the previous section is that the process requires iterative
    optimization, as summarized in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80a70ed5-d413-45d0-bd9c-6fb5e73bdff4.png)'
  prefs: []
  type: TYPE_IMG
- en: This optimization is akin to training, in terms of performing many iterations
    to minimize the loss. Therefore, it typically takes a considerable amount of time,
    even when using a modest computer. As implied at the start of this book, we ideally
    want to restrict ourselves to performing inference on the edge as it requires
    significantly less compute power and can be run in near-real time, allowing us
    to adopt it for interactive applications. Luckily for us, in their paper *Perceptual
    Losses for Real-Time Style Transfer and Super-Resolution*, J. Johnson, A. Alahi,
    and L. Fei-Fei describe a technique that decouples training (optimization) and
    inference for style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, we described a network that took as its input a generated image,
    a style image, and a content image. The network minimized loss by iteratively
    adjusting the generated image using the loss functions for content and style;
    this provided the flexibility of allowing us to plug in any style and content
    image, but came at the cost of being computationally expensive, that is, slow.
    What if we sacrifice this flexibility for performance by restraining ourselves
    to a single style and, instead of performing the optimization to generate the
    image, train a CNN? The CNN would learn the style and, once trained, could generate
    a stylized image given a content image with a single pass through the network
    (inference). This is, in essence, what the paper *Perceptual Losses for Real-Time
    Style Transfer and Super-Resolution*, describes, and it is the network we will
    use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better elucidate the difference between the previous approach and this approach,
    take a moment to review and compare the preceding figure with the following one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c05bedd-135f-46f8-b726-125d44a0cb3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike the previous approach, where we optimized for a given set of content,
    style, and generated images and adjusted the generated image to minimize loss,
    we now feed a CNN with a set of content images and have the network generate the
    image. We then perform the same loss functions as described earlier for a single
    style. But, instead of adjusting the generated image, we adjust the weights of
    the networks using the gradients from the loss function. And we repeat until we
    have sufficiently minimized the mean loss across all of our content images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with our model trained, we can have our network stylize an image with
    a single pass, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b348bf5b-bde1-4e9f-8950-9653fa333ae9.png)'
  prefs: []
  type: TYPE_IMG
- en: Over the last two sections we have described, at a high-level, how these networks
    work. Now, it's time to build an application that takes advantage of all this.
    In the next section, we will quickly walk through converting the trained Keras
    model to Core ML before moving on to the main topic of this chapter—implementing
    custom layers for Core ML.
  prefs: []
  type: TYPE_NORMAL
- en: Converting a Keras model to Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to what we did in the previous chapter, in this section we will be converting
    a trained Keras model into a Core ML model using the **Core ML Tools** package.
    To avoid any complications of setting up the environment on your local or remote
    machine, we will leverage the free Jupyter cloud service provided by Microsoft.
    Head over to [https://notebooks.azure.com](https://notebooks.azure.com) and log
    in (or register if you haven't already).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once logged in, click on the Libraries menu link from the navigation bar, which
    will take you to a page containing a list of all of your libraries, similar to
    what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cdb2fe87-6e5c-4c63-a213-e61ec24d4608.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, click on the + New Library link to bring up the Create New Library dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb802108-7164-4387-8abb-c608e8383555.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, click on the From GitHub tab and enter `https://github.com/packtpublishing/machine-learning-with-core-ml` in
    the GitHub repository field. After that, give your library a meaningful name and
    click on the Import button to begin the process of cloning the repository and
    creating the library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the library has been created, you will be redirected to the root. From
    there, click on the `Chapter6/Notebooks` folder to open up the relevant folder
    for this chapter, and finally click on the Notebook `FastNeuralStyleTransfer_Keras2CoreML.ipynb`.
    Here is a screenshot of what you should see after clicking on the `Chapter6` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3705777f-bd95-4b87-b458-3baf5a7249d0.png)'
  prefs: []
  type: TYPE_IMG
- en: It's beyond the scope of this book to walk you through the details of the Notebook,
    including the details of the network and training. For the curious reader, I have
    included the original Notebooks for each of the models used throughout this book
    in the accompanying `chapters` folder within the `training` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our Notebook now loaded, it''s time to walk through each of the cells
    to create our Core ML model; all of the required code exists and all that remains
    is executing each of the cells sequentially. To execute a cell, you can either
    use the shortcut keys *Shift* + *Enter* or click on the Run button in the toolbar
    (which will run the currently selected cell), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/228ba8b2-345c-4aca-a980-c5b6eac51b3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I will provide a brief explanation of what each cell does. Ensure that you
    execute each cell as we walk through them so that we all end up with the converted
    model, which we can then download and import into our iOS project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We first import a module that includes the a function that will create and
    return the Keras model we want to convert:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We then use our `helpers` method `build_model` to create the model, passing
    in the style image that the model was trained on. Remember that we are using a
    feedforward network that has been trained on a single style; while the network
    can be reused for different styles, the weights are unique per style.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `build_model` will take some time to return; this is because the model
    uses a trained model (VGG16) that is downloaded before returning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Talking of weights (previously trained model), let''s now load them by running
    the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the aforementioned code, we are passing in the weights for the model
    that was trained on Vincent van Gogh's *Starry Night* painting for its style.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s inspect the architecture of the model by calling the `summary`
    method on the model itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling this will return, as the name suggests, a summary of our model. Here
    is an extract of the summary produced:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As previously mentioned, it''s out of scope to go into the details of Python,
    Keras, or the specifics of this model. Instead I present an extract here to highlight
    the custom layers embedded in the model (the bold lines). In the context of Core
    ML Tools, custom layers are layers that have not been defined and, therefore,
    are not handled during the conversion process, so it is our responsibility to
    handle these. You can think of the conversion process as a process of mapping
    layers from a machine learning framework, such as Keras, to Core ML. If no mapping
    exists, then it is left up to us to fill in the details, as illustrated in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1022c0b7-25f4-4e6f-877f-5e7728ac0d4e.png)'
  prefs: []
  type: TYPE_IMG
- en: The two custom layers shown previously are both Lambda layers; a Lambda layer
    is a special Keras class that conveniently allows writing quick-and-dirty layers
    using just a function or a Lambda expression (similar to a closure in Swift).
    Lambda is useful for layers that don’t have a state and are commonly seen in Keras
    models for doing basic computations. Here, we see two being used, `res_crop` and
    `rescale_output`.
  prefs: []
  type: TYPE_NORMAL
- en: '`res_crop` is part of the ResNet block that crops (as implied by the name)
    the output; the function is simple enough, with its definition shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I refer you to the paper *Deep Residual Learning for Image Recognition*, by
    K. He, X. Zhang, S. Ren, and J. Sun to learn more about ResNet and residual blocks,
    available here at [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, all that this is doing is cropping the outputs with a padding
    of 2 for the width and height axis. We can further interrogate this by inspecting
    the input and output shapes of this layer, by running the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This cell prints the input and output shape of the layer `res_crop_3_layer`;
    the layer receives a tensor of shape `(None, 88, 88, 64)` and outputs a tensor
    of shape `(None, 84, 84, 64)`. Here the tuple is broken down into: (batch size,
    height, width, channels). The batch size is set to `None`, indicating that it
    is dynamically set during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our next Lambda layer is `rescale_output`; this is used at the end of the network
    to rescale the outputs from the Convolution 2D layer, which passes its data through
    a tanh activation. This forces our data to be constrained between -1.0 and 1.0,
    where as we want it in a range of 0 and 255 so that we can convert it into an
    image. As we did before, let''s look at its definition to get a better idea of
    what this layer does, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This method performs an element-wise operation that maps the values -1.0 and
    1.0 to 0 and 255\. Similar to the preceding method (`res_crop`), we can inspect
    the input and output shapes of this layer by running the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Once run, this cell prints the layer's input shape of `(None, 320, 320, 3)`
    and output shape of `(None, 320, 320, 3)`. This tells us that this layer doesn't
    change the shape of the tensor, as well as shows us the output dimensions of our
    image as 320 x 320 with three channels (RGB).
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now reviewed the custom layers and seen what they actually do; the
    next step is to perform the actual conversion. Run the following cell to ensure
    that the environment has the Core ML Tools modules installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, we can load the required modules by running the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this instance, I have prewarned you that our model contains custom layers;
    in some (if not most) instances, you may discover this only when the conversion
    process fails. Let''s see exactly what this looks like by running the following
    cell and examining its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippet, we are passing our model to the method `coremltools.converters.keras.convert`,
    which is responsible for converting our Keras model to Core ML. Along with the
    model, we pass in the input and output names for our model, as well as setting `image_input_names` to
    inform the method that we want the input `image` to be treated as an image rather
    than a multidimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, after running this cell, you will receive an error. If you scroll
    to the bottom of the output, you will see the line `ValueError: Keras layer ''<class
    ''keras.layers.core.Lambda''>'' not supported`. At this stage, you will need to
    review the architecture of your model to identify the layer that caused the error
    and proceed with what you are about to do.'
  prefs: []
  type: TYPE_NORMAL
- en: By enabling the parameter `add_custom_layers` in the conversion call, we prevent
    the method from failing when the converter encounters a layer it doesn't recognize.
    A placeholder layer named custom will be inserted as part of the conversion process.
    In addition to recognizing custom layers, we can pass in a `delegate` function
    to the parameter `custom_conversion_functions`, which allows us to add metadata
    to the model's specification stating how the custom layer will be handled.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create this `delegate` method now; run the cell with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This `delegate` is passed each custom layer the converter comes across. Because
    we are dealing with two different layers, we first check which layer we are dealing
    with and then proceed to create and return an instance of `CustomLayerParams`.
    This class allows us to add some metadata used when creating the model's specification
    for the Core ML conversion. Here we are setting its `className`, which is the
    name of the Swift (or Objective-C) class in our iOS project that implements this
    layer, and `description`, which is the text shown in Xcode 's ML model viewer.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our `delegate` method now implemented, let''s rerun the converter, passing
    in the appropriate parameters, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'If all goes well, you should see the converter output each layer it visits,
    with no error messages, and finally returning a Core ML model instance. We can
    now add metadata to our model, which is what is displayed in Xcode ''s ML model
    views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, we could save the model and import into Xcode , but there is
    just one more thing I would like to do to make our life a little easier. At its
    core (excuse the pun), the Core ML model is a specification of the network (including
    the model description, model parameters, and metadata) used by Xcode to build
    the model when imported. We can get a reference to this specification by calling
    the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'With reference to the specification of the models, we next search for the output
    layer, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the output simply by printing it out; run the cell with the
    following code to do just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Take note of the type, which is currently `multiArrayType` (its iOS equivalent
    is `MLMultiArray`). This is fine but would require us to explicitly convert it
    to an image; it would be more convenient to just have our model output an image
    instead of a multidimensional array. We can do this by simply modifying the specification.
    Specifically, in this instance, this means populating the type''s `imageType`
    properties to hint to Xcode that we are expecting an image. Let''s do that now
    by running the cell with this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We first set the color space to RGB, then we set the expected width and height
    of the image. Finally, we create a new model by passing in the updated specification
    with the statement `coremltools.models.MLModel(spec)`. Now, if you interrogate
    the output, you should see something like the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now saved ourselves a whole lot of code to perform this conversion;
    our final step is to save the model before importing it into Xcode . Run the last
    cell, which does just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Before closing the browser, let''s download the model. You can do this by returning
    the `Chapter6/Notebooks` directory and drilling down into the `output` folder.
    Here you should see the file `FastStyleTransferVanGoghStarryNight.mlmodel`; simply right-click
    on it and select the Download menu item (or do it by left-clicking and selecting
    the Download toolbar item):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0792c6c-82da-4984-8e22-5463596401dd.png)'
  prefs: []
  type: TYPE_IMG
- en: With our model in hand, it's now time to jump into Xcode and implement those
    custom layers.
  prefs: []
  type: TYPE_NORMAL
- en: Building custom layers in Swift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be mainly focusing on implementing the custom layers
    that our model is dependent on, and we'll omit a lot of the application's details
    by working with an existing template—a structure you have no doubt become quite
    familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven''t done so already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter6/Start/StyleTransfer/` and
    open the project `StyleTransfer.xcodeproj`. Once loaded, you will see the project
    for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0de45713-7155-44ad-8787-cd62a5d02fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: The application consists of two view controllers. The first, `CameraViewController`,
    provides the user with a live stream of the camera and the ability to take a photo.
    When a photo is taken, the controller presents the other view controller, `StyleTransferViewController`,
    passing along with the captured photo. `StyleTransferViewController` then presents
    the image, along with a horizontal `CollectionView` at the bottom containing a
    set of styles that the user can select by tapping on them.
  prefs: []
  type: TYPE_NORMAL
- en: Each time the user selects a style, the controller updates the `ImageProcessors`
    style property and then calls its method, `processImage`, passing in the assigned
    image. It is here that we will implement the functionality responsible for passing
    the image to the model and returning the result via the assigned delegates `onImageProcessorCompleted`
    method, which is then presented to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, with our project loaded, let''s import the model we have just created;
    locate the downloaded `.mlmodel` file and drag it onto Xcode . Once imported,
    we select it from the left-hand \ panel to inspect the metadata, to remind ourselves
    what we need to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a97e6658-5e37-4a88-8712-177aedd2b358.png)'
  prefs: []
  type: TYPE_IMG
- en: By inspecting the model, we can see that it is expecting an input RGB image
    of size 320 x 320, and it will output an image with the same dimensions. We can
    also see that the model is expecting two custom layers named `ResCropBlockLambda`
    and `RescaleOutputLambda`. Before implementing these classes, let's hook the model
    up and, just for fun, see what happens when we try to run it without the custom
    layers implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select `ImageProcessor.swift` from the left-hand-side panel; in this project,
    we will get the Vision framework to do all the preprocessing. Start by adding
    the following properties within the body of the `ImageProcessor` class, somewhere
    such as underneath the `style` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The first property returns an instance of `VNCoreMLModel`, wrapping our `FastStyleTransferVanGoghStarryNight`
    model. Wrapping our model is necessary to make it compatible with the Vision framework's
    requests classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just underneath, add the following snippet, which will be responsible for returning
    the appropriate `VNCoreMLModel`, based on the selected style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create the method that will be responsible for returning an instance
    of `VNCoreMLRequest`, based on the currently selected model (determined by the
    current `style`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`VNCoreMLRequest` is responsible for performing the necessary preprocessing
    on the input image before passing it to the assigned Core ML model. We instantiate
    `VNCoreMLRequest`, passing in a completion handler that will simply pass its results
    to the `processRequest` method, of the `ImageProcessor` class, when called. We
    also set the `imageCropAndScaleOption` to `.centerCrop` so that our image is resized
    to 320 x 320 whilst maintaining its aspect ratio (cropping the centered image
    on its longest side, if necessary).'
  prefs: []
  type: TYPE_NORMAL
- en: 'With our properties now defined, it''s time to jump into the `processImage` method
    to initiate the actual work; add the following code (shown in bold, and replacing the `//
    TODO` comments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding method is our entry point to stylizing an image; we start by instantiating
    an instance of `VNImageRequestHandler`, passing in the image, and initiating the
    process by calling the `perform` method. Once the analysis has finished, the request
    will call the `delegate` we assigned to it, `processRequest`, passing in a reference
    of the associated request and the results (or errors if any). Let''s flesh out
    this method now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: While `VNCoreMLRequest` is responsible for the image analysis, `VNImageRequestHandler`
    is responsible for executing the request (or requests).
  prefs: []
  type: TYPE_NORMAL
- en: If no errors occurred during the analysis, we should be returned the instance
    of our request with its results property set. As we are only expecting one request
    and result type, we cast the results to an array of `VNPixelBufferObservation`,
    a type of observation suitable for image analysis with a Core ML model whose role
    is image-to-image processing, such as our style transfer model.
  prefs: []
  type: TYPE_NORMAL
- en: We can get a reference to our stylized image via the property `pixelBuffer`,
    from the observation obtained from the results. And then we can call the extension
    method `toCGImage` (found in `CVPixelBuffer+Extension.swift`) to conveniently
    obtain the output in a format we can easily use, in this case, updating the image
    view.
  prefs: []
  type: TYPE_NORMAL
- en: As previously discussed, let's see what happens when we try to run an image
    through our model without implementing the custom layers. Build and deploy to
    a device and proceed to take a photo, then select the Van Cogh style from the
    styles displayed. In doing so, you will observe the build failing and reporting
    the error: `Error creating Core ML custom layer implementation from factory for
    layer "RescaleOutputLambda"` (as we were expecting).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s address this now by implementing each of our custom layers, starting
    with the `RescaleOutputLambda` class. Create a new Swift file named `RescaleOutputLamdba.class`
    and replace the template code with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have created a concrete class of the protocol `MLCustomLayer`, a protocol
    that defines the behavior of a custom layer in our neural network model. The protocol
    consists of four required methods and one optional method, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init(parameters)`: Initializes the custom layer implementation that is passed
    the dictionary `parameters` that includes any additional configuration options
    for the layer. As you may recall, we created an instance of `NeuralNetwork_pb2.CustomLayerParams`
    for each of our custom layers when converting our Keras model. Here we can add
    more entries, which will be passed into this dictionary. This provides some flexibility,
    such as allowing you to adjust your layer based on the set parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setWeightData()`: Assigns the weights for the connections within the layer
    (for layers with trainable weights).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outputShapes(forInputShapes)`: This determines how the layer modifies the
    size of the input data. Our `RescaleOutputLambda` layer doesn''t change the size
    of the layer, so we simply need to return the input shape, but we will make use
    of this when implementing the next custom layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`evaluate(inputs, outputs)`: This performs the actual computation; this method
    is required and gets called when the model is run on the CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`encode(commandBuffer, inputs, outputs)`: This method is optional and acts
    as an alternative to the method `evaluate`, which uses the GPU rather than the
    CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because we are not passing in any custom parameters or setting any trainable
    weights, we can skip the constructor and `setWeightData` methods; let's walk through
    the remaining methods, starting with `outputShapes(forInputShapes)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously mentioned, this layer doesn''t change the shape of the input,
    therefore, we can simply return the input shape, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: With our `outputShapes(forInputShapes)` method now implemented, let's turn our
    attention to the workhorse of the layer responsible for performing the actual
    computation, the `evaluate` method. The `evaluate` method receives an array of `MLMultiArray`
    objects as inputs, along with another array of `MLMultiArray` objects, where it
    is expected to store the results. Having the `evaluate` method accept arrays for
    its input and outputs allows for greater flexibility in supporting different architectures,
    but, in this example, we are expecting only one input and one output.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, this layer is for scaling each element from a range of -1.0
    - 1.0 to a range of 0 - 255 (that''s what a typical image would be expecting).
    The simplest approach is to iterate through each element and scale it using the
    equation we saw in Python: `((x+1)*127.5`. This is exactly what we''ll do; add
    the following code (in bold) to the body of your `evaluate` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The bulk of this method is made up of code used to create the index for obtaining
    the appropriate value from the input and pointing to its output counterpart. Once
    an index has been created, the Python formula is ported across to Swift: `input[index].doubleValue
    + rescaleAddition) * rescaleMulitplier`. This concludes our first custom layer;
    let's now implement our second customer layer, `ResCropBlockLambda`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file called `ResCropBlockLambda.swift` and add the following code,
    overwriting any existing code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As we have done with the previous custom layer, we have stubbed out all the
    required methods as determined by the `MLCustomLayer` protocol. Once again, we
    can ignore the constructor and `setWeightData` method as neither are used in this
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall, and as the name suggests, the function of this layer is to crop
    the width and height of one of the inputs of the residual block. We need to reflect
    this within the `outputShapes(forInputShapes)` method, so that the network knows
    the input dimensions for subsequent layers. Update the `outputShapes(forInputShapes)`
    method with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are removing a constant of `4` from the width and height, essentially
    padding 2 from the width and height. Next, we implement the `evaluate` method,
    which performs this cropping. Replace the `evaluate` method with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the `evaluate` method of our `RescaleOutputLambda` layer, the bulk
    of this method has to do with creating the indices for the input and output arrays.
    We simply pad it by restraining the ranges of our loops to the desired width and
    height.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you build and run the project, you will be able to run an image through
    the Van Gogh network getting a stylized version of it back, similar to what is
    shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74bc12a2-2c87-4e58-8bd2-b30ead776cf2.png)'
  prefs: []
  type: TYPE_IMG
- en: When running on the simulator, the whole process took approximately **22.4 seconds**.
    In the following two sections, we will spend some time looking at how we can reduce
    this.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerating our layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s return to the layer `RescaleOutputLambda` and see where we might be
    able to shed a second or two off the processing time. As a reminder, the function
    of this layer is to rescale each element in the output, where our output can be
    thought of as a large vector. Luckily for us, Apple provides an efficient framework
    and API for just this. Instead of operating on each element within a loop, we
    will take advantage of the `Accelerate` framework and its vDSPAPI to perform this
    operation in a single step. This process is called **vectorization** and is made
    possible by exploiting the CPU''s **Single Instruction, Multiple Data** (**SIMD**) instruction
    set. Return to the `RescaleOutputLambda` class and update the `evaluate` method
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we first get a reference to the pointers to each of the
    input and output buffers, wrapping them in `UnsafeMutablePointer`, as required
    by the vDSP functions. Then, it's simply a matter of applying each of our scaling
    operations using the equivalent vDSP functions, which we will walk through.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add our constant of `1` to the input and save the results in the
    output buffer, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Where the function `vDSP_vsadd` takes in a pointer to our vector (`inputPointer`)
    and adds `rescaleAddition` to each of its elements before storing it into the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we apply our multiplier to each of the elements of the output (which
    currently has each of its values set to the input with 1 added to it); the code
    for this is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Similar to `vDSP_vsadd`, `vDSP_vsmul` takes in the input (in this case, our
    output); the scalar we want to multiply each element by; the output; the stride
    for persisting the result; and finally, the number of elements we want to operate
    on.
  prefs: []
  type: TYPE_NORMAL
- en: If you rerun the application, you will see that we have managed to shed a few
    seconds off the total execution time—not bad considering this layer is run only once
    at the end of our network. Can we do better?
  prefs: []
  type: TYPE_NORMAL
- en: Taking advantage of the GPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may recall that when we introduced the `MLCustomLayer` protocol, there was
    an optional method, `encode(commandBuffer, inputs, outputs)`, reserved for performing
    the evaluation on the GPU if the hosting device supported it. This flexibility
    is one of the advantages Core ML has over other machine learning frameworks; it
    allows mixing layers, which run on the CPU and GPU, and allows them to work coherently
    together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the GPU, we will be using Apple''s `Metal` framework, a graphics framework
    equivalent to OpenGL and DirectX (and now Vulkan), for those who are familiar
    with 3D graphics. Unlike our previous solutions, which included all code in a
    single method, we need to write the code that performs the computation in an external
    file called a **Metal shader** file. Within this file we will define a kernel,
    which will be complied and stored on the GPU (when loaded), allowing it to fan
    out the data in parallel across the GPU. Let''s create this kernel now; create
    a new `metal` file called `rescale.metal` and add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: It is out of scope to discuss the details of `metal`, so instead, we'll just
    highlight some of the key differences and commonalities between this and the previous
    approaches. First, it's worth recognizing why GPUs have been a major catalyst
    for the resurgence of neural networks. The GPU architecture allows a kernel (seen
    earlier) to be spawned for each element in our array—mass parallelism!
  prefs: []
  type: TYPE_NORMAL
- en: Because GPU frameworks were traditionally built with graphics manipulation in
    mind, there are some nuances with how we operate on data and what we operate on.
    The most notable of them is that we have swapped `MLMultiArray` for `texture2d_array`
    (textures) and we access them through sampling, using `thread_position_in_grid`.
    Nonetheless, the actual computation should look familiar from the original Python
    code, `const float4 y = (1.0f + x) * 127.5f`. Once calculated, we cast it to float
    16 (half) and write it to the output texture.
  prefs: []
  type: TYPE_NORMAL
- en: Our next step is to configure our `RescaleOutputLambda` class to use `Metal`
    and the GPU, rather than the CPU. Return to the `RescaleOutputLambda.swift` file
    and make the following amendments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by importing the `Metal` framework by adding the following statement
    at the top of your file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a class variable of the type `MTLComputePipelineState` as a
    handler to the kernel we have just created, along with setting this up within
    the constructor of the `RescaleOutputLambda` class. Make the following amendments
    to the class and constructor, as shown in bold in the snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'If no errors are thrown, we will have reference to a complied version of our
    rescale kernel; the final step is making use of it. Within the `RescaleOutputLambda`
    class, add the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As mentioned before, we will omit the details here and only highlight some key
    differences and commonalities between this approach and the previous approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, the bulk of this method is responsible for passing data through to
    the compute kernel via the encoder and then dispatching it across the GPU. We
    first pass the input and output textures, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we''re setting the handler, which points to the rescale kernel we
    created in the preceding snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, dispatch the job to the GPU; in this instance, our compute kernel
    is invoked for every pixel in every channel of the input texture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: If you build and run again, you will hopefully get the same result but in less
    time. We have now seen two approaches to optimizing our network; I leave optimizing
    `ResCropBlockLambda` as an exercise for you. For now, let's shift our focus to
    talking about your model's weight before we wrap up this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing your model's weight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have spent considerable time discussing layers of a network; we have learned
    that layers are made up of weights, configured in such a way that they can transform
    an input into a desirable output. These weights come at a cost, though; each one
    (by default) is a 32-bit floating-point number with a typical model, especially
    in computer vision, having millions resulting in networks that are hundreds of
    megabytes in size. On top of that; it's plausible that your application will have
    multiple models (with this chapter being a good example, requiring a model for
    each style).
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, our model in this chapter has a moderate number of weights and
    weighs in at a mere 2.2 MB; but this is possibly an exception. So we''ll use this
    chapter as an excuse to explore some ways we can reduce our model''s weights.
    But before doing so, let''s quickly discuss why, even though it''s probably obvious.
    The three main reasons why you should be conscious of your model''s size include:'
  prefs: []
  type: TYPE_NORMAL
- en: Download time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application footprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demands on memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These could all hinder the user experience and are reasons for a user to either
    uninstall the application quickly or not even download it in the first place.
    So how do you reduce your model''s size to avoid deterring the user. There are
    three broad approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the number of layers your network uses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the number of units in each of those layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the size of the weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first two require that you have access to the original network and tools
    to re-architect and train the model; the last is the most accessible and it's
    the one we will discuss now.
  prefs: []
  type: TYPE_NORMAL
- en: 'In iOS 11.2, Apple allowed your networks to use half-precision floating-point
    numbers (16-bit). Now, with the release of iOS 12, Apple has taken this even further and
    introduced quantization, which allows us to use eight or less bits to encode our
    model''s weights. In the following figure, we can see how these options compare
    with one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/982b4300-9cc8-4a31-8249-7c6fe6d16583.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's discuss each in turn, starting with reducing our weights precision by
    converting it's floating points from 32-bits to 16-bits.
  prefs: []
  type: TYPE_NORMAL
- en: For both of these techniques (half-precision and quantization), we will be using
    the Core ML Tools Python package; so, begin by opening up your browser and heading
    over to [https://notebooks.azure.com](https://notebooks.azure.com). Once the page
    is loaded navigate to the folder `Chapter6/Notebooks/` and open the Jupyter Notebook `FastNeuralStyleTransfer_OptimizeCoreML.ipynb`.
    As we did before, we'll walk through each of the Notebook's cells here with the
    assumption that you will be executing each one as we cover it (if you are working
    along).
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the Core ML Tools package; execute the cell with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: For convenience, we have wrapped the `import` in an exception block, so that
    it automatically installs the package if it doesn't exist.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Core ML 2 was still in beta and only recently publicly
    announced. If you're using a version of Core ML Tools that is less than 2.0 then
    replace `!pip install coremltools` with `!pip install coremltools>=2.0b1` to install
    the latest beta version to have access to the necessary modules for this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will load our `mlmodel` file that we had previously saved, using the
    following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we perform the conversion by simply calling `coremltools.utils.convert_neural_network_weights_to_fp16`
    and passing in your model. If successful, this method will return an equivalent
    model (that you passed in), using half precision weights instead of 32-bits for
    storing its weights. Run the cell with the following code to do just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save it so we can later download it and import into our project;
    run the next cell with the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: And with that executed (essentially three lines of code), we have managed to
    have our models size, going from 2.2 MB to 1.1 MB - so, what's the catch?
  prefs: []
  type: TYPE_NORMAL
- en: As you might suspect, there is a trade-off here; reducing the precision of your
    models weights will affect its accuracy, but possibly not enough to be concerned
    with. The only way you will know is by comparing the optimized model with the
    original and re-evaluating it on your test data, ensuring that it satisfies your
    required accuracy/results. For this, Core ML Tools provides a collection of utilities
    that makes this fairly seamless, which you can learn at the official website [https://apple.github.io/coremltools/index.html](https://apple.github.io/coremltools/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Quantization is no more complicated (with respect to using it via Core ML Tools
    rather than concept); it's a clever technique, so let's quickly discuss how it
    achieves 8-bit compression before running through the code.
  prefs: []
  type: TYPE_NORMAL
- en: At a high-level, quantization is a technique that maps a continuous range of
    values to a discrete set; you can think of it as a process of clustering your
    values into a discrete set of groups and then creating a lookup table which maps
    your values to the closest group. The size is then dependent on the number of
    clusters used (index) rather than value, which allows you to encode your weights
    using anything from 8-bits to 2-bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this concept more concrete, the following figure illustrates the results
    of color quantization; where a 24-bit image is mapped to 16 discrete colors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9c1e2b-d93e-4767-ba5b-ae5894189d61.png)'
  prefs: []
  type: TYPE_IMG
- en: Instead of each pixel representing its color (using 24-bits/8-bits per channel),
    they now are indexes to the 16 color palette, that is, from 24-bits to 4-bits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving onto how we optimize our models using quantization with the Core
    ML Tools package, you maybe wondering how this palette (or discrete set of values)
    is derived. The short answer is that there are many ways, from linearly separating
    the values into groups, to using an unsupervised learning technique such as k-means,
    or even using a custom, domain-specific technique. Core ML Tools allows for all
    variations and the choice will be dependent on your data distribution and that
    results achieved during testing. Let''s jump into it; first, we will start by
    importing the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With this statement, we have imported the module and assigned it the alias
    `quant_utils`; the next cell, we optimize our model using a variation of sizes
    and methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is completed, let''s save each of our optimized models to the output
    directory before downloading them to our local disk to import them into Xcode
    (this may take some time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'I will omit the details of downloading and importing the model into your project
    as we have already gone through these steps previously in this chapter, but I
    do encourage that you to inspect the results from each model to get a feel for
    how each optimization affects the results - of course, these effects are highly
    dependent on the model, data, and domain. The following figure shows the results
    of each of the optimizations along with the model''s size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e59a2efc-e65e-40b9-8700-9ec242b9285c.png)'
  prefs: []
  type: TYPE_IMG
- en: Admittedly, it's difficult to see the differences due to the low-resolution
    of the image (and possibly because you're reading this in black and white) but
    generally, the quality appears minimal between the original and k-means 8-bit
    version.
  prefs: []
  type: TYPE_NORMAL
- en: With the release of Core ML 2, Apple offers another powerful feature to optimize
    you Core ML models; specifically around consolidating multiple models into a single
    package. This not only reduces the size of your application but also convenient
    for you, the developer, when interfacing with your model. For example, flexible
    shapes and sizes allows for variable input and output dimensions, that is, instead
    of a single fixed input and output dimension, you have the flexibility of having
    multiple variants or a variable range within a limit. You can learn more about
    this feature on their official website at [https://developer.apple.com/machine-learning](https://developer.apple.com/machine-learning);
    but for now, we will wrap up this chapter with a quick summary before moving on
    to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of style transfer; a technique that
    aims to separate the content of an image from its style. We discussed how it achieves
    this by leveraging a trained CNN, where we saw how deeper layers of a network
    extract features that distill information about the content of an image, while
    discarding any extraneous information.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we saw that shallower layers extracted the finer details, such as
    texture and color, which we could use to isolate the style of a given image by
    looking for the correlations between the feature maps (also known as **convolutional
    kernels** or **filters**) in each layer. These correlations are what we use to
    measure style and how we steer our network. Having isolated the content and style,
    we generated a new image by combining the two.
  prefs: []
  type: TYPE_NORMAL
- en: We then highlighted the limitations of performing style transfer in real time
    (with current technologies) and introduced a slight variation. Instead of optimizing
    the style and content each time, we could train a model to learn a particular
    style. This would allow us to generate a stylized image for a given image with
    a single pass through the network, as we have done with many other examples we
    have worked through in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Having introduced the concepts, we then walked through converting the Keras
    model to Core ML and used this as an opportunity to implement custom layers, a
    Swift-centric way of implementing layers that have no direct mapping between the
    machine learning framework and Core ML. Having implemented custom layers, we then
    spent some time looking at how we can optimize them using the `Accelerate` (SIMD)
    and `Metal` frameworks (GPU).
  prefs: []
  type: TYPE_NORMAL
- en: The theme of optimization continued into the next section, where we discussed
    some of the tools available for reducing a model's size; there, we looked at two
    approaches and how we could make use of them using the Core ML Tools package along
    with a cautionary warning of the trade-off between size and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we look at how we can apply what we have learned to recognizing
    user sketches.
  prefs: []
  type: TYPE_NORMAL
