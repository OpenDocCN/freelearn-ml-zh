- en: Creating Art with Style Transfer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用风格迁移创作艺术
- en: In this chapter, we will explore what was one of the most popular mainstream
    applications of deep learning in 2017—style transfer. We begin by introducing
    the concepts of style transfer and then its faster alternative, appropriately
    named fast neural style transfer. Similar to other chapters, we will provide the
    intuition behind the models (rather than granular details) and, in doing so, you
    will gain a deeper understanding and appreciation for the potential of deep learning
    algorithms. Unlike previous chapters, this chapter will focus more on the steps
    involved in getting the model working on iOS rather than building up the application,
    in order to keep it concise.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨2017年最流行的主流深度学习应用之一——风格迁移。我们首先介绍风格迁移的概念，然后是其更快的替代方案，即快速神经风格迁移。与其他章节类似，我们将提供模型背后的直觉（而不是细节），通过这样做，您将获得对深度学习算法潜力的更深入理解和欣赏。与之前的章节不同，本章将更多地关注使模型在iOS上工作所需的步骤，而不是构建应用程序，以保持内容的简洁性。
- en: 'By the end of this chapter you will have achieved the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将实现以下目标：
- en: Gained an intuitive understanding of how style transfer works
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对风格迁移的工作原理有了直观的理解
- en: Gained hands-on experience of working with the Core ML Tools Python package
    and custom layers to get Keras models working in Core ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用Core ML Tools Python包和自定义层，获得了在Core ML中使Keras模型工作的实际经验
- en: Let's get started by introducing style transfer and building our understanding
    of how it works.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从介绍风格迁移并建立对其工作原理的理解开始。
- en: Transferring style from one image to another
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将风格从一个图像转移到另一个图像
- en: 'Imagine being able to have one of the greatest painters in history, such as Vincent
    van Gogh or Pablo Picasso, recreate a photo of your liking using their own unique
    style. In a nutshell, this is what style transfer allows us to do. Quite simply,
    it''s the process of generating a photo using the style of one with the content
    of another, as shown here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，能够让历史上最伟大的画家之一，如文森特·梵高或巴勃罗·毕加索，用他们独特的风格重新创作一张您喜欢的照片。简而言之，这就是风格迁移允许我们做的事情。简单来说，它是一个使用另一个内容生成照片的过程，其风格来自另一个，如下所示：
- en: '![](img/1c79f7dd-e436-4a7f-8dbe-0a8917fdd108.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c79f7dd-e436-4a7f-8dbe-0a8917fdd108.png)'
- en: In this section, we will describe, albeit at a high level, how this works and
    then move on to an alternative that allows us to perform a similar process in
    significantly less time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述（尽管是高层次地）它是如何工作的，然后转向一个允许我们在显著更短的时间内执行类似过程的替代方案。
- en: I encourage you to read the original paper, *A Neural Algorithm of Artistic
    Style*, by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, for a more
    comprehensive overview. This paper is available at [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励您阅读Leon A. Gatys、Alexander S. Ecker和Matthias Bethge撰写的原始论文，《艺术风格神经算法》，以获得更全面的概述。这篇论文可在[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)找到。
- en: At this stage, we have learned that neural networks learn by iteratively reducing
    a loss, calculated using some specified cost function that is to indicate how
    well the neural network did with respect to the expected output. The difference
    between the **predicted output** and **expected output** is then used to adjust
    the model's weights, through a process known as **backpropagation**, such to minimize
    this loss.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到神经网络通过迭代减少损失来学习，损失是通过使用某些指定的成本函数计算的，该函数用于指示神经网络在预期输出方面的表现如何。然后，**预测输出**与**预期输出**之间的差异被用来通过称为**反向传播**的过程调整模型的权重，以最小化这种损失。
- en: The preceding description (intentionally) skips the details of this process
    as our goal here is to provide an intuitive understanding, rather than the granular
    details. I recommend reading Andrew Trask's *Grokking Deep Learning* for a gentle
    introduction to the underlying details of neural networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述（有意）省略了此过程的细节，因为我们的目标是提供直观的理解，而不是细节。我建议阅读Andrew Trask的《Grokking Deep Learning》以获得对神经网络底层细节的温和介绍。
- en: Unlike the classification models we have worked with thus far, where the output
    is a probability distribution across some set of labels, we are instead interested
    in the model's generative abilities. That is, instead of adjusting the model's
    weights, we want to adjust the generated image's pixel values so as to reduce
    some defined cost function.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止所使用的分类模型不同，其中输出是跨越某些标签集的概率分布，我们更感兴趣的是模型生成能力。也就是说，我们不是调整模型的权重，而是想调整生成图像的像素值，以减少一些定义的成本函数。
- en: 'So if we were to define a cost function that could measure the loss between
    the generated image and content image, and another to measure the loss between
    the generated image and style image, we could then simply combine them. Thus we
    obtain the overall loss and use this to adjust the generated image pixels values,
    to create something that has the targets content in the style of our targets style
    as illustrated in the following image:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们定义一个成本函数来衡量生成图像和内容图像之间的损失，以及另一个来衡量生成图像和风格图像之间的损失，我们就可以简单地合并它们。这样我们就得到了整体损失，并使用这个损失来调整生成图像的像素值，以创建一个具有目标内容的目标风格的图像，如图所示：
- en: '![](img/17c518be-eba6-4893-9bad-489861742bae.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17c518be-eba6-4893-9bad-489861742bae.png)'
- en: At this point, we have a general idea of the required process; what is left
    is building some intuition behind these cost functions. That is, how do you determine
    how well your generated image is, with respect to some content of the content
    image and with respect to a style of the style image? For this, we will backtrack
    a little and review what other layers of a CNN learn by inspecting each of their
    activations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经对所需的过程有一个大致的了解；剩下的是建立对这些成本函数背后的直觉。也就是说，你是如何确定你生成的图像在内容图像的某些内容和风格图像的风格方面有多好？为此，我们将稍微回顾一下，通过检查每个激活来了解CNN的其他层是如何学习的。
- en: The details and images demonstrating what **convolutional neural networks**
    (**CNNs**) learn have been taken from the paper *Visualizing and Understanding
    Convolutional Networks*, by Matthew D. Zeiler and Rob Fergus, which is available
    at [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于**卷积神经网络**（**CNNs**）学习细节和图像的详细信息，来自Matthew D. Zeiler和Rob Fergus的论文《可视化与理解卷积网络》，可在[https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)找到。
- en: 'A typical architecture of a CNN consists of a series of convolutional and pooling
    layers, which is then fed into a fully connected network (for case of classification),
    as illustrated in this image:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的典型架构由一系列卷积和池化层组成，然后输入到一个全连接网络（用于分类情况），如图所示：
- en: '![](img/68df6574-ed16-4453-9739-0762baa74183.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/68df6574-ed16-4453-9739-0762baa74183.png)'
- en: This flat representation misses an important property of a CNN, which is how,
    after each subsequent pair of convolution and pooling layers, the input's width
    and height reduce in size. The consequence of this is that the receptive field
    increases depth into the network; that is, deeper layers have a larger receptive
    field and thus capture higher level features than shallower layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种平面表示忽略了CNN的一个重要特性，即在每一对后续的卷积和池化层之后，输入的宽度和高度都会减小。这种结果就是感受野向网络深处增加；也就是说，深层有更大的感受野，因此比浅层捕获更高层次的特征。
- en: 'To better illustrate what each layer learns, we will reference the paper *Visualizing
    and Understanding Convolutional Networks*, by Matthew D. Zeiler and Rob Fergus.
    In their paper (previously referenced), they pass through images from their training
    set to identify the image patches that maximize each layer''s activations; by
    visualizing these patches, we get a sense of what each neuron (hidden unit) at
    each of the layers learns. Here is an screenshot showing some of these patches
    across a CNN:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地说明每一层学习的内容，我们将参考Matthew D. Zeiler和Rob Fergus的论文《可视化与理解卷积网络》。在他们之前的论文中，他们通过将训练集中的图像传递过去，以识别最大化每一层激活的图像块；通过可视化这些块，我们可以了解每一层的每个神经元（隐藏单元）学习到了什么。以下是CNN中一些这些块的截图：
- en: '![](img/b02ef6da-b1d9-4d10-8df6-40fdae4fd5b8.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b02ef6da-b1d9-4d10-8df6-40fdae4fd5b8.png)'
- en: Source: Visualizing and Understanding Convolutional Networks; Matthew D Zeiler,
    Rob Fergus
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：《可视化与理解卷积网络》；Matthew D Zeiler, Rob Fergus
- en: What you can see in the preceding figure are nine image patches that maximize
    an individual hidden unit at each of the layers of this particular network. What
    has been omitted from the preceding figure is the variance in size; that is, the
    deeper you go, the larger the image patch will be.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到九个图像块，这些图像块在每个网络层的每个隐藏单元中最大化。前面图中省略的是尺寸上的变化；也就是说，你走得越深，图像块就会越大。
- en: What is hopefully obvious from the preceding image is that the shallower layers
    extract simple features. For example, we can see that a single hidden unit at
    **Layer 1** is activated by a diagonal edge and a single hidden unit at **Layer
    2** is activated with a vertically striped patch. While the deeper layers extract
    higher-level features, or more complex features, again, in the preceding figure,
    we can see that a single hidden unit at **Layer 4** is activated by patches of
    dog faces.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图像中可以明显看出，较浅的层提取简单的特征。例如，我们可以看到**层1**的单个隐藏单元被对角线边缘激活，而**层2**的单个隐藏单元则被垂直条纹块激活。而较深的层提取高级特征或更复杂的特征，再次，在前面图中，我们可以看到**层4**的单个隐藏单元被狗脸的块激活。
- en: 'We return to our task of defining a cost function for content and style, starting
    with the cost function for content. Given a content image and a generated image,
    we want to measure how close we are so as to minimize this difference, so that
    we retain the content. We can achieve this by selecting one of the deeper layers
    from our CNN, which we saw before have a large receptive field, and capture complex
    features. We pass through both the content images and the generated image and
    measure the distance between outputted activations (on this layer). This will
    hopefully seem logical given that the deeper layers learn complex features, such
    as a dog''s face or a car, but decouple them from lower-level features such as
    edges, color, and textures. The following figure depicts this process:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回到定义内容与风格成本函数的任务，首先从内容成本函数开始。给定一个内容图像和一个生成图像，我们想要测量我们有多接近，以便最小化这种差异，从而保留内容。我们可以通过选择我们之前看到的具有大感受野的CNN中的一个较深层来实现这一点，它能够捕捉复杂的特征。我们通过内容图像和生成图像传递，并测量输出激活（在这一层）之间的距离。这可能会在更深层的网络学习复杂特征，如狗的脸或汽车的情况下显得合乎逻辑，但将它们与较低级别的特征（如边缘、颜色和纹理）解耦。以下图展示了这一过程：
- en: '![](img/6116a977-45e8-465f-9eb2-281c004ab46b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6116a977-45e8-465f-9eb2-281c004ab46b.png)'
- en: This takes care of our cost function for the content which can be easily tested
    by running a network that implements this. If implemented correctly, it should
    result in a generated image that looks similar to that of the input (content image).
    Let's now turn our attention to measuring style.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这就解决了我们的内容成本函数问题，可以通过运行实现此功能的网络来轻松测试。如果实现正确，它应该会产生一个看起来与输入（内容图像）相似的生成图像。现在，让我们将注意力转向测量风格。
- en: We saw in the preceding figure that shallower layers of a network learn simple
    features such as edges, textures, and color combinations. This gives us a clue
    as to which layers would be useful when trying to measure style, but we still
    need a way of extracting and measuring style. However, before we start, what exactly
    is style?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们看到了网络的较浅层学习简单的特征，如边缘、纹理和颜色组合。这为我们提供了关于在尝试测量风格时哪些层可能有用的线索，但我们仍然需要一种提取和测量风格的方法。然而，在我们开始之前，究竟什么是风格？
- en: 'A quick search on [http://www.dictionary.com/](http://www.dictionary.com/)
    reveals style being defined as *a distinctive appearance, typically determined
    by the principles according to which something is designed*. Let''s take Katsushika
    Hokusai''s *The Great Wave off Kanagawa* as an example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://www.dictionary.com/](http://www.dictionary.com/)上进行快速搜索，可以发现“风格”被定义为“*一种独特的外观，通常由设计时所依据的原则决定*”。让我们以葛饰北斋的*《神奈川冲浪里》*为例：
- en: '![](img/ed306f8b-c601-42bb-b2a2-f5aa88ea612c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed306f8b-c601-42bb-b2a2-f5aa88ea612c.png)'
- en: '*The Great Wave off Kanagawa* is an output of a process known as **woodblock
    printing**; this is where an artist''s sketch is broken down into layers (carved
    wooden blocks), with each layer (usually one for each color) used to reproduce
    the art piece. It''s similar to a manual printing press; this process produces
    a distinctive flat and simplistic style. Another dominate style (and possibly
    side-effect) that can be seen in the preceding image is that a limited range of
    colors is being used; for example, the water consists of no more than four colors.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*神奈川冲浪里*是称为**木版印刷**的过程的输出；这是艺术家草图被分解成层（雕刻的木块），每个层（通常每个颜色一个）用于复制艺术品。它类似于手动印刷机；这个过程产生了一种独特的平坦和简化的风格。在前面的图像中还可以看到另一种主导风格（以及可能的副作用），那就是使用了有限的颜色范围；例如，水由不超过四种颜色组成。'
- en: The way we can capture style is as defined in the paper *A Neural Algorithm
    of Artistic Style*, by L. Gatys, A. Ecker, and M. Bethge. This way is to use a
    style matrix (also known as **gram matrix**) to find the correlation between the
    activations across different channels for a given layer. It is these correlations
    that define the style and something we can then use to measure the difference
    between our style image and generated image to influence the style of the generated
    image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以捕获风格的方式，如L. Gatys、A. Ecker和M. Bethge在论文《艺术风格神经算法》中定义的那样。这种方式是使用风格矩阵（也称为**gram矩阵**）来找到给定层中不同通道之间激活的相关性。正是这些相关性定义了风格，并且我们可以用它来衡量我们的风格图像和生成的图像之间的差异，从而影响生成的图像的风格。
- en: 'To make this more concrete, borrowing from an example used by Andrew Ng in
    his Coursera course on deep learning, let''s take **Layer 2** from the earlier
    example. What the style matrix calculates is the correlation across all channels
    for a given layer. If we use the following illustration, showing nine activations
    from two channels, we can see that a correlation exists between vertical textures
    from the first channel with orange patches from the second channel. That is, when
    we see a vertical texture in the first channel, we would expect the image patches
    that maximize the second channel''s activations to have an orange tint:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加具体，借鉴Andrew Ng在他的Coursera深度学习课程中使用的例子，让我们从之前的例子中取**层2**。风格矩阵计算的是给定层中所有通道之间的相关性。如果我们使用以下插图，展示两个通道的九个激活，我们可以看到第一通道的垂直纹理与第二通道的橙色块之间存在相关性。也就是说，当我们看到第一通道中的垂直纹理时，我们预计最大化第二通道激活的图像块将带有橙色：
- en: '![](img/2f0c959f-2afa-49f8-a2eb-e100a3fa9196.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2f0c959f-2afa-49f8-a2eb-e100a3fa9196.png)'
- en: 'This style matrix is calculated for both the style image and generated image,
    with our optimization forcing our generated image to adopt these correlations.
    With both style matrices calculated, we can then calculate the loss by simply
    finding the sum of the square difference between the two matrices. The following
    figure illustrates this process, as we have previously done when describing the
    content loss function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个风格矩阵为风格图像和生成的图像都进行了计算，我们的优化迫使生成的图像采用这些相关性。计算完这两个风格矩阵后，我们可以通过简单地找到两个矩阵之间平方差的和来计算损失。以下图示说明了这个过程，就像我们之前在描述内容损失函数时做的那样：
- en: '![](img/d0245269-6f46-4dbd-888c-e71fa9a0f5cc.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d0245269-6f46-4dbd-888c-e71fa9a0f5cc.png)'
- en: With that, we have now concluded our introduction to style transfer, and hopefully
    given you some intuition of how we can use the network's perceptual understanding
    of images to extract content and style. This approach works well, but there is
    one drawback that we will address in the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们现在已经完成了对风格迁移的介绍，并希望给你一些关于如何使用网络对图像的感知理解来提取内容和风格的直观感受。这种方法效果很好，但有一个缺点，我们将在下一节中解决。
- en: A faster way to transfer style
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移风格的更快方法
- en: 'As you may have inferred from the title of this section, the big drawback of
    the approach introduced in the previous section is that the process requires iterative
    optimization, as summarized in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所从本节标题中推断出的，前节中介绍的方法的一个主要缺点是，该过程需要迭代优化，以下图示总结了这一点：
- en: '![](img/80a70ed5-d413-45d0-bd9c-6fb5e73bdff4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/80a70ed5-d413-45d0-bd9c-6fb5e73bdff4.png)'
- en: This optimization is akin to training, in terms of performing many iterations
    to minimize the loss. Therefore, it typically takes a considerable amount of time,
    even when using a modest computer. As implied at the start of this book, we ideally
    want to restrict ourselves to performing inference on the edge as it requires
    significantly less compute power and can be run in near-real time, allowing us
    to adopt it for interactive applications. Luckily for us, in their paper *Perceptual
    Losses for Real-Time Style Transfer and Super-Resolution*, J. Johnson, A. Alahi,
    and L. Fei-Fei describe a technique that decouples training (optimization) and
    inference for style transfer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优化在执行许多迭代以最小化损失方面类似于训练。因此，即使使用一台普通的计算机，这也通常需要相当多的时间。正如本书开头所暗示的，我们理想情况下希望将自己限制在边缘进行推理，因为它需要的计算能力显著较低，并且可以在接近实时的情况下运行，使我们能够将其用于交互式应用程序。幸运的是，在他们的论文《用于实时风格迁移和超分辨率的感知损失》中，J.
    Johnson、A. Alahi和L. Fei-Fei描述了一种将风格迁移的训练（优化）和推理解耦的技术。
- en: Previously, we described a network that took as its input a generated image,
    a style image, and a content image. The network minimized loss by iteratively
    adjusting the generated image using the loss functions for content and style;
    this provided the flexibility of allowing us to plug in any style and content
    image, but came at the cost of being computationally expensive, that is, slow.
    What if we sacrifice this flexibility for performance by restraining ourselves
    to a single style and, instead of performing the optimization to generate the
    image, train a CNN? The CNN would learn the style and, once trained, could generate
    a stylized image given a content image with a single pass through the network
    (inference). This is, in essence, what the paper *Perceptual Losses for Real-Time
    Style Transfer and Super-Resolution*, describes, and it is the network we will
    use in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们描述了一个网络，它以生成图像、风格图像和内容图像作为输入。该网络通过迭代调整生成图像，使用内容和风格的损失函数来最小化损失；这提供了灵活性，允许我们插入任何风格和内容图像，但代价是计算成本高，即速度慢。如果我们牺牲这种灵活性以换取性能，将自己限制在单一风格，并且不执行生成图像的优化，而是训练一个CNN会怎样？CNN将学习风格，一旦训练完成，就可以通过网络的单次遍历（推理）来生成风格化的图像。这正是论文《用于实时风格迁移和超分辨率的感知损失》所描述的，也是我们将在本章中使用的网络。
- en: 'To better elucidate the difference between the previous approach and this approach,
    take a moment to review and compare the preceding figure with the following one:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地阐明先前方法和这种方法之间的区别，请花一点时间回顾并比较前面的图与以下图：
- en: '![](img/6c05bedd-135f-46f8-b726-125d44a0cb3e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6c05bedd-135f-46f8-b726-125d44a0cb3e.png)'
- en: Unlike the previous approach, where we optimized for a given set of content,
    style, and generated images and adjusted the generated image to minimize loss,
    we now feed a CNN with a set of content images and have the network generate the
    image. We then perform the same loss functions as described earlier for a single
    style. But, instead of adjusting the generated image, we adjust the weights of
    the networks using the gradients from the loss function. And we repeat until we
    have sufficiently minimized the mean loss across all of our content images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前的方法不同，在先前的方法中，我们针对一组给定内容、风格和生成图像进行优化，并调整生成图像以最小化损失，我们现在向CNN提供一组内容图像，并让网络生成图像。然后，我们执行与之前描述的相同损失函数，针对单一风格。但是，我们不是调整生成图像，而是使用损失函数的梯度来调整网络的权重。我们重复这个过程，直到我们足够地最小化了所有内容图像的平均损失。
- en: 'Now, with our model trained, we can have our network stylize an image with
    a single pass, as shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们的模型训练完成，我们可以让我们的网络通过单次遍历来风格化图像，如图所示：
- en: '![](img/b348bf5b-bde1-4e9f-8950-9653fa333ae9.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b348bf5b-bde1-4e9f-8950-9653fa333ae9.png)'
- en: Over the last two sections we have described, at a high-level, how these networks
    work. Now, it's time to build an application that takes advantage of all this.
    In the next section, we will quickly walk through converting the trained Keras
    model to Core ML before moving on to the main topic of this chapter—implementing
    custom layers for Core ML.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两节中，我们以高层次概述了这些网络的工作原理。现在，是时候构建一个利用所有这些功能的应用程序了。在下一节中，我们将快速浏览如何将训练好的Keras模型转换为Core
    ML，然后再继续本章的主要主题——为Core ML实现自定义层。
- en: Converting a Keras model to Core ML
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Keras模型转换为Core ML
- en: Similar to what we did in the previous chapter, in this section we will be converting
    a trained Keras model into a Core ML model using the **Core ML Tools** package.
    To avoid any complications of setting up the environment on your local or remote
    machine, we will leverage the free Jupyter cloud service provided by Microsoft.
    Head over to [https://notebooks.azure.com](https://notebooks.azure.com) and log
    in (or register if you haven't already).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在上一章中所做的一样，在本节中，我们将使用 **Core ML Tools** 包将训练好的 Keras 模型转换为 Core ML 模型。为了避免在您的本地或远程机器上设置环境的任何复杂性，我们将利用微软提供的免费
    Jupyter 云服务。访问 [https://notebooks.azure.com](https://notebooks.azure.com) 并登录（如果您还没有，请先注册）。
- en: 'Once logged in, click on the Libraries menu link from the navigation bar, which
    will take you to a page containing a list of all of your libraries, similar to
    what is shown in the following screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，点击导航栏中的“库”菜单链接，这将带您到一个包含您所有库列表的页面，类似于以下截图所示：
- en: '![](img/cdb2fe87-6e5c-4c63-a213-e61ec24d4608.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cdb2fe87-6e5c-4c63-a213-e61ec24d4608.png)'
- en: 'Next, click on the + New Library link to bring up the Create New Library dialog:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击“新建库”链接以打开创建新库对话框：
- en: '![](img/cb802108-7164-4387-8abb-c608e8383555.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cb802108-7164-4387-8abb-c608e8383555.png)'
- en: Then, click on the From GitHub tab and enter `https://github.com/packtpublishing/machine-learning-with-core-ml` in
    the GitHub repository field. After that, give your library a meaningful name and
    click on the Import button to begin the process of cloning the repository and
    creating the library.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击“从 GitHub”标签，并在 GitHub 仓库字段中输入 `https://github.com/packtpublishing/machine-learning-with-core-ml`。之后，给您的库起一个有意义的名字，并点击导入按钮以开始克隆仓库并创建库的过程。
- en: 'Once the library has been created, you will be redirected to the root. From
    there, click on the `Chapter6/Notebooks` folder to open up the relevant folder
    for this chapter, and finally click on the Notebook `FastNeuralStyleTransfer_Keras2CoreML.ipynb`.
    Here is a screenshot of what you should see after clicking on the `Chapter6` folder:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 创建库后，您将被重定向到根目录。从那里，点击 `Chapter6/Notebooks` 文件夹以打开本章的相关文件夹，最后点击笔记本 `FastNeuralStyleTransfer_Keras2CoreML.ipynb`。以下是点击
    `Chapter6` 文件夹后您应该看到的截图：
- en: '![](img/3705777f-bd95-4b87-b458-3baf5a7249d0.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3705777f-bd95-4b87-b458-3baf5a7249d0.png)'
- en: It's beyond the scope of this book to walk you through the details of the Notebook,
    including the details of the network and training. For the curious reader, I have
    included the original Notebooks for each of the models used throughout this book
    in the accompanying `chapters` folder within the `training` folder.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论笔记本的细节，包括网络和训练的细节超出了本书的范围。对于好奇的读者，我在 `training` 文件夹中的 `chapters` 文件夹内包含了本书中使用的每个模型的原始笔记本。
- en: 'With our Notebook now loaded, it''s time to walk through each of the cells
    to create our Core ML model; all of the required code exists and all that remains
    is executing each of the cells sequentially. To execute a cell, you can either
    use the shortcut keys *Shift* + *Enter* or click on the Run button in the toolbar
    (which will run the currently selected cell), as shown in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的笔记本现在已加载，是时候逐个遍历每个单元格来创建我们的 Core ML 模型了；所有必要的代码都已存在，剩下的只是依次执行每个单元格。要执行一个单元格，你可以使用快捷键
    *Shift* + *Enter* 或者点击工具栏中的运行按钮（这将运行当前选中的单元格），如下面的截图所示：
- en: '![](img/228ba8b2-345c-4aca-a980-c5b6eac51b3d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/228ba8b2-345c-4aca-a980-c5b6eac51b3d.png)'
- en: 'I will provide a brief explanation of what each cell does. Ensure that you
    execute each cell as we walk through them so that we all end up with the converted
    model, which we can then download and import into our iOS project:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我将简要解释每个单元格的作用。确保我们在遍历它们时执行每个单元格，这样我们最终都能得到转换后的模型，然后我们可以将其下载并导入到我们的 iOS 项目中：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We first import a module that includes the a function that will create and
    return the Keras model we want to convert:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入一个包含创建并返回我们想要转换的 Keras 模型的函数的模块：
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We then use our `helpers` method `build_model` to create the model, passing
    in the style image that the model was trained on. Remember that we are using a
    feedforward network that has been trained on a single style; while the network
    can be reused for different styles, the weights are unique per style.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着使用我们的 `helpers` 方法 `build_model` 来创建模型，传入模型训练所用的风格图像。请记住，我们正在使用一个在单个风格上训练的前馈网络；虽然该网络可以用于不同的风格，但每个风格的权重是唯一的。
- en: Calling `build_model` will take some time to return; this is because the model
    uses a trained model (VGG16) that is downloaded before returning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `build_model` 将需要一些时间来返回；这是因为模型使用了一个在返回之前下载的已训练模型（VGG16）。
- en: 'Talking of weights (previously trained model), let''s now load them by running
    the following cell:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 说到权重（之前训练的模型），现在让我们通过运行以下单元格来加载它们：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similar to the aforementioned code, we are passing in the weights for the model
    that was trained on Vincent van Gogh's *Starry Night* painting for its style.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述代码类似，我们传递了在文森特·梵高的《星夜》画作上训练的模型的权重，用于其风格。
- en: 'Next, let''s inspect the architecture of the model by calling the `summary`
    method on the model itself:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们通过在模型本身上调用 `summary` 方法来检查模型的架构：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calling this will return, as the name suggests, a summary of our model. Here
    is an extract of the summary produced:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此方法将返回，正如其名称所暗示的，我们模型的摘要。以下是生成的摘要摘录：
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As previously mentioned, it''s out of scope to go into the details of Python,
    Keras, or the specifics of this model. Instead I present an extract here to highlight
    the custom layers embedded in the model (the bold lines). In the context of Core
    ML Tools, custom layers are layers that have not been defined and, therefore,
    are not handled during the conversion process, so it is our responsibility to
    handle these. You can think of the conversion process as a process of mapping
    layers from a machine learning framework, such as Keras, to Core ML. If no mapping
    exists, then it is left up to us to fill in the details, as illustrated in the
    following figure:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深入探讨 Python、Keras 或该模型的细节超出了范围。相反，我在这里提供了一段摘录，以突出模型中嵌入的自定义层（粗体行）。在 Core
    ML Tools 的上下文中，自定义层是指尚未定义的层，因此它们在转换过程中不会被处理，因此处理这些层的责任在我们。您可以将转换过程视为将机器学习框架（如
    Keras）中的层映射到 Core ML 的过程。如果没有映射存在，那么就由我们来填写细节，如下面的图示所示：
- en: '![](img/1022c0b7-25f4-4e6f-877f-5e7728ac0d4e.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1022c0b7-25f4-4e6f-877f-5e7728ac0d4e.png)'
- en: The two custom layers shown previously are both Lambda layers; a Lambda layer
    is a special Keras class that conveniently allows writing quick-and-dirty layers
    using just a function or a Lambda expression (similar to a closure in Swift).
    Lambda is useful for layers that don’t have a state and are commonly seen in Keras
    models for doing basic computations. Here, we see two being used, `res_crop` and
    `rescale_output`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前显示的两个自定义层都是 Lambda 层；Lambda 层是一个特殊的 Keras 类，它方便地允许使用函数或 Lambda 表达式（类似于 Swift
    中的闭包）来编写快速且简单的层。Lambda 层对于没有状态的层非常有用，在 Keras 模型中常见，用于执行基本计算。这里我们看到两个被使用，`res_crop`
    和 `rescale_output`。
- en: '`res_crop` is part of the ResNet block that crops (as implied by the name)
    the output; the function is simple enough, with its definition shown in the following
    code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`res_crop` 是 ResNet 模块的一部分，它裁剪输出（正如其名称所暗示的）；该函数足够简单，其定义如下面的代码所示：'
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: I refer you to the paper *Deep Residual Learning for Image Recognition*, by
    K. He, X. Zhang, S. Ren, and J. Sun to learn more about ResNet and residual blocks,
    available here at [https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您阅读 K. He、X. Zhang、S. Ren 和 J. Sun 的论文《用于图像识别的深度残差学习》，以了解更多关于 ResNet 和残差块的信息，该论文可在以下链接找到：[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)。
- en: 'Essentially, all that this is doing is cropping the outputs with a padding
    of 2 for the width and height axis. We can further interrogate this by inspecting
    the input and output shapes of this layer, by running the following cell:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这所做的一切就是使用宽度高度轴上的填充 2 来裁剪输出。我们可以通过运行以下单元格来进一步调查这个层的输入和输出形状：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This cell prints the input and output shape of the layer `res_crop_3_layer`;
    the layer receives a tensor of shape `(None, 88, 88, 64)` and outputs a tensor
    of shape `(None, 84, 84, 64)`. Here the tuple is broken down into: (batch size,
    height, width, channels). The batch size is set to `None`, indicating that it
    is dynamically set during training.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此单元格打印了层 `res_crop_3_layer` 的输入和输出形状；该层接收形状为 `(None, 88, 88, 64)` 的张量，并输出形状为
    `(None, 84, 84, 64)` 的张量。这里元组被分解为：（批量大小，高度，宽度，通道）。批量大小设置为 `None`，表示它在训练过程中动态设置。
- en: 'Our next Lambda layer is `rescale_output`; this is used at the end of the network
    to rescale the outputs from the Convolution 2D layer, which passes its data through
    a tanh activation. This forces our data to be constrained between -1.0 and 1.0,
    where as we want it in a range of 0 and 255 so that we can convert it into an
    image. As we did before, let''s look at its definition to get a better idea of
    what this layer does, as shown in the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个 Lambda 层是 `rescale_output`；这是在网络末尾用于将 Convolution 2D 层的输出重新缩放，该层通过 tanh
    激活传递数据。这迫使我们的数据被限制在 -1.0 和 1.0 之间，而我们需要它在 0 到 255 的范围内，以便将其转换为图像。像之前一样，让我们看看它的定义，以更好地了解这个层的作用，如下面的代码所示：
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This method performs an element-wise operation that maps the values -1.0 and
    1.0 to 0 and 255\. Similar to the preceding method (`res_crop`), we can inspect
    the input and output shapes of this layer by running the following cell:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法执行一个元素级操作，将值 -1.0 和 1.0 映射到 0 和 255。类似于前面的方法 (`res_crop`)，我们可以通过运行以下单元格来检查这个层的输入和输出形状：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once run, this cell prints the layer's input shape of `(None, 320, 320, 3)`
    and output shape of `(None, 320, 320, 3)`. This tells us that this layer doesn't
    change the shape of the tensor, as well as shows us the output dimensions of our
    image as 320 x 320 with three channels (RGB).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行，这个单元格将打印出层的输入形状为 `(None, 320, 320, 3)` 和输出形状为 `(None, 320, 320, 3)`。这告诉我们这个层不会改变张量的形状，同时也显示了我们的图像输出维度为
    320 x 320，具有三个通道（RGB）。
- en: 'We have now reviewed the custom layers and seen what they actually do; the
    next step is to perform the actual conversion. Run the following cell to ensure
    that the environment has the Core ML Tools modules installed:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经审查了自定义层，并看到了它们实际的功能；下一步是执行实际的转换。运行以下单元格以确保环境中已安装 Core ML Tools 模块：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once installed, we can load the required modules by running the following cell:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装，我们可以通过运行以下单元格来加载所需的模块：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In this instance, I have prewarned you that our model contains custom layers;
    in some (if not most) instances, you may discover this only when the conversion
    process fails. Let''s see exactly what this looks like by running the following
    cell and examining its output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我提前警告你我们的模型包含自定义层；在某些（如果不是大多数）情况下，你可能会在转换过程失败时发现这一点。让我们通过运行以下单元格并检查其输出来看一下具体是什么样子：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding snippet, we are passing our model to the method `coremltools.converters.keras.convert`,
    which is responsible for converting our Keras model to Core ML. Along with the
    model, we pass in the input and output names for our model, as well as setting `image_input_names` to
    inform the method that we want the input `image` to be treated as an image rather
    than a multidimensional array.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们将我们的模型传递给 `coremltools.converters.keras.convert` 方法，该方法负责将我们的 Keras
    模型转换为 Core ML。除了模型外，我们还传递了模型的输入和输出名称，以及设置 `image_input_names` 以告知该方法我们希望输入 `image`
    被视为图像而不是多维数组。
- en: 'As expected, after running this cell, you will receive an error. If you scroll
    to the bottom of the output, you will see the line `ValueError: Keras layer ''<class
    ''keras.layers.core.Lambda''>'' not supported`. At this stage, you will need to
    review the architecture of your model to identify the layer that caused the error
    and proceed with what you are about to do.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '如预期，运行这个单元格后，你会收到一个错误。如果你滚动到输出的底部，你会看到行 `ValueError: Keras layer ''<class ''keras.layers.core.Lambda''>''
    not supported`。在这个阶段，你需要审查你模型的架构以确定导致错误的层，并继续你即将要做的事情。'
- en: By enabling the parameter `add_custom_layers` in the conversion call, we prevent
    the method from failing when the converter encounters a layer it doesn't recognize.
    A placeholder layer named custom will be inserted as part of the conversion process.
    In addition to recognizing custom layers, we can pass in a `delegate` function
    to the parameter `custom_conversion_functions`, which allows us to add metadata
    to the model's specification stating how the custom layer will be handled.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在转换调用中启用参数 `add_custom_layers`，我们防止转换器在遇到它不认识的层时失败。作为转换过程的一部分，将插入一个名为 custom
    的占位符层。除了识别自定义层外，我们还可以将 `delegate` 函数传递给参数 `custom_conversion_functions`，这允许我们向模型的规范中添加元数据，说明如何处理自定义层。
- en: 'Let''s create this `delegate` method now; run the cell with the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建这个 `delegate` 方法；运行以下代码的单元格：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This `delegate` is passed each custom layer the converter comes across. Because
    we are dealing with two different layers, we first check which layer we are dealing
    with and then proceed to create and return an instance of `CustomLayerParams`.
    This class allows us to add some metadata used when creating the model's specification
    for the Core ML conversion. Here we are setting its `className`, which is the
    name of the Swift (or Objective-C) class in our iOS project that implements this
    layer, and `description`, which is the text shown in Xcode 's ML model viewer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`delegate`会传递转换器遇到的每个自定义层。因为我们处理的是两个不同的层，所以我们首先检查我们正在处理哪个层，然后继续创建并返回一个`CustomLayerParams`实例。这个类允许我们在创建模型规范时添加一些元数据，用于
    Core ML 转换。在这里，我们设置其`className`，这是我们在 iOS 项目中实现这个层的 Swift（或 Objective-C）类的名称，以及`description`，这是在
    Xcode 的 ML 模型查看器中显示的文本。
- en: 'With our `delegate` method now implemented, let''s rerun the converter, passing
    in the appropriate parameters, as shown in the following code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了`delegate`方法，让我们重新运行转换器，传递适当的参数，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If all goes well, you should see the converter output each layer it visits,
    with no error messages, and finally returning a Core ML model instance. We can
    now add metadata to our model, which is what is displayed in Xcode ''s ML model
    views:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该会看到转换器输出它访问的每一层，没有错误消息，最后返回一个 Core ML 模型实例。现在我们可以给我们的模型添加元数据，这是在 Xcode
    的 ML 模型视图中显示的内容：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At this stage, we could save the model and import into Xcode , but there is
    just one more thing I would like to do to make our life a little easier. At its
    core (excuse the pun), the Core ML model is a specification of the network (including
    the model description, model parameters, and metadata) used by Xcode to build
    the model when imported. We can get a reference to this specification by calling
    the following statement:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以保存模型并将其导入 Xcode，但我想再做一些事情来让我们的生活更轻松。在本质上（请原谅这个双关语），Core ML 模型是 Xcode
    在导入时用于构建模型的网络规范（包括模型描述、模型参数和元数据）。我们可以通过调用以下语句来获取对这个规范的引用：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With reference to the specification of the models, we next search for the output
    layer, as shown in the following snippet:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 参考模型的规范，我们接下来搜索输出层，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can inspect the output simply by printing it out; run the cell with the
    following code to do just that:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地打印出来来检查输出；运行以下代码的单元格来完成这个操作：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should see something similar to this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到类似以下的内容：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Take note of the type, which is currently `multiArrayType` (its iOS equivalent
    is `MLMultiArray`). This is fine but would require us to explicitly convert it
    to an image; it would be more convenient to just have our model output an image
    instead of a multidimensional array. We can do this by simply modifying the specification.
    Specifically, in this instance, this means populating the type''s `imageType`
    properties to hint to Xcode that we are expecting an image. Let''s do that now
    by running the cell with this code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意类型，目前是`multiArrayType`（它在 iOS 中的对应物是`MLMultiArray`）。这没问题，但需要我们显式地将其转换为图像；如果模型直接输出图像而不是多维数组会方便得多。我们可以通过简单地修改规范来实现这一点。具体来说，在这个例子中，这意味着填充类型的`imageType`属性，以提示
    Xcode 我们期望一个图像。现在让我们通过运行带有以下代码的单元格来完成这个操作：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We first set the color space to RGB, then we set the expected width and height
    of the image. Finally, we create a new model by passing in the updated specification
    with the statement `coremltools.models.MLModel(spec)`. Now, if you interrogate
    the output, you should see something like the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先设置颜色空间为 RGB，然后设置图像的预期宽度和高度。最后，我们通过传递更新后的规范并使用语句`coremltools.models.MLModel(spec)`创建一个新的模型。现在，如果你查询输出，你应该会看到以下类似输出：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We have now saved ourselves a whole lot of code to perform this conversion;
    our final step is to save the model before importing it into Xcode . Run the last
    cell, which does just that:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为执行这个转换节省了大量代码；我们的最后一步是在将其导入 Xcode 之前保存模型。运行最后一个单元格，它正是这样做的：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Before closing the browser, let''s download the model. You can do this by returning
    the `Chapter6/Notebooks` directory and drilling down into the `output` folder.
    Here you should see the file `FastStyleTransferVanGoghStarryNight.mlmodel`; simply right-click
    on it and select the Download menu item (or do it by left-clicking and selecting
    the Download toolbar item):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在关闭浏览器之前，让我们下载模型。你可以通过返回到 `Chapter6/Notebooks` 目录并深入到 `output` 文件夹来完成此操作。在这里，你应该能看到文件
    `FastStyleTransferVanGoghStarryNight.mlmodel`；只需右键单击它并选择下载菜单项（或者通过左键单击并选择下载工具栏项）：
- en: '![](img/c0792c6c-82da-4984-8e22-5463596401dd.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0792c6c-82da-4984-8e22-5463596401dd.png)'
- en: With our model in hand, it's now time to jump into Xcode and implement those
    custom layers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有我们的模型后，现在是时候跳入 Xcode 并实现那些自定义层了。
- en: Building custom layers in Swift
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Swift 中构建自定义层
- en: In this section, we will be mainly focusing on implementing the custom layers
    that our model is dependent on, and we'll omit a lot of the application's details
    by working with an existing template—a structure you have no doubt become quite
    familiar with.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将主要关注实现模型所依赖的自定义层，并且通过使用现有的模板——你无疑已经非常熟悉的结构——来省略应用的大部分细节。
- en: 'If you haven''t done so already, pull down the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter6/Start/StyleTransfer/` and
    open the project `StyleTransfer.xcodeproj`. Once loaded, you will see the project
    for this chapter:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从配套的仓库中拉取最新的代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载后，导航到目录
    `Chapter6/Start/StyleTransfer/` 并打开项目 `StyleTransfer.xcodeproj`。一旦加载，你将看到本章的项目：
- en: '![](img/0de45713-7155-44ad-8787-cd62a5d02fa3.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0de45713-7155-44ad-8787-cd62a5d02fa3.png)'
- en: The application consists of two view controllers. The first, `CameraViewController`,
    provides the user with a live stream of the camera and the ability to take a photo.
    When a photo is taken, the controller presents the other view controller, `StyleTransferViewController`,
    passing along with the captured photo. `StyleTransferViewController` then presents
    the image, along with a horizontal `CollectionView` at the bottom containing a
    set of styles that the user can select by tapping on them.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序由两个视图控制器组成。第一个，`CameraViewController`，为用户提供相机实时流和拍照的能力。拍照时，控制器会展示另一个视图控制器
    `StyleTransferViewController`，并传递捕获的相片。`StyleTransferViewController` 然后展示图像，并在底部包含一个水平
    `CollectionView`，其中包含用户可以通过点击选择的样式集。
- en: Each time the user selects a style, the controller updates the `ImageProcessors`
    style property and then calls its method, `processImage`, passing in the assigned
    image. It is here that we will implement the functionality responsible for passing
    the image to the model and returning the result via the assigned delegates `onImageProcessorCompleted`
    method, which is then presented to the user.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每当用户选择一种样式时，控制器都会更新 `ImageProcessors` 样式属性，然后调用其方法 `processImage`，传入指定的图像。正是在这里，我们将实现将图像传递给模型并通过指定的代理
    `onImageProcessorCompleted` 方法返回结果的功能，然后将其展示给用户。
- en: 'Now, with our project loaded, let''s import the model we have just created;
    locate the downloaded `.mlmodel` file and drag it onto Xcode . Once imported,
    we select it from the left-hand \ panel to inspect the metadata, to remind ourselves
    what we need to implement:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们的项目已加载，让我们导入我们刚刚创建的模型；找到下载的 `.mlmodel` 文件并将其拖放到 Xcode 中。一旦导入，我们从左侧面板中选择它来检查元数据，以提醒自己需要实现的内容：
- en: '![](img/a97e6658-5e37-4a88-8712-177aedd2b358.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a97e6658-5e37-4a88-8712-177aedd2b358.png)'
- en: By inspecting the model, we can see that it is expecting an input RGB image
    of size 320 x 320, and it will output an image with the same dimensions. We can
    also see that the model is expecting two custom layers named `ResCropBlockLambda`
    and `RescaleOutputLambda`. Before implementing these classes, let's hook the model
    up and, just for fun, see what happens when we try to run it without the custom
    layers implemented.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查模型，我们可以看到它期望一个大小为 320 x 320 的输入 RGB 图像，并且它将以相同的尺寸输出图像。我们还可以看到模型期望两个名为 `ResCropBlockLambda`
    和 `RescaleOutputLambda` 的自定义层。在实现这些类之前，让我们将模型连接起来，并且为了好玩，看看在没有实现自定义层的情况下尝试运行它会发生什么。
- en: 'Select `ImageProcessor.swift` from the left-hand-side panel; in this project,
    we will get the Vision framework to do all the preprocessing. Start by adding
    the following properties within the body of the `ImageProcessor` class, somewhere
    such as underneath the `style` property:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从左侧面板选择`ImageProcessor.swift`；在这个项目中，我们将使用Vision框架来完成所有预处理。首先，在`ImageProcessor`类的主体中添加以下属性，例如在`style`属性下方：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first property returns an instance of `VNCoreMLModel`, wrapping our `FastStyleTransferVanGoghStarryNight`
    model. Wrapping our model is necessary to make it compatible with the Vision framework's
    requests classes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个属性返回一个`VNCoreMLModel`实例，封装了我们的`FastStyleTransferVanGoghStarryNight`模型。封装我们的模型是必要的，以便使其与Vision框架的请求类兼容。
- en: 'Just underneath, add the following snippet, which will be responsible for returning
    the appropriate `VNCoreMLModel`, based on the selected style:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面添加以下片段，它将负责根据所选样式返回适当的`VNCoreMLModel`：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we create the method that will be responsible for returning an instance
    of `VNCoreMLRequest`, based on the currently selected model (determined by the
    current `style`):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个方法，它将负责根据当前所选模型（由当前的`style`确定）返回一个`VNCoreMLRequest`实例：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`VNCoreMLRequest` is responsible for performing the necessary preprocessing
    on the input image before passing it to the assigned Core ML model. We instantiate
    `VNCoreMLRequest`, passing in a completion handler that will simply pass its results
    to the `processRequest` method, of the `ImageProcessor` class, when called. We
    also set the `imageCropAndScaleOption` to `.centerCrop` so that our image is resized
    to 320 x 320 whilst maintaining its aspect ratio (cropping the centered image
    on its longest side, if necessary).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`VNCoreMLRequest`负责在将输入图像传递给分配的Core ML模型之前对其进行必要的预处理。我们实例化`VNCoreMLRequest`，传入一个完成处理程序，当调用时，它将简单地将其结果传递给`ImageProcessor`类的`processRequest`方法。我们还设置了`imageCropAndScaleOption`为`.centerCrop`，以便我们的图像在保持其宽高比的同时调整大小到320
    x 320（如果需要，裁剪中心图像的最长边）。'
- en: 'With our properties now defined, it''s time to jump into the `processImage` method
    to initiate the actual work; add the following code (shown in bold, and replacing the `//
    TODO` comments):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了属性，是时候跳转到`processImage`方法来启动实际的工作了；添加以下代码（以粗体显示，并替换`// TODO`注释）：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding method is our entry point to stylizing an image; we start by instantiating
    an instance of `VNImageRequestHandler`, passing in the image, and initiating the
    process by calling the `perform` method. Once the analysis has finished, the request
    will call the `delegate` we assigned to it, `processRequest`, passing in a reference
    of the associated request and the results (or errors if any). Let''s flesh out
    this method now:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法是我们的图像风格化入口点；我们首先实例化一个`VNImageRequestHandler`实例，传入图像，并通过调用`perform`方法启动过程。一旦分析完成，请求将调用我们分配给它的`delegate`，即`processRequest`，传入相关请求和结果（如果有错误）。现在让我们具体实现这个方法：
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: While `VNCoreMLRequest` is responsible for the image analysis, `VNImageRequestHandler`
    is responsible for executing the request (or requests).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`VNCoreMLRequest`负责图像分析，但`VNImageRequestHandler`负责执行请求（或请求）。
- en: If no errors occurred during the analysis, we should be returned the instance
    of our request with its results property set. As we are only expecting one request
    and result type, we cast the results to an array of `VNPixelBufferObservation`,
    a type of observation suitable for image analysis with a Core ML model whose role
    is image-to-image processing, such as our style transfer model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分析过程中没有发生错误，我们应该返回带有其结果属性的请求实例。因为我们只期望一个请求和结果类型，我们将结果转换为`VNPixelBufferObservation`数组的实例，这是一种适合使用Core
    ML模型进行图像分析的观察类型，其作用是图像到图像处理，例如我们的风格转换模型。
- en: We can get a reference to our stylized image via the property `pixelBuffer`,
    from the observation obtained from the results. And then we can call the extension
    method `toCGImage` (found in `CVPixelBuffer+Extension.swift`) to conveniently
    obtain the output in a format we can easily use, in this case, updating the image
    view.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过属性`pixelBuffer`从结果中获取的观察结果中获取我们风格化图像的引用。然后我们可以调用扩展方法`toCGImage`（在`CVPixelBuffer+Extension.swift`中找到）以方便地以我们可以轻松使用的格式获取输出，在这种情况下，更新图像视图。
- en: As previously discussed, let's see what happens when we try to run an image
    through our model without implementing the custom layers. Build and deploy to
    a device and proceed to take a photo, then select the Van Cogh style from the
    styles displayed. In doing so, you will observe the build failing and reporting
    the error: `Error creating Core ML custom layer implementation from factory for
    layer "RescaleOutputLambda"` (as we were expecting).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，让我们看看当我们尝试在不实现自定义层的情况下运行图像通过我们的模型时会发生什么。构建并部署到设备上，然后拍照，然后从显示的样式中选择梵高风格。这样做时，你会观察到构建失败并报告错误：“从工厂创建
    Core ML 自定义层实现时出错，层名为 "RescaleOutputLambda"`（正如我们所预期的）。
- en: 'Let''s address this now by implementing each of our custom layers, starting
    with the `RescaleOutputLambda` class. Create a new Swift file named `RescaleOutputLamdba.class`
    and replace the template code with the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过实现每个自定义层来解决这个问题，从 `RescaleOutputLambda` 类开始。创建一个名为 `RescaleOutputLamdba.class`
    的新 Swift 文件，并用以下代码替换模板代码：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, we have created a concrete class of the protocol `MLCustomLayer`, a protocol
    that defines the behavior of a custom layer in our neural network model. The protocol
    consists of four required methods and one optional method, which are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个名为 `MLCustomLayer` 的具体类，这是一个定义我们神经网络模型中自定义层行为的协议。该协议包含四个必需的方法和一个可选方法，具体如下：
- en: '`init(parameters)`: Initializes the custom layer implementation that is passed
    the dictionary `parameters` that includes any additional configuration options
    for the layer. As you may recall, we created an instance of `NeuralNetwork_pb2.CustomLayerParams`
    for each of our custom layers when converting our Keras model. Here we can add
    more entries, which will be passed into this dictionary. This provides some flexibility,
    such as allowing you to adjust your layer based on the set parameters.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init(parameters)`: 初始化传递字典 `parameters` 的自定义层实现，该字典包含任何额外的配置选项。如您所回忆的，我们在将我们的
    Keras 模型转换为自定义层时为每个自定义层创建了一个 `NeuralNetwork_pb2.CustomLayerParams` 实例。在这里，我们可以添加更多条目，这些条目将被传递到这个字典中。这提供了一些灵活性，例如允许您根据设置的参数调整您的层。'
- en: '`setWeightData()`: Assigns the weights for the connections within the layer
    (for layers with trainable weights).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setWeightData()`: 为层内连接分配权重（对于具有可训练权重的层）。'
- en: '`outputShapes(forInputShapes)`: This determines how the layer modifies the
    size of the input data. Our `RescaleOutputLambda` layer doesn''t change the size
    of the layer, so we simply need to return the input shape, but we will make use
    of this when implementing the next custom layer.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputShapes(forInputShapes)`: 这个方法确定层如何修改输入数据的大小。我们的 `RescaleOutputLambda`
    层不会改变层的大小，所以我们只需返回输入形状，但我们将利用这个方法来实现下一个自定义层。'
- en: '`evaluate(inputs, outputs)`: This performs the actual computation; this method
    is required and gets called when the model is run on the CPU.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate(inputs, outputs)`: 这个方法执行实际的计算；这是一个必需的方法，当模型在 CPU 上运行时会被调用。'
- en: '`encode(commandBuffer, inputs, outputs)`: This method is optional and acts
    as an alternative to the method `evaluate`, which uses the GPU rather than the
    CPU.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encode(commandBuffer, inputs, outputs)`: 这个方法是可选的，作为 `evaluate` 方法的替代，后者使用
    GPU 而不是 CPU。'
- en: Because we are not passing in any custom parameters or setting any trainable
    weights, we can skip the constructor and `setWeightData` methods; let's walk through
    the remaining methods, starting with `outputShapes(forInputShapes)`.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有传递任何自定义参数或设置任何可训练权重，我们可以跳过构造函数和 `setWeightData` 方法；让我们逐一介绍剩余的方法，从 `outputShapes(forInputShapes)`
    开始。
- en: 'As previously mentioned, this layer doesn''t change the shape of the input,
    therefore, we can simply return the input shape, as shown in the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个层不会改变输入的形状，因此我们可以简单地返回输入形状，如下面的代码所示：
- en: '[PRE28]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With our `outputShapes(forInputShapes)` method now implemented, let's turn our
    attention to the workhorse of the layer responsible for performing the actual
    computation, the `evaluate` method. The `evaluate` method receives an array of `MLMultiArray`
    objects as inputs, along with another array of `MLMultiArray` objects, where it
    is expected to store the results. Having the `evaluate` method accept arrays for
    its input and outputs allows for greater flexibility in supporting different architectures,
    but, in this example, we are expecting only one input and one output.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了`outputShapes(forInputShapes)`方法，让我们将注意力转向层的实际计算工作，即`evaluate`方法。`evaluate`方法接收一个`MLMultiArray`对象的数组作为输入，以及另一个`MLMultiArray`对象的数组，其中它预期存储结果。让`evaluate`方法接受输入和输出数组，这为支持不同的架构提供了更大的灵活性，但在这个例子中，我们只期望有一个输入和一个输出。
- en: 'As a reminder, this layer is for scaling each element from a range of -1.0
    - 1.0 to a range of 0 - 255 (that''s what a typical image would be expecting).
    The simplest approach is to iterate through each element and scale it using the
    equation we saw in Python: `((x+1)*127.5`. This is exactly what we''ll do; add
    the following code (in bold) to the body of your `evaluate` method:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，这个层是为了将每个元素从-1.0 - 1.0的范围缩放到0 - 255的范围（这是典型图像所期望的）。最简单的方法是遍历每个元素并使用我们在Python中看到的方程进行缩放：`((x+1)*127.5`。这正是我们将要做的；将以下（加粗）代码添加到`evaluate`方法的主体中：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The bulk of this method is made up of code used to create the index for obtaining
    the appropriate value from the input and pointing to its output counterpart. Once
    an index has been created, the Python formula is ported across to Swift: `input[index].doubleValue
    + rescaleAddition) * rescaleMulitplier`. This concludes our first custom layer;
    let's now implement our second customer layer, `ResCropBlockLambda`.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主体是由用于创建索引的代码组成的，该索引用于从输入中获取适当的值并指向其输出对应项。一旦创建了索引，Python公式就被移植到Swift中：`input[index].doubleValue
    + rescaleAddition) * rescaleMulitplier`。这标志着我们第一个自定义层的结束；现在让我们实现第二个自定义层，`ResCropBlockLambda`。
- en: 'Create a new file called `ResCropBlockLambda.swift` and add the following code,
    overwriting any existing code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`ResCropBlockLambda.swift`的新文件，并添加以下代码，覆盖任何现有代码：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As we have done with the previous custom layer, we have stubbed out all the
    required methods as determined by the `MLCustomLayer` protocol. Once again, we
    can ignore the constructor and `setWeightData` method as neither are used in this
    layer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一个自定义层中所做的那样，我们已经根据`MLCustomLayer`协议确定了所有必需的方法。再次强调，我们可以忽略构造函数和`setWeightData`方法，因为在这个层中它们都没有被使用。
- en: 'If you recall, and as the name suggests, the function of this layer is to crop
    the width and height of one of the inputs of the residual block. We need to reflect
    this within the `outputShapes(forInputShapes)` method, so that the network knows
    the input dimensions for subsequent layers. Update the `outputShapes(forInputShapes)`
    method with the following code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，正如其名称所暗示的，这个层的功能是裁剪残差块的一个输入的宽度和高度。我们需要在`outputShapes(forInputShapes)`方法中反映这一点，以便网络知道后续层的输入维度。使用以下代码更新`outputShapes(forInputShapes)`方法：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here, we are removing a constant of `4` from the width and height, essentially
    padding 2 from the width and height. Next, we implement the `evaluate` method,
    which performs this cropping. Replace the `evaluate` method with the following
    code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从宽度和高度中减去了一个常数`4`，实际上是在宽度和高度上填充了2。接下来，我们实现`evaluate`方法，它执行这个裁剪。用以下代码替换`evaluate`方法：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Similar to the `evaluate` method of our `RescaleOutputLambda` layer, the bulk
    of this method has to do with creating the indices for the input and output arrays.
    We simply pad it by restraining the ranges of our loops to the desired width and
    height.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的`RescaleOutputLambda`层的`evaluate`方法类似，这个方法的主体必须与创建输入和输出数组的索引有关。我们只是通过限制循环的范围来调整它，以达到所需的宽度和高度。
- en: 'Now, if you build and run the project, you will be able to run an image through
    the Van Gogh network getting a stylized version of it back, similar to what is
    shown in the following image:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你构建并运行项目，你将能够将图像通过梵高网络运行，并得到它的风格化版本，类似于以下图像所示：
- en: '![](img/74bc12a2-2c87-4e58-8bd2-b30ead776cf2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74bc12a2-2c87-4e58-8bd2-b30ead776cf2.png)'
- en: When running on the simulator, the whole process took approximately **22.4 seconds**.
    In the following two sections, we will spend some time looking at how we can reduce
    this.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在模拟器上运行时，整个过程大约花费了**22.4秒**。在接下来的两个部分中，我们将花时间探讨如何减少这个时间。
- en: Accelerating our layers
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速我们的层
- en: 'Let''s return to the layer `RescaleOutputLambda` and see where we might be
    able to shed a second or two off the processing time. As a reminder, the function
    of this layer is to rescale each element in the output, where our output can be
    thought of as a large vector. Luckily for us, Apple provides an efficient framework
    and API for just this. Instead of operating on each element within a loop, we
    will take advantage of the `Accelerate` framework and its vDSPAPI to perform this
    operation in a single step. This process is called **vectorization** and is made
    possible by exploiting the CPU''s **Single Instruction, Multiple Data** (**SIMD**) instruction
    set. Return to the `RescaleOutputLambda` class and update the `evaluate` method
    with the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到`RescaleOutputLambda`层，看看我们可能在哪里能减少一秒或两秒的处理时间。作为提醒，这个层的作用是对输出中的每个元素进行缩放，其中我们的输出可以被视为一个大向量。幸运的是，苹果为我们提供了高效的框架和API来处理这种情况。我们不会在循环中对每个元素进行操作，而是将利用`Accelerate`框架及其`vDSPAPI`在单步中执行此操作。这个过程被称为**向量化**，是通过利用CPU的**单指令多数据**（**SIMD**）指令集来实现的。回到`RescaleOutputLambda`类，并使用以下代码更新`evaluate`方法：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we first get a reference to the pointers to each of the
    input and output buffers, wrapping them in `UnsafeMutablePointer`, as required
    by the vDSP functions. Then, it's simply a matter of applying each of our scaling
    operations using the equivalent vDSP functions, which we will walk through.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先获取每个输入和输出缓冲区的指针的引用，将它们包装在`UnsafeMutablePointer`中，这是vDSP函数所要求的。然后，只需简单地使用等效的vDSP函数应用我们的缩放操作，我们将逐一介绍这些函数。
- en: 'First, we add our constant of `1` to the input and save the results in the
    output buffer, as shown in the following snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将常数`1`添加到输入中，并将结果保存到输出缓冲区中，如下面的代码片段所示：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Where the function `vDSP_vsadd` takes in a pointer to our vector (`inputPointer`)
    and adds `rescaleAddition` to each of its elements before storing it into the
    output.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中函数`vDSP_vsadd`接收指向我们的向量（`inputPointer`）的指针，并将`rescaleAddition`添加到其每个元素中，然后再将其存储到输出中。
- en: 'Next, we apply our multiplier to each of the elements of the output (which
    currently has each of its values set to the input with 1 added to it); the code
    for this is shown in the following snippet:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的乘数应用于输出（当前每个值都设置为输入加1）的每个元素；此代码的示例如下：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Similar to `vDSP_vsadd`, `vDSP_vsmul` takes in the input (in this case, our
    output); the scalar we want to multiply each element by; the output; the stride
    for persisting the result; and finally, the number of elements we want to operate
    on.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与`vDSP_vsadd`类似，`vDSP_vsmul`接收输入（在这种情况下，我们的输出）；我们想要乘以每个元素的标量；输出；用于持久化结果的步长；最后，我们想要操作的元素数量。
- en: If you rerun the application, you will see that we have managed to shed a few
    seconds off the total execution time—not bad considering this layer is run only once
    at the end of our network. Can we do better?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你重新运行应用程序，你会看到我们已经成功将总执行时间减少了几秒钟——考虑到这个层只在我们的网络末尾运行一次，这已经很不错了。我们能做得更好吗？
- en: Taking advantage of the GPU
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用GPU
- en: You may recall that when we introduced the `MLCustomLayer` protocol, there was
    an optional method, `encode(commandBuffer, inputs, outputs)`, reserved for performing
    the evaluation on the GPU if the hosting device supported it. This flexibility
    is one of the advantages Core ML has over other machine learning frameworks; it
    allows mixing layers, which run on the CPU and GPU, and allows them to work coherently
    together.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，当我们介绍`MLCustomLayer`协议时，有一个可选方法`encode(commandBuffer, inputs, outputs)`，它被保留用于在宿主设备支持的情况下在GPU上执行评估。这种灵活性是Core
    ML相对于其他机器学习框架的优势之一；它允许混合运行在CPU和GPU上的层，并允许它们协同工作。
- en: 'To use the GPU, we will be using Apple''s `Metal` framework, a graphics framework
    equivalent to OpenGL and DirectX (and now Vulkan), for those who are familiar
    with 3D graphics. Unlike our previous solutions, which included all code in a
    single method, we need to write the code that performs the computation in an external
    file called a **Metal shader** file. Within this file we will define a kernel,
    which will be complied and stored on the GPU (when loaded), allowing it to fan
    out the data in parallel across the GPU. Let''s create this kernel now; create
    a new `metal` file called `rescale.metal` and add the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用GPU，我们将使用苹果的`Metal`框架，这是一个与OpenGL和DirectX（现在还有Vulkan）相当的图形框架，对于那些熟悉3D图形的人来说。与我们的先前解决方案不同，它们将所有代码包含在一个方法中，我们需要在外部文件中编写执行计算的代码，这个文件被称为**Metal着色器**文件。在这个文件中，我们将定义一个内核，该内核将被编译并存储在GPU上（当加载时），允许它并行地在GPU上分散数据。现在让我们创建这个内核；创建一个名为`rescale.metal`的新`metal`文件，并添加以下代码：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: It is out of scope to discuss the details of `metal`, so instead, we'll just
    highlight some of the key differences and commonalities between this and the previous
    approaches. First, it's worth recognizing why GPUs have been a major catalyst
    for the resurgence of neural networks. The GPU architecture allows a kernel (seen
    earlier) to be spawned for each element in our array—mass parallelism!
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论`metal`的细节超出了范围，所以我们只突出一些与之前方法的关键差异和相似之处。首先，值得认识到为什么GPU已经成为神经网络复兴的主要催化剂。GPU架构允许为我们的数组中的每个元素生成一个内核（之前已看到）——大规模并行！
- en: Because GPU frameworks were traditionally built with graphics manipulation in
    mind, there are some nuances with how we operate on data and what we operate on.
    The most notable of them is that we have swapped `MLMultiArray` for `texture2d_array`
    (textures) and we access them through sampling, using `thread_position_in_grid`.
    Nonetheless, the actual computation should look familiar from the original Python
    code, `const float4 y = (1.0f + x) * 127.5f`. Once calculated, we cast it to float
    16 (half) and write it to the output texture.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU框架传统上是为了图形操作而构建的，我们在操作数据和操作内容上存在一些细微差别。其中最显著的是，我们将`MLMultiArray`替换为`texture2d_array`（纹理），并通过`thread_position_in_grid`进行采样来访问它们。不过，实际的计算应该与原始Python代码相似，`const
    float4 y = (1.0f + x) * 127.5f`。一旦计算完成，我们将它转换为float 16（半精度）并写入输出纹理。
- en: Our next step is to configure our `RescaleOutputLambda` class to use `Metal`
    and the GPU, rather than the CPU. Return to the `RescaleOutputLambda.swift` file
    and make the following amendments.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是配置`RescaleOutputLambda`类以使用`Metal`和GPU，而不是CPU。回到`RescaleOutputLambda.swift`文件，并做出以下修改。
- en: 'Start by importing the `Metal` framework by adding the following statement
    at the top of your file:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过在文件顶部添加以下语句来导入`Metal`框架：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we define a class variable of the type `MTLComputePipelineState` as a
    handler to the kernel we have just created, along with setting this up within
    the constructor of the `RescaleOutputLambda` class. Make the following amendments
    to the class and constructor, as shown in bold in the snippet:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个类型为`MTLComputePipelineState`的类变量，作为我们刚刚创建的内核的处理程序，并在`RescaleOutputLambda`类的构造函数中设置它。按照代码片段中加粗的部分对类和构造函数进行以下修改：
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'If no errors are thrown, we will have reference to a complied version of our
    rescale kernel; the final step is making use of it. Within the `RescaleOutputLambda`
    class, add the following method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有抛出错误，我们将有一个编译后的缩放内核的引用；最后一步是利用它。在`RescaleOutputLambda`类中添加以下方法：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As mentioned before, we will omit the details here and only highlight some key
    differences and commonalities between this approach and the previous approaches.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将省略细节，只突出这种方法与之前方法的关键差异和相似之处。
- en: 'In short, the bulk of this method is responsible for passing data through to
    the compute kernel via the encoder and then dispatching it across the GPU. We
    first pass the input and output textures, as shown in the following snippet:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个方法的大部分工作是通过编码器将数据传递给计算内核，然后在GPU上分发它。我们首先传递输入和输出纹理，如下面的代码片段所示：
- en: '[PRE40]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'And then we''re setting the handler, which points to the rescale kernel we
    created in the preceding snippet:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们设置处理器，它指向我们在前面的代码片段中创建的缩放内核：
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, dispatch the job to the GPU; in this instance, our compute kernel
    is invoked for every pixel in every channel of the input texture:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将任务分发给GPU；在这种情况下，我们的计算内核对输入纹理的每个通道的每个像素进行调用：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: If you build and run again, you will hopefully get the same result but in less
    time. We have now seen two approaches to optimizing our network; I leave optimizing
    `ResCropBlockLambda` as an exercise for you. For now, let's shift our focus to
    talking about your model's weight before we wrap up this chapter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次构建和运行，你可能会希望得到相同的结果，但用时更少。我们已经看到了两种优化我们网络的方法；我将优化`ResCropBlockLambda`作为一个练习留给你。现在，在我们结束这一章之前，让我们将注意力转移到讨论你的模型权重上。
- en: Reducing your model's weight
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少你的模型权重
- en: We have spent considerable time discussing layers of a network; we have learned
    that layers are made up of weights, configured in such a way that they can transform
    an input into a desirable output. These weights come at a cost, though; each one
    (by default) is a 32-bit floating-point number with a typical model, especially
    in computer vision, having millions resulting in networks that are hundreds of
    megabytes in size. On top of that; it's plausible that your application will have
    multiple models (with this chapter being a good example, requiring a model for
    each style).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经花费了大量时间讨论网络的层；我们了解到层由权重组成，这些权重被配置成能够将输入转换为期望的输出。然而，这些权重是有代价的；每一个（默认情况下）都是一个32位的浮点数，特别是在计算机视觉中，典型的模型有数百万个，导致网络大小达到数百兆字节。除此之外；你的应用程序可能需要多个模型（本章就是一个很好的例子，需要为每种风格创建一个模型）。
- en: 'Fortunately, our model in this chapter has a moderate number of weights and
    weighs in at a mere 2.2 MB; but this is possibly an exception. So we''ll use this
    chapter as an excuse to explore some ways we can reduce our model''s weights.
    But before doing so, let''s quickly discuss why, even though it''s probably obvious.
    The three main reasons why you should be conscious of your model''s size include:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们本章中的模型权重数量适中，仅重2.2 MB；但这可能是一个例外。所以我们将利用这一章作为借口来探索一些我们可以减少模型权重的途径。但在这样做之前，让我们快速讨论一下，尽管这可能是显而易见的。你应该注意你的模型大小的三个主要原因包括：
- en: Download time
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载时间
- en: Application footprint
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序占用空间
- en: Demands on memory
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对内存的需求
- en: 'These could all hinder the user experience and are reasons for a user to either
    uninstall the application quickly or not even download it in the first place.
    So how do you reduce your model''s size to avoid deterring the user. There are
    three broad approaches:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都可能会阻碍用户体验，并且是用户快速卸载应用程序或根本不下载的原因。那么，你如何减少你的模型大小以避免阻碍用户。有三种主要的方法：
- en: Reduce the number of layers your network uses
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少你的网络使用的层数
- en: Reduce the number of units in each of those layers
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少每个层中的单元数量
- en: Reduce the size of the weights
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少权重的尺寸
- en: The first two require that you have access to the original network and tools
    to re-architect and train the model; the last is the most accessible and it's
    the one we will discuss now.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个要求你能够访问原始网络和工具来重新架构和训练模型；最后一个是最容易获得的，也是我们现在要讨论的。
- en: 'In iOS 11.2, Apple allowed your networks to use half-precision floating-point
    numbers (16-bit). Now, with the release of iOS 12, Apple has taken this even further and
    introduced quantization, which allows us to use eight or less bits to encode our
    model''s weights. In the following figure, we can see how these options compare
    with one another:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在iOS 11.2中，苹果允许你的网络使用半精度浮点数（16位）。现在，随着iOS 12的发布，苹果更进一步，引入了量化，这允许我们使用八个或更少的位来编码我们的模型权重。在下面的图中，我们可以看到这些选项之间的比较：
- en: '![](img/982b4300-9cc8-4a31-8249-7c6fe6d16583.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/982b4300-9cc8-4a31-8249-7c6fe6d16583.png)'
- en: Let's discuss each in turn, starting with reducing our weights precision by
    converting it's floating points from 32-bits to 16-bits.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一讨论，首先从通过将它的浮点数从32位转换为16位来降低我们的权重精度开始。
- en: For both of these techniques (half-precision and quantization), we will be using
    the Core ML Tools Python package; so, begin by opening up your browser and heading
    over to [https://notebooks.azure.com](https://notebooks.azure.com). Once the page
    is loaded navigate to the folder `Chapter6/Notebooks/` and open the Jupyter Notebook `FastNeuralStyleTransfer_OptimizeCoreML.ipynb`.
    As we did before, we'll walk through each of the Notebook's cells here with the
    assumption that you will be executing each one as we cover it (if you are working
    along).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两种技术（半精度和量化），我们将使用Core ML Tools Python包；因此，首先打开您的浏览器并转到 [https://notebooks.azure.com](https://notebooks.azure.com)。页面加载后，导航到文件夹
    `Chapter6/Notebooks/` 并打开Jupyter Notebook `FastNeuralStyleTransfer_OptimizeCoreML.ipynb`。像之前一样，我们将在这里逐个介绍Notebook的单元格，假设您将按照我们介绍的内容执行每个单元格（如果您正在一起工作的话）。
- en: 'We begin by importing the Core ML Tools package; execute the cell with the
    following code:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入Core ML Tools包；执行以下代码的单元格：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: For convenience, we have wrapped the `import` in an exception block, so that
    it automatically installs the package if it doesn't exist.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们将 `import` 包裹在一个异常块中，这样如果它不存在，它会自动安装该包。
- en: At the time of writing, Core ML 2 was still in beta and only recently publicly
    announced. If you're using a version of Core ML Tools that is less than 2.0 then
    replace `!pip install coremltools` with `!pip install coremltools>=2.0b1` to install
    the latest beta version to have access to the necessary modules for this section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Core ML 2仍然处于测试版，并且最近才公开宣布。如果您使用的Core ML Tools版本低于2.0，请将 `!pip install
    coremltools` 替换为 `!pip install coremltools>=2.0b1` 以安装最新的测试版，以便访问本节所需的模块。
- en: 'Next, we will load our `mlmodel` file that we had previously saved, using the
    following statement:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下语句加载我们之前保存的 `mlmodel` 文件：
- en: '[PRE44]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we perform the conversion by simply calling `coremltools.utils.convert_neural_network_weights_to_fp16`
    and passing in your model. If successful, this method will return an equivalent
    model (that you passed in), using half precision weights instead of 32-bits for
    storing its weights. Run the cell with the following code to do just that:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过简单地调用 `coremltools.utils.convert_neural_network_weights_to_fp16` 并传入您的模型来执行转换。如果成功，此方法将返回一个等效模型（您传入的模型），使用半精度权重而不是32位来存储其权重。运行以下代码的单元格来完成此操作：
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we save it so we can later download it and import into our project;
    run the next cell with the code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将其保存下来，以便我们可以在以后下载并导入到我们的项目中；运行下一个单元格的代码：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: And with that executed (essentially three lines of code), we have managed to
    have our models size, going from 2.2 MB to 1.1 MB - so, what's the catch?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述操作（本质上只有三行代码）后，我们已经成功将模型的大小从2.2 MB减小到1.1 MB——那么，有什么问题吗？
- en: As you might suspect, there is a trade-off here; reducing the precision of your
    models weights will affect its accuracy, but possibly not enough to be concerned
    with. The only way you will know is by comparing the optimized model with the
    original and re-evaluating it on your test data, ensuring that it satisfies your
    required accuracy/results. For this, Core ML Tools provides a collection of utilities
    that makes this fairly seamless, which you can learn at the official website [https://apple.github.io/coremltools/index.html](https://apple.github.io/coremltools/index.html).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所料，这里有一个权衡；降低模型权重的精度将影响其准确性，但可能不足以引起关注。您唯一知道的方法是通过比较优化后的模型和原始模型，并在测试数据上重新评估它，确保它满足您所需的准确度/结果。为此，Core
    ML Tools提供了一系列工具，使得这个过程相当无缝，您可以在官方网站 [https://apple.github.io/coremltools/index.html](https://apple.github.io/coremltools/index.html)
    上了解这些工具。
- en: Quantization is no more complicated (with respect to using it via Core ML Tools
    rather than concept); it's a clever technique, so let's quickly discuss how it
    achieves 8-bit compression before running through the code.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过Core ML Tools使用概念相比，量化并不复杂；它是一种巧妙的技术，所以让我们快速讨论它是如何实现8位压缩的，然后再运行代码。
- en: At a high-level, quantization is a technique that maps a continuous range of
    values to a discrete set; you can think of it as a process of clustering your
    values into a discrete set of groups and then creating a lookup table which maps
    your values to the closest group. The size is then dependent on the number of
    clusters used (index) rather than value, which allows you to encode your weights
    using anything from 8-bits to 2-bits.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，量化是一种将连续值范围映射到离散集的技术；你可以将其视为将你的值聚类到一组离散的组中，然后创建一个查找表，将你的值映射到最近的组。大小现在取决于使用的聚类数量（索引），而不是值，这允许你使用从8位到2位的任何位数来编码你的权重。
- en: 'To make this concept more concrete, the following figure illustrates the results
    of color quantization; where a 24-bit image is mapped to 16 discrete colors:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个概念更具体，以下图表说明了颜色量化的结果；其中24位图像被映射到16种离散颜色：
- en: '![](img/ff9c1e2b-d93e-4767-ba5b-ae5894189d61.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ff9c1e2b-d93e-4767-ba5b-ae5894189d61.png)'
- en: Instead of each pixel representing its color (using 24-bits/8-bits per channel),
    they now are indexes to the 16 color palette, that is, from 24-bits to 4-bits.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是每个像素代表其颜色（使用24位/8位/通道），现在它们现在是16色调色板的索引，即从24位到4位。
- en: 'Before moving onto how we optimize our models using quantization with the Core
    ML Tools package, you maybe wondering how this palette (or discrete set of values)
    is derived. The short answer is that there are many ways, from linearly separating
    the values into groups, to using an unsupervised learning technique such as k-means,
    or even using a custom, domain-specific technique. Core ML Tools allows for all
    variations and the choice will be dependent on your data distribution and that
    results achieved during testing. Let''s jump into it; first, we will start by
    importing the module:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用Core ML Tools包进行量化优化模型之前，你可能想知道这种调色板（或离散值集）是如何得到的。简短的回答是，有多种方法，从将值线性分组，到使用k-means等无监督学习技术，甚至使用自定义的、特定领域的技巧。Core
    ML Tools允许所有变体，选择将取决于你的数据分布和测试期间获得的结果。让我们开始吧；首先，我们将从导入模块开始：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With this statement, we have imported the module and assigned it the alias
    `quant_utils`; the next cell, we optimize our model using a variation of sizes
    and methods:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个声明，我们已经导入了模块并将其分配给别名`quant_utils`；下一个单元，我们将使用不同的大小和方法来优化我们的模型：
- en: '[PRE48]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once this is completed, let''s save each of our optimized models to the output
    directory before downloading them to our local disk to import them into Xcode
    (this may take some time):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，在我们将它们下载到本地磁盘并导入Xcode之前，让我们将每个优化后的模型保存到输出目录（这可能需要一些时间）：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'I will omit the details of downloading and importing the model into your project
    as we have already gone through these steps previously in this chapter, but I
    do encourage that you to inspect the results from each model to get a feel for
    how each optimization affects the results - of course, these effects are highly
    dependent on the model, data, and domain. The following figure shows the results
    of each of the optimizations along with the model''s size:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在本章中详细介绍了下载和将模型导入项目的步骤，因此我将省略这些细节，但我确实鼓励你检查每个模型的输出结果，以了解每种优化如何影响结果——当然，这些影响高度依赖于模型、数据和领域。以下图表显示了每种优化的结果以及模型的大小：
- en: '![](img/e59a2efc-e65e-40b9-8700-9ec242b9285c.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e59a2efc-e65e-40b9-8700-9ec242b9285c.png)'
- en: Admittedly, it's difficult to see the differences due to the low-resolution
    of the image (and possibly because you're reading this in black and white) but
    generally, the quality appears minimal between the original and k-means 8-bit
    version.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 承认，由于图像分辨率低（以及可能因为你正在以黑白阅读），很难看到差异，但一般来说，原始图像和k-means 8位版本之间的质量差异很小。
- en: With the release of Core ML 2, Apple offers another powerful feature to optimize
    you Core ML models; specifically around consolidating multiple models into a single
    package. This not only reduces the size of your application but also convenient
    for you, the developer, when interfacing with your model. For example, flexible
    shapes and sizes allows for variable input and output dimensions, that is, instead
    of a single fixed input and output dimension, you have the flexibility of having
    multiple variants or a variable range within a limit. You can learn more about
    this feature on their official website at [https://developer.apple.com/machine-learning](https://developer.apple.com/machine-learning);
    but for now, we will wrap up this chapter with a quick summary before moving on
    to the next chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Core ML 2 的发布，Apple 提供了另一个强大的功能来优化您的 Core ML 模型；具体来说，是关于将多个模型合并成一个单一包。这不仅减少了您应用程序的大小，而且对您，即开发者，与模型交互时也方便。例如，灵活的形状和大小允许变量输入和输出维度，也就是说，您有多个变体或在一个限制范围内的变量范围。您可以在他们的官方网站上了解更多关于这个功能的信息：[https://developer.apple.com/machine-learning](https://developer.apple.com/machine-learning)；但在此阶段，我们将在进入下一章之前，对这个章节做一个简要总结。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the concept of style transfer; a technique that
    aims to separate the content of an image from its style. We discussed how it achieves
    this by leveraging a trained CNN, where we saw how deeper layers of a network
    extract features that distill information about the content of an image, while
    discarding any extraneous information.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了风格迁移的概念；这是一种旨在将图像的内容与其风格分离的技术。我们讨论了它是如何通过利用一个训练好的 CNN 来实现这一点的，我们看到了网络的深层如何提取关于图像内容的特征，同时丢弃任何无关信息。
- en: Similarly, we saw that shallower layers extracted the finer details, such as
    texture and color, which we could use to isolate the style of a given image by
    looking for the correlations between the feature maps (also known as **convolutional
    kernels** or **filters**) in each layer. These correlations are what we use to
    measure style and how we steer our network. Having isolated the content and style,
    we generated a new image by combining the two.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们看到较浅的层提取了更细微的细节，如纹理和颜色，我们可以通过寻找每个层的特征图（也称为 **卷积核** 或 **过滤器**）之间的相关性来使用这些细节来隔离给定图像的风格。这些相关性就是我们用来衡量风格和引导我们网络的方法。在隔离了内容和风格之后，我们通过结合两者生成了一个新的图像。
- en: We then highlighted the limitations of performing style transfer in real time
    (with current technologies) and introduced a slight variation. Instead of optimizing
    the style and content each time, we could train a model to learn a particular
    style. This would allow us to generate a stylized image for a given image with
    a single pass through the network, as we have done with many other examples we
    have worked through in this book.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们指出了在实时进行风格迁移（使用当前技术）的局限性，并介绍了一个轻微的变化。我们不是每次都优化风格和内容，而是训练一个模型来学习特定的风格。这将允许我们通过网络的单次通过为给定的图像生成一个风格化的图像，正如我们在本书中处理的其他许多示例中所做的那样。
- en: Having introduced the concepts, we then walked through converting the Keras
    model to Core ML and used this as an opportunity to implement custom layers, a
    Swift-centric way of implementing layers that have no direct mapping between the
    machine learning framework and Core ML. Having implemented custom layers, we then
    spent some time looking at how we can optimize them using the `Accelerate` (SIMD)
    and `Metal` frameworks (GPU).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了这些概念之后，我们接着展示了如何将 Keras 模型转换为 Core ML，并借此机会实现自定义层，这是一种以 Swift 为中心的实现层的方法，这些层在机器学习框架和
    Core ML 之间没有直接的映射。在实现了自定义层之后，我们花了些时间研究如何使用 `Accelerate`（SIMD）和 `Metal` 框架（GPU）来优化它们。
- en: The theme of optimization continued into the next section, where we discussed
    some of the tools available for reducing a model's size; there, we looked at two
    approaches and how we could make use of them using the Core ML Tools package along
    with a cautionary warning of the trade-off between size and accuracy.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的主题延续到下一节，我们讨论了一些可用于减少模型大小的工具；在那里，我们研究了两种方法，以及我们如何使用 Core ML 工具包以及一个关于大小和精度之间权衡的警告来利用它们。
- en: In the next chapter, we look at how we can apply what we have learned to recognizing
    user sketches.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何将我们所学应用到识别用户草图。
