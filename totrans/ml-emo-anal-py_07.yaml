- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Support Vector Machines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: In [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes*, we looked
    at using Bayes’ Theorem to find the emotions that are associated with individual
    tweets. The conclusion there was that the standard Naive Bayes algorithm worked
    well with some datasets and less well with others. In the following chapters,
    we will look at several other algorithms to see whether we can get any improvements,
    starting in this chapter with the well-known **support vector machine** (**SVM**)
    (Boser et al., 1992) approach.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B18714_06.xhtml#_idTextAnchor134)中，*朴素贝叶斯*，我们探讨了使用贝叶斯定理来查找与单个推文相关的情绪。那里的结论是，标准的朴素贝叶斯算法对某些数据集效果良好，而对其他数据集效果较差。在接下来的章节中，我们将探讨几种其他算法，看看我们是否能取得任何改进，本章将从众所周知的**支持向量机**（SVM）（Boser等人，1992）方法开始。
- en: We will start this chapter by giving a brief introduction to SVMs. This introduction
    will take a geometric approach that may be easier for you than the standard presentation.
    Bennett and Bredensteiner (see the *References* section) give detailed formal
    proof that the two approaches are equivalent – the discussion in this chapter
    is intended simply to provide an intuitive grasp of the issues. We will then show
    you how to use the `sklearn.svm.LinearSVC` implementation for our current task.
    As with the previous approaches, we will start with a simple application of the
    approach that will work well for some examples but less so for others; we will
    then introduce two ways of refining the approach to work with multi-label datasets,
    and finally, we will reflect on the results we have obtained.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章开始于对SVMs的简要介绍。这种介绍将采用几何方法，这可能比标准的介绍更容易理解。Bennett和Bredensteiner（见*参考文献*部分）给出了两个方法等效的详细形式证明——本章的讨论旨在简单地提供对问题的直观理解。然后，我们将向您展示如何使用`sklearn.svm.LinearSVC`实现来处理我们的当前任务。与先前的方法一样，我们将从一个简单应用该方法开始，该方法对某些示例效果良好，但对其他示例效果较差；然后，我们将介绍两种改进方法，以便与多标签数据集一起使用，最后，我们将反思我们获得的结果。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The basic ideas behind SVMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM背后的基本思想
- en: Application of simple SVMs for standard datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单SVMs在标准数据集上的应用
- en: Ways of extending SVMs to cover multi-label datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将SVM扩展到覆盖多标签数据集的方法
- en: A geometric introduction to SVMs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVMs的几何介绍
- en: 'Suppose we have two groups of entities, called B and R, where each entity is
    described by a pair of numerical coordinates. B includes objects with coordinates
    (6.38, -10.62), (4.29, -8.99), (8.68, -4.54), and so on and R contains objects
    with coordinates (6.50, -3.82), (7.39, -3.13), (7.64, -10.02), and so on (the
    example used in this discussion has been taken from [https://scikit-learn.org/stable/modules/svm.xhtml#classification](https://scikit-learn.org/stable/modules/svm.xhtml#classification)).
    Plotting these points on a graph gives us the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个实体组，称为B和R，其中每个实体都由一对数值坐标描述。B包括坐标为（6.38，-10.62）、（4.29，-8.99）、（8.68，-4.54）等的对象，而R包含坐标为（6.50，-3.82）、（7.39，-3.13）、（7.64，-10.02）等的对象（本讨论中使用的示例已从[https://scikit-learn.org/stable/modules/svm.xhtml#classification](https://scikit-learn.org/stable/modules/svm.xhtml#classification)中选取）。将这些点绘制在图上，我们得到以下结果：
- en: '![Figure 7.1 – Plot of the R and B points](img/B18714_07_01.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – R点和B点的绘图](img/B18714_07_01.jpg)'
- en: Figure 7.1 – Plot of the R and B points
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – R点和B点的绘图
- en: It looks as though you should be able to draw a straight line to separate the
    two groups, and if you could, then you could use it to decide whether some new
    point was an instance of R or B.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来你应该能够画一条直线来分隔两组，如果你能这样做，那么你可以用它来决定某个新点是否是R或B的实例。
- en: There are numerous ways of finding such a line for a simple case like this.
    One approach would be to find the **convex hulls** (Graham, 1972) for the two
    groups – that is, the polygons that include them. The easiest way to visualize
    this involves taking the leftmost point in the set as a starting point. Then,
    you should pick the most clockwise point from there and set that as the next point
    on the list, and then do the same again with that one until you get back to the
    original.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如此简单的情况，有许多找到这样一条线的方法。一种方法是为两组找到**凸包**（Graham，1972）——即包括它们的 polygons。可视化这种方法的简单方式是，从集合中的最左端点作为起点。然后，你应该从那里选择最顺时针的点，并将其设置为列表中的下一个点，然后再次用那个点做同样的事情，直到你回到起点。
- en: 'To see how to pick the most clockwise point from a given starting point, consider
    the two diagrams shown here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何从一个给定的起点选择最顺时针的点，请考虑这里显示的两个图表：
- en: '![Figure 7.2 – Turning counter-clockwise and clockwise](img/B18714_07_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 逆时针和顺时针转向](img/B18714_07_02.jpg)'
- en: Figure 7.2 – Turning counter-clockwise and clockwise
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 逆时针和顺时针转向
- en: 'In the left-hand diagram, the slope from A to B is less steep than the slope
    from B to C, which means that you have to turn counter-clockwise when you get
    to B if you want to go from A to B to C, which, in turn, means that C is further
    counter-clockwise from A than B is. In the right-hand diagram, the slope from
    A’ to B’ is steeper than the slope from B’ to C’, which means that C’ is less
    counter-clockwise from A’ than B’ is. Thus, to see whether C is more or less counter-clockwise
    from A than B is, we need to calculate the slopes of the lines joining them and
    see which is steeper: the slope of the line from A to C is (C[1]-A[1])/(C[0]-A[0]),
    and likewise for the line joining A and B, so C is further counter-clockwise from
    A than B if (C[1]-A[1])/(C[0]-A[0]) > (B[1]-A[1])/(B[0]-A[0]). Rearranging this
    gives us `ccw`, as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧图中，从 A 到 B 的斜率比从 B 到 C 的斜率平缓，这意味着如果你想从 A 到 B 到 C，你到达 B 时必须逆时针转向，这反过来又意味着
    C 比 B 更远离 A。在右侧图中，从 A' 到 B' 的斜率比从 B' 到 C' 的斜率更陡，这意味着 C' 比 B' 更远离 A'。因此，为了判断 C
    比 B 更远离 A 还是更近，我们需要计算连接它们的线的斜率，并看哪条更陡：从 A 到 C 的线的斜率是 (C[1]-A[1])/(C[0]-A[0])，同样对于连接
    A 和 B 的线也是如此，所以如果 (C[1]-A[1])/(C[0]-A[0]) > (B[1]-A[1])/(B[0]-A[0])，则 C 比 B 更远离
    A。重新排列这个公式，我们得到 `ccw`，如下所示：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then use this to find the convex hull. We sort the points by their *Y*
    and *X* coordinates, which lets us find the lowest point, `p` (picking the leftmost
    of these if there is a tie). This point must lie on the hull, so we add it to
    the hull. We then pick the next item, `q`, in the list of points (or go back to
    the beginning if `p` was the last point – `(p+1)%n` will be 0 if `p` is `n` and
    `p+1` otherwise). We now go through the entire list of points starting at `q`
    using `ccw` to see whether going from `p` to `i` to `q` satisfies the constraint
    given previously: if it does, then `i` is further counter-clockwise from `p` than
    `q` is, so we replace `q` with it. At the end of this, we know that `q` is the
    furthest counter-clockwise point from `p`, so we add it to the hull and continue.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个算法来找到凸包。我们按点的 *Y* 和 *X* 坐标对点进行排序，这使我们能够找到最低点 `p`（如果有平局，则选择最左边的点）。这个点必须位于凸包上，因此我们将其添加到凸包中。然后，我们从点的列表中选择下一个项目
    `q`（如果 `p` 是最后一个点，则回到列表的开始 – 如果 `p` 是 `n`，则 `(p+1)%n` 将为 0，否则为 `p+1`）。现在，我们从 `q`
    开始遍历整个点的列表，使用 `ccw` 来判断从 `p` 到 `i` 到 `q` 是否满足之前给出的约束：如果满足，则 `i` 比 `q` 更远离 `p`，因此我们用
    `i` 替换 `q`。在这个过程中，我们知道 `q` 是从 `p` 最远离的点，因此我们将其添加到凸包中并继续。
- en: 'The complexity of this algorithm is *o(H*N)*, where *H* is the size of the
    hull and *N* is the total number of points – *H* because the main loop terminates
    after the hull has been constructed by adding one item for each iteration, *N*
    because on each pass through the main loop, we have to look at every point to
    find the most counter-clockwise one. There are more complicated algorithms that
    are more efficient than this under certain circumstances, but the one given here
    is efficient enough for our purposes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的复杂度为 *o(H*N)*，其中 *H* 是凸包的大小，*N* 是点的总数 – *H* 因为主循环在添加每个迭代一个项目后终止，构建了凸包，*N*
    因为在主循环的每次遍历中，我们必须查看每个点以找到最远离的点。在某些情况下，存在更复杂的算法，其效率高于此，但这里给出的算法对于我们的目的来说已经足够高效：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following figure illustrates how this algorithm progresses around the set
    of B points in our example:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了该算法在我们示例中围绕 B 点集的进展情况：
- en: '![Figure 7.3 – Growing the convex hull for B](img/B18714_07_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 为 B 增长凸包](img/B18714_07_03.jpg)'
- en: Figure 7.3 – Growing the convex hull for B
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 为 B 增长凸包
- en: 'There are more efficient algorithms for growing the hull (see `scipy.spatial.ConvexHull`:
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml)),
    but this one is simple to understand. We use it to calculate the convex hulls
    of R and B:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 存在更多用于生长船体的有效算法（参见 `scipy.spatial.ConvexHull`：[https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml)），但这个算法易于理解。我们用它来计算
    R 和 B 的凸包：
- en: '![Figure 7.4 – Convex hulls of B and R](img/B18714_07_04.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – B 和 R 的凸包](img/B18714_07_04.jpg)'
- en: Figure 7.4 – Convex hulls of B and R
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – B 和 R 的凸包
- en: 'If any lines separate R and B (if they are **linearly separable**), then at
    least one of the segments of the convex hull must be one. If we pick the edge
    from the hull of B that is nearest to some edge from the hull of R, we can see
    that it is a separator – all the B items are above or on the dashed orange line
    and all the R ones are below it, and likewise, all the R items are on or below
    the dotted green line and all the B ones are above it:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何线可以分隔R和B（如果它们是**线性可分**的），那么凸包的至少一个段必须是。如果我们选择B的凸包上离R的凸包上某条边最近的边，我们可以看到它是一个分隔符——所有的B项目都在虚线橙色线以上或在其上，同样，所有的R项目都在虚线绿色线以下或在其以下：
- en: '![Figure 7.5 – Hull segments as candidate separators for R and B](img/B18714_07_05.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 作为R和B候选分隔符的边界段](img/B18714_07_05.jpg)'
- en: Figure 7.5 – Hull segments as candidate separators for R and B
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 作为R和B候选分隔符的边界段
- en: But they are not very good separators. All items that fell just below the dashed
    orange line would be classified as R, even if they were only just below this line
    and hence were much nearer to the Bs than to the Rs; and all items that appeared
    just above the dotted green line would be classified as B, even if they were only
    just above it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们并不是很好的分隔符。所有刚好低于虚线橙色线的项目都会被分类为R，即使它们只是刚好低于这条线，因此它们比R更接近B；并且所有刚好高于虚线绿色线的项目都会被分类为B，即使它们只是刚好高于它。
- en: 'So, we want some way of finding a separator that will deal with cases that
    fall between these two extreme lines appropriately. We can do this by finding
    the line from the closest point on one of the segments to the other (the dotted
    gray line) and then drawing our separator through the middle of, and perpendicular
    to, this line (the solid black line):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望找到一种方法来找到一个分隔符，可以适当地处理介于这两条极端线之间的案例。我们可以通过找到一条线（虚线灰色线）从一条段的最接近点到另一条（另一条段）来做到这一点，然后在这条线的中间和垂直于这条线的地方画我们的分隔符（实线黑色线）：
- en: '![Figure 7.6 – Optimal separator for R and B](img/B18714_07_06.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – R和B的最佳分隔符](img/B18714_07_06.jpg)'
- en: Figure 7.6 – Optimal separator for R and B
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – R和B的最佳分隔符
- en: 'The solid black line is an optimal separator in that it makes the separation
    between the two groups as great as possible: the distance from the nearest point
    in each group to the line is as great as possible, so any unseen point that falls
    above it will be assigned to B, which is the best place for it to go, and any
    point that falls below it will be assigned to R.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实线黑色线是一个最佳分隔符，因为它使得两组之间的分离尽可能大：每个组中离线最近的点的距离尽可能大，因此任何落在它上面的未观察到的点将被分配到B，这是它最好的去处，任何落在它下面的点将被分配到R。
- en: 'This is what we want. Unfortunately, it will all go wrong if some points are
    outliers – that is, if some R points fall within the main body of B points or
    some B points fall within the main body of R points. In the following example,
    we have switched two so that there is a B near the top left of the Rs and an R
    near the bottom of the Bs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们想要的。不幸的是，如果有些点是异常值——也就是说，如果有些R点落在B点的主体内或有些B点落在R点的主体内，那么一切都会出错。在以下示例中，我们切换了两个点，使得有一个B点位于R点的左上角附近，有一个R点位于B点的底部附近：
- en: '![Figure 7.7 – One R point and one B point switched](img/B18714_07_07.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 一个R点和一个B点被切换](img/B18714_07_07.jpg)'
- en: Figure 7.7 – One R point and one B point switched
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 一个R点和一个B点被切换
- en: 'The convex hulls of the two groups now overlap, and cannot sensibly be used
    for finding separators:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 两组的凸包现在重叠，不能合理地用于寻找分隔符：
- en: '![Figure 7.8 – Convex hulls with outliers](img/B18714_07_08.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 带有异常值的凸包](img/B18714_07_08.jpg)'
- en: Figure 7.8 – Convex hulls with outliers
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 带有异常值的凸包
- en: 'We can try to identify outliers and exclude them from their groups, for example,
    by finding the center of mass of the entire group, marked as black ovals, and
    removing a point from it if it is nearer to the center of mass of the other group:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试识别异常值并将它们从其组中排除，例如，通过找到整个组的质量中心，标记为黑色椭圆，如果它离另一个组的质量中心更近，则从其中移除一个点：
- en: '![Figure 7.9 – Centers of mass of the two groups](img/B18714_07_09.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图7.9 – 两组的重心](img/B18714_07_09.jpg)'
- en: Figure 7.9 – Centers of mass of the two groups
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 两组的重心
- en: 'It is clear that the outliers are nearer to the centers of mass of the “wrong”
    groups, and hence can be identified and removed from consideration when trying
    to find the separators. If we remove them both, we get non-overlapping hulls.
    The separator does not put the outliers on the “right” sides, but then no straight
    line could do that – any straight line that included the outlying R with the other
    Rs would have to include the outlying B and vice versa:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，异常点更接近“错误”组的质心，因此可以在尝试找到分隔符时识别并移除它们。如果我们都移除它们，我们就会得到非重叠的凸包。分隔符没有将异常点放在“正确”的一侧，但那时任何直线都无法做到这一点——任何包含异常R和其他R的直线都必须包含异常B，反之亦然：
- en: '![Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two
    outliers (brown Xs are ignored)](img/B18714_07_10.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图7.10 – 忽略两个异常点（棕色X被忽略）的非重叠凸包和分隔符](img/B18714_07_10.jpg)'
- en: Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two
    outliers (brown Xs are ignored)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 忽略两个异常点（棕色X被忽略）的非重叠凸包和分隔符
- en: 'However, we can also get non-overlapping hulls by ignoring just one of them:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也可以通过只忽略其中一个来得到非重叠的凸包：
- en: '![Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying
    R (left) and B (right) points](img/B18714_07_11.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11 – 忽略R（左）和B（右）异常点的非重叠凸包和分隔符](img/B18714_07_11.jpg)'
- en: Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying
    R (left) and B (right) points
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 忽略R（左）和B（右）异常点的非重叠凸包和分隔符
- en: This time, we are ignoring just one of the outlying points when calculating
    the hulls. In each case, we get two non-overlapping hulls and a separator, but
    this time, we have used more of the original data points, with just one of them
    lying on the wrong side of the line.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们在计算凸包时只忽略一个异常点。在每种情况下，我们都得到两个非重叠的凸包和一个分隔符，但这次，我们使用了更多的原始数据点，只有一个点落在线的错误一侧。
- en: Which is better? To use more of the data points when trying to find the separator,
    to have more of the data points lying on the right-hand side of the line, or to
    minimize the total distance between the line and the points in the two sets? If
    we do decide to ignore a point, which one should we choose (is the separator at
    the top of *Figure 7**.11* better than the one at the bottom)? Is the bottom left
    B point a normal member of B or is it an outlier? If we did not have the outlying
    R point then there would never have been any reason to doubt that this one was
    indeed a B.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个更好？在尝试找到分隔符时使用更多的数据点，让更多的数据点落在线的右侧，还是最小化线与两个集合中点之间的总距离？如果我们决定忽略一个点，我们应该选择哪一个（*图7.11*顶部的分隔符比底部的更好吗）？左下角的B点是不是B的正常成员，还是它是异常点？如果我们没有R的异常点，那么就永远不会有任何理由怀疑这个确实是B。
- en: We need to have a view of how important each of these issues is, and then we
    have to optimize our choice of points to get the best outcome. That turns this
    into an optimization algorithm, where we make successive changes to the sets of
    points to try to optimize the criteria given previously – how many points are
    included, how close the separator is to the nearest points, and in the most general
    case what the equation that defines the separator should be (there are tricks
    to allow for circular or curved separators or to allow the separator to be a bit
    bendy). If the separating line is straight – that is, the classes are linearly
    separable – then for the 2-dimensional cases, the line will have an equation such
    as ![<mml:math  ><mml:mi>A</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/15.png).
    When we move to three dimensions, the separator becomes a plane with an equation
    such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/16.png).
    When we move to even higher dimensions, it becomes a **hyperplane**, with an equation
    such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math>](img/17.png)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解每个问题的重要性，然后我们必须优化我们的点选择以获得最佳结果。这使它成为一个优化算法，其中我们通过连续改变点的集合来尝试优化先前给出的标准——包含多少个点，分隔线与最近点有多接近，以及在最一般的情况下，定义分隔线的方程应该是什么（有一些技巧可以允许圆形或曲线分隔线，或者允许分隔线稍微弯曲）。如果分隔线是直的——也就是说，类别是线性可分的——那么对于二维情况，线将有一个如![<mml:math  ><mml:mi>A</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/15.png)的方程。当我们移动到三维时，分隔线变成一个如![<mml:math  ><mml:mi>A</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/16.png)的平面。当我们移动到更高的维度时，它变成一个**超平面**，其方程如![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math>](img/17.png)
- en: The simple programs that we used for illustration purposes are not powerful
    enough to deal with all these issues. We are going to want to work with very high-dimensional
    spaces where, in most cases, most of the dimensions are zero. Fortunately, there
    are plenty of efficient implementations that we can use. We will use the Python
    `LinearSVC` implementation from `sklearn.svm` – there are plenty of other implementations
    in Python, but `sklearn` packages tend to be stable and well-integrated with other
    parts of `sklearn`, and `LinearSVC` is known to be particularly efficient for
    large sparse linear tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于说明目的的简单程序不足以处理所有这些问题。我们将希望与非常高维的空间一起工作，在大多数情况下，大多数维度都是零。幸运的是，有许多高效的实现可供我们使用。我们将使用来自`sklearn.svm`的Python
    `LinearSVC`实现——Python中还有许多其他实现，但`sklearn`包通常很稳定，并且与其他`sklearn`部分很好地集成，而且`LinearSVC`已知特别适用于大型稀疏线性任务。
- en: Using SVMs for sentiment mining
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机进行情感挖掘
- en: We have now seen how SVMs provide classifiers by finding hyperplanes that separate
    the data into classes and have seen a graphical explanation of how such hyperplanes
    can be found, even when the data is not linearly separable. Now, we’ll look at
    how SVMs can be applied to our datasets to find the boundaries between sentiments,
    with an analysis of their behavior on single-label and multi-label datasets and
    a preliminary investigation into how their performance on multi-label datasets
    might be improved.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到SVM如何通过找到将数据分离成类别的超平面来提供分类器，并看到了即使数据不是线性可分时如何找到这样的超平面的图形解释。现在，我们将探讨如何将SVM应用于我们的数据集以找到情感之间的边界，分析它们在单标签和多标签数据集上的行为，并对如何提高多标签数据集上的性能进行初步研究。
- en: Applying our SVMs
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用我们的SVM
- en: 'As with the previous classifiers, we can define the `SVMCLASSIFIERs` class
    as a subclass of `SKLEARNCLASSIFIER` by using the following initialization code
    (`useDF` is a flag to decide whether to use the TF-IDF algorithm from [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*
    when building the training set; `max_iter` sets an upper bound on the number of
    iterations the SVM algorithm should carry out – for our examples, the scores tend
    to converge by 2,000 steps, so we generally use that as the limit):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的分类器一样，我们可以通过以下初始化代码定义`SVMCLASSIFIERs`类作为`SKLEARNCLASSIFIER`的子类（`useDF`是一个标志，用于决定在构建训练集时是否使用来自[*第5章*](B18714_05.xhtml#_idTextAnchor116)*，情感词典和向量空间模型*的TF-IDF算法；`max_iter`设置SVM算法应执行的迭代次数的上限——在我们的例子中，分数通常在2,000步时收敛，所以我们通常将其作为限制）：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is exactly like the constructor for `NBCLASSIFIERS` – just use `readTrainingData`
    to get the data into the right format and then use the `sklearn` implementation
    to construct the SVM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这与`NBCLASSIFIERS`的构造函数非常相似——只需使用`readTrainingData`将数据格式化，然后使用`sklearn`实现来构建SVM。
- en: 'As usual, we start by applying this to our standard datasets:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先将此方法应用于我们的标准数据集：
- en: '|  | Precision | Recall | micro F1 | macro F1 | Jaccard |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 精确率 | 召回率 | 微平均F1 | 宏平均F1 | Jaccard |'
- en: '| **SEM4-EN** | **0.916** | **0.916** | **0.916** | **0.916** | **0.845** |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-EN** | **0.916** | **0.916** | **0.916** | **0.916** | **0.845** |'
- en: '| SEM11-EN | 0.620 | 0.260 | 0.366 | 0.372 | 0.224 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.620 | 0.260 | 0.366 | 0.372 | 0.224 |'
- en: '| **WASSA-EN** | **0.870** | **0.870** | **0.870** | **0.870** | **0.770**
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **WASSA-EN** | **0.870** | **0.870** | **0.870** | **0.870** | **0.770**
    |'
- en: '| CARER-EN | 0.870 | 0.870 | 0.870 | 0.870 | 0.770 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.870 | 0.870 | 0.870 | 0.870 | 0.770 |'
- en: '| IMDB-EN | 0.848 | 0.848 | 0.848 | 0.848 | 0.736 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.848 | 0.848 | 0.848 | 0.848 | 0.736 |'
- en: '| **SEM4-AR** | **0.679** | **0.679** | **0.679** | **0.679** | **0.514** |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **SEM4-AR** | **0.679** | **0.679** | **0.679** | **0.679** | **0.514** |'
- en: '| SEM11-AR | 0.586 | 0.255 | 0.356 | 0.367 | 0.216 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.586 | 0.255 | 0.356 | 0.367 | 0.216 |'
- en: '| KWT.M-AR | 0.781 | 0.767 | 0.774 | 0.778 | 0.631 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.781 | 0.767 | 0.774 | 0.778 | 0.631 |'
- en: '| SEM4-ES | 0.592 | 0.574 | 0.583 | 0.494 | 0.412 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | 0.592 | 0.574 | 0.583 | 0.494 | 0.412 |'
- en: '| SEM11-ES | 0.493 | 0.295 | 0.369 | 0.372 | 0.226 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.493 | 0.295 | 0.369 | 0.372 | 0.226 |'
- en: Figure 7.12 – SVM applied to the standard datasets
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – SVM应用于标准数据集
- en: The basic SVM gets us the best scores we have seen so far for the WASSA and
    two of the SEM4 datasets, with scores for most of the other datasets that are
    close to the best we have obtained so far. As with the previous algorithms, if
    we use it with the standard settings, it does very poorly on the multi-label problems,
    simply because an algorithm that returns exactly one label will fail to cope with
    datasets where items can have zero or more than one label.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基本SVM为我们带来了迄今为止WASSA和两个SEM4数据集的最佳分数，对于大多数其他数据集的分数也接近我们迄今为止获得的最佳分数。与之前的算法一样，如果我们使用标准设置，它在多标签问题上的表现非常差，仅仅是因为返回确切一个标签的算法无法处理具有零个或多个标签的项目集。
- en: Training an SVM takes significantly longer than training any of the classifiers
    we have looked at so far. Therefore, it is worth looking briefly at how accuracy
    and training time for CARER vary as we vary the size of the training data – if
    we find that having more data has little effect on accuracy but makes training
    take much longer, we may decide that getting more data is not worth the bother.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练SVM比我们迄今为止看到的任何分类器都要长得多。因此，简要地看一下随着训练数据大小的变化，CARER的准确性和训练时间如何变化是值得的——如果我们发现更多的数据对准确性影响不大，但会使训练时间大大增加，我们可能会决定获取更多数据不值得麻烦。
- en: 'When we plot accuracy (because there is no empty class, the values of recall
    and macro- and micro-F measure are all the same) and Jaccard score for CARER against
    the size of the training set, we see that we do not need the entire dataset –
    these two measures converge fairly rapidly, and if anything, performance starts
    to go down after a while:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将准确率（因为没有任何空类，召回率和宏平均及微平均F度量值都相同）和Jaccard分数与训练集的大小进行对比时，我们发现我们不需要整个数据集——这两个度量值相当快地趋于一致，而且如果有什么的话，性能在一段时间后开始下降：
- en: '![Figure 7.13 – Accuracy versus training data size for SVMs](img/B18714_07_13.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13 – SVM的准确率与训练数据大小的关系](img/B18714_07_13.jpg)'
- en: Figure 7.13 – Accuracy versus training data size for SVMs
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – SVM的准确率与训练数据大小的关系
- en: The decrease in performance after about 30K tweets could just be noise or it
    could be the result of over-training – as machine learning algorithms see more
    and more data, they can start to pick up on things that are specific to the training
    data and are not present in the test data. Either way, the performance is substantially
    better than anything we saw in *Chapters 5* and *6* and appears to have leveled
    out at around 0.89 accuracy and 0.80 Jaccard.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约30K条推文之后性能的下降可能只是噪声，也可能是过拟合的结果——随着机器学习算法看到越来越多的数据，它们可以开始注意到训练数据中特有的东西，而这些东西在测试数据中并不存在。无论如何，性能显著优于我们在*第5章*和*第6章*中看到的所有内容，并且似乎在约0.89的准确率和0.80的Jaccard分数上趋于平稳。
- en: 'We also plotted training time against data size to see whether it would be
    feasible to run it with more data if we could get it, and the time went up more
    or less linearly with the data size (this is commonly reported for this kind of
    problem). However, since the accuracy has already leveled off by about 40K, it
    seems unlikely that adding more data would make any difference anyway:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还绘制了训练时间与数据大小的关系图，以查看如果我们能获得更多数据，是否可行运行它，时间大致与数据大小呈线性增长（这类问题通常有这种报道）。然而，由于准确率已经在约40K时趋于平稳，因此增加更多数据似乎不会产生任何影响：
- en: '![Figure 7.14 – Training time versus data size for SVMs](img/B18714_07_14.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – SVM的训练时间与数据大小的关系](img/B18714_07_14.jpg)'
- en: Figure 7.14 – Training time versus data size for SVMs
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – SVM的训练时间与数据大小的关系
- en: Note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy that’s obtained by any classifier with any amount of data must
    level off before reaching 1\. Most of the curves that can easily be fitted to
    plots such as the one shown here (for example, polynomial curves) continue increasing
    as the *X*-value increases, so they can only be approximations to the real curve,
    but they do let you get an impression of whether increasing the amount of training
    data is worthwhile.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分类器在拥有任何数量的数据后，其准确率都必须在达到1之前趋于平稳。大多数可以轻易拟合到如图所示（例如，多项式曲线）的图表中的曲线，随着X值的增加而持续增加，因此它们只能是对真实曲线的近似，但它们确实让你对增加训练数据量是否值得有一个印象。
- en: Experiments with datasets with tweets with no labels and tweets with multiple
    labels
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对无标签推文和有多个标签的推文数据集进行的实验
- en: Why is the behavior of the standard SVM so much worse for the multi-label datasets
    than for most of the others?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么标准支持向量机（SVM）在多标签数据集上的表现比大多数其他情况都要差得多？
- en: SVMs, like any other standard classifier, are designed for assigning each item
    to a single class. Each point in the training set is given a set of features and
    a label and the learning algorithm works out how the features and labels are connected.
    It is fairly straightforward to adapt this to include data where some points have
    no label, simply by saying that there is an extra label, called something such
    as neutral, or none of the above, or something like that, to be used when a point
    has not been given a label. This is a slightly artificial way to proceed because
    it means that the classifier finds words that are associated with having no emotion,
    whereas the real situation is that such points simply don’t have any words that
    carry emotional weight. However, it usually works reasonably well and can be assimilated
    into the standard SVM training algorithm (the definition of `onehot2value` shown
    in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes*, allows for
    exactly this kind of situation).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs（支持向量机）和其他任何标准分类器一样，旨在将每个项目分配到单个类别。训练集中的每个点都有一组特征和标签，学习算法会找出特征和标签之间的联系。将这种方法适应包括某些点没有标签的数据相当直接，只需说有一个额外的标签，比如中性、以上皆非，或者类似的东西，当某个点没有给出标签时使用。这是一种稍微有些人为的方法，因为它意味着分类器找到与没有情绪相关的单词，而实际情况是，这样的点根本没有任何带有情感重量的单词。然而，这通常效果不错，并且可以被纳入标准的SVM训练算法（[*第6章*](B18714_06.xhtml#_idTextAnchor134)，*朴素贝叶斯*中显示的`onehot2value`的定义允许这种情况）。
- en: 'The SEM11 and KWT.M-AR examples belong to a harder, and arguably more realistic,
    class of problem, where a single tweet may express zero *or more* emotions. The
    second tweet in the test set for SEM11-EN, *I’m doing all this to make sure you
    smiling down on me bro*, expresses all three of joy, love, and optimism, and the
    second to last, *# ThingsIveLearned The wise # shepherd never trusts his flock
    to a # smiling wolf. # TeamFollowBack # fact # wisewords*, has no emotion linked
    to it.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'SEM11和KWT.M-AR示例属于更难、也许更现实的类别问题，其中一条推文可能表达零个或多个情绪。SEM11-EN测试集中的第二条推文，“我正在做这一切以确保你对我微笑，兄弟”，表达了快乐、爱和乐观这三种情绪，而倒数第二条，“#
    ThingsIveLearned 聪明的牧羊人永远不会把羊群托付给一个微笑的狼。# TeamFollowBack # fact # wisewords”，与之没有关联的情绪。'
- en: 'It is easy enough to encode these as vectors, given the possible set of emotions
    [‘anger’, ‘anticipation’, ‘disgust’, ‘fear’, ‘joy’, ‘love’, ‘optimism’, ‘pessimism’,
    ‘sadness’, ‘surprise’, ‘trust’]: we use [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] for
    joy, love, and optimism and [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0] for no emotion.
    *But these are not one-hot encodings*, and `onehot2value` will not deal with them
    properly. In particular, it will interpret [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] as
    joy since that is the first non-zero item it encounters.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可能的情绪集合[‘愤怒’，‘期待’，‘厌恶’，‘恐惧’，‘快乐’，‘爱’，‘乐观’，‘悲观’，‘悲伤’，‘惊讶’，‘信任’]，将这些情绪编码为向量是足够的：我们用[0,
    0, 0, 0, 1, 1, 1, 0, 0, 0, 0]表示快乐、爱和乐观，用[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]表示没有情绪。*但是这些不是独热编码*，`onehot2value`无法正确处理它们。特别是，它将[0,
    0, 0, 0, 1, 1, 1, 0, 0, 0, 0]解释为快乐，因为这是它遇到的第一个非零项。
- en: There is no simple way around this – SEM11 data has multiple emotions, and SVMs
    expect single emotions. This has two consequences. During training, only one of
    the emotions associated with a tweet will be used – if the preceding example had
    occurred during training, it would have led to the words *smiling* and *bro* (which
    are the only low document frequency words in this tweet) being associated with
    joy but not with love and optimism, which could lead to lower precision and lower
    recall; and if it occurred during testing, then it would inevitably have led to
    a loss of recall because only one of the three could be returned. This is borne
    out by the results shown previously, where SEM11-EN in particular has quite good
    precision but very poor recall, in contrast to the best of the results in [*Chapter
    6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes* where it has good recall
    but poor precision.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 没有简单的解决办法——SEM11数据具有多种情绪，而SVMs期望单一的情绪。这有两个后果。在训练期间，只会使用与推文相关联的情绪中的一个——如果前面的例子发生在训练期间，它会导致单词*smiling*和*bro*（这是这条推文中唯一的低文档频率单词）与快乐相关联，而不是与爱和乐观相关联，这可能导致较低的精确度和较低的召回率；如果在测试期间发生，那么不可避免地会导致召回率的损失，因为只能返回三个中的一个。这在前面的结果中得到了证实，特别是SEM11-EN在[*第6章*](B18714_06.xhtml#_idTextAnchor134)，*朴素贝叶斯*中，它的召回率很好，但精确度较差。
- en: The illustration earlier in this chapter of how SVMs work showed a two-class
    problem – Rs and Bs. There are two obvious ways of coping with the extension to
    multiclass problems (such as the CARER and WASSA datasets). Suppose we had Rs,
    Bs, and Gs. We could train three two-class classifiers, one for R versus B, one
    for R versus G, and one for B versus G (**one versus one**), and combine the results;
    alternatively, we could train a different set of two-class classifiers, one for
    R versus (B or G), one for B versus (R or G), and one for G versus (R or B) (**one
    versus many**). Both give similar results, but when there are a lot of classes,
    you have to train N*(N+1)/2 classifiers (N for the first class versus each of
    the rest + N-1 for the second class versus each of the rest + ...) for one versus
    one but only N for one versus many. For CARER, for instance, where there are six
    classes, we would have to train 21 classifiers and combine their results for one
    versus one, whereas for one versus many, we would only have to train six classifiers
    and combine their results.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前面关于SVM工作原理的插图显示了一个双分类问题——Rs和Bs。有两种明显的方法可以处理扩展到多分类问题（如CARER和WASSA数据集）。假设我们有Rs、Bs和Gs。我们可以训练三个双分类器，一个用于R对B，一个用于R对G，一个用于B对G（**一对多**），然后组合结果；或者，我们可以训练另一组双分类器，一个用于R对（B或G），一个用于B对（R或G），一个用于G对（R或B）（**一对多**）。两者都给出相似的结果，但当有大量类别时，你必须训练N*(N+1)/2个分类器（N为第一个类别与剩余每个类别的组合
    + N-1为第二个类别与剩余每个类别的组合 + ...）用于一对多，但只有N用于一对多。例如，对于有六个类别的CARER，我们不得不训练21个分类器并将它们的结果组合起来用于一对多，而对于一对多，我们只需要训练六个分类器并将它们的结果组合起来。
- en: We need to follow one of these strategies for all our datasets since they all
    have several possible outcomes. Fortunately, `sklearn.svm.LinearSVC` does this
    automatically (using one versus many) for problems where there are a range of
    possible labels. This by itself, however, will not solve the problem of multi-label
    datasets – there is a difference between having an outcome consisting of one label
    drawn from several options and an outcome having an unknown number of labels drawn
    from several options. The standard multiple-classifiers, which are combined as
    one versus one or one versus many, will solve the first of these problems but
    not the second.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有数据集都有几种可能的输出，我们需要为所有数据集遵循以下策略之一。幸运的是，`sklearn.svm.LinearSVC`会自动（使用一对多）为存在一系列可能标签的问题执行此操作。然而，仅此并不能解决多标签数据集的问题——从几个选项中抽取一个标签的输出与从几个选项中抽取未知数量标签的输出是有区别的。标准的多分类器，它们被组合为一对一或一对多，将解决第一个问题但不会解决第二个问题。
- en: 'There are two ways we can adapt our SVM classifier to deal with this problem:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两种方法可以将我们的SVM分类器适应这个问题：
- en: We can follow the same strategy as with the Naive Bayes classifier of taking
    the real-valued scores for each emotion and using a threshold to determine whether
    or not a tweet satisfies each emotion.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以遵循与朴素贝叶斯分类器相同的策略，即对每个情感的真实值得分进行训练，并使用阈值来确定推文是否满足每个情感。
- en: We can train several classifiers, one versus many style, and simply accept the
    results from each of them. In the preceding example, if the R versus (B or G)
    classifier says R, then we accept R as one of the labels for a test case; if B
    versus (R or G) says B, then we accept that *as well*.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以训练多个分类器，采用一对多的风格，并简单地接受它们中的每一个的结果。在前面的例子中，如果R对（B或G）分类器说R，那么我们接受R作为测试案例的一个标签；如果B对（R或G）说B，那么我们也接受它**同样**。
- en: We will look at each of these in turn in the next two sections.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的两个部分中依次探讨这些问题。
- en: Using a standard SVM with a threshold
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用带有阈值的标准SVM
- en: 'To use Naive Bayes with multi-label datasets, we changed `applyToTweet` like
    so:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用朴素贝叶斯处理多标签数据集，我们像这样修改了`applyToTweet`：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code used the fact that `predict_log_proba` returns a value for
    every label. In the standard version of Naive Bayes, we just pick the highest
    scoring label for each case, but using a threshold allows us to pick any number
    of labels, starting from 0.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用了`predict_log_proba`为每个标签返回一个值的事实。在朴素贝叶斯的常规版本中，我们只为每个案例选择得分最高的标签，但使用阈值允许我们选择任意数量的标签，从0开始。
- en: 'This won’t quite work for SVMs because they do not have a method called `predict_log_proba`.
    What they do have is a method called `decision_function`, which produces a score
    for each label. Rather than changing the definition of `applyToTweet` to use `decision_function`
    instead of `predict_log_proba`, we simply set the value of `predict_log_proba`
    to be `decision_function` in the constructor for SVMs, and then use `applyToTweet`,
    as we did previously. So, we must adapt the constructor for SVMs, as shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于SVM来说不太适用，因为它们没有名为`predict_log_proba`的方法。它们所拥有的方法是名为`decision_function`的方法，它为每个标签生成一个分数。我们不是将`applyToTweet`的定义改为使用`decision_function`而不是`predict_log_proba`，而是在SVM的构造函数中简单地将`predict_log_proba`的值设置为`decision_function`，然后使用`applyToTweet`，就像我们之前做的那样。因此，我们必须调整SVM的构造函数，如下所示：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In other words, once we have made the underlying SVM, we must set a couple
    of standard properties that we will find useful and that do not have the same
    names in all the `sklearn` classifiers. The results of this for the multi-label
    cases are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，一旦我们制作了底层的SVM，我们必须设置一些我们将发现有用的标准属性，这些属性在所有`sklearn`分类器中名称并不相同。对于多标签情况的结果如下：
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
- en: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
- en: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
- en: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
- en: Figure 7.15 – SVMs using thresholds to deal with multi-label problems
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 使用阈值处理多标签问题的SVM
- en: The SEM11 cases are better than those for the simple SVM that we looked at earlier
    but are not better than the scores we obtained using the earlier algorithms, and
    the scores for KWT.M-AR are worse than with the simple SVMs. Just using the values
    that the decision function for the SVM assigns to each label does not solve the
    problem of multi-label datasets. We will refer to SVMs that use the set of values
    for each label plus a threshold as SVM (multi) classifiers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: SEM11案例比我们之前查看的简单SVM更好，但并不比我们使用早期算法获得的分数更好，而KWT.M-AR的分数比简单SVM更差。仅仅使用SVM为每个标签分配的决策函数值并不能解决多标签数据集的问题。我们将把使用每个标签的值集加上阈值的SVM称为SVM（多）分类器。
- en: Making multiple SVMs
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 制作多个SVM
- en: 'The second option is to make a set of one versus rest classifiers and accept
    every label for which the relevant classifier succeeds. The key is to take each
    label in turn and `squeeze` the *N* labels in the training data into two – one
    for the target label and one for all the others. Consider a tweet labeled as joy.
    The representation of this as a vector would be [0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
    0] – that is, with 1 in the column for joy. If we squeeze this to be joy versus
    the rest, then it will come out as [1, 0] – that is, with a 1 in the new column
    for joy and a 0 in the column for not-joy. If we squeeze it to be angry versus
    not-angry, then it would be [0, 1], with a 0 in the new column for anger and a
    1 in not-angry. If it had been labeled as joy and love, then the vector would
    have been [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0] and the squeezed version would have
    been [1, 1]: 1 in the first column because it does express joy and 1 in the second
    because it *also* expresses something else.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是制作一系列一对多分类器，并接受所有相关分类器成功的标签。关键是依次处理每个标签，并将训练数据中的*N*个标签压缩成两个——一个用于目标标签，一个用于所有其他标签。考虑一个被标记为快乐的推文。这个推文作为向量的表示将是[0,
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0]——也就是说，在快乐这一列中有一个1。如果我们将其压缩为快乐与剩余标签的对立，那么它将输出为[1,
    0]——也就是说，在新列中快乐这一列有一个1，而在非快乐这一列中有一个0。如果我们将其压缩为愤怒与非愤怒的对立，那么它将是[0, 1]，在新列中愤怒这一列有一个0，而在非愤怒这一列中有一个1。如果它被标记为快乐和爱，那么向量将是[0,
    0, 0, 0, 1, 1, 0, 0, 0, 0, 0]，压缩后的版本将是[1, 1]：第一列中的1因为它确实表达了快乐，第二列中的1因为它*也*表达了其他东西。
- en: 'Suppose we have a vector, `gs`, that represents the emotions for a multi-label
    tweet and we want to squeeze it on the `I` column. The first column is easy –
    we just set it to `gs[i]`. To get the second, which represents whether some column
    other than `I` is non-zero, we use `numpy.sign(sum(gs[:i]+gs[i+1:])`: `gs[:i]`
    and `gs[i+1:]` are the other columns. Taking their sum will be greater than 0
    if at least one of them is non-zero while taking the sign of that will be 0 if
    the sum was 0 and 1 if it was greater than zero. Note that it is possible for
    both `gs[i]` and `numpy.sign(sum(gs[:i]+gs[i+1:])` to be `0` and for both of them
    to be `1`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个向量，`gs`，它代表了一个多标签推文的情感，并且我们想要在`I`列上压缩它。第一列很简单——我们只需将其设置为`gs[i]`。要得到第二列，它代表除了`I`列之外的某些列是否非零，我们使用`numpy.sign(sum(gs[:i]+gs[i+1:]))`：`gs[:i]`和`gs[i+1:]`是其他列。它们的和如果至少有一个非零，则将大于0，而取其符号，如果和为0，则符号为0，如果大于零，则符号为1。请注意，`gs[i]`和`numpy.sign(sum(gs[:i]+gs[i+1:]))`都可能是`0`，同时它们也可能是`1`：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The constructor for `MULTISVMCLASSIFIER` is straightforward – just make one
    standard `SVMCLASSIFIER` for each emotion. To apply one to a tweet, we must apply
    each of the standard ones and gather the positive results. So, if the classifier
    that has been trained on joy versus not-joy says some tweet expresses joy, then
    we mark the tweet as satisfying joy, but we ignore what it says about not-joy
    since a positive score on not-joy simply tells us that the tweet also expresses
    some other emotion, and we are allowing tweets to express more than one emotion:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`MULTISVMCLASSIFIER`的构造函数很简单——只需为每个情感创建一个标准的`SVMCLASSIFIER`。要将其中一个应用于推文，我们必须应用每个标准的，并收集正面的结果。所以，如果训练在快乐与不快乐之间的分类器说某个推文表达了快乐，那么我们就标记这个推文为满足快乐，但我们忽略它关于不快乐的描述，因为不快乐的正分只告诉我们推文也表达了其他某种情感，而我们允许推文表达多种情感：'
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is pretty much the standard one versus many approach to training an SVM
    with multiple labels. The key difference is in the way that the results of the
    individual *X* versus not-*X* classifiers are combined – we accept *all* the positive
    results, whereas the standard approach just accepts one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是标准的一个对多个方法来训练具有多个标签的SVM。关键的区别在于如何组合单个*X*与不-*X*分类器的结果——我们接受*所有*正面的结果，而标准方法只接受一个。
- en: 'The following table is a repeat of the table for the multi-label problems using
    SVM (multi) for comparison:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下表是使用SVM（多）的多标签问题的表的重复，用于比较：
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
- en: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
- en: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
- en: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
- en: Figure 7.16 – Multi-label datasets, SVM (multi)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 多标签数据集，SVM（多）
- en: 'When we use multiple SVMs, one per label, we get an improvement in each case,
    with the score for SEM11-EN being the best so far:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用多个SVM时，每个标签一个，我们会在每个情况下得到改进，其中SEM11-EN的得分目前是最好的：
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| **SEM11-EN** | **0.580** | **0.535** | **0.556** | **0.529** | **0.385**
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **SEM11-EN** | **0.580** | **0.535** | **0.556** | **0.529** | **0.385**
    |'
- en: '| SEM11-AR | 0.531 | 0.485 | 0.507 | 0.478 | 0.340 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.531 | 0.485 | 0.507 | 0.478 | 0.340 |'
- en: '| KWT.M-AR | 0.648 | 0.419 | 0.509 | 0.340 | 0.341 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.648 | 0.419 | 0.509 | 0.340 | 0.341 |'
- en: '| SEM11-ES | 0.498 | 0.368 | 0.423 | 0.378 | 0.268 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.498 | 0.368 | 0.423 | 0.378 | 0.268 |'
- en: Figure 7.17 – Multi-label datasets, multiple SVMs
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – 多标签数据集，多个SVM
- en: This is better than the results for the SVM (multi) case and is the best so
    far for SEM11-EN. The improvement over the SVM for the SEM11 datasets comes from
    the huge improvement in recall. Remember that the standard SVM can only return
    one result per datapoint, so its recall *must* be poor in cases where the Gold
    Standard contains more than one emotion – if a tweet has three emotions associated
    with it and the classifier reports just one, then the recall for that tweet is
    1/3\. The improvement for KWT.M-AR comes from the improvement in precision – if
    a tweet has zero emotions associated with it, as is common in this dataset, then
    the standard SVM must produce a false positive for it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这比SVM（多）案例的结果要好，并且是SEM11-EN迄今为止最好的结果。对于SEM11数据集的SVM改进来自于召回率的巨大提升。记住，标准的SVM只能对每个数据点返回一个结果，所以当黄金标准包含多个情感时，它的召回率*必须*很差——如果一个推文有三个与之相关的情感，而分类器只报告一个，那么该推文的召回率是1/3。KWT.M-AR的改进来自于精度的提升——如果一个推文没有与之相关的情感，这在数据集中很常见，那么标准的SVM必须为它产生一个假阳性。
- en: Numerous tweaks can be applied to `sklearn.svm.LinearSVC`, and we can also try
    the tweaks from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector Space Models* – using IDF to get the feature values, for instance,
    produces a small improvement across the board. These are worth trying once you
    have reasonable results with the default values, but it is easy to get carried
    away trying variations to try to gain a few percentage points on a given dataset.
    For now, we will simply note that even the default values provide good results
    in cases where the dataset has exactly one emotion per tweet, with the multi-SVM
    providing the best results yet for some of the more difficult cases.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对`sklearn.svm.LinearSVC`进行多种调整，我们还可以尝试[*第5章*](B18714_05.xhtml#_idTextAnchor116)*，情感词典和向量空间模型*中的调整方法——例如，使用IDF获取特征值，这在整体上产生了一些小的改进。一旦你使用默认值获得了合理的结果，这些方法都值得一试，但尝试各种变体以在给定数据集上获得几个百分点的提升很容易让人沉迷。目前，我们只需注意，即使默认值在数据集中每条推文恰好有一个情感的情况下也能提供良好的结果，对于一些更困难的情况，多SVM提供了迄今为止最好的结果。
- en: 'Using an SVM can easily be seen as yet another way of extracting a lexicon
    with weights from a corpus. The dimensions of the SVMs that were used in this
    chapter are just the words in the lexicon, and we can play the same games with
    that as in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector Space Models* – using different tokenizers, stemming, and eliminating
    uncommon words. We will not repeat these variations here: we know from [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*
    that different combinations suit different datasets, and simply running through
    all the variations will not tell us anything new. It is, however, worth reflecting
    on exactly how SVMs use the weights that they assign to individual words.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVM可以很容易地被视为从语料库中提取带权重的词典的另一种方式。本章中使用的SVM的维度仅仅是词典中的单词，我们可以像[*第5章*](B18714_05.xhtml#_idTextAnchor116)*，情感词典和向量空间模型*中那样进行相同的操作——使用不同的分词器、词干提取和消除不常见的单词。我们在这里不会重复这些变体：从[*第5章*](B18714_05.xhtml#_idTextAnchor116)*，情感词典和向量空间模型*我们知道不同的组合适合不同的数据集，简单地运行所有变体不会告诉我们任何新的东西。然而，值得思考的是SVM如何使用它们分配给单个单词的权重。
- en: 'The SVM for the CARER dataset, for instance, has an array of six rows by 74,902
    columns as its coefficients: six rows because there are six emotions in this dataset,
    and 75K columns because there are 75K distinct words. If we pick several words
    more or less at random, some of which are associated with some emotion and some
    that have very little emotional significance, we will see that their weights for
    the various emotions reflect our intuition:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CARER数据集的SVM具有一个6行74,902列的系数数组：6行是因为这个数据集中有六个情感，75K列是因为有75K个不同的单词。如果我们随机挑选一些单词，其中一些与某些情感相关，而一些几乎没有情感意义，我们会看到它们对各种情感的权重反映了我们的直觉：
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
- en: '| sorrow | -0.033 | -0.233 | 0.014 | 0.026 | 0.119 | 0.068 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| sorrow | -0.033 | -0.233 | 0.014 | 0.026 | 0.119 | 0.068 |'
- en: '| scared | -0.508 | 1.392 | -1.039 | -0.474 | -0.701 | -0.290 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| scared | -0.508 | 1.392 | -1.039 | -0.474 | -0.701 | -0.290 |'
- en: '| disgust | 1.115 | -0.293 | -0.973 | -0.185 | -0.855 | -0.121 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| disgust | 1.115 | -0.293 | -0.973 | -0.185 | -0.855 | -0.121 |'
- en: '| happy | -0.239 | -0.267 | 0.546 | -0.210 | -0.432 | -0.080 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| happy | -0.239 | -0.267 | 0.546 | -0.210 | -0.432 | -0.080 |'
- en: '| adores | 0.000 | 0.000 | 0.412 | -0.060 | -0.059 | -0.000 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 喜爱 | 0.000 | 0.000 | 0.412 | -0.060 | -0.059 | -0.000 |'
- en: '| and | -0.027 | -0.008 | 0.001 | -0.008 | -0.020 | -0.004 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 和 | -0.027 | -0.008 | 0.001 | -0.008 | -0.020 | -0.004 |'
- en: '| the | 0.001 | -0.012 | -0.004 | 0.001 | -0.002 | -0.002 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 这 | 0.001 | -0.012 | -0.004 | 0.001 | -0.002 | -0.002 |'
- en: Figure 7.18 – Associations between words and emotions, SVM as the classifier,
    with the CARER dataset
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 单词与情感之间的关联，使用SVM作为分类器，基于CARER数据集
- en: '*sorrow* is strongly linked to **sadness**, *scared* is strongly linked to
    **fear**, *disgust* is strongly linked to **anger**, *happy* is strongly linked
    to **joy**, and *adores* is strongly linked to **joy** (but not, interestingly,
    to **love**: words always throw up surprises); and neutral words are not strongly
    linked to any particular emotion. The main thing that is different from the lexicons
    in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134)*, Naive Bayes* is that some
    words also vote very strongly *against* some emotions – if you are *scared*, then
    you are not joyous, and if you are *happy*, then you are not angry, fearful, or
    sad.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*悲伤*与**悲伤**紧密相连，*害怕*与**恐惧**紧密相连，*厌恶*与**愤怒**紧密相连，*快乐*与**喜悦**紧密相连，而*喜爱*与**喜悦**紧密相连（但有趣的是，并不与**爱**相连：单词总是带来惊喜）；中性词并不与任何特定的情感强烈相连。与[*第6章*](B18714_06.xhtml#_idTextAnchor134)*，朴素贝叶斯*中的词典不同之处在于，一些单词也强烈反对某些情感——如果你是*害怕*的，那么你就不是喜悦的，如果你是*快乐*的，那么你就不是愤怒的、恐惧的或悲伤的。'
- en: The way that SVMs use these weights for classification is the same as in [*Chapter
    6*](B18714_06.xhtml#_idTextAnchor134)*, Naive Bayes* – if you are given a vector
    of values, V = [v0, v1, ..., vn], and a set of coefficients, C = [w0, w1, ...,
    wn], then checking whether `V.dot(C)` is greater than some threshold is exactly
    what we did with the weights in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134)*,
    Naive Bayes* (given that V and C are sparse arrays in `sklearn.svm.LinearSVC`,
    this may be a fairly fast way to do this sum, but it is the same sum). The only
    differences lie in the way that SVMs obtain the weights and the fact that an SVM
    can assign negative weights to words. We will return to ways of handling multi-label
    datasets in [*Chapter 10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
    For now, we will just note that SVMs and the simple lexicon-based approaches end
    up using the same decision function on the same features, but that the way that
    SVMs arrive at the weights for those features is generally better, and in some
    cases much better.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs使用这些权重进行分类的方式与[*第6章*](B18714_06.xhtml#_idTextAnchor134)*，朴素贝叶斯*相同，即如果你给定一个值向量V
    = [v0, v1, ..., vn]和一组系数C = [w0, w1, ..., wn]，那么检查`V.dot(C)`是否大于某个阈值，这正是我们在[*第6章*](B18714_06.xhtml#_idTextAnchor134)*，朴素贝叶斯*中处理权重时所做的事情（假设V和C在`sklearn.svm.LinearSVC`中是稀疏数组，这可能是一种相当快速的方式来计算这个和，但它仍然是同一个和）。唯一的区别在于SVMs获取权重的方式以及SVM可以为单词分配负权重。我们将在[*第10章*](B18714_10.xhtml#_idTextAnchor193)，*多分类器*中返回处理多标签数据集的方法。现在，我们只需注意，SVMs和简单的基于词典的方法最终会在相同的特征上使用相同的决策函数，但SVMs到达这些特征的权重的方式通常更好，在某些情况下要好得多。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Figure 7**.17* shows the best classifiers that we have seen so far, with Jaccard
    scores, for each of the datasets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.17*显示了迄今为止我们看到的最佳分类器，以及每个数据集的Jaccard分数：'
- en: '|  | **LEX** | **CP (unstemmed)** | **CP (****stemmed)** | **NB (****single)**
    | **NB (multi)** | **SVM (****single)** | **SVM (****multi)** | **MULTI-SVM**
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX** | **CP (未分词)** | **CP (分词)** | **NB (单)** | **NB (多)** | **SVM
    (单)** | **SVM (多)** | **MULTI-SVM** |'
- en: '| SEM4-EN | 0.497 | 0.593 | 0.593 | 0.775 | 0.778 | *******0.845*** | 0.836
    |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.497 | 0.593 | 0.593 | 0.775 | 0.778 | *******0.845*** | 0.836
    |  |'
- en: '| SEM11-EN | 0.348 | 0.352 | 0.353 | 0.227 | 0.267 | 0.224 | 0.249 | *******0.385***
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.348 | 0.352 | 0.353 | 0.227 | 0.267 | 0.224 | 0.249 | *******0.385***
    |'
- en: '| WASSA-EN | 0.437 | 0.512 | 0.505 | 0.709 | 0.707 | *******0.770*** | 0.749
    |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.437 | 0.512 | 0.505 | 0.709 | 0.707 | *******0.770*** | 0.749
    |  |'
- en: '| CARER-EN | 0.350 | 0.414 | 0.395 | 0.776 | 0.774 | 0.770 | *******0.796***
    |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.350 | 0.414 | 0.395 | 0.776 | 0.774 | 0.770 | *******0.796***
    |  |'
- en: '| IMDB-EN | 0.667 | 0.721 | 0.722 | 0.738 | *******0.740*** | 0.736 | 0.736
    |  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.667 | 0.721 | 0.722 | 0.738 | *******0.740*** | 0.736 | 0.736
    |  |'
- en: '| SEM4-AR | 0.509 | 0.493 | 0.513 | 0.531 | *******0.532*** | 0.514 | 0.494
    |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.509 | 0.493 | 0.513 | 0.531 | *******0.532*** | 0.514 | 0.494
    |  |'
- en: '| SEM11-AR | *******0.386*** | 0.370 | 0.382 | 0.236 | 0.274 | 0.216 | 0.229
    | 0.340 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | *******0.386*** | 0.370 | 0.382 | 0.236 | 0.274 | 0.216 | 0.229
    | 0.340 |'
- en: '| KWT.M-AR | 0.663 | *******0.684*** | 0.666 | 0.494 | 0.507 | 0.631 | 0.128
    | 0.341 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.663 | *******0.684*** | 0.666 | 0.494 | 0.507 | 0.631 | 0.128
    | 0.341 |'
- en: '| SEM4-ES | *******0.420*** | 0.191 | 0.177 | 0.360 | 0.331 | 0.412 | 0.336
    |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | *******0.420*** | 0.191 | 0.177 | 0.360 | 0.331 | 0.412 | 0.336
    |  |'
- en: '| SEM11-ES | 0.271 | 0.276 | *******0.278*** | 0.230 | 0.255 | 0.226 | 0.235
    | 0.268 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.271 | 0.276 | *******0.278*** | 0.230 | 0.255 | 0.226 | 0.235
    | 0.268 |'
- en: Figure 7.19 – Best classifier for each dataset
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 每个数据集的最佳分类器
- en: 'As we can see, different classifiers work well with different datasets. The
    major lesson here is that you should not just accept that there is a single best
    classification algorithm: do experiments, try out variations, and see for yourself
    what works best with your data. It is also worth noting that the multi-label datasets
    (SEM11-EN, SEM11-AR, SEM11-ES, and KWT.M-AR) score very poorly with simple SVMs,
    and the only one where the multi-SVM wins is SEM11-EN, with simple algorithms
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and
    Vector-Space Models*, still producing the best scores for the other cases.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，不同的分类器与不同的数据集配合得很好。这里的主要教训是，你不应该仅仅接受存在一个最佳的分类算法：进行实验，尝试不同的变体，并亲自看看哪种算法对你的数据效果最好。还值得注意的是，多标签数据集（SEM11-EN、SEM11-AR、SEM11-ES和KWT.M-AR）在使用简单的SVM时得分非常低，唯一一个多SVM获胜的是SEM11-EN，来自[*第5章*](B18714_05.xhtml#_idTextAnchor116)的简单算法，*情感词典和向量空间模型*在其他情况下仍然产生最佳得分。
- en: References
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章所涉及的主题，请查看以下资源：
- en: Bennett, K. P., & Bredensteiner, E. J. (2000). *Duality and Geometry in SVM
    Classifiers*. Proceedings of the Seventeenth International Conference on Machine
    Learning, 57–64.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett, K. P., & Bredensteiner, E. J. (2000). *Duality and Geometry in SVM
    Classifiers*. Proceedings of the Seventeenth International Conference on Machine
    Learning, 57–64.
- en: Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). *A Training Algorithm for
    Optimal Margin Classifiers*. Proceedings of the Fifth Annual Workshop on Computational
    Learning Theory, 144–152\. [https://doi.org/10.1145/130385.130401](https://doi.org/10.1145/130385.130401).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). *A Training Algorithm for
    Optimal Margin Classifiers*. Proceedings of the Fifth Annual Workshop on Computational
    Learning Theory, 144–152\. [https://doi.org/10.1145/130385.130401](https://doi.org/10.1145/130385.130401).
- en: Graham, R. L. (1972). *An efficient algorithm for determining the convex hull
    of a finite planar set*. Information Processing Letters, 1(4), 132–133\. [https://doi.org/10.1016/0020-0190(72)90045-2](https://doi.org/10.1016/0020-0190(72)90045-2).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graham, R. L. (1972). *An efficient algorithm for determining the convex hull
    of a finite planar set*. Information Processing Letters, 1(4), 132–133\. [https://doi.org/10.1016/0020-0190(72)90045-2](https://doi.org/10.1016/0020-0190(72)90045-2).
