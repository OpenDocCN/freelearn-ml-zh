- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes*, we looked
    at using Bayes’ Theorem to find the emotions that are associated with individual
    tweets. The conclusion there was that the standard Naive Bayes algorithm worked
    well with some datasets and less well with others. In the following chapters,
    we will look at several other algorithms to see whether we can get any improvements,
    starting in this chapter with the well-known **support vector machine** (**SVM**)
    (Boser et al., 1992) approach.
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter by giving a brief introduction to SVMs. This introduction
    will take a geometric approach that may be easier for you than the standard presentation.
    Bennett and Bredensteiner (see the *References* section) give detailed formal
    proof that the two approaches are equivalent – the discussion in this chapter
    is intended simply to provide an intuitive grasp of the issues. We will then show
    you how to use the `sklearn.svm.LinearSVC` implementation for our current task.
    As with the previous approaches, we will start with a simple application of the
    approach that will work well for some examples but less so for others; we will
    then introduce two ways of refining the approach to work with multi-label datasets,
    and finally, we will reflect on the results we have obtained.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basic ideas behind SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application of simple SVMs for standard datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ways of extending SVMs to cover multi-label datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A geometric introduction to SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have two groups of entities, called B and R, where each entity is
    described by a pair of numerical coordinates. B includes objects with coordinates
    (6.38, -10.62), (4.29, -8.99), (8.68, -4.54), and so on and R contains objects
    with coordinates (6.50, -3.82), (7.39, -3.13), (7.64, -10.02), and so on (the
    example used in this discussion has been taken from [https://scikit-learn.org/stable/modules/svm.xhtml#classification](https://scikit-learn.org/stable/modules/svm.xhtml#classification)).
    Plotting these points on a graph gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Plot of the R and B points](img/B18714_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Plot of the R and B points
  prefs: []
  type: TYPE_NORMAL
- en: It looks as though you should be able to draw a straight line to separate the
    two groups, and if you could, then you could use it to decide whether some new
    point was an instance of R or B.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous ways of finding such a line for a simple case like this.
    One approach would be to find the **convex hulls** (Graham, 1972) for the two
    groups – that is, the polygons that include them. The easiest way to visualize
    this involves taking the leftmost point in the set as a starting point. Then,
    you should pick the most clockwise point from there and set that as the next point
    on the list, and then do the same again with that one until you get back to the
    original.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how to pick the most clockwise point from a given starting point, consider
    the two diagrams shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Turning counter-clockwise and clockwise](img/B18714_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Turning counter-clockwise and clockwise
  prefs: []
  type: TYPE_NORMAL
- en: 'In the left-hand diagram, the slope from A to B is less steep than the slope
    from B to C, which means that you have to turn counter-clockwise when you get
    to B if you want to go from A to B to C, which, in turn, means that C is further
    counter-clockwise from A than B is. In the right-hand diagram, the slope from
    A’ to B’ is steeper than the slope from B’ to C’, which means that C’ is less
    counter-clockwise from A’ than B’ is. Thus, to see whether C is more or less counter-clockwise
    from A than B is, we need to calculate the slopes of the lines joining them and
    see which is steeper: the slope of the line from A to C is (C[1]-A[1])/(C[0]-A[0]),
    and likewise for the line joining A and B, so C is further counter-clockwise from
    A than B if (C[1]-A[1])/(C[0]-A[0]) > (B[1]-A[1])/(B[0]-A[0]). Rearranging this
    gives us `ccw`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use this to find the convex hull. We sort the points by their *Y*
    and *X* coordinates, which lets us find the lowest point, `p` (picking the leftmost
    of these if there is a tie). This point must lie on the hull, so we add it to
    the hull. We then pick the next item, `q`, in the list of points (or go back to
    the beginning if `p` was the last point – `(p+1)%n` will be 0 if `p` is `n` and
    `p+1` otherwise). We now go through the entire list of points starting at `q`
    using `ccw` to see whether going from `p` to `i` to `q` satisfies the constraint
    given previously: if it does, then `i` is further counter-clockwise from `p` than
    `q` is, so we replace `q` with it. At the end of this, we know that `q` is the
    furthest counter-clockwise point from `p`, so we add it to the hull and continue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The complexity of this algorithm is *o(H*N)*, where *H* is the size of the
    hull and *N* is the total number of points – *H* because the main loop terminates
    after the hull has been constructed by adding one item for each iteration, *N*
    because on each pass through the main loop, we have to look at every point to
    find the most counter-clockwise one. There are more complicated algorithms that
    are more efficient than this under certain circumstances, but the one given here
    is efficient enough for our purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure illustrates how this algorithm progresses around the set
    of B points in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Growing the convex hull for B](img/B18714_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Growing the convex hull for B
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more efficient algorithms for growing the hull (see `scipy.spatial.ConvexHull`:
    [https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.xhtml)),
    but this one is simple to understand. We use it to calculate the convex hulls
    of R and B:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Convex hulls of B and R](img/B18714_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Convex hulls of B and R
  prefs: []
  type: TYPE_NORMAL
- en: 'If any lines separate R and B (if they are **linearly separable**), then at
    least one of the segments of the convex hull must be one. If we pick the edge
    from the hull of B that is nearest to some edge from the hull of R, we can see
    that it is a separator – all the B items are above or on the dashed orange line
    and all the R ones are below it, and likewise, all the R items are on or below
    the dotted green line and all the B ones are above it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Hull segments as candidate separators for R and B](img/B18714_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Hull segments as candidate separators for R and B
  prefs: []
  type: TYPE_NORMAL
- en: But they are not very good separators. All items that fell just below the dashed
    orange line would be classified as R, even if they were only just below this line
    and hence were much nearer to the Bs than to the Rs; and all items that appeared
    just above the dotted green line would be classified as B, even if they were only
    just above it.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we want some way of finding a separator that will deal with cases that
    fall between these two extreme lines appropriately. We can do this by finding
    the line from the closest point on one of the segments to the other (the dotted
    gray line) and then drawing our separator through the middle of, and perpendicular
    to, this line (the solid black line):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Optimal separator for R and B](img/B18714_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Optimal separator for R and B
  prefs: []
  type: TYPE_NORMAL
- en: 'The solid black line is an optimal separator in that it makes the separation
    between the two groups as great as possible: the distance from the nearest point
    in each group to the line is as great as possible, so any unseen point that falls
    above it will be assigned to B, which is the best place for it to go, and any
    point that falls below it will be assigned to R.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what we want. Unfortunately, it will all go wrong if some points are
    outliers – that is, if some R points fall within the main body of B points or
    some B points fall within the main body of R points. In the following example,
    we have switched two so that there is a B near the top left of the Rs and an R
    near the bottom of the Bs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – One R point and one B point switched](img/B18714_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – One R point and one B point switched
  prefs: []
  type: TYPE_NORMAL
- en: 'The convex hulls of the two groups now overlap, and cannot sensibly be used
    for finding separators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Convex hulls with outliers](img/B18714_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Convex hulls with outliers
  prefs: []
  type: TYPE_NORMAL
- en: 'We can try to identify outliers and exclude them from their groups, for example,
    by finding the center of mass of the entire group, marked as black ovals, and
    removing a point from it if it is nearer to the center of mass of the other group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Centers of mass of the two groups](img/B18714_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Centers of mass of the two groups
  prefs: []
  type: TYPE_NORMAL
- en: 'It is clear that the outliers are nearer to the centers of mass of the “wrong”
    groups, and hence can be identified and removed from consideration when trying
    to find the separators. If we remove them both, we get non-overlapping hulls.
    The separator does not put the outliers on the “right” sides, but then no straight
    line could do that – any straight line that included the outlying R with the other
    Rs would have to include the outlying B and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two
    outliers (brown Xs are ignored)](img/B18714_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Non-overlapping convex hulls and a separator ignoring the two
    outliers (brown Xs are ignored)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we can also get non-overlapping hulls by ignoring just one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying
    R (left) and B (right) points](img/B18714_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – Non-overlapping convex hulls and a separator ignoring outlying
    R (left) and B (right) points
  prefs: []
  type: TYPE_NORMAL
- en: This time, we are ignoring just one of the outlying points when calculating
    the hulls. In each case, we get two non-overlapping hulls and a separator, but
    this time, we have used more of the original data points, with just one of them
    lying on the wrong side of the line.
  prefs: []
  type: TYPE_NORMAL
- en: Which is better? To use more of the data points when trying to find the separator,
    to have more of the data points lying on the right-hand side of the line, or to
    minimize the total distance between the line and the points in the two sets? If
    we do decide to ignore a point, which one should we choose (is the separator at
    the top of *Figure 7**.11* better than the one at the bottom)? Is the bottom left
    B point a normal member of B or is it an outlier? If we did not have the outlying
    R point then there would never have been any reason to doubt that this one was
    indeed a B.
  prefs: []
  type: TYPE_NORMAL
- en: We need to have a view of how important each of these issues is, and then we
    have to optimize our choice of points to get the best outcome. That turns this
    into an optimization algorithm, where we make successive changes to the sets of
    points to try to optimize the criteria given previously – how many points are
    included, how close the separator is to the nearest points, and in the most general
    case what the equation that defines the separator should be (there are tricks
    to allow for circular or curved separators or to allow the separator to be a bit
    bendy). If the separating line is straight – that is, the classes are linearly
    separable – then for the 2-dimensional cases, the line will have an equation such
    as ![<mml:math  ><mml:mi>A</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/15.png).
    When we move to three dimensions, the separator becomes a plane with an equation
    such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mi>C</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math>](img/16.png).
    When we move to even higher dimensions, it becomes a **hyperplane**, with an equation
    such as ![<mml:math  ><mml:mi>A</mml:mi><mml:mn>1</mml:mn><mml:mi mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mn>2</mml:mn><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mo>+</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>+</mml:mo><mml:mi>A</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">*</mml:mi><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>.</mml:mo></mml:math>](img/17.png)
  prefs: []
  type: TYPE_NORMAL
- en: The simple programs that we used for illustration purposes are not powerful
    enough to deal with all these issues. We are going to want to work with very high-dimensional
    spaces where, in most cases, most of the dimensions are zero. Fortunately, there
    are plenty of efficient implementations that we can use. We will use the Python
    `LinearSVC` implementation from `sklearn.svm` – there are plenty of other implementations
    in Python, but `sklearn` packages tend to be stable and well-integrated with other
    parts of `sklearn`, and `LinearSVC` is known to be particularly efficient for
    large sparse linear tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using SVMs for sentiment mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now seen how SVMs provide classifiers by finding hyperplanes that separate
    the data into classes and have seen a graphical explanation of how such hyperplanes
    can be found, even when the data is not linearly separable. Now, we’ll look at
    how SVMs can be applied to our datasets to find the boundaries between sentiments,
    with an analysis of their behavior on single-label and multi-label datasets and
    a preliminary investigation into how their performance on multi-label datasets
    might be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with the previous classifiers, we can define the `SVMCLASSIFIERs` class
    as a subclass of `SKLEARNCLASSIFIER` by using the following initialization code
    (`useDF` is a flag to decide whether to use the TF-IDF algorithm from [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*
    when building the training set; `max_iter` sets an upper bound on the number of
    iterations the SVM algorithm should carry out – for our examples, the scores tend
    to converge by 2,000 steps, so we generally use that as the limit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly like the constructor for `NBCLASSIFIERS` – just use `readTrainingData`
    to get the data into the right format and then use the `sklearn` implementation
    to construct the SVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we start by applying this to our standard datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Precision | Recall | micro F1 | macro F1 | Jaccard |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-EN** | **0.916** | **0.916** | **0.916** | **0.916** | **0.845** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.620 | 0.260 | 0.366 | 0.372 | 0.224 |'
  prefs: []
  type: TYPE_TB
- en: '| **WASSA-EN** | **0.870** | **0.870** | **0.870** | **0.870** | **0.770**
    |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.870 | 0.870 | 0.870 | 0.870 | 0.770 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.848 | 0.848 | 0.848 | 0.848 | 0.736 |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM4-AR** | **0.679** | **0.679** | **0.679** | **0.679** | **0.514** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.586 | 0.255 | 0.356 | 0.367 | 0.216 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.781 | 0.767 | 0.774 | 0.778 | 0.631 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | 0.592 | 0.574 | 0.583 | 0.494 | 0.412 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.493 | 0.295 | 0.369 | 0.372 | 0.226 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.12 – SVM applied to the standard datasets
  prefs: []
  type: TYPE_NORMAL
- en: The basic SVM gets us the best scores we have seen so far for the WASSA and
    two of the SEM4 datasets, with scores for most of the other datasets that are
    close to the best we have obtained so far. As with the previous algorithms, if
    we use it with the standard settings, it does very poorly on the multi-label problems,
    simply because an algorithm that returns exactly one label will fail to cope with
    datasets where items can have zero or more than one label.
  prefs: []
  type: TYPE_NORMAL
- en: Training an SVM takes significantly longer than training any of the classifiers
    we have looked at so far. Therefore, it is worth looking briefly at how accuracy
    and training time for CARER vary as we vary the size of the training data – if
    we find that having more data has little effect on accuracy but makes training
    take much longer, we may decide that getting more data is not worth the bother.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we plot accuracy (because there is no empty class, the values of recall
    and macro- and micro-F measure are all the same) and Jaccard score for CARER against
    the size of the training set, we see that we do not need the entire dataset –
    these two measures converge fairly rapidly, and if anything, performance starts
    to go down after a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Accuracy versus training data size for SVMs](img/B18714_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.13 – Accuracy versus training data size for SVMs
  prefs: []
  type: TYPE_NORMAL
- en: The decrease in performance after about 30K tweets could just be noise or it
    could be the result of over-training – as machine learning algorithms see more
    and more data, they can start to pick up on things that are specific to the training
    data and are not present in the test data. Either way, the performance is substantially
    better than anything we saw in *Chapters 5* and *6* and appears to have leveled
    out at around 0.89 accuracy and 0.80 Jaccard.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also plotted training time against data size to see whether it would be
    feasible to run it with more data if we could get it, and the time went up more
    or less linearly with the data size (this is commonly reported for this kind of
    problem). However, since the accuracy has already leveled off by about 40K, it
    seems unlikely that adding more data would make any difference anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Training time versus data size for SVMs](img/B18714_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.14 – Training time versus data size for SVMs
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy that’s obtained by any classifier with any amount of data must
    level off before reaching 1\. Most of the curves that can easily be fitted to
    plots such as the one shown here (for example, polynomial curves) continue increasing
    as the *X*-value increases, so they can only be approximations to the real curve,
    but they do let you get an impression of whether increasing the amount of training
    data is worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with datasets with tweets with no labels and tweets with multiple
    labels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Why is the behavior of the standard SVM so much worse for the multi-label datasets
    than for most of the others?
  prefs: []
  type: TYPE_NORMAL
- en: SVMs, like any other standard classifier, are designed for assigning each item
    to a single class. Each point in the training set is given a set of features and
    a label and the learning algorithm works out how the features and labels are connected.
    It is fairly straightforward to adapt this to include data where some points have
    no label, simply by saying that there is an extra label, called something such
    as neutral, or none of the above, or something like that, to be used when a point
    has not been given a label. This is a slightly artificial way to proceed because
    it means that the classifier finds words that are associated with having no emotion,
    whereas the real situation is that such points simply don’t have any words that
    carry emotional weight. However, it usually works reasonably well and can be assimilated
    into the standard SVM training algorithm (the definition of `onehot2value` shown
    in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes*, allows for
    exactly this kind of situation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The SEM11 and KWT.M-AR examples belong to a harder, and arguably more realistic,
    class of problem, where a single tweet may express zero *or more* emotions. The
    second tweet in the test set for SEM11-EN, *I’m doing all this to make sure you
    smiling down on me bro*, expresses all three of joy, love, and optimism, and the
    second to last, *# ThingsIveLearned The wise # shepherd never trusts his flock
    to a # smiling wolf. # TeamFollowBack # fact # wisewords*, has no emotion linked
    to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easy enough to encode these as vectors, given the possible set of emotions
    [‘anger’, ‘anticipation’, ‘disgust’, ‘fear’, ‘joy’, ‘love’, ‘optimism’, ‘pessimism’,
    ‘sadness’, ‘surprise’, ‘trust’]: we use [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] for
    joy, love, and optimism and [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0] for no emotion.
    *But these are not one-hot encodings*, and `onehot2value` will not deal with them
    properly. In particular, it will interpret [0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0] as
    joy since that is the first non-zero item it encounters.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no simple way around this – SEM11 data has multiple emotions, and SVMs
    expect single emotions. This has two consequences. During training, only one of
    the emotions associated with a tweet will be used – if the preceding example had
    occurred during training, it would have led to the words *smiling* and *bro* (which
    are the only low document frequency words in this tweet) being associated with
    joy but not with love and optimism, which could lead to lower precision and lower
    recall; and if it occurred during testing, then it would inevitably have led to
    a loss of recall because only one of the three could be returned. This is borne
    out by the results shown previously, where SEM11-EN in particular has quite good
    precision but very poor recall, in contrast to the best of the results in [*Chapter
    6*](B18714_06.xhtml#_idTextAnchor134), *Naive Bayes* where it has good recall
    but poor precision.
  prefs: []
  type: TYPE_NORMAL
- en: The illustration earlier in this chapter of how SVMs work showed a two-class
    problem – Rs and Bs. There are two obvious ways of coping with the extension to
    multiclass problems (such as the CARER and WASSA datasets). Suppose we had Rs,
    Bs, and Gs. We could train three two-class classifiers, one for R versus B, one
    for R versus G, and one for B versus G (**one versus one**), and combine the results;
    alternatively, we could train a different set of two-class classifiers, one for
    R versus (B or G), one for B versus (R or G), and one for G versus (R or B) (**one
    versus many**). Both give similar results, but when there are a lot of classes,
    you have to train N*(N+1)/2 classifiers (N for the first class versus each of
    the rest + N-1 for the second class versus each of the rest + ...) for one versus
    one but only N for one versus many. For CARER, for instance, where there are six
    classes, we would have to train 21 classifiers and combine their results for one
    versus one, whereas for one versus many, we would only have to train six classifiers
    and combine their results.
  prefs: []
  type: TYPE_NORMAL
- en: We need to follow one of these strategies for all our datasets since they all
    have several possible outcomes. Fortunately, `sklearn.svm.LinearSVC` does this
    automatically (using one versus many) for problems where there are a range of
    possible labels. This by itself, however, will not solve the problem of multi-label
    datasets – there is a difference between having an outcome consisting of one label
    drawn from several options and an outcome having an unknown number of labels drawn
    from several options. The standard multiple-classifiers, which are combined as
    one versus one or one versus many, will solve the first of these problems but
    not the second.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways we can adapt our SVM classifier to deal with this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: We can follow the same strategy as with the Naive Bayes classifier of taking
    the real-valued scores for each emotion and using a threshold to determine whether
    or not a tweet satisfies each emotion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can train several classifiers, one versus many style, and simply accept the
    results from each of them. In the preceding example, if the R versus (B or G)
    classifier says R, then we accept R as one of the labels for a test case; if B
    versus (R or G) says B, then we accept that *as well*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look at each of these in turn in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Using a standard SVM with a threshold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To use Naive Bayes with multi-label datasets, we changed `applyToTweet` like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code used the fact that `predict_log_proba` returns a value for
    every label. In the standard version of Naive Bayes, we just pick the highest
    scoring label for each case, but using a threshold allows us to pick any number
    of labels, starting from 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'This won’t quite work for SVMs because they do not have a method called `predict_log_proba`.
    What they do have is a method called `decision_function`, which produces a score
    for each label. Rather than changing the definition of `applyToTweet` to use `decision_function`
    instead of `predict_log_proba`, we simply set the value of `predict_log_proba`
    to be `decision_function` in the constructor for SVMs, and then use `applyToTweet`,
    as we did previously. So, we must adapt the constructor for SVMs, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In other words, once we have made the underlying SVM, we must set a couple
    of standard properties that we will find useful and that do not have the same
    names in all the `sklearn` classifiers. The results of this for the multi-label
    cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.15 – SVMs using thresholds to deal with multi-label problems
  prefs: []
  type: TYPE_NORMAL
- en: The SEM11 cases are better than those for the simple SVM that we looked at earlier
    but are not better than the scores we obtained using the earlier algorithms, and
    the scores for KWT.M-AR are worse than with the simple SVMs. Just using the values
    that the decision function for the SVM assigns to each label does not solve the
    problem of multi-label datasets. We will refer to SVMs that use the set of values
    for each label plus a threshold as SVM (multi) classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Making multiple SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second option is to make a set of one versus rest classifiers and accept
    every label for which the relevant classifier succeeds. The key is to take each
    label in turn and `squeeze` the *N* labels in the training data into two – one
    for the target label and one for all the others. Consider a tweet labeled as joy.
    The representation of this as a vector would be [0, 0, 0, 0, 1, 0, 0, 0, 0, 0,
    0] – that is, with 1 in the column for joy. If we squeeze this to be joy versus
    the rest, then it will come out as [1, 0] – that is, with a 1 in the new column
    for joy and a 0 in the column for not-joy. If we squeeze it to be angry versus
    not-angry, then it would be [0, 1], with a 0 in the new column for anger and a
    1 in not-angry. If it had been labeled as joy and love, then the vector would
    have been [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0] and the squeezed version would have
    been [1, 1]: 1 in the first column because it does express joy and 1 in the second
    because it *also* expresses something else.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have a vector, `gs`, that represents the emotions for a multi-label
    tweet and we want to squeeze it on the `I` column. The first column is easy –
    we just set it to `gs[i]`. To get the second, which represents whether some column
    other than `I` is non-zero, we use `numpy.sign(sum(gs[:i]+gs[i+1:])`: `gs[:i]`
    and `gs[i+1:]` are the other columns. Taking their sum will be greater than 0
    if at least one of them is non-zero while taking the sign of that will be 0 if
    the sum was 0 and 1 if it was greater than zero. Note that it is possible for
    both `gs[i]` and `numpy.sign(sum(gs[:i]+gs[i+1:])` to be `0` and for both of them
    to be `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor for `MULTISVMCLASSIFIER` is straightforward – just make one
    standard `SVMCLASSIFIER` for each emotion. To apply one to a tweet, we must apply
    each of the standard ones and gather the positive results. So, if the classifier
    that has been trained on joy versus not-joy says some tweet expresses joy, then
    we mark the tweet as satisfying joy, but we ignore what it says about not-joy
    since a positive score on not-joy simply tells us that the tweet also expresses
    some other emotion, and we are allowing tweets to express more than one emotion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is pretty much the standard one versus many approach to training an SVM
    with multiple labels. The key difference is in the way that the results of the
    individual *X* versus not-*X* classifiers are combined – we accept *all* the positive
    results, whereas the standard approach just accepts one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table is a repeat of the table for the multi-label problems using
    SVM (multi) for comparison:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.511 | 0.328 | 0.399 | 0.387 | 0.249 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.521 | 0.290 | 0.373 | 0.361 | 0.229 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.135 | 0.694 | 0.227 | 0.131 | 0.128 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.434 | 0.338 | 0.380 | 0.361 | 0.235 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.16 – Multi-label datasets, SVM (multi)
  prefs: []
  type: TYPE_NORMAL
- en: 'When we use multiple SVMs, one per label, we get an improvement in each case,
    with the score for SEM11-EN being the best so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Precision** | **Recall** | **micro F1** | **macro F1** | **Jaccard**
    |'
  prefs: []
  type: TYPE_TB
- en: '| **SEM11-EN** | **0.580** | **0.535** | **0.556** | **0.529** | **0.385**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.531 | 0.485 | 0.507 | 0.478 | 0.340 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.648 | 0.419 | 0.509 | 0.340 | 0.341 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.498 | 0.368 | 0.423 | 0.378 | 0.268 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.17 – Multi-label datasets, multiple SVMs
  prefs: []
  type: TYPE_NORMAL
- en: This is better than the results for the SVM (multi) case and is the best so
    far for SEM11-EN. The improvement over the SVM for the SEM11 datasets comes from
    the huge improvement in recall. Remember that the standard SVM can only return
    one result per datapoint, so its recall *must* be poor in cases where the Gold
    Standard contains more than one emotion – if a tweet has three emotions associated
    with it and the classifier reports just one, then the recall for that tweet is
    1/3\. The improvement for KWT.M-AR comes from the improvement in precision – if
    a tweet has zero emotions associated with it, as is common in this dataset, then
    the standard SVM must produce a false positive for it.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous tweaks can be applied to `sklearn.svm.LinearSVC`, and we can also try
    the tweaks from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector Space Models* – using IDF to get the feature values, for instance,
    produces a small improvement across the board. These are worth trying once you
    have reasonable results with the default values, but it is easy to get carried
    away trying variations to try to gain a few percentage points on a given dataset.
    For now, we will simply note that even the default values provide good results
    in cases where the dataset has exactly one emotion per tweet, with the multi-SVM
    providing the best results yet for some of the more difficult cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using an SVM can easily be seen as yet another way of extracting a lexicon
    with weights from a corpus. The dimensions of the SVMs that were used in this
    chapter are just the words in the lexicon, and we can play the same games with
    that as in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector Space Models* – using different tokenizers, stemming, and eliminating
    uncommon words. We will not repeat these variations here: we know from [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*
    that different combinations suit different datasets, and simply running through
    all the variations will not tell us anything new. It is, however, worth reflecting
    on exactly how SVMs use the weights that they assign to individual words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The SVM for the CARER dataset, for instance, has an array of six rows by 74,902
    columns as its coefficients: six rows because there are six emotions in this dataset,
    and 75K columns because there are 75K distinct words. If we pick several words
    more or less at random, some of which are associated with some emotion and some
    that have very little emotional significance, we will see that their weights for
    the various emotions reflect our intuition:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  prefs: []
  type: TYPE_TB
- en: '| sorrow | -0.033 | -0.233 | 0.014 | 0.026 | 0.119 | 0.068 |'
  prefs: []
  type: TYPE_TB
- en: '| scared | -0.508 | 1.392 | -1.039 | -0.474 | -0.701 | -0.290 |'
  prefs: []
  type: TYPE_TB
- en: '| disgust | 1.115 | -0.293 | -0.973 | -0.185 | -0.855 | -0.121 |'
  prefs: []
  type: TYPE_TB
- en: '| happy | -0.239 | -0.267 | 0.546 | -0.210 | -0.432 | -0.080 |'
  prefs: []
  type: TYPE_TB
- en: '| adores | 0.000 | 0.000 | 0.412 | -0.060 | -0.059 | -0.000 |'
  prefs: []
  type: TYPE_TB
- en: '| and | -0.027 | -0.008 | 0.001 | -0.008 | -0.020 | -0.004 |'
  prefs: []
  type: TYPE_TB
- en: '| the | 0.001 | -0.012 | -0.004 | 0.001 | -0.002 | -0.002 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.18 – Associations between words and emotions, SVM as the classifier,
    with the CARER dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*sorrow* is strongly linked to **sadness**, *scared* is strongly linked to
    **fear**, *disgust* is strongly linked to **anger**, *happy* is strongly linked
    to **joy**, and *adores* is strongly linked to **joy** (but not, interestingly,
    to **love**: words always throw up surprises); and neutral words are not strongly
    linked to any particular emotion. The main thing that is different from the lexicons
    in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134)*, Naive Bayes* is that some
    words also vote very strongly *against* some emotions – if you are *scared*, then
    you are not joyous, and if you are *happy*, then you are not angry, fearful, or
    sad.'
  prefs: []
  type: TYPE_NORMAL
- en: The way that SVMs use these weights for classification is the same as in [*Chapter
    6*](B18714_06.xhtml#_idTextAnchor134)*, Naive Bayes* – if you are given a vector
    of values, V = [v0, v1, ..., vn], and a set of coefficients, C = [w0, w1, ...,
    wn], then checking whether `V.dot(C)` is greater than some threshold is exactly
    what we did with the weights in [*Chapter 6*](B18714_06.xhtml#_idTextAnchor134)*,
    Naive Bayes* (given that V and C are sparse arrays in `sklearn.svm.LinearSVC`,
    this may be a fairly fast way to do this sum, but it is the same sum). The only
    differences lie in the way that SVMs obtain the weights and the fact that an SVM
    can assign negative weights to words. We will return to ways of handling multi-label
    datasets in [*Chapter 10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
    For now, we will just note that SVMs and the simple lexicon-based approaches end
    up using the same decision function on the same features, but that the way that
    SVMs arrive at the weights for those features is generally better, and in some
    cases much better.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 7**.17* shows the best classifiers that we have seen so far, with Jaccard
    scores, for each of the datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX** | **CP (unstemmed)** | **CP (****stemmed)** | **NB (****single)**
    | **NB (multi)** | **SVM (****single)** | **SVM (****multi)** | **MULTI-SVM**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.497 | 0.593 | 0.593 | 0.775 | 0.778 | *******0.845*** | 0.836
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.348 | 0.352 | 0.353 | 0.227 | 0.267 | 0.224 | 0.249 | *******0.385***
    |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.437 | 0.512 | 0.505 | 0.709 | 0.707 | *******0.770*** | 0.749
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.350 | 0.414 | 0.395 | 0.776 | 0.774 | 0.770 | *******0.796***
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.667 | 0.721 | 0.722 | 0.738 | *******0.740*** | 0.736 | 0.736
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.509 | 0.493 | 0.513 | 0.531 | *******0.532*** | 0.514 | 0.494
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | *******0.386*** | 0.370 | 0.382 | 0.236 | 0.274 | 0.216 | 0.229
    | 0.340 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.663 | *******0.684*** | 0.666 | 0.494 | 0.507 | 0.631 | 0.128
    | 0.341 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | *******0.420*** | 0.191 | 0.177 | 0.360 | 0.331 | 0.412 | 0.336
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.271 | 0.276 | *******0.278*** | 0.230 | 0.255 | 0.226 | 0.235
    | 0.268 |'
  prefs: []
  type: TYPE_TB
- en: Figure 7.19 – Best classifier for each dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, different classifiers work well with different datasets. The
    major lesson here is that you should not just accept that there is a single best
    classification algorithm: do experiments, try out variations, and see for yourself
    what works best with your data. It is also worth noting that the multi-label datasets
    (SEM11-EN, SEM11-AR, SEM11-ES, and KWT.M-AR) score very poorly with simple SVMs,
    and the only one where the multi-SVM wins is SEM11-EN, with simple algorithms
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and
    Vector-Space Models*, still producing the best scores for the other cases.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Bennett, K. P., & Bredensteiner, E. J. (2000). *Duality and Geometry in SVM
    Classifiers*. Proceedings of the Seventeenth International Conference on Machine
    Learning, 57–64.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). *A Training Algorithm for
    Optimal Margin Classifiers*. Proceedings of the Fifth Annual Workshop on Computational
    Learning Theory, 144–152\. [https://doi.org/10.1145/130385.130401](https://doi.org/10.1145/130385.130401).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graham, R. L. (1972). *An efficient algorithm for determining the convex hull
    of a finite planar set*. Information Processing Letters, 1(4), 132–133\. [https://doi.org/10.1016/0020-0190(72)90045-2](https://doi.org/10.1016/0020-0190(72)90045-2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
