- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating and Enhancing Efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the important aspects of rigorously evaluating
    the performance of active machine learning systems. We will cover various topics
    such as automation, testing, monitoring, and determining the stopping criteria.
    In this chapter we will use a paid cloud service, such as AWS, to demonstrate
    how an automatic, efficient active learning pipeline can be implemented in the
    real world.
  prefs: []
  type: TYPE_NORMAL
- en: By thoroughly understanding these concepts and techniques, we can ensure a comprehensive
    active ML process that yields accurate and reliable results. Through this exploration,
    we will gain insights into the effectiveness and efficiency of active ML systems,
    enabling us to make informed decisions and improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this chapter, we will have covered the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating efficient active ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring active ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining when to stop active ML runs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing production model monitoring with active ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A MongoDB account: ([https://www.mongodb.com/](https://www.mongodb.com/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A ClearML account: ([https://app.clear.ml/](https://app.clear.ml/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPU: You may check out the specific hardware requirements from the web page
    of the tool you will be using'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An EC2 instance, factoring in cost considerations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, you will need to install these packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Creating efficient active ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the previous chapter, efficient active ML pipelines consist
    of end-to-end pipelines. This means that the active ML algorithm needs to be able
    to access the unlabeled data, select the most informative frames, and then seamlessly
    send them to the labeling platform. All these steps need to happen one after the
    other in an automatic manner in order to reduce manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, it is essential to test this pipeline to ensure that each step works
    properly. An example of a cloud-hosted active ML pipeline would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlabeled data is stored in an AWS S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An active ML algorithm runs on an EC2 instance that can access the S3 bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The results of the active ML run are saved in a dedicated S3 bucket specifically
    for this purpose and are linked to the labeling platform used for the project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The final step of the active ML run is to link the selected frames to the labeling
    platform and create the annotation project, ready for the labelers to work on
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When selecting and configuring the EC2 instance for running the active ML code,
    it is essential to consider efficiency. It is highly likely that a GPU will be
    required to perform the inference and compute the active ML embeddings. For example,
    if you are using Lightly, you can refer to their hardware recommendations on this
    page ([https://docs.lightly.ai/docs/hardware-recommendations](https://docs.lightly.ai/docs/hardware-recommendations)).
    Additionally, it is important to take into account the cost of the chosen EC2
    instance and determine if it aligns with your budget. You can find the AWS EC2
    on-demand pricing here ([https://aws.amazon.com/ec2/pricing/on-demand/](https://aws.amazon.com/ec2/pricing/on-demand/)).
    When you are not running any active ML process, stopping the instance is a good
    practice to save money.
  prefs: []
  type: TYPE_NORMAL
- en: Other good practices include having a requirements.txt file that lists all the
    required versions of the packages for the run. For example, for the packages used
    in [*Chapter 5*](B21789_05.xhtml#_idTextAnchor069), *Leveraging Active Learning
    for Big Data*, the `requirements.txt` file would look something like this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can replace any version with the desired version; ideally, using the latest
    versions of the packages would be better.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, making the pipeline configurable through parameters enables easier
    scaling. For example, specifying options such as sampling strategy, model selection,
    and data source via a YAML configuration file. This allows for changing pipeline
    behavior without code changes, simplifying integration into workflows. As a reminder,
    we have explored different sampling strategies in [*Chapter 2*](B21789_02.xhtml#_idTextAnchor027),
    *Designing Query Strategy Frameworks*, and we have explored model selection for
    computer vision tasks in [*Chapter 4*](B21789_04.xhtml#_idTextAnchor056), *Applying
    Active Learning to* *Computer Vision*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple example of configuring a YAML file using our Lightly example from
    [*Chapter 5*](B21789_05.xhtml#_idTextAnchor069), *Leveraging Active Learning for
    Big Data*, might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, those parameters can be accessed using this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Followed by this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Those variables can then be used in the code in [*Chapter 5*](B21789_05.xhtml#_idTextAnchor069),
    *Leveraging Active Learning for Big Data*, and will make it scalable to all workflows
    and applications.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for your projects, you only need to change the YAML file and can then
    use the scripts for all your projects without modifying the scripts themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will explore solutions to monitor our active ML runs and make sure
    that we are able to have a full overview of the whole process.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring active ML pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `logging`, which is a versatile and built-in library.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating an MLOps platform such as **ClearML** ([https://clear.ml/](https://clear.ml/))
    can significantly streamline the monitoring of pipeline runs. ClearML provides
    real-time statistics, graphical data visualizations, extensive logging, model
    artifacts, and the ability to compare pipeline runs in a side-by-side format.
    While traditionally used for improving the observability of ML training and deployment
    pipelines, ClearML is also highly effective for active ML pipelines, enhancing
    their management and oversight.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.1 – Console log automatically saved after initializing the ClearML
    run](img/B21789_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Console log automatically saved after initializing the ClearML
    run
  prefs: []
  type: TYPE_NORMAL
- en: 'This integration not only streamlines the process of monitoring and managing
    ML runs but also ensures that all relevant data and metrics are systematically
    captured and made accessible for analysis within the ClearML platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We opt for the ClearML task type `TaskTypes.inference`, as it is specifically
    tailored to facilitate the comprehensive logging and meticulous monitoring of
    the inference processes in machine learning. This task type is adept at meticulously
    tracking the input data, the predictions outputted by the model, and a range of
    relevant metrics or performance indicators, making it particularly suited for
    active ML runs. Indeed, as we have seen in previous chapters, active ML runs consist
    of conducting inference to identify the most advantageous frames to add to the
    labeling queue. Therefore, `TaskTypes.inference` is the ideal choice here. This
    task type is instrumental in enabling the systematic collection and thorough analysis
    of key performance metrics unique to the inference stage, such as latency. Furthermore,
    utilizing `TaskTypes.inference` empowers teams to accrue critical insights regarding
    the model’s behavior when interfacing with real-time data—a fundamental aspect
    for the success of active ML systems. This detailed understanding of a run’s real-time
    performance is invaluable for optimizing active ML strategies and enhancing overall
    model efficacy.
  prefs: []
  type: TYPE_NORMAL
- en: Another valuable tool for monitoring active ML pipelines is **MongoDB** ([https://www.mongodb.com/](https://www.mongodb.com/)),
    which is a widely used database known for its user-friendly nature. Its flexibility
    makes it an excellent choice for ML pipelines, which often evolve over time. In
    the context of active ML pipelines, MongoDB can be employed to generate a labeling
    queue automatically, for instance. This application of MongoDB not only streamlines
    the data handling process but also contributes to the overall efficiency and adaptability
    of the ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a MongoDB collection specifically for this project, which we
    will name `ml_demo_project`. Within this collection, we will create a table titled
    `ml_labeling_queue_demo`. This organizational structure in MongoDB will facilitate
    the efficient management and retrieval of data pertinent to our project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step involves populating our labeling queue table with data derived
    from our active ML run. This is a general example and should be tailored to fit
    the specific requirements of individual projects. By integrating this step, we
    ensure that the information from the ML run is accurately and efficiently transferred
    to the labeling queue, setting the stage for subsequent processing and analysis
    tailored to the unique needs of each project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This MongoDB collection streamlines the process of monitoring progress in the
    labeling queue, facilitating efficient communication with labelers about upcoming
    items. This setup eliminates the need for the manual entry of each new annotation
    project, significantly enhancing workflow efficiency. By automating the tracking
    and updating of the queue, it ensures a more seamless and coordinated approach
    to managing labeling tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Actively monitoring via logs, MLOps platforms, databases, and other tools is
    essential for maintaining visibility and quickly catching any issues in production-active
    ML pipelines. This helps minimize risks and improve system reliability.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that this monitoring is effectively utilized in decision-making processes,
    it’s crucial to establish clear criteria for critical actions, such as determining
    when to stop active ML runs, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Determining when to stop active ML runs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Active ML runs are dynamic and iterative processes that require careful monitoring,
    as we have already seen. But they also require strategic decision-making to determine
    the optimal point for cessation. The decision to stop an active ML run is critical
    as it impacts both the performance and efficiency of the learning model. This
    section focuses on the key considerations and strategies to effectively determine
    when to stop active machine learning runs.
  prefs: []
  type: TYPE_NORMAL
- en: In active ML, establishing clear performance goals specific to the project is
    crucial. For instance, consider a project aimed at developing a facial recognition
    system. Here, accuracy and precision might be the chosen performance metrics.
    A diverse test set, mirroring real-world conditions and varied facial features,
    is crucial for evaluating the model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say the pre-defined threshold on the established test set for accuracy
    is set at 95% and for precision, at 90%. The active ML process should continue
    until the model consistently achieves or surpasses these metrics on the test set.
    If the model shows an accuracy of 95% or more and a precision of 90% or more on
    the test set, it suggests the model has learned to generalize well across different
    faces and scenarios. This consistent performance on a diverse test set indicates
    the model is ready for real-world application, having been effectively tailored
    through the active learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional considerations play a vital role in determining when to stop the
    active ML process. In the preceding facial recognition example, there are several
    simple but important additional factors to consider when deciding to stop the
    active ML process. Here are some steps you could take before deciding to stop
    the active ML process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watch out for overfitting**: This happens when the model does much better
    on training data than on the test set. If we see this, it’s time to stop and adjust
    the model to avoid overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Think about our resources**: Resources such as time, computing power, and
    money are scarce. Even if our model hasn’t hit the 95% accuracy or 90% precision
    we want, we might have to stop if we’re running low on these resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Be aware of diminishing returns**: This means if training more isn’t really
    improving our model, it might have learned as much as it can. Continuing to train
    it in this case won’t help.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keep reassessing the model with feedback loops**: As the world and data change,
    our model’s goals might need to change, too. Regularly checking that our model
    still meets our current needs helps keep it relevant and effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision to stop an active ML run should be based on a combination of reaching
    predefined performance metrics, maintaining stability on a diverse test set, monitoring
    resource constraints, and being vigilant about overfitting and diminishing returns.
    By carefully considering these factors, we can ensure that active ML models are
    both effective and efficient, aligning with the overarching goals of the project.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss how we can use active learning in a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing production model monitoring with active ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having already established a comprehensive understanding of active ML, this
    section shifts focus to its practical application in monitoring machine learning
    models in production environments. The dynamic nature of user data and market
    conditions presents a unique challenge for maintaining the accuracy and relevance
    of deployed models. Active ML emerges as a pivotal tool in this context, offering
    a proactive approach to identify and adapt to changes in real time. This section
    will explore the methodologies and strategies through which active ML can be harnessed
    to continuously improve and adjust models based on evolving user data, ensuring
    that these models remain robust, efficient, and aligned with current trends and
    user behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in monitoring production models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several challenges when it comes to monitoring production models.
    First, we have data drift and model decay.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift** refers to the change in the input data fed into a machine learning
    model over time. This change can occur due to various reasons, such as evolving
    user behaviors, seasonal effects, economic shifts, or changes in the broader environment
    in which the model operates. The key characteristic of data drift is that the
    statistical properties of the current input data differ from those of the original
    training data, as shown in *Figure 6**.2*. Data drift can significantly impact
    the performance of a model since the assumptions the model was originally trained
    on no longer hold true. It can lead to a decrease in accuracy and reliability,
    making the model less effective at making predictions or classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Illustration of data drift](img/B21789_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Illustration of data drift
  prefs: []
  type: TYPE_NORMAL
- en: '**Model decay**, also known as model degradation or performance decay, refers
    to the decline in the performance of a machine learning model over time. This
    phenomenon is closely related to data drift, as shown in *Figure 6**.3*, as one
    of the primary causes of model decay is the changing nature of the data the model
    encounters in a live environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Model decay over time](img/B21789_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Model decay over time
  prefs: []
  type: TYPE_NORMAL
- en: 'However, model decay can also occur due to other factors, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Changes in relationships**: Over time, the relationships between variables
    might change, making the model’s learned patterns outdated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback loops**: If a model’s predictions are used as part of a decision-making
    process that influences future data, it can create feedback loops that gradually
    degrade the model’s performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External factors**: Unforeseen external factors such as policy changes, natural
    disasters, or global events can also lead to model decay'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both data drift and model decay highlight the need for continuous monitoring
    and updating machine learning models in production. Identifying when and how these
    changes occur is crucial for maintaining the effectiveness and accuracy of the
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Active ML to monitor models in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Active ML is particularly well suited to combat data drift and model decay due
    to its dynamic and responsive nature. Indeed, active ML stands out for its targeted
    data acquisition, as previously discussed in our chapters, where it excels in
    identifying and acquiring the most informative data points. This approach is particularly
    advantageous in addressing data drift and model decay, as the algorithm actively
    queries for new data that accurately represents the current environment or user
    behavior. This method is not only more efficient than passively collecting large
    datasets but also ensures that the data is relevant and not redundant. Active
    ML systems are inherently adaptable, quickly adjusting their understanding and
    predictions in response to new data, a feature crucial for maintaining effectiveness
    amidst changing data distributions. This adaptability is augmented by the system’s
    capacity for continuous learning and improvement. As active ML systems receive
    new data points and feedback, they are constantly updating and refining their
    models, thereby mitigating the effects of model decay and ensuring that the models
    evolve in line with changes in the data and environment.
  prefs: []
  type: TYPE_NORMAL
- en: Active ML addresses the cost and time-intensive nature of data labeling by selecting
    the most informative samples, a process that proves especially beneficial in adapting
    to data drift. The efficient use of resources in labeling ensures maximum benefit
    to the model. Additionally, active ML algorithms are designed for the **early
    detection of shifts in data patterns** or **performance drops**, acting as an
    early warning system for data drift and model decay. This early detection capability
    allows for prompt interventions, such as model adjustments or retraining, to prevent
    significant performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 2*](B21789_02.xhtml#_idTextAnchor027), *Designing Query Strategy
    Frameworks*, we also discussed how active ML provides customizable query strategies,
    including uncertainty sampling and query by committee, which can be tailored to
    the specific needs of an application. This flexibility enables more effective
    responses to the unique challenges of data drift and model decay in various scenarios,
    underlining the comprehensive adaptability of active ML in dynamic data environments.
  prefs: []
  type: TYPE_NORMAL
- en: The early detection of drift and decay is crucial to sustain the performance
    of machine learning models once they are deployed. Active ML assumes an indispensable
    role here, functioning effectively as an early warning system. This capability
    is crucial for the pre-emptive identification and mitigation of potential issues
    before they escalate, thus preserving the model’s integrity and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore the mechanisms and strategies through which active ML can
    be used to accomplish this, highlighting its significance in the proactive management
    and maintenance of ML models in dynamic environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some mechanisms for early detection using active ML methods that we
    have seen in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uncertainty sampling**: Active ML algorithms often employ uncertainty sampling,
    where the model identifies data points for which it has the lowest confidence
    in its predictions. A sudden increase in the number of such points can signal
    a change in the underlying data distribution, indicating potential data drift.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Anomaly detection**: Active ML systems can be equipped with anomaly detection
    capabilities to spot unusual patterns in incoming data. These anomalies might
    be indicative of changes in the data landscape that could lead to model decay
    if not addressed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query by committee**: This approach involves maintaining multiple models
    or versions of a model (the committee) and using their disagreement to identify
    challenging data points. A growing disparity in the predictions of committee members
    can indicate emerging data drift or model decay, as it suggests that the models
    are becoming increasingly uncertain about the current data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feedback loops**: In scenarios where user feedback or real-world outcomes
    are available, active ML can use this feedback to assess model performance. Rapid
    changes in user feedback patterns can provide early indications of shifts in data
    trends or declining model effectiveness.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Active ML’s capability for early issue detection is essential, allowing for
    prompt and timely interventions. This proactive strategy is markedly more effective
    than conventional reactive methods, where issues are only addressed following
    notable performance declines. By identifying problems early, active ML ensures
    that resources allocated for model retraining or adjustments are utilized efficiently
    and judiciously. This aspect is particularly crucial in environments where computational
    resources or labeled data are scarce. Moreover, in end-user applications, the
    consistency of model performance is essential for maintaining user trust. Through
    the early detection and timely correction of data drift or model decay, active
    ML contributes significantly to a reliable and consistent user experience, underlining
    its value in sustaining the credibility and effectiveness of machine learning
    models in various real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Early detection for data drift and model decay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The effective implementation of early detection for data drift and model decay
    in active ML necessitates several key considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right metrics for monitoring is the key. The metrics should align
    closely with the model’s objectives and the unique characteristics of the data
    and application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting realistic thresholds for alerts is also crucial, striking a balance
    between sensitivity and practicality to avoid frequent false alarms or missing
    critical changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating active ML systems with existing data pipelines is crucial for real-time
    monitoring and quick responses to detected issues, thereby enhancing system efficiency
    and responsiveness. For practical implementation, this means linking the active
    ML algorithm directly to the storage of user data, enabling the system to engage
    and analyze new data automatically as soon as it’s uploaded. This integration
    ensures continuous, up-to-date monitoring, which is vital for the timely detection
    and handling of potential data drift or model decay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider an example from a retail use case, specifically in the context
    of a computer vision system used for inventory management. Imagine a retail store
    using an active ML system equipped with computer vision for inventory management.
    This system is designed to monitor the store’s shelves using cameras to track
    stock levels, detect when items are running low, and identify when restocking
    is needed. The computer vision model is initially trained on a dataset of images
    depicting various states of shelf stock, from fully stocked to nearly empty. It
    learns to recognize different products, their locations, and their quantities.
    Over time, the store introduces new products and changes the layout of some items.
    The active ML system, during its routine monitoring, starts detecting anomalies
    in the images; the model encounters unfamiliar images, leading to uncertainty
    in its predictions. This uncertainty, often reflected in lower confidence scores,
    indicates potential anomalies. The system also employs statistical methods to
    identify outliers—data points that significantly deviate from established patterns
    of product arrangements. Additionally, it analyzes changes over time, comparing
    current images against historical data to spot deviations in product types or
    arrangements. When the system flags an anomaly, it alerts store managers for further
    inspection. If the change is intentional, such as with the introduction of new
    products, this information is used to update and retrain the model, ensuring it
    adapts to the new store layout and inventory. If the change is unintentional,
    such as a misplaced product, it can be corrected to maintain inventory accuracy.
    This adaptive process ensures the ML model remains effective in real-time inventory
    management, adjusting to both gradual and sudden changes in the retail environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the active ML system’s anomaly detection capability is crucial
    for maintaining the effectiveness of the inventory management system. It ensures
    that the computer vision model remains accurate and reliable in tracking inventory
    despite changes in the store’s product range and layout, thus preventing model
    decay and ensuring operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, by focusing on the most informative data points and integrating user
    feedback, active ML provides a dynamic approach to maintaining the relevance and
    accuracy of models amidst evolving user data and market conditions. The adaptability
    and efficiency delivered by this approach are vital for the long-term success
    of machine learning applications in various sectors.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have delved deeply into the crucial aspects of rigorously
    evaluating the performance of active ML systems. We began by understanding the
    significance of automating processes to enhance efficiency and accuracy. The chapter
    then guided us through various testing methodologies, emphasizing their role in
    ensuring robust and reliable active ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: A significant portion of our discussion focused on the criticality of the continuous
    monitoring of active ML pipelines. This monitoring is not just about observing
    the performance but also involves understanding and interpreting the results to
    make data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most pivotal topics we covered was determining the appropriate stopping
    criteria for active ML runs. We explored how setting pre-defined performance metrics,
    such as accuracy and precision, is crucial in guiding these decisions. We also
    emphasized the importance of a diverse and representative test set to ensure the
    model’s applicability in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we discussed the need to be mindful of overfitting, resource limitations,
    diminishing returns, and the importance of implementing feedback loops. These
    considerations play a key role in not only determining when to stop the ML run
    but also in ensuring the overall success and relevance of the model in a constantly
    evolving environment.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have established that active ML is exceptionally adept at monitoring
    models in production environments. Its application extends to the early detection
    of data drift and model decay, particularly when seamlessly integrated into user
    data pipelines. This integration enables the active ML system to monitor data,
    ensuring that any deviations or anomalies are promptly detected continuously.
    Moreover, the system can be configured to trigger alerts when these irregularities
    occur, allowing for immediate attention and action. This capability not only enhances
    the model’s reliability and accuracy but also ensures its adaptability and resilience
    in the face of evolving data landscapes, making active ML a powerful tool in production
    model monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, *Utilizing Tools and Packages for Active ML*, we will turn
    our attention to the various Python libraries, frameworks, and tools commonly
    used in active ML. We will provide an overview of these resources, highlighting
    their value in implementing various active ML techniques. This will equip you
    with the necessary knowledge and skills to elevate your active ML projects and
    define what tools are best suited for them. This chapter promises to be a comprehensive
    guide to the current active ML tools.
  prefs: []
  type: TYPE_NORMAL
