- en: '*Chapter 5:*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：'
- en: Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Define binary classification in supervised machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义监督机器学习中的二元分类
- en: 'Perform binary classification using white-box models: logistic regression and
    decision trees'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用白盒模型进行二元分类：逻辑回归和决策树
- en: Evaluate the performance of supervised classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估监督分类模型的表现
- en: Perform binary classification using black-box ensemble models – Random Forest
    and XGBoost
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用黑盒集成模型进行二元分类 - 随机森林和XGBoost
- en: Design and develop deep neural networks for classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和开发用于分类的深度神经网络
- en: Select the best model for a given classification use case
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为给定的分类用例选择最佳模型
- en: In this chapter, we will focus on solving classification use cases for supervised
    learning. We will use a dataset designed for a classification use case, frame
    a business problem around it, and explore a few popular techniques to solve the
    problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于解决监督学习的分类用例。我们将使用一个为分类用例设计的数据库，围绕它构建一个业务问题，并探索一些流行的技术来解决该问题。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简介
- en: Let's quickly brush up on the topics we learned in *Chapter 3*, *Introduction
    to Supervised Learning*. Supervised learning, as you already know by now, is the
    branch of machine learning and artificial intelligence that helps machines learn
    without explicit programming. A more simplified way of describing supervised learning
    would be developing algorithms that learn from labeled data. The broad categories
    in supervised learning are classification and regression, differentiated fundamentally
    by the type of label, that is, **continuous** or **categorical**. Algorithms that
    deal with continuous variables are known as **regression algorithms**, and those
    with categorical variables are called **classification algorithms**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下我们在*第三章*中学到的内容，*监督学习简介*。正如你现在所知道的，监督学习是机器学习和人工智能的一个分支，它帮助机器在没有明确编程的情况下学习。描述监督学习的一种更简单的方式是开发从标记数据中学习的算法。监督学习的广泛类别包括分类和回归，它们的基本区别在于标签的类型，即**连续**或**分类**。处理连续变量的算法被称为**回归算法**，而处理分类变量的算法被称为**分类算法**。
- en: 'In classification algorithms, our target, dependent, or criterion variable
    is a **categorical variable**. Based on the number of classes, we can further
    divide them into the following groups:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类算法中，我们的目标、依赖或标准变量是一个**分类变量**。根据类别的数量，我们可以进一步将它们分为以下几组：
- en: Binary classification
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类
- en: Multinomial classification
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式分类
- en: Multi-label classification
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签分类
- en: In this chapter, we will focus on **binary classification**. Discussing the
    specifics and practical examples of multinomial and multi-class classification
    is beyond the scope of this chapter; however, a few additional reading references
    for advanced topics will be listed before wrapping up the chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于**二元分类**。讨论多项式分类和多类分类的细节和实际例子超出了本章的范围；然而，在结束本章之前，我们将列出一些高级主题的额外阅读参考。
- en: Binary classification algorithms are the most popular class of algorithms within
    machine learning and have numerous applications in business, research, and academia.
    Simple models that classify a student's chances of passing a future exam based
    on their past performance as pass or fail, predict whether it will rain or not,
    predict whether a customer will default on a loan or not, predict whether a patient
    has cancer or not, and so on are all common use cases that are solved by classification
    algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类算法是机器学习中最受欢迎的算法类别，在商业、研究和学术界有众多应用。简单的模型可以根据学生的过去表现（通过或失败）来预测他们未来考试通过的可能性，预测是否会下雨，预测客户是否会违约，预测患者是否患有癌症等等，这些都是由分类算法解决的常见用例。
- en: Before diving deeper into algorithms, we will first get started with a use case
    that will help us solve a supervised learning classification problem with hands-on
    exercises.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究算法之前，我们将首先从一个小案例开始，这个小案例将帮助我们通过实际练习解决监督学习分类问题。
- en: Getting Started with the Use Case
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用用例
- en: In this chapter, we will refer to the `weather` dataset, obtained from the Australian
    Commonwealth Bureau of Meteorology and made available through R. The dataset has
    two target variables, `RainTomorrow`, a flag indicating whether it will rain tomorrow,
    and `RISK_MM`, which measures the amount of rainfall for the following day.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将参考来自澳大利亚联邦气象局并通过R提供的`weather`数据集。该数据集有两个目标变量，`RainTomorrow`，一个表示明天是否会下雨的标志，以及`RISK_MM`，它衡量的是下一天降雨量。
- en: In a nutshell, we can use this dataset for `RainTomorrow`, for our classification
    exercise. The metadata and additional details about the dataset are available
    to explore at https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather.
    Since the dataset is readily available through R, we don't need to separately
    download it; instead, we can directly use the R function within the `rattle` library
    to load the data into system memory.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以使用这个数据集的`RainTomorrow`进行分类练习。有关数据集的元数据和附加详细信息可在https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather上探索。由于数据集可以通过R直接使用，我们不需要单独下载它；相反，我们可以直接使用`rattle`库中的R函数将数据加载到系统内存中。
- en: Some Background on the Use Case
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于用例的一些背景信息
- en: Several weather parameters, such as temperature, direction, pressure, cloud
    cover, humidity, and sunshine, were recorded daily for one year. The rainfall
    for the next day is already engineered in the dataset as the target variable,
    `RainTomorrow`. We can leverage this data to define a machine learning model that
    learns from the present day's weather parameters and predicts the chances of rain
    for the next day.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 记录了温度、方向、气压、云量、湿度和日照等天气参数，持续一年。下一天的降雨量已经在数据集中作为目标变量`RainTomorrow`进行工程化。我们可以利用这些数据定义一个机器学习模型，该模型从当天的天气参数中学习，并预测下一天的降雨概率。
- en: Rainfall prediction is of paramount importance to many industries. Long-haul
    journeys by train and buses usually look at changing weather patterns, primarily
    rainfall, to estimate the arrival time and journey length. Similarly, most brick
    and mortar stores, small restaurants and food joints, and others are all heavily
    impacted by rainfall. Gaining visibility of the weather conditions for the next
    day can help businesses better prepare in several ways, to combat business losses
    and, in some cases, maximize business outcomes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 雨量预测对许多行业至关重要。火车和长途汽车的长途旅行通常关注天气模式的变化，主要是降雨，以估计到达时间和旅行长度。同样，大多数实体店、小型餐馆和食品摊位等也受到降雨的严重影响。了解明天的天气条件可以帮助企业从多个方面更好地准备，以应对业务损失，在某些情况下，甚至可以最大化业务成果。
- en: To build nice intuition around the problem-solving exercise, let's frame a business
    problem using the dataset and develop the problem statement for the use case.
    Since the data is about rainfall prediction, we will choose a popular business
    problem faced by today's hyper-local food-delivery services. Start-ups such as
    DoorDash, Skip the Dishes, FoodPanda, Swiggy, Foodora, and many others offer hyper-local
    food delivery services to customers in different countries. A common trend observed
    in most countries is the rise in food delivery orders with the onset of rain.
    In general, most delivery companies expect around a 30%-40% increase in the total
    number of deliveries on a given day. Given the limited number of delivery agents,
    the delivery time is impacted immensely due to increased orders on rainy days.
    To keep costs optimal, it is not viable for these companies to increase the number
    of full-time agents; therefore, a common strategy is to dynamically hire more
    agents for days when demand for the service is expected to be high. To plan better,
    visibility of rainfall predictions for the next day is of paramount importance.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在问题解决练习中建立良好的直觉，让我们使用数据集构建一个业务问题，并为用例制定问题陈述。由于数据是关于降雨预测的，我们将选择当今超本地食品配送服务面临的流行业务问题。DoorDash、Skip
    the Dishes、FoodPanda、Swiggy、Foodora等初创公司为不同国家的客户提供超本地食品配送服务。在大多数国家，一个普遍的趋势是随着雨季的到来，食品配送订单量增加。一般来说，大多数配送公司预计在给定的一天内总配送量会增加30%-40%。由于配送代理人数有限，雨天订单的增加对配送时间影响巨大。为了保持成本最优，这些公司不可能增加全职代理人数；因此，一个常见的策略是在预计服务需求高的日子里动态雇佣更多代理。为了更好地规划，了解下一天的降雨预测至关重要。
- en: Defining the Problem Statement
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义问题陈述
- en: With the context of the problem set up, let's try to define our problem statement
    for a hyper-local food-delivery service company to predict the rainfall for the
    next day. To keep things simple and consistent, let's frame the problem statement
    using the frameworks we studied previously, in *Chapter 2*, *Exploratory Analysis
    of Data*. This will help us distill the end goal we want to solve in a business-first
    approach while keeping the machine learning perspective at the forefront.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure creates a simple visual for the **Situation** - **Complication**
    - **Question** (**SCQ**) framework for the previously defined use case:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: SCQ for the classification use case'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_01.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.1: SCQ for the classification use case'
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can clearly answer the question from the SCQ: we would need a predictive
    model to predict the chances of rain for the next day as a solution to the problem.
    Let''s move on to the next step – gathering data to build a predictive model that
    will help us solve the business problem.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Data Gathering
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `rattle.data` package provides us with the data for the use case, which
    can be accessed using the internal dataset methods of R. In case you have not
    already installed the packages, you can easily install them using the `install.packages("rattle.data")`
    command.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 63: Exploring Data for the Use Case'
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform the initial exploration of the dataset we
    have gathered for the use case. We will explore the shape of the data, that is,
    the number of rows and columns, and study the content within each column.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the shape (rows x columns) and content of the data, perform the
    following steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the `rattle` package using the following command:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the data for our use case, which is available from the `rattle` package:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The `weatherAUS` dataset is a DataFrame containing more than 1,40,000 daily
    observations from over 45 Australian weather stations.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, load the weather data directly into a DataFrame called `df`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Explore the DataFrame''s content using the `str` command:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2: Final output'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_02.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.2: Final output'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have almost 1,50,000 rows of data and 24 variables. We would need to drop
    the `RISK_MM` variable, as it will be the target variable for the regression use
    case (that is, predicting how much it will rain the next day). Therefore, we are
    left with 22 independent variables and 1 dependent variable, `RainTomorrow`, for
    our use case. We can also see a good mix of continuous and categorical variables.
    The `Location`, `WindDir`, `RainToday`, and many more variables are categorical,
    and the remainder are continuous.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vwgu8Q.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will calculate the total percentage of the null values
    in each column.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 64: Calculating the Null Value Percentage in All Columns'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset we explored in *Exercise 1*, *Exploring Data for the Use Case* has
    quite a few null values. In this exercise, we will write a script to calculate
    the percentage of null values within each column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*练习 1*，*探索数据集以用于用例*中探索的数据集有相当多的空值。在这个练习中，我们将编写一个脚本来计算每个列中空值的百分比。
- en: We can see the presence of null values in a few variables. Let's check the percentage
    of null values in each column within the `df` dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到几个变量中存在空值。让我们检查`df`数据集中每个列的空值百分比。
- en: 'Perform the following steps to calculate the percentage of null values in each
    column of the dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以计算数据集中每列的空值百分比：
- en: 'First, remove the column named `RISK_MM`, since it is supposed to be used as
    a target variable for regression use. (Adding this to our model will result in
    data leakage.):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，移除名为`RISK_MM`的列，因为它打算用作回归用途的目标变量。（将其添加到我们的模型会导致数据泄露）：
- en: '[PRE4]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `temp_df` DataFrame object and store the value in it:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`temp_df` DataFrame对象并将其值存储在其中：
- en: '[PRE5]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, use the `print` function to display the percentage null values in each
    column using the following command:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用`print`函数显示每列的空值百分比，使用以下命令：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see that the last four variables have more than *30%* missing or null
    values. This is a significantly huge drop. It would be best to drop these variables
    from our analysis. Also, we can see that there are a few other variables that
    have roughly *1%*-*2%*, and in some cases, up to *10%* missing or null values.
    We can treat these variables using various missing value treatment techniques,
    such as replacing them with mean or mode. In some important cases, we can also
    use additional techniques, such as clustering-based mean and mode replacement,
    for improved treatment. Additionally, in very critical scenarios, we can use a
    regression model to estimate the remainder of the missing values by defining a
    model where the column with the required missing value is treated as a function
    of the remaining variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到最后四个变量有超过*30%*的缺失或空值。这是一个相当大的下降。最好从我们的分析中删除这些变量。此外，我们还可以看到一些其他变量大约有*1%*到*2%，在某些情况下，高达*10%*的缺失或空值。我们可以使用各种缺失值处理技术来处理这些变量，例如用均值或众数替换它们。在某些重要情况下，我们还可以使用基于聚类的均值和众数替换等额外技术来提高处理效果。此外，在非常关键的场景中，我们可以使用回归模型来估计剩余缺失值的剩余部分，通过定义一个模型，其中所需的缺失值列被视为剩余变量的函数。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2ViZEp1.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2ViZEp1。
- en: In the following exercise, we will remove null values. We will revisit data
    if we do not have a good model in place.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将移除空值。如果没有合适的模型，我们将重新审视数据。
- en: 'Exercise 65: Removing Null Values from the Dataset'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 65：从数据集中移除空值
- en: John is working on the newly created dataset, and while doing analysis, he has
    found out that the dataset contains significant null values. To make the dataset
    useful for further analysis, he must remove the null values from it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰正在处理新创建的数据集，在进行分析时，他发现数据集中存在显著的空值。为了使数据集对进一步分析有用，他必须从其中移除空值。
- en: 'Perform the following steps to remove the null values from the `df` dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以从`df`数据集中移除空值：
- en: 'First, select the last four columns to drop that have more than *30%* null
    values using the following command:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下命令选择最后四列，这些列的空值超过*30%*：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Remove all the rows from the DataFrame that will have one or more columns with
    null values using the `na.omit` command, which removes all of the null rows from
    the DataFrame:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`na.omit`命令从DataFrame中移除所有包含一个或多个空值列的行，该命令会从DataFrame中移除所有空行：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, print the newly formatted data using the following `print` commands:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下`print`命令打印新格式化的数据：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the following command, verify whether the newly created dataset contains
    null values or not:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，验证新创建的数据集中是否存在空值：
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, print the dataset using the following `print` command:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下`print`命令打印数据集：
- en: '[PRE13]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can now double check and see that the new dataset has no more missing values
    and the overall number of rows in the dataset also reduced to 112,000, which is
    around a *20%* loss of training data. We should use missing value treatment techniques
    such as replacing missing values with the mean, mode, or median to combat such
    high losses due to the omission of missing values. A rule of thumb would be to
    safely ignore anything less than a *5%* loss. Since, we have more than 1,00,000
    records (a reasonably high number of records for a simple use case), we are ignoring
    this rule of thumb.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以再次检查，看看新的数据集没有更多的缺失值，数据集的总行数也减少到112,000行，这大约是训练数据的*20%*损失。我们应该使用替换缺失值（如平均值、众数或中位数）等技术来对抗由于缺失值的省略而导致的高损失。一个经验法则是安全地忽略小于*5%*的损失。由于我们有超过100,000条记录（对于简单用例来说是一个相当高的记录数），我们正在忽略这个经验法则。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q3HIgT.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2Q3HIgT。
- en: Additionally, we can also engineer date- and time-related features using the
    `Date` column. The following exercise creates numeric features such as day, month,
    day of the week, and quarter of the year as additional time-related features and
    drops the original `Date` variable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用`Date`列来构建日期和时间相关的特征。以下练习创建了诸如日、月、星期和季度等数值特征作为额外的时相关特征，并删除了原始的`Date`变量。
- en: We will use the `lubridate` library in R to work with date and time-related
    features. It provides us with extremely easy-to-use functions to perform date
    and time operations. If you have not already installed the package, please install
    the library using the `install.packages('lubridate')` command.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用R中的`lubridate`库来处理日期和时间相关特征。它为我们提供了执行日期和时间操作的极其易于使用的函数。如果您尚未安装此包，请使用`install.packages('lubridate')`命令安装库。
- en: 'Exercise 66: Engineer Time-Based Features from the Date Variable'
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习66：从日期变量中构建基于时间的特征
- en: Time- and date-related attributes cannot be directly used in a supervised classification
    model. To extract meaningful properties from date- and time-related variables,
    it is a common practice to create month, year, week, and quarter from the date
    as features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 时间和日期相关的属性不能直接用于监督分类模型。为了从日期和时间相关的变量中提取有意义的属性，通常的做法是从日期中创建月份、年份、星期和季度作为特征。
- en: 'Perform the following steps to work with the data and time function in R:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以在R中处理数据和时间函数：
- en: 'Import the `lubridate` library into RStudio using the following command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将`lubridate`库导入RStudio：
- en: '[PRE15]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `lubridate` library provides handy date- and time-related functions.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`lubridate`库提供了方便的日期和时间相关函数。'
- en: 'Extract `day`, `month`, `dayofweek`, and `quarter` as new features from the
    `Date` variable using the `lubridate` function:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lubridate`函数从`Date`变量中提取`day`、`month`、`dayofweek`和`quarter`作为新特征：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Examine the newly created variables:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查新创建的变量：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have created all of the date- and time-related features, we won''t
    need the actual `Date` variable. Therefore, delete the previous `Date` column:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了所有日期和时间相关的特征，我们不再需要实际的`Date`变量。因此，删除之前的`Date`列：
- en: '[PRE18]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this exercise, we have extracted meaningful features from date- and time-related
    attributes from the data and removed the actual date-related columns.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们从日期和时间相关的属性中提取了有意义的特征，并删除了实际的日期相关列。
- en: Note
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/2E4hOEU.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/2E4hOEU。
- en: 'Next, we need to process or clean another feature within the DataFrame: `location`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要处理或清理DataFrame中的另一个特征：`location`。
- en: 'Exercise 67: Exploring the Location Frequency'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习67：探索位置频率
- en: The `Location` variable defines the actual location where the weather data was
    captured for the specified time. Let's do a quick check on the number of distinct
    values that are captured within this variable and see whether there are any interesting
    patterns that might be of importance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`Location`变量定义了在指定时间实际捕获天气数据的实际位置。让我们快速检查这个变量中捕获的不同值的数量，看看是否有任何可能重要的有趣模式。'
- en: In the following exercise, we will be using the `Location` variable to define
    the actual location where the weather data was captured for the specified time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，我们将使用`Location`变量来定义在指定时间实际捕获天气数据的实际位置。
- en: 'Perform the following steps:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Calculate the frequency of rain across each location using the grouping functions
    from the `dplyr` package:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dplyr`包中的分组函数计算每个位置的降雨频率：
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Examine the number of distinct locations for sanity:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查不同位置的数量以确保正确：
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Print `summary` to examine the aggregation performed:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`summary`以检查执行的聚合：
- en: '[PRE23]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that there are 44 distinct locations in the data. The `cnt` variable,
    which defines the number of records (in the previous transformed data) for each
    location, has an average 2,566 records. The similar number distribution between
    the first quartile, median, and third quartile denote that the locations are evenly
    distributed in the data. However, if we investigate the percentage of records
    where rain was recorded (`pct`), we see an interesting trend. Here, we have locations
    with around a *6%* chance of rain and some with around a *36%* chance of rain.
    There is a huge difference in the possibility of rain, based on the location.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据中有44个不同的位置。定义每个位置记录数（在之前转换的数据中）的`cnt`变量，平均有2,566条记录。第一四分位数、中位数和第三四分位数的相似数量分布表明，位置在数据中分布均匀。然而，如果我们调查记录降雨的记录百分比（`pct`），我们会看到一个有趣的趋势。在这里，我们有一些位置大约有6%的降雨概率，还有一些位置大约有36%的降雨概率。根据位置的不同，降雨的可能性有很大的差异。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30aKUMx.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到完整的代码：http://bit.ly/30aKUMx。
- en: Since we have around 44 distinct locations, it is difficult to utilize this
    variable directly as a categorical feature. In R, most supervised learning algorithms
    internally convert the categorical column into a numerical form that can be interpreted
    by the model. However, with an increased number of classes within the categorical
    variable, the complexity of the model increases with no additional value. To keep
    things simple, we can transform the `Location` variable as a new variable with
    a reduced number of levels. We will select the top five and the bottom five locations
    with chances of rain and tag all other locations as `Others`. This will reduce
    the number of distinct levels in the variable as 10+1 and will be more suitable
    for the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有大约44个不同的位置，直接将此变量作为分类特征使用是有困难的。在R中，大多数监督学习算法内部将分类列转换为模型可以解释的数值形式。然而，随着分类变量中类别的数量增加，模型的复杂性也随之增加，但没有额外的价值。为了保持简单，我们可以将`Location`变量转换为一个具有较少级别的新的变量。我们将选择降雨概率最高的五个和最低的五个位置，并将所有其他位置标记为`Others`。这将减少变量中不同级别的数量为10+1，这将更适合模型。
- en: 'Exercise 68: Engineering the New Location with Reduced Levels'
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习68：创建具有较少级别的新的位置
- en: The `location` variable has too many distinct values (44 locations), and machine
    learning models in general do not perform well with categorical variables with
    a high frequency of distinct classes. We therefore need to prune the variable
    by reducing the number of distinct classes within it. We will select the top five
    and the bottom five locations with chances of rain and tag all other locations
    as `Others`. This will reduce the number of distinct levels in the variable as
    10+1 and will be more suitable for the model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`location`变量有太多的不同值（44个位置），通常机器学习模型在具有高频率不同类别的分类变量上表现不佳。因此，我们需要通过减少变量中不同类别的数量来修剪变量。我们将选择降雨概率最高的五个和最低的五个位置，并将所有其他位置标记为`Others`。这将减少变量中不同级别的数量为10+1，这将更适合模型。'
- en: 'Perform the following steps to engineer a new variable for location with a
    reduced number of distinct levels:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建一个具有较少不同级别的位置新变量：
- en: 'Convert the `location` variable from a factor into a character:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`location`变量从因子转换为字符：
- en: '[PRE25]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a list with the top five and the bottom five locations with respect
    to the chances of rain. We can do this by using the `head` command for the top
    five and the `tail` command for the bottom five locations after ordering the DataFrame
    in ascending order:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含降雨概率最高和最低的五个位置的列表。我们可以通过在按升序排序的DataFrame中使用`head`命令获取前五个位置，以及使用`tail`命令获取后五个位置来实现这一点：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Print the list to double-check that we have the locations correctly stored:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印列表以确认我们已经正确存储了位置：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Convert the `Location` variable in the main `df_new` DataFrame into a `character`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将主`df_new` DataFrame中的`Location`变量转换为`字符`：
- en: '[PRE29]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Reduce the number of distinct locations in the variable. This can be done by
    tagging all the locations that are not a part of the `location_list` list as `Others`:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 减少变量中不同位置的数量。这可以通过将所有不属于`location_list`列表的位置标记为`Others`来实现：
- en: '[PRE30]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Delete the old `Location` variable using the following command:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令删除旧的`Location`变量：
- en: '[PRE31]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To ensure that the fifth step was correctly performed, we can create a temporary
    DataFrame and summarize the frequency of records against the new `location` variable
    we created:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保第五步正确执行，我们可以创建一个临时DataFrame，并总结记录频率与我们所创建的新`location`变量之间的对比：
- en: '[PRE32]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Print the temporary test DataFrame and observe the results. We should see only
    11 distinct location values:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印临时测试DataFrame并观察结果。我们应该只看到11个不同的位置值：
- en: '[PRE33]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We first convert the `Location` variable from a factor to a character to ease
    the string operation's tasks. The DataFrame is sorted in descending order according
    to the percentage chance of rain. The `head` and the `tail` commands are used
    to extract the top and bottom five locations in a list. This list is then used
    as a reference check to reduce the number of levels in the new feature. Finally,
    after engineering the new feature with the reduced levels, we do a simple check
    to ensure that our feature has been engineered in the way we expect.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将`Location`变量从因子转换为字符，以简化字符串操作任务。DataFrame根据降雨概率的百分比降序排序。`head`和`tail`命令用于提取列表中的前五个和后五个位置。然后，这个列表被用作参考检查，以减少新特征中的级别数量。最后，在工程化减少级别的新的特征后，我们进行简单的检查以确保我们的特征已经按照预期的方式工程化。
- en: Note
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the complete code on GitHub: http://bit.ly/30fnR31.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到完整的代码：http://bit.ly/30fnR31。
- en: Let's now get into the most interesting topic of the chapter and explore classification
    techniques for supervised learning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在进入本章最有趣的主题，并探讨监督学习的分类技术。
- en: Classification Techniques for Supervised Learning
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习的分类技术
- en: To approach a **supervised classification algorithm**, we first need to understand
    the basic functioning of the algorithm, explore a bit of the math in an abstract
    way, and then develop the algorithm using readily available packages in R. We
    will cover a few basic algorithms, such as white-box algorithms such as Logistic
    Regression and Decision Trees, and then we will move on to advanced modeling techniques,
    such as black-box models such as Random Forest, XGBoost, and neural networks.
    The list of algorithms we plan to cover is not exhaustive, but these five algorithms
    will help you gain a broad understanding of the topic.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要接近**监督分类算法**，我们首先需要理解算法的基本功能，以抽象的方式探索一点数学，然后使用R中现成的包来开发算法。我们将介绍几个基本算法，例如透明算法如逻辑回归和决策树，然后我们将转向高级建模技术，例如黑盒模型如随机森林、XGBoost和神经网络。我们计划涵盖的算法列表并不全面，但这五个算法将帮助您对主题有一个广泛的理解。
- en: Logistic Regression
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '**Logistic regression** is the most favorable white-box model used for binary
    classification. White-box models are defined as models where we have visibility
    of the entire reasoning used for the prediction. For each prediction made, we
    can leverage the model''s mathematical equation and decode the reasons for the
    prediction made. There are also a set of classification models that are completely
    black-box, that is, by no means can we understand the reasoning for the prediction
    leveraged by the model. In situations where we want to focus on only the end outcome,
    we should prefer black-box models, as they are more powerful.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**逻辑回归**是用于二元分类的最受欢迎的透明模型。透明模型定义为我们可以看到用于预测的整个推理过程的模型。对于每个做出的预测，我们可以利用模型的数学方程式来解码预测的原因。也存在一组完全黑盒的分类模型，也就是说，我们根本无法理解模型利用的预测推理。在我们只想关注最终结果的情况下，我们应该选择黑盒模型，因为它们更强大。'
- en: Though the name ends with *regression*, logistic regression is a technique used
    to predict binary categorical outcomes. We would need a different approach to
    model for a categorical outcome. This can be done by transforming the outcome
    into a log of odds ratio or the probability of the event happening.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称以*回归*结尾，但逻辑回归是一种用于预测二元分类结果的技巧。我们需要不同的方法来对分类结果进行建模。这可以通过将结果转换为优势比的对数或事件发生的概率来实现。
- en: Let's distill this approach into simpler constructs. Assume the probability
    of success for an event is 0.8\. Then, the probability of failure for the same
    event would be defined as *(1-0.8) = 0.2*. The odds of success are defined as
    the ratio of the probability of success over the probability of failure.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种方法提炼成更简单的结构。假设一个事件成功的概率为0.8。那么，同一事件失败的概率将被定义为*(1-0.8) = 0.2*。成功的优势被定义为成功概率与失败概率的比率。
- en: 'In the following example, the odds of success would be *(0.8/0.2) = 4*. That
    is, the odds of success are four to one. If the probability of success is 0.5,
    that is, a 50-50 percent chance, then the odds of success are 0.5 to 1\. The logistic
    regression model can be mathematically represented as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，成功的优势将是*(0.8/0.2) = 4*。这意味着成功的优势是四比一。如果成功的概率是0.5，即50-50的机会，那么成功的优势是0.5比1。逻辑回归模型可以用以下方式数学表示：
- en: '![](img/C12624_05_10.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_10.jpg)'
- en: Where, ![](img/C12624_05_11.png) is the log of odds ratio.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![图片](img/C12624_05_11.png)是优势比的对数。
- en: 'Solving the math further, we can deduce the probability of the outcome as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步解决数学问题，我们可以推导出以下结果：
- en: '![](img/C12624_05_12.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/C12624_05_12.jpg)'
- en: Discussing the mathematical background and derivation of the equations is beyond
    the scope of the chapter. To summarize, the `logit` function, that is, the link
    function, helps logistic regression reframe the problem (predicted outcome) intuitively
    as the log of odds ratio. When solved, it helps us predict the probability of
    a binary dependent variable.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论方程的数学背景和推导超出了本章的范围。为了总结，`logit`函数，即连接函数，帮助逻辑回归直观地将问题（预测结果）重新表述为优势比的对数。当求解时，它帮助我们预测二元因变量的概率。
- en: How Does Logistic Regression Work?
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归是如何工作的？
- en: Just like linear regression, where the beta coefficients for the variables are
    estimated using the **Ordinary Least Squares** (**OLS**) method, a logistic regression
    model leverages the **maximum-likelihood estimation** (**MLE**). The MLE function
    estimates the best set of values of the model parameters or beta coefficients
    such that it maximizes the likelihood function, that is, the probability estimates,
    which can be also defined as the *agreement* of the selected model with the observed
    data. When the best set of parameter values are estimated, plugging these values
    or beta coefficients into the model equation as previously defined would help
    in estimating the probability of the outcome for a given sample. Akin to OLS,
    MLE is also an iterative process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归中，变量的贝塔系数是通过**普通最小二乘法**（**OLS**）来估计的，逻辑回归模型利用**最大似然估计**（**MLE**）。MLE函数估计模型参数或贝塔系数的最佳值集，以最大化似然函数，即概率估计，这也可以定义为所选模型与观察数据的**一致性**。当最佳参数值集被估计出来后，将这些值或贝塔系数按先前定义的方式插入到模型方程中，将有助于估计给定样本的输出概率。类似于OLS，MLE也是一个迭代过程。
- en: Let's see a logistic regression model in action on our dataset. To get started,
    we will use only a small subset of variables for the model. Ideally, it is recommended
    to start with the most important variables based on the EDA exercise and then
    incrementally add remainder variables. For now, we will start with a temperature-related
    variable for the maximum and minimum values, a wind speed-related variable, pressure
    and humidity at 3 P.M., and the rainfall for the current day.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看逻辑回归模型在我们数据集上的实际应用。为了开始，我们将只使用模型的一小部分变量。理想情况下，建议根据EDA练习开始于最重要的变量，然后逐步添加剩余变量。现在，我们将从一个与最高和最低温度值相关的变量开始，一个与风速相关的变量，下午3点的气压和湿度，以及当天的降雨量。
- en: We will divide the entire dataset into train (70%) and test (30%). While fitting
    the data to the model, we will only use the train dataset and will later evaluate
    the performance of the model on the train, as well as the unseen test data. This
    approach will help us understand whether our model is overfitting and provide
    a more realistic model performance on unseen data.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整个数据集分为训练集（70%）和测试集（30%）。在将数据拟合到模型时，我们只将使用训练集，稍后将在训练集以及未见过的测试数据上评估模型的表现。这种方法将帮助我们了解我们的模型是否过拟合，并在未见过的数据上提供更现实的模型性能。
- en: 'Exercise 69: Build a Logistic Regression Model'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习69：构建逻辑回归模型
- en: We will build a binary classification model using logistic regression and the
    dataset we explored in the Exercises 1-6\. We will divide the data into train
    and test (70% and 30%, respectively) and leverage the training data to fit the
    model and the test data to evaluate the model's performance on unseen data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set `seed` for reproducibility using the following command:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, create a list of indexes for the training dataset (70%):'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, split the data into test and train datasets using the following commands:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Build the logistic regression model with `RainTomorrow` as the dependent variable
    and a few independent variables (we selected `MinTemp`, `Rainfall`, `WindGustSpeed`,
    `WindSpeed3pm`, `Humidity3pm`. `Pressure3pm`, `RainToday`, `Temp3pm`, and `Temp9am`).
    We can add all the available independent variables in the DataFrame too:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Print the summary of the dataset using the `summary` function:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The `set.seed` command ensures that the random selections used for the train
    and test data split can be reproduced. We divide the data into 70% train and 30%
    test. The set seed function ensures that, for the same seed, we get the same split
    every time. The `glm` function is used in R to build generalized linear models.
    Logistic regression is defined in the model using the `family` parameter value
    set to `binomial(link ='logit')`. The `glm` function can be used to build several
    other models too (such as gamma, Poisson, and binomial). The formula defines the
    dependent, as well as the set of independent, variables. It takes the general
    form *Var1 ~ Var2 + Var3 + …*, which denotes `Var1` as the dependent or target
    variable and the remainder as the independent variables. If we want to use all
    of the variables in the DataFrame as independent variables, we can instead use
    `formula = Var1 ~ .`, which would indicate that `Var1` is the dependent variable
    and the rest are all independent variables.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2HwwUUX.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Results of Logistic Regression
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We previously had a glimpse of logistic regression in *Chapter 2*, *Exploratory
    Analysis of Data*, but we didn't get into the specifics of the model results.
    The results demonstrated in the previous output snippet will look like what you
    observed in linear regression, but with some differences. Let's explore and interpret
    the results part by part.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we have the `glm` function calculates two types of residuals, that
    is, **Null Deviance** and **Residual Deviance**. The difference between the two
    is that one reports the goodness of fit when only the intercept (that is, no dependent
    variables) is used and the other reports when all the provided independent variables
    are used. The reduction in deviance between null and residual deviance helps us
    understand the quantified value added by the independent variables in defining
    the variance or the predictive correctness. The distribution of deviance residuals
    is reported right after the formula.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the **beta coefficients** and the associated **standard error**,
    the *z-value* and the *p-value*, which is the probability of significance. For
    each variable provided, R internally calculates the coefficients and, along with
    the parameter value, it also reports additional test results to help us interpret
    how effective these coefficients are. The absolute value of the coefficient is
    a simple way to understand how important that variable is to the final predictive
    power, that is, how impactful the variable is in determining the end outcome of
    the prediction. We can see that all variables have a low value for the coefficient.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Next, the standard error helps us quantify how stable the value will be. A lower
    value for the standard error would indicate more consistent or stable values for
    the beta coefficients. The standard errors for all the variables in our exercise
    are low. The *z-value* and the probability of significance together help us take
    a call as to whether the results are statistically significant or just appear
    as they are due to random chance. This idea follows on from the same principle
    we learned about the null and alternate hypothesis in *Chapter 2*, *Exploratory
    Analysis of Data*, and is akin to linear regression parameter significance, which
    we learned about in *Chapter 4*, *Regression*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to interpret the significance would be to study the *asterix*
    besides each independent variable, that is, `*`. The number of `*` is defined
    by the actual probability value, as defined below the parameter values. In our
    exercise, notice that the `MinTemp` variable is not statistically significant,
    that is, *p-value > 0.05*. The rest are all statistically significant variables.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The **Akaike Information Criterion** (**AIC**) is again a metric reported by
    R to assess the goodness of fit of the model or the quality of the model. This
    number comes in handy to compare different models for the same use case. Say you
    fit several models using a combination of independent variables but the same dependent
    variable, the AIC can be used to study the best model by way of a simple comparison
    of the value in all models. The calculation of the metric is derived from the
    deviance between the model's prediction and the actual labels, but factors in
    the presence of variables that are not adding any value. Therefore, akin to **R
    Squared** and **adjusted R Squared** in linear regression, the AIC helps us to
    avoid building complicated models. To select the best model from a list of candidate
    models, we should select the model with the lowest AIC.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end of the previous output, we can see the results from **Fisher's
    Scoring** algorithm, which is a derivative of Newton's method for solving maximum
    likelihood problems numerically. We see that it required five iterations to fit
    the data to the model, but beyond that, this information is not of much value
    to us. It is a simple indication for us to conclude that the model did converge.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how logistic regression works and have interpreted the results
    reported by the model in R. However, we still need to evaluate the model results
    using our train and test dataset and ensure that the model performs well on unseen
    data. To study the performance of a classification model, we would need to leverage
    various metrics, such as accuracy, precision, and recall. Though we already explored
    them in *Chapter 4*, *Regression*, let's now study them in more detail.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Classification Models
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification models require a bunch of different metrics to be thoroughly
    evaluated, unlike regression models. Here, we don't have something as intuitive
    as **R Squared**. Moreover, the performance requirements completely change based
    on a specific use case. Let's take a brief look at the various metrics that we
    already studied in *Chapter 3*, *Introduction to Supervised Learning*, for classification.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix and Its Derived Metrics
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first basis for studying model performance for classification algorithms
    starts with a **confusion matrix**. A confusion matrix is a simple representation
    of the distribution of predictions of each class across the actuals of each class:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Confusion matrix'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_03.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.3: Confusion matrix'
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The previous table is a simple representation of a confusion matrix. Here, we
    assume that the `1`) and `0`); when the result is correctly predicted, then we
    assign `1` correctly predicted as `1` and so on for the remaining outcomes.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the confusion matrix and the values defined from it, we can further
    define a couple of metrics that will help us better understand the model''s performance.
    We will now use the abbreviations **TP** for **True Positive**, **FP** for **False
    Positive**, **TN** for **True Negative**, and **FN** for **False Negative** going
    forward:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall accuracy**: Overall accuracy is defined as the ratio of total correct
    predictions to the total number of predictions in the entire test sample. So,
    this would be simply the sum of **True Positives** and **True Negatives** divided
    by all the metrics in the confusion matrix:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_13.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: '**Precision** or **Positive Predictive Value** (**PPV**): Precision is defined
    as the ratio of correctly predicted positive labels to the total number of positively
    predicted labels:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_14.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: '**Recall** or **Sensitivity**: Recall measures how sensitive your model is
    by representing the ratio of the number of correctly predicted positive labels
    to the total number of actual positive labels:'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_15.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: '**Specificity** or **True Negative Rate** (**TNR**): Specificity defines the
    ratio of correctly predicted negative labels to the total number of actual negative
    labels:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_16.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: '**F1 Score**: The F1 score is the harmonic mean between precision and recall.
    It is a better metric to consider than overall accuracy for most cases:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_17.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: What Metric Should You Choose?
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important aspect to consider on a serious note, is which metric we should
    consider while evaluating a model. There is no straightforward answer, as the
    best combination of metrics completely depend on the type of classification use
    case we are dealing with. One situation that commonly arises in classification
    use cases is imbalanced classes. It is not necessary for us to always have an
    equal distribution of positive and negative labels in data. In fact, in most cases,
    we would be dealing with a scenario where the positive class would be less than
    *30%* of the data. In such cases, the overall accuracy would not be the ideal
    metric to consider.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a simple example to understand this better. Consider the example
    of predicting fraud in credit card transactions. In a realistic scenario, for
    every 100 transactions there may be just one or two fraud transactions. Now, if
    we use overall accuracy as the only metric to evaluate a model, even if we predict
    all the labels as **No**, that is, **Not Fraud**, we would have approximately
    *99%* accuracy, *0%* precision, and *0%* recall. The *99%* accuracy might seem
    a great number for model performance; however, in this case, it would not be the
    ideal metric to evaluate.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: To deal with such a situation, there is often additional business context required
    to make a tangible call, but in most cases (for this type of a scenario), the
    business would want a higher recall with a bit of compromise on the overall accuracy
    and precision. The rationale to use high recall as the metric for model evaluation
    is that it would still be fine to predict a transaction as fraud even if it is
    authentic; however, it would be a mistake to predict a fraud transaction as authentic;
    the business losses would be colossal.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Often, the evaluation of a model is taken with a combination of metrics based
    on business demands. The biggest decision maker would be the trade-off between
    precision and recall. As indicated by the confusion matrix, whenever we try to
    improve precision, it hurts recall and vice versa.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some business situations in which we prioritize different metrics:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicting a rare event with catastrophic consequences**: When predicting
    whether a patient has cancer or not, whether a transaction is fraud, and so on,
    it is OK to predict a person without cancer as having cancer, but the other way
    around would result in the loss of life. Such scenarios demand high recall by
    compromising *precision* and *overall accuracy*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicting a rare event with not such catastrophic consequences**: When predicting
    whether a customer will churn or whether a customer will positively respond to
    a marketing campaign, the business outcome is not jeopardized by an incorrect
    prediction, but would be the campaign. In such cases, based on the situation,
    it would make sense to have high precision with a bit of compromise on recall.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicting a regular (non-rare) event with not such catastrophic consequences**:
    This would deal with most classification use cases, where the cost of correctly
    predicting a class is almost equal to the cost of incorrectly predicting the class.
    In such cases, we can use the F1 score, which represents a harmonic mean between
    precision and recall. It would be ideal to use overall accuracy in conjunction
    with the F1 score, as accuracy is more easily interpretable.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating Logistic Regression
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now evaluate the logistic regression model that we built previously.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 70: Evaluate a Logistic Regression Model'
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models fitted on a training dataset cannot be evaluated using
    the same dataset. We would need to leverage a separate test dataset and compare
    the model's performance on a train as well as a test dataset. The `caret` package
    has some handy functions to compute the model evaluation metrics previously discussed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to evaluate the logistic regression model we built
    in *Exercise 7*, *Build a Logistic Regression Model*:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the distribution of records for the `RainTomorrow` target variable
    in the `df_new` DataFrame:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Predict the `RainTomorrow` target variable on the train data using the `predict`
    function and cast observations with values (probability >0.5) as `Yes`, else `No`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create the confusion matrix and print the results for the train data:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Predict the results on the test data, similar to the second step:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create a confusion matrix for the test data predictions and print the results:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We first load the necessary `caret` library, which will provide the functions
    to compute the desired metrics, as discussed. We then use the `predict` function
    in R to predict the results using the previously fitted model on the train as
    well as the test data (separately). The `predict` function for logistic regression
    returns the value of the `link` function, by default. Using the `type= 'response'`
    parameter, we can override the function to return probabilities for the target.
    For simplicity, we use `0.5` as a threshold on the predictions. Therefore, anything
    above 0.5 would be `confusionMatrix` function from the `caret` library provides
    us with a simple way to construct the confusion matrix and calculate an exhaustive
    list of metrics. We would need to pass the actual, as well the predicted labels,
    to the function.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q6mYW0.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of the target label is imbalanced: *77%* no and *23%* yes.
    In such a scenario, we cannot rely only on the overall accuracy as a metric to
    evaluate the model''s performance. Also, the confusion matrix, as shown in the
    output for steps 3 and 5, is inverted when compared to the illustration shown
    in the previous section, *Confusion Matrix and Its Derived Metrics*. We have the
    predictions as rows and actual values as columns. However, the interpretation
    and results will remain the same. The next set of output reports the metrics of
    interest, along with a few others we have not explored. We have covered the most
    important ones (sensitivity and precision, that is, positive predictive value);
    however, it is recommended to explore the remaining metrics, such as negative
    predicted value, prevalence and detection rate. We can see that we are getting
    precision of around *73%* and *50%* recall and overall accuracy of *85%*. The
    results are similar on the train and test datasets; therefore, we can conclude
    that the model doesn''t overfit.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The results are not bad overall. Please don't be surprised to see the low recall
    rate; in scenarios where we have imbalanced datasets, the metrics that are used
    to assess model performance are business-driven.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that we would correctly predict at least half of the time whenever
    there is a possibility of rain, and whenever we predict, we are *73%* correct.
    From a business perspective, if we try to contemplate whether we should strive
    for high recall or precision, we would need to estimate the cost of misclassification.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, whenever we predict that there is rainfall predicted for the
    next day, the operations management team would prepare the team with a higher
    number of agents to deliver faster. Since there isn't a pre-existing technique
    to combat rainfall-related problems, we have an opportunity to cover even if we
    recall only 50% of the times when there is rain. In this problem, since the cost
    of incorrectly predicting rain will be more expensive for the business, that is,
    if the chances of rainfall are predicted, the team would invest in pooling more
    agents for delivery, which comes at an additional cost. Therefore, we would want
    higher precision, while we are OK to compromise on recall.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-259
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ideal scenario is to have high precision and high recall. However, there
    is always a trade-off in achieving one over the other. In most real-life machine
    learning use cases, a business-driven decision finalizes the priority to choose
    either precision or recall.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: The previous model developed in *Exercise 8*, *Evaluate a Logistic Regression
    Model*, was developed only using a few variables that were available in the `df_new`
    dataset. Let's build an improved model with all the available variables in the
    dataset and check the performance on the test dataset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: The best way to iterate for model improvements would be with feature selection
    and hyperparameter tuning. Feature selection involves selecting the best set of
    features from the available list through various validation approaches and finalizing
    a model with the best performance and the least number of features. Hyperparameter
    tuning deals with building generalized models that will not overfit, that is,
    a model that performs well on training as well as unseen test data. These topics
    will be covered in detail in *Chapter 6*, *Feature Selection and Dimensionality
    Reduction*, and *Chapter 7*, *Model Improvements*. For now, the scope of the chapter
    will be restricted to demonstrate model evaluation only. We will touch on the
    same use case for hyperparameter tuning and feature selection in upcoming chapters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 71: Develop a Logistic Regression Model with All of the Independent
    Variables Available in Our Use Case'
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous exercise, we limited the number of independent variables to
    only a few. In this example, we will use all the available independent variables
    in our `df_new` dataset and create an improved model. We will again use the train
    dataset to fit the model and test to evaluate the model's performance.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build a logistic regression model with all of
    the independent variables available within the use case:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the logistic regression model with all the available independent variables:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Predict on the train dataset:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create the confusion matrix:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Predict the results on the test data:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create the confusion matrix:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We leverage all the variables within the dataset to create a logistic regression
    model using the `glm` function. We then use the **fitted** model to predict the
    outcomes for the train and the test datasets; akin to the previous exercise.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2HgwjaU.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the overall accuracy, precision, and recall has improved a bit (though
    marginally). The results are fair and we can iterate with logistic regression
    to improving them further. For now, let's explore a few other classification techniques
    and study the performance of the model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this exercise, we have not printed the model's summary statistics, akin to
    the first model, with a few variables. If printed, the results would consume less
    than two pages of the chapter. For now, we will ignore that since we are not exploring
    the model characteristics that are reported by R; instead, we are evaluating a
    model purely from the accuracy, precision, and recall metrics on the train and
    test dataset.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The ideal way to get the best model would be to eliminate all statistically
    insignificant variables, remove multicollinearity, and treat the data for outliers,
    and so on. All these steps have been ignored for now, given the scope of the chapter.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Building a Logistic Regression Model with Additional Features'
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We built a simple model with few features in *Exercise 8*, *Evaluate a Logistic
    Regression Model*, and then with all the features in *Exercise 9*, *Develop a
    Logistic Regression Model with All of the Independent Variables Available in Our
    Use Case*. In this activity, we will build a logistic regression model with additional
    features that we can generate using simple mathematical transformations. It is
    good practice to add additional transformations of numeric features with log transformations,
    square and cube power transformations, square root transformations, and so on.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to develop a logistic regression model with additional
    features engineered:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Create a copy of the `df_new` dataset in `df_copy` for the activity and select
    any three numeric features (for example, `MaxTemp`, `Rainfall` and `Humidity3pm`).
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineer new features with square and cube power and square root transformations
    for each of the selected features.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the `df_copy` dataset into train and test in a 70:30 ratio.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model with the new train data, evaluate it on test data, and finally,
    compare the results.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Note
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 451.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Decision Trees
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like logistic regression, there is another popular classification technique
    that is very popular due to its simplicity and white-box nature. A decision tree
    is a simple flowchart that is represented in the form of a tree (an inverted tree).
    It starts with a root node and branches into several nodes, which can be traversed
    based on a decision, and ends with a leaf node where the *final outcome* is determined.
    Decision trees can be used for regression, as well as classification use cases.
    There are several variations of decision trees implemented in machine learning.
    A few popular choices are listed here:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative Dichotomiser 3** (**ID3**)'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Successor to ID3** (**C4.5**)'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification and Regression Tree** (**CART**)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHi-squared Automatic Interaction Detector** (**CHAID**)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional Inference Trees** (**C Trees**)'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is not exhaustive. There are other alternatives, and each
    of them has small variations in how they approach the tree creation process. In
    this chapter, we will limit our exploration to **CART Decision Trees**, which
    are the most widely used. R provides a few packages that house the implementation
    of the CART algorithm. Before we delve into the implementation, let's explore
    a few important aspects of decision trees in the following sections.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: How Do Decision Trees Work?
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each variation of decision trees has a slightly different approach. Overall,
    if we try to simplify the pseudocode for a generic decision tree, it can be summarized
    as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Select the root node (the node corresponds to a variable).
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition the data into groups.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each group from the previous step:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a decision node or leaf node (based on the splitting criteria).
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Repeat until node size <= threshold or features = empty.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Variations between different forms of tree implementations include the way categorical
    and numerical variables are handled, the approach used to select the root node
    and consecutive nodes in the tree, the rules to branch each decision node, and
    so on.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'The following visual is a sample decision tree. The root node and the decision
    nodes are the independent variables we provide to the algorithm. The leaf nodes
    denote the final outcome, whereas the root node and the intermediate decision
    nodes help in traversing the data to the leaf node. The simplicity of a decision
    tree is what makes it so effective and easy to interpret. This helps in easily
    identifying rules for a prediction task. Often, many research and business initiatives
    leverage decision trees to design a set of rules for a simple classification system:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Sample decision tree'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_04.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.4: Sample decision tree'
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a general sense, given a combination of dependent and several independent
    variables, the decision tree algorithm calculates a metric that represents the
    goodness of fit between the dependent target variable and all independent variables.
    For classification use cases, entropy and information gain are commonly used metrics
    in CART decision trees. The variable with the best fit for the metric is chosen
    as the root node and the next best is used as the decision nodes in the descending
    order of fit. The nodes are terminated into leaf nodes based on a defined threshold.
    The tree keeps growing till it exhausts the number of variables for decision nodes
    or when a predefined threshold for the number of nodes is reached.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: To improve tree performance and reduce overfitting, a few strategies, such as
    restricting the depth or breadth of the tree or additional rules for leaf nodes
    or decision nodes help in generalizing a tree for prediction.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement the same use case using CART decision trees in R. The CART model
    is available through the `rpart` package in R. This algorithm was developed by
    Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone in 1984 and has
    been widely adopted in the industry.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 72: Create a Decision Tree Model in R'
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create a decision tree model in R using the same data
    and the use case we leveraged in *Exercise 9*, *Develop a Logistic Regression
    Model with All of the Independent Variables Available in Our Use Case*. We will
    try to study whether there are any differences in the performance of a decision
    tree model over a logistic regression model.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create a decision tree model in R:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `rpart` and `rpart.plot` packages using the following command:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Build the CART model with all of the variables:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Plot the cost parameter:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5: Decision tree model'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_05_05.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.5: Decision tree model'
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the tree using the following command:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6: Predicting rainfall'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_05_06.jpg)'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.6: Predicting rainfall'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Make predictions on the train data:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Make predictions on the test data:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The `rpart` library provides us with the CART implementation of decision trees.
    There are additional libraries that help us visualize the decision tree in R.
    We have used `rpart.plot` here. If the package is not already installed, please
    install it using the `install.packages` command. We use the `rpart` function to
    create the tree model and we use all the available independent variables. We then
    use the `plotcp` function to visualize the complexity parameter's corresponding
    validation error on different iterations. We also use the `plot.rpart` function
    to plot the decision tree.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we make predictions on the train as well as the test data and build
    the confusion matrix and calculate the metrics of interest using the `confusionMatrix`
    function for the train and test datasets individually.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2WECLgZ.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: The CART decision tree implemented in R has several optimizations already in
    place. The function, by default, sets a ton of parameters for optimum results.
    In a decision tree, there are several parameters that we can manually set to tune
    the performance based on our requirements. However, the R implementation does
    a great job of setting a wide number of parameters with a relatively good value
    by default. These additional settings can be added to the `rpart` tree with the
    `control` parameter.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add the following parameter to the tree model:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: One parameter of interest would be the `0.01`. We can further change this to
    a lower number that would make the tree grow deeper and become more complicated.
    The `plotcp` function visualizes the relative validation error for different values
    of `cp`, that is, the complexity parameter. The most ideal value for `cp` is the
    leftmost value below the dotted line in the plot in *Figure 5.4*. In this case
    (as shown in the plot), the best value is 0.017\. Since this value is not very
    different from the default value, we don't change it further.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The next plot in *Figure 5.5* helps us visualize the actual decision tree constructed
    by the algorithm. We can see the simple set of rules being constructed using the
    available data. As you can see, only two independent variables, that is, `Humidity3pm`
    and `WindGustSpeed`, have been selected for the tree. If we change the complexity
    parameter to *0.001* instead of *0.01*, we can see a much deeper tree (which could
    overfit the model) would have been constructed. Finally, we can see the results
    from the confusion matrix (step 6) along with additional metrics of interest for
    the train and test dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the results are similar for the train and test dataset. We can
    therefore conclude that the model doesn't overfit. However, there is a significant
    drop in accuracy (*83%*) and recall (*35%*), while the precision has increased
    to a slightly higher value (*77%*).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: We have now worked with a few white-box modeling techniques. Given the simplicity
    and ease of interpretation of white-box models, they are the most preferred technique
    for classification use cases in business, where reasoning and driver analysis
    is of paramount importance. However, there are a few scenarios where a business
    might be more interested in the *net outcome* of the model rather than the entire
    interpretation of the outcome. In such cases, the end model performance is of
    more interest. In our use case, we want to achieve high precision. Let's explore
    a few black-box models that are superior (in most cases) to white-box models in
    terms of model performance and that can be achieved with far less effort and more
    training data.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Create a Decision Tree Model with Additional Control Parameters'
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision tree model we created in *Exercise 10*, *Create a Decision Tree
    Model in R*, used the default control parameters for the tree. In this activity,
    we will override a few control parameters and study its impact on the overall
    tree-fitting process.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create a decision tree model with additional
    control parameters:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Load the `rpart` library.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the control object for the decision tree with new values: `minsplit
    =15` and `cp = 0.00`.'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the tree model with the train data and pass the control object to the `rpart`
    function.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the complexity parameter plot to see how the tree performs at different
    values of `CP`.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fitted model to make predictions on the train data and create the confusion
    matrix.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fitted model to make predictions on the test data and create the confusion
    matrix.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Note
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 454.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ensemble Modelling
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ensemble modeling is one of the most popular approaches used in classification
    and regression modeling techniques when there is a need for improved performance
    with a larger training sample. In simple words, ensemble modeling can be defined
    by breaking down the name into individual terms: **ensemble** and **modeling**.
    We have already studied modeling in this book; an ensemble in simple terms is
    a **group**. Therefore, the process of building several models for the same task
    instead of just one model and then combining the results into a single outcome
    through any means, such as averaging or voting, and many others, is called **ensemble
    modeling**.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build ensembles of any models, such as linear models or tree models,
    and in fact can even build an ensemble of ensemble models. However, the most popular
    approach is using tree models as the base for ensembles. There are two broad types
    of ensemble models:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Here, each model is built in parallel with some randomization
    introduced within each model, and the results of all models are combined using
    a simple voting mechanism. Say we built 100 tree models and 60 models predicted
    the outcome as *Yes* and 40 predicted it as *No*. The end result would be a *Yes*.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: Here, models are built sequentially and the results of the first
    model are used to tune the next model. Each model iteratively learns from errors
    made by the previous model and tries to improve with successive iterations. The
    result is usually a weighted average of all the individual outcomes.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several implementations available in bagging as well as boosting.
    **Bagging** itself is an ensemble model available in R. By far the most popular
    bagging technique used is random forest. Another bagging technique along similar
    lines as random forest is **extra trees**. Similarly, a few examples of boosting
    techniques are AdaBoost, Stochastic Gradient Boosting, BrownBoost, and many others.
    However, the most popular boosting technique is **XGBoost**, which is derived
    from the name **EXtreme Gradient Boosting**. In most cases, for classification
    as well as regression use cases, data scientists prefer using random forests or
    XGBoost models. A recent survey on Kaggle (an online data science community) revealed
    the most popular technique used for most machine learning competitions were always
    random forest and XGBoost. In this chapter, we will take a closer look at both
    models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random forest** is the most popular bagging technique used in machine learning.
    It was developed by Leo Brieman, the author of CART. This simple technique is
    so effective that it is almost always the first choice of algorithm for a data
    scientist given a supervised use case. Random forest is a good choice for classification
    as well as regression use cases. It is a highly effective method for reducing
    overfitting with a bare minimum amount of effort. Let''s have a deeper understanding
    of how random forests work.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: As we already know, random forest is an ensemble modeling technique, where we
    build several models and combine their results using a simple voting technique.
    In random forests, we use decision trees as the base model. The inner workings
    of the algorithm can be fairly guessed from the name itself, that is, random (since
    it induces a layer of randomization in every model that is built) and forest (since
    there are several *tree* models we build). Before we get into the actual workings
    of the algorithm, we first need to understand the story of its predecessor, **bagging**,
    and study why we need ensembles.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Why Are Ensemble Models Used?
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first question that would have surfaced in your thoughts may be, why do
    we need to build several models for the same task in the first place? Is it necessary?
    Well, yes! When we build ensembles, we don't build the exact same model several
    times; instead, every model we build will be different from the others in some
    way. The intuition behind this can be understood using a simple example from our
    day-to-day lives. It is built on the principle that several weak learners combined
    together build a stronger and more robust model.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this idea using a simple example. Say you reach a new city
    and want to know the chances of there being rain in the city the next day. Assuming
    technology is not an available option, the easiest way you could find this out
    would be to ask someone in the neighborhood who has been a dweller of the place
    for a while. Maybe the answer would not always be correct; if someone said that
    there was a very high chance of rain the next day, it doesn't necessarily mean
    that it would certainly rain. Therefore, to make an improved guess, you ask several
    people in the neighborhood. Now, if 7 out of the 10 people you asked mentioned
    that there was a high chance of rain the next day, then it almost certainly would
    rain the very next day. The reason this works effectively is because every person
    you reached out to would have some understanding about rain patterns and also
    every person's understanding about those patterns would be a bit different. Though
    the differences are not miles apart, some level of randomness among the people's
    understanding when aggregated for a collective answer would yield a better answer.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Bagging – Predecessor to Random Forest
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble modeling works on the same principle. Here, in each model, we induce
    some level of randomness. The bagging algorithm brings in this randomness for
    each model on the training data. The name bagging is derived from **Bootstrap
    Aggregation**; a process where we sample two-thirds of the available data with
    replacement data for training and the remainder for testing and validation. Here,
    each model, that is, a decision tree model, trains on a slightly different dataset
    and therefore might have a slightly different outcome for the same test sample.
    Bagging, in a way, mimics the real-world example that we discussed and therefore
    combines several weak learners (decision tree models) into a strong learner.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: How Does Random Forest Work?
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random forest** is basically a successor to bagging. Here, apart from the
    randomness in the training data, random forest adds an additional layer of randomness
    with the feature set. Therefore, each decision tree not only has bootstrap aggregation,
    that is, two thirds of the training data with replacement, but also a subset of
    features randomly selected from the available list. Thus, each individual decision
    tree in the ensemble has a slightly different training dataset and a slightly
    different set of features to train. This additional layer of randomness works
    effectively in generalizing the model and reduces variance.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 73: Building a Random Forest Model in R'
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will build a random forest model on the same dataset we
    leveraged in Exercises 8, 9, and 10\. We will leverage ensemble modelling and
    test whether the overall model performance improves compared to decision trees
    and logistic regression.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To get started, we can quickly build a random forest model using the same dataset
    we used earlier. The `randomForest` package in R provides the implementation for
    the model, along with a few additional functions to optimize the model.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a basic random forest model. Perform the following steps:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `randomForest` library using the following command:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Build a random forest model with all of the independent features available:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Evaluate on the training data:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Evaluate on the test data:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Plot the feature importance:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output is as follows:'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '![Figure 5.7: Random Forest model'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_07.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.7: Random Forest model'
  id: totrans-407
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q2xKwd.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Build a Random Forest Model with a Greater Number of Trees'
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 11*, *Building a Random Forest Model in R*, we created a random
    forest model with just 100 trees; we can build a more robust model with a higher
    number of trees. In this activity, we will create a random forest model with 500
    trees and study the impact of the model having only 100 trees. In general, we
    expect the model's performance to improve (at least marginally with an increased
    number of trees). This comes with higher computational time for the model to converge.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build a random forest model with 500 trees:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Develop a random forest model with a higher number of trees; say, 500\. Readers
    are encouraged to try higher numbers such as 1,000, 2,000, and so on, and study
    the incremental improvements in each version.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leverage the fitted model to predict estimates on the train-and-test data and
    study whether there was any improvement compared to the model with 100 trees.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 457.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: XGBoost
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**XGBoost** is the most popular boosting technique in recent times. Although
    there have been various new versions that have been developed by large corporations,
    XGBoost still remains the undisputed king. Let''s look at a brief history of boosting.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Boosting Process Work?
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting differs from bagging in its core principles; the learning process is,
    in fact, sequential. Every model built in an ensemble is ideally an improved version
    of the previous model. To understand boosting in simple terms, imagine you are
    playing a game where you must remember all the objects placed on the table that
    you are shown just once for 30 seconds. The moderator of the game arranges around
    50-100 different objects on a table, such as a bat, ball, clock, die, coins, and
    so on, and covers them with a large piece of cloth. When the game begins, he withdraws
    the cloth from the table and gives you exactly 30 seconds to see them and puts
    the curtain back. You now must recollect all the objects you can remember. The
    participant who can recollect the most, aces the game.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: In this game, let's add one new dimension. Assume you are a team and the players
    take turns one by one to announce all the objects they can recollect, while the
    others listen to them. Say there are 10 participants; each participant steps forward
    and announces out loud the objects they can recollect from the table. By the time
    the second player steps forward, they have heard all the objects called out by
    the first player. They would have mentioned a few objects that the second player
    might not have recollected. To improve on the first player, the second player
    learns a few new objects from the first player, adds them to his list, and then
    announces them out loud. By the time the last player steps forward, they have
    already learned several objects that other players recollected, which they failed
    to recollect themselves.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Putting those together, that player creates the most exhaustive list and aces
    the competition. The fact that each player announces the list sequentially helps
    the next player learn from their mistakes and improvise on it.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Boosting works in the same way. Each model trained sequentially is imparted
    with additional knowledge, such that the errors of the first model are learned
    better in the second model. Say the first model learns to classify well for most
    cases of a specific independent variable; however, it fails to correctly predict
    for just one specific category. The next model is imparted with a different training
    sample, such that the model learns better for the category where the previous
    model fails. A simple example would be oversampling based on the variable or category
    of interest. Boosting effectively reduces bias and therefore improves the model's
    performance.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: What Are Some Popular Boosting Techniques?
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The boosting techniques introduced earlier were not very popular, because they
    were easily overfit and often required, relatively, a lot of effort in tuning
    to achieve great performance. AdaBoost, BrownBoost, Gradient Boosting, and Stochastic
    Gradient Boosting are all boosting techniques that were popular for a long time.
    However, in 2014, when T Chen and others introduced XGBoost (**Extreme Gradient
    Boosting**), it ushered in a new height in the boosting performance.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: How Does XGBoost Work?
  id: totrans-426
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost natively introduced regularization, which helps models combat overfitting
    and thus delivered high performance. Compared to other available boosting techniques
    at the time, XGBoost reduced the overfitting problem significantly and with the
    least amount of effort. With current implementations of the model in R or any
    other language, XGBoost almost always performs great with the default parameter
    setting. (Though, this is not always true; in many cases, random forest outperforms
    XGBoost). XGBoost has been among the most popular choice of algorithms used in
    data science hackathons and enterprise projects.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, XGBoost has regularization introduced in the objective function,
    which penalizes the model when it gets more complicated in a training iteration.
    Discussing the depth of mathematical constructs that goes into XGBoosting is beyond
    the scope of this chapter. You can refer to T Chen''s paper here (https://arxiv.org/abs/1603.02754)
    for further notes. Also, this blog will help you to understand the mathematical
    differences between GBM and XGBoost in a simple way: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: Implementing XGBoost in R
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can leverage the XGBoost package, which provides a neat implementation of
    the algorithm. There are a few differences in the implementation approach that
    we will need to take care of before getting started. Unlike other implementations
    of algorithms in R, XGBoost does not handle categorical data (others take care
    of converting it into numeric data internally). The internal functioning of XGBoost
    in R doesn't handle the automatic conversion of categorical columns into numeric
    columns. Therefore, we manually convert categorical columns into numeric or one-hot
    encoded form.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: A one-hot encoded form basically represents a single categorical column as a
    binary encoded form. Say we have a categorical column with values such as **Yes**/**No**/**Maybe**;
    then, we transform this single variable, where we have an individual variable
    for each value of the categorical variable indicating its value as **0** or **1**.
    So, the values for the columns **Yes**, **No**, and **Maybe** will take **0**
    and **1** based on the original value.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 'One-hot encoding is demonstrated in the following table:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: One-hot encoding'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_08.jpg)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.8: One-hot encoding'
  id: totrans-435
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's transform the data into the required form and build an XGBoost model on
    the dataset.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 74: Building an XGBoost Model in R'
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we did in *Exercise 11*, *Building a Random Forest Model in R*, we will
    try to improve the performance of the classification model by building an XGBoost
    model for the same use case and dataset as in *Exercise 11*, *Building a Random
    Forest Model in R*.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to build an XGBoost model in R.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Create list placeholders for the target, categorical, and numeric variables:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Convert the categorical factor variables into character. This will be useful
    for converting them into one-hot-encoded forms:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Convert the categorical variables into one-hot encoded forms using the `dummyVars`
    function from the `caret` package:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Combine numeric variables and the one-hot encoded variables from the third
    step into a single DataFrame named `df_final`:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Convert the target variable into numeric form, as the XGBoost implementation
    in R doesn''t accept factor or character forms:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Split the `df_final` dataset into train (70%) and test (30%) datasets:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Build an XGBoost model using the `xgboost` function. Pass the train data and
    `y_train` target variable and define the `eta = 0.01`, `max_depth = 6`, `nrounds
    = 200`, and `colsample_bytree = 1` hyperparameters, define the evaluation metric
    as `logloss,` and the `objective` function as `binary:logistic`, since we are
    dealing with binary classification:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Make a prediction using the fitted model on the train dataset and create the
    confusion matrix to evaluate the model''s performance on the train data:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, as in the previous step, make predictions using the fitted model on the
    test dataset and create the confusion matrix to evaluate the model''s performance
    on the test data:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The output is as follows:'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: If we take a closer look at the results from the model, we can see a slight
    improvement in the performance compared to random forest model results. The `0.54`
    instead of `0.5`, we can increase the precision (to match random forest) while
    still having slightly higher recall than random forest. The increase in recall
    for XGBoost is significantly higher than the decrease in precision. The threshold
    value for the probability cutoff is not a defined, hard cutoff. We can tweak the
    threshold based on our use case. The best number can be studied with empirical
    experiments or by studying the sensitivity, specificity distribution.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-463
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30gzSW0.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: The following exercise uses 0.54 instead of 0.5 as the probability cutoff to
    study the improvement in precision at the cost of recall.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 75: Improving the XGBoost Model''s Performance'
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can tweak the model performance of binary classification models by adjusting
    the threshold value of the output. By default, we select 0.5 as the default probability
    cutoff. So, all responses above 0.5 are tagged as `Yes`, else `No`. Adjusting
    the threshold can help us achieve more sensitive or more precise models.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to improve the XGBoost model''s performance by
    adjusting the threshold for the probability cutoff:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase the probability cutoff for the prediction on the train dataset from
    0.5 to 0.53 and print the results:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output is as follows:'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Increase the probability cutoff for the prediction on the test dataset from
    0.5 to 0.53 and print the results:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: We see that, at 44% recall, we have 80% precision on the test dataset, and the
    difference in performance between the train and test datasets is also negligible.
    We can therefore conclude that the model performance of XGBoost is a bit better
    than random forest, though only a bit.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30c5DQ9.'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: Before wrapping up our chapter, let's experiment with the last supervised technique
    for classification, that is, deep neural networks.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Networks
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last type of technique that we will be discussing before wrapping up our
    chapter is deep neural networks or deep learning. This is a long and complicated
    topic, which by no means will we be able to do justice in a short section of this
    chapter. A complete book may not even suffice to cover the surface of the topic!
    We will explore the topic from 100 feet and quickly study an easy implementation
    in R.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks, which are primarily used in the field of computer vision
    and natural language processing, have also found significance in machine learning
    use cases for regression and classification on tabular cross-sectional data. With
    large amounts of data, deep neural networks have been proved to be very effective
    at learning latent patterns and thus training models with better performance.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: A Deeper Look into Deep Neural Networks
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep neural networks were inspired by the neural structure of the human brain.
    The field of deep learning became popular for solving computer vision problems,
    that is, the area of problems that were easily solved by humans, but computers
    struggled with for a long time. The motivation for designing deep neural networks
    akin to a miniature and highly simplified human brain was to solve problems that
    were specifically easy for humans. Later, with the success of deep learning in
    the field of computer vision, it was embraced in several other fields, including
    traditional machine learning supervised use cases.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is organized as a hierarchy of neurons, just like the neurons
    in the human brain. Each neuron is connected to other neurons, which enables communication
    between them that traverses as a signal to other neurons and results in a large
    complex network that can learn with a feedback mechanism.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure demonstrates a simple neural network:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Simple neural network](img/C12624_05_09.jpg)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Simple neural network'
  id: totrans-489
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input data forms the 0th layer in the network. This layer then connects
    to the neurons within the next layer, which is hidden. It is called **hidden**
    as the network can be perceived as a black box where we provide input to the network
    and directly see the output. The intermediate layers are hidden. In a neural network,
    a layer can have any number of neurons and each network can have any number of
    layers. The larger the number of layers, the 'deeper' the network will be. Hence
    the name deep learning and deep neural networks. Every neuron in each hidden layer
    computes a mathematical function, which is called the activation function in deep
    learning. This function helps in mimicking the signal between two neurons. If
    the function (activation) computes a value greater than a threshold, it sends
    a signal to the immediate connected neuron in the next layer. The connection between
    these two neurons is moderated by a weight. The weight decides how important the
    incoming neuron's signal is for the receiving neuron. The learning method in the
    deep learning model updates the weights between neurons such that the end prediction,
    akin to machine learning models, is the most accurate one.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Deep Learning Model Work?
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how a neural network works and learns to make predictions on data,
    let's consider a simple task that is, relatively, very easy for humans. Consider
    the task of learning to identify different people by their faces. Most of us meet
    a few different people every day; say, at work, school, or on the street. Every
    person we meet is different from each other in some dimension. Though everyone
    would have a ton of similar features, such as two eyes, two ears, lips, two hands,
    and so on, our brain easily distinguishes between two individuals. The second
    time we meet a person, we would most probably recognize them and distinguish them
    as someone we met previously. Given the scale at which this happens and the fact
    that our brain effectively works to solve this mammoth problem with ease, it makes
    us wonder how exactly this happens.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: To understand this and appreciate the beauty of our brain, we need to understand
    how the brain fundamentally learns. The brain is a large, complex structure of
    interconnected neurons. Each neuron gets activated when it senses something essential
    and passes a message or signal to other neurons it is connected to. The connection
    between neurons is strengthened by constant learning from the feedback they receive.
    Here, when we see a new face, rather than learning the structure of the face to
    identify people, the brain learns how different the given face is from a generic
    baseline face. This can be further simplified as calculating the difference between
    important facial features, such as eye shape, nose, lips, ears, and lip structure,
    color deviations of the skin and hair, and other attributes. These differences,
    which are quantified by different neurons, are then orchestrated in a systematic
    fashion for the brain to distinguish one face from another and recall a face from
    memory. This entire computation happens subconsciously, and we barely realize
    this as the results are instant for us to notice anything specific.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: A neural network essentially tries to mimic the learning functionality of the
    brain in an extremely simplified form. Neurons are connected to each other in
    a layer-wise fashion and initialized with random weights. A mathematical calculation
    across the network combines the inputs from all neurons layer-wise and finally
    reaches the end outcome. The deviation of the end outcome (the predicted value)
    is then quantified as an error and is given as feedback to the network. Based
    on the error, the network tries updating the weights of the connections and tries
    to reduce the error in the prediction iteratively. With several iterations, the
    network updates its weights in an ordered fashion and thus learns to recognize
    patterns to make a correct prediction.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: What Framework Do We Use for Deep Learning Models?
  id: totrans-495
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For now, we will experiment with deep neural networks for our classification
    use case, using Keras for R. For deep learning model development, we would need
    to write a ton of code, which would render the building blocks for the network.
    To speed up our process, we can leverage Keras, a deep learning framework that
    provides neat abstraction for deep learning components. Keras has an R interface
    and works on top of a low-level deep learning framework.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning frameworks available in today's AI community are either low-level
    or high-level. Frameworks such as TensorFlow, Theano, PyTorch, PaddlePaddle, and
    mxnet are low-level frameworks that provide the basic building blocks for deep
    learning models. Using low-level frameworks offers a ton of flexibility and customization
    to the end network design. However, we would still need to write quite a lot of
    code to get a relatively large network working. To simplify this further, there
    are a few high-level frameworks available that work on top of the low-level frameworks
    and provide a second layer of abstraction in the process of building deep learning
    models. Keras, Gluon, and Lasagne are a few frameworks that leverage the aforementioned
    low-level framework as a backend and provide a new API that makes the overall
    development process far easier. This reduces the flexibility when compared to
    directly using a low-level framework such as TensorFlow, and offers a robust solution
    for most networks. For our use case, we can directly leverage Keras with the R
    interface.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: Using the `install.packages('keras')` command would install the R interface
    to Keras and would also automatically install TensorFlow as the low-level backend
    for Keras.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deep Neural Network in Keras
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To leverage Keras in R, we would need additional data augmentations to our existing
    training dataset. In most machine learning functions available under R, we can
    pass the categorical column directly coded as a factor. However, we saw that XGBoost
    had a mandate that the data needs to be rendered into one-hot encoded form, as
    it does not internally transform the data into the required format. We therefore
    used the `dummyVars` function in R to transform the training and test dataset
    into a one-hot encoded version, such that we have only numerical data in the dataset.
    In Keras, we would need to feed a matrix instead of a DataFrame as the training
    dataset. Therefore, in addition to transforming the data into a one-hot encoded
    form, we would also need to convert the dataset into a matrix.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is also recommended that we standardize, normalize, or scale all
    our input dimensions. The process of normalization rescales data values into the
    range 0 to 1\. Similarly, standardization rescales data to have a mean (*μ*) of
    0 and standard deviation (*σ*) of 1 (unit variance). This transformation is a
    good feature to have in machine learning, as some algorithms tend to benefit and
    learn better. However, in deep learning, this transformation becomes crucial,
    as the model learning process suffers if we provide an input training dataset
    such that all dimensions are in a different range or scale. The reason behind
    this issue is the type of activation function used in neurons.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet implements a basic neural network in Keras. Here,
    we use an architecture that has three layers with 250 neurons each. Finding the
    right architecture is an empirical process and does not have a definitive guide.
    The deeper network is designed, the more computation it will need to fit the data.
    The dataset used in the following snippet is the same as was used for XGBoost
    and already has the one-hot encoded forms.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 76: Build a Deep Neural Network in R using R Keras'
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will leverage deep neural networks to build a classification
    model for the same use case as *Exercise 13*, *Improving XGBoost Model Performance*,
    and try to improve the performance. Deep neural networks will not always perform
    better than ensemble models. They are usually a preferred choice when we have
    a very high number of training samples, say 10 million. However, we will experiment
    and check whether we can achieve any better performance than the models we built
    in exercises 10-13.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to build a deep neural network in R.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale the input dataset in the range 0 to 1\. We would first need to initiate
    a `preProcess` object on the training data. This will be later used to scale the
    train as well as the test data. Neural networks perform better with scaled data.
    The train data alone is used for creating the object to scale:'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-507
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Use the `standardizer` object created in the previous step to scale the train
    and test data:'
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Store the number of predictor variables in a variable called **predictors**.
    We will use this information to construct the network:'
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Define the structure for a deep neural network. We will use the `keras_model_sequential`
    method. We will create a network with three hidden layers, having 250 neurons
    each and `relu` as the activation function. The output layer will have one neuron
    with the `sigmoid` activation function (since we are developing a binary classification
    mode):'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define the model optimizer as `adam`, loss function, and the metrics to capture
    for the model''s training iteration:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The output is as follows:'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Fit the model structure we created in steps 4-5 with the training and test
    data from steps 1-2:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Predict the responses using the fitted model on the train dataset:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'The output is as follows:'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Predict the responses using the fitted model on the test dataset:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output is as follows:'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Note
  id: totrans-530
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vz8Omb.'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preprocessor function helps to transform the data into the required scale
    or range. Here, we scale the data to a 0 to 1 scale. We should only consider using
    the train data as the input to the function generator and use the fitted method
    to scale the test data. This is essential, as we won't have access to the test
    data in a real-time scenario. Once the `preProcess` method is fit, we use it to
    transform the train and test data. We then define the architecture for the deep
    neural network model. R provides the easy to extend pipe operator with `%>%`,
    which enables the easy concatenation of the operators. We design a network with
    three layers and 250 neurons each. The input data will form the 0th layer and
    the last layer will be the predicted outcome. The activation function used in
    the network for the hidden layers is `relu`, the most recommended activation function
    for any deep learning use case. The final layer has the `sigmoid` activation function,
    as we have a binary classification use case. There are a ton of activation functions
    to choose from in Keras, such as `prelu`, `tanh`, `swish`, and so on. Once, the
    model architecture is defined, we define the loss function, `binary_crossentropy`,
    which is analogous to binary `logloss` (akin to XGBoost), the optimizer, that
    is, technique, used by the model to learn and backpropagate. The errors in the
    prediction are backpropagated to the network so that it can adjust the weights
    in the right direction and iteratively reduce the error.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical intuitiveness of this functionality can take various approaches.
    Adam optimization, which is based on adaptive estimates of lower-order moments,
    is the most popular choice, which we can almost blindly experiment with for most
    use cases in deep learning. Some of the other options are `rmsprop`, stochastic
    gradient descent, and `Adagrad`. We also define the metrics to calculate on the
    validation dataset after each epoch, that is, one complete presentation of training
    samples to the network. The `summary` function displays the resultant architecture
    we defined in the preceding section using the Keras constructs. The `summary`
    function gives us a brief idea of the number of parameters in each layer and additionally
    represents the network in a hierarchical structure to help us visualize the model
    architecture. Lastly, we use the `fit` function which trains or 'fits' the data
    to the network. We also define the number of epochs the model should iterate;
    the higher the number of epochs, the longer the training process will take to
    compute.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: The batch size indicates the number of training samples the network consumes
    in one single pass before updating the weights of the network; a lower number
    for the batch indicates more frequent weight updates and helps the RAM memory
    to be effectively utilized. The validation split defines the percentage of training
    samples to be used for validation at the end of each epoch. Finally, we validate
    the model's performance on the train and test data.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This explanation in the code snippet will by no means be a justification for
    the topic. A deep neural network is an extremely vast and complex topic that might
    need a complete book for a basic introduction. We have wrapped the context into
    a short paragraph for you to understand the constructs used in the model development
    process. Exploring the depth of the topic would be beyond the scope of this book.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the results, we can see similar results as for the previous models.
    The results are almost comparable with the XGBoost model we developed previously.
    We have around 48% recall and 75% precision on the test dataset. The results can
    be further tweaked to reduce recall and enhance precision (if necessary).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: 'We can therefore conclude that we got fairly good results from our simple logistic
    regression model, XGBoost, and the deep neural network model. The differences
    between all three models were relatively slight. This might bring important questions
    into your mind: Is it worth iterating for various models on the same use case?
    Which model will ideally give the best results? Though there are no straightforward
    answers to these questions, we can say that, overall, simple models always do
    great; ensemble models perform better with lots of data; and deep learning models
    perform better with a ton of data. In the use case that we experimented with in
    this chapter, we will get improved results from all the models with hyperparameter
    tuning and; most importantly; feature engineering. We will explore hyperparameter
    tuning in *Chapter 7*, *Model Improvements*, and feature engineering on a light
    node in *Chapter 6*, *Feature Selection and Dimensionality Reduction*. The process
    of feature engineering is very domain-specific and can only be generalized to
    a certain extent. We will have a look at this in more detail in the next chapter.
    The primary agenda for this chapter was to introduce the range of modeling techniques
    that cover a substantial area in the field and can help you build the foundations
    for any machine learning technique to be developed for a classification use case.'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Model for Your Use Case
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have explored a set of white-box models and a couple of black-box
    machine learning models for the same classification use case. We also extended
    the same use case with a deep neural network in Keras and studied its performance.
    With the results from several models and various iterations, we need to decide
    which model would be the best for a classification use case. There isn't a simple
    and straightforward answer to this. In a more general sense, we can say that the
    best model would be a Random Forest or XGBoost for most use cases. However, this
    is not true for all types of data. There will be numerous scenarios where ensemble
    modeling may not be the right fit and a linear model would outperform it and vice
    versa. In most experiments conducted by data scientists for classification use
    cases, the approach would be an exploratory and iterative one. There is no one-size-fits-all
    model in machine learning. The process of designing and training a machine learning
    model is arduous and extremely iterative and will always depend on the type of
    data used to train it.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach to proceed, given the task of building a supervised machine
    learning model, would be as follows:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 0**: **EDA, Data Treatment and Feature Engineering**: Study the data
    extensively using a combination of visualization techniques and then treat the
    data for missing values, remove outliers, engineer new features, and build the
    train and test datasets. (If necessary, create a validation dataset too.)'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 1**: **Start with a simple white-box model such as logistic regression**:
    The best starting point in the modeling iterations is a simple white-box model
    that helps us study the impact of each predictor on the dependent variable in
    an easy-to-quantify way. A couple of model iterations will help with feature selection
    and getting a clear understanding of the best predictors and a model benchmark.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: **Repeat the modeling experiments with a decision tree model**:
    Leveraging decision tree models will always help us get a new perspective on the
    model and feature patterns. It might give us simple rules and thereby new ideas
    to engineer features for an improved model.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: If there is enough data, experiment with ensemble modeling; otherwise,
    try alternative approaches, such as support vector machines.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensemble modeling with Random Forest and XGBoost is almost always a safe option
    to experiment with. But in cases where there is a scarcity of data to train, ensemble
    modeling might not be an effective approach to proceed. In such cases, a black
    box kernel-based model would be more effective at learning data patterns and,
    thus, would improve model performance. We have not covered **Support Vector Machines**
    (**SVM**) in this chapter, given the scope. However, with the wide range of topics
    covered in the chapter, getting started with SVMs would be a straightforward task
    for you. This blog provides a simple and easy to understand guide to SVMs: https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/.'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, to understand whether the number of training samples is less or
    more, you can use a simple rule of thumb. If there are at least 100 rows of training
    samples for every feature in the dataset, then there is enough data for ensemble
    models; if the number of samples is lower than that, then ensemble models might
    not always be effective. It is still worth a try, though. For example, if there
    are 15 features (independent variables) and 1 dependent variable, and then if
    we have *15 x 100 = 1500* training samples, ensemble models might have better
    performance on a white-box model.
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: If there is more than enough data, try deep neural networks. If
    there are at least 10,000 samples for every feature in the dataset, experimenting
    with deep neural networks might be a good idea. The problem with neural networks
    is mainly the huge training data and large number of iterations required to get
    good performance. In most generic cases for classification using tabular cross-sectional
    data (the type of use case we solved in this book), deep neural networks are just
    as effective as ensemble models but require significantly more effort in training
    and tuning to achieve the same results. They do outperform ensemble models when
    there is a significantly large number of samples to train. Investing the effort
    in deep neural networks only returns favorable results when there is a significantly
    higher number of training samples.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-549
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored different types of classification algorithms for
    supervised machine learning. We leveraged the Australian weather data, designed
    a business problem around it, and explored various machine learning techniques
    on the same use case. We studied how to develop these models in R and studied
    the functioning of these algorithms in depth with mathematical abstractions. We
    summarized the results from each technique and studied a generalized approach
    to tackle common classification use cases.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study feature selection, dimensionality reduction,
    and feature engineering for machine learning models.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
