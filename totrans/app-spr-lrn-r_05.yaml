- en: '*Chapter 5:*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Define binary classification in supervised machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform binary classification using white-box models: logistic regression and
    decision trees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the performance of supervised classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform binary classification using black-box ensemble models – Random Forest
    and XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design and develop deep neural networks for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the best model for a given classification use case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will focus on solving classification use cases for supervised
    learning. We will use a dataset designed for a classification use case, frame
    a business problem around it, and explore a few popular techniques to solve the
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's quickly brush up on the topics we learned in *Chapter 3*, *Introduction
    to Supervised Learning*. Supervised learning, as you already know by now, is the
    branch of machine learning and artificial intelligence that helps machines learn
    without explicit programming. A more simplified way of describing supervised learning
    would be developing algorithms that learn from labeled data. The broad categories
    in supervised learning are classification and regression, differentiated fundamentally
    by the type of label, that is, **continuous** or **categorical**. Algorithms that
    deal with continuous variables are known as **regression algorithms**, and those
    with categorical variables are called **classification algorithms**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In classification algorithms, our target, dependent, or criterion variable
    is a **categorical variable**. Based on the number of classes, we can further
    divide them into the following groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multinomial classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-label classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will focus on **binary classification**. Discussing the
    specifics and practical examples of multinomial and multi-class classification
    is beyond the scope of this chapter; however, a few additional reading references
    for advanced topics will be listed before wrapping up the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification algorithms are the most popular class of algorithms within
    machine learning and have numerous applications in business, research, and academia.
    Simple models that classify a student's chances of passing a future exam based
    on their past performance as pass or fail, predict whether it will rain or not,
    predict whether a customer will default on a loan or not, predict whether a patient
    has cancer or not, and so on are all common use cases that are solved by classification
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving deeper into algorithms, we will first get started with a use case
    that will help us solve a supervised learning classification problem with hands-on
    exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with the Use Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will refer to the `weather` dataset, obtained from the Australian
    Commonwealth Bureau of Meteorology and made available through R. The dataset has
    two target variables, `RainTomorrow`, a flag indicating whether it will rain tomorrow,
    and `RISK_MM`, which measures the amount of rainfall for the following day.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, we can use this dataset for `RainTomorrow`, for our classification
    exercise. The metadata and additional details about the dataset are available
    to explore at https://www.rdocumentation.org/packages/rattle/versions/5.2.0/topics/weather.
    Since the dataset is readily available through R, we don't need to separately
    download it; instead, we can directly use the R function within the `rattle` library
    to load the data into system memory.
  prefs: []
  type: TYPE_NORMAL
- en: Some Background on the Use Case
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several weather parameters, such as temperature, direction, pressure, cloud
    cover, humidity, and sunshine, were recorded daily for one year. The rainfall
    for the next day is already engineered in the dataset as the target variable,
    `RainTomorrow`. We can leverage this data to define a machine learning model that
    learns from the present day's weather parameters and predicts the chances of rain
    for the next day.
  prefs: []
  type: TYPE_NORMAL
- en: Rainfall prediction is of paramount importance to many industries. Long-haul
    journeys by train and buses usually look at changing weather patterns, primarily
    rainfall, to estimate the arrival time and journey length. Similarly, most brick
    and mortar stores, small restaurants and food joints, and others are all heavily
    impacted by rainfall. Gaining visibility of the weather conditions for the next
    day can help businesses better prepare in several ways, to combat business losses
    and, in some cases, maximize business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: To build nice intuition around the problem-solving exercise, let's frame a business
    problem using the dataset and develop the problem statement for the use case.
    Since the data is about rainfall prediction, we will choose a popular business
    problem faced by today's hyper-local food-delivery services. Start-ups such as
    DoorDash, Skip the Dishes, FoodPanda, Swiggy, Foodora, and many others offer hyper-local
    food delivery services to customers in different countries. A common trend observed
    in most countries is the rise in food delivery orders with the onset of rain.
    In general, most delivery companies expect around a 30%-40% increase in the total
    number of deliveries on a given day. Given the limited number of delivery agents,
    the delivery time is impacted immensely due to increased orders on rainy days.
    To keep costs optimal, it is not viable for these companies to increase the number
    of full-time agents; therefore, a common strategy is to dynamically hire more
    agents for days when demand for the service is expected to be high. To plan better,
    visibility of rainfall predictions for the next day is of paramount importance.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the context of the problem set up, let's try to define our problem statement
    for a hyper-local food-delivery service company to predict the rainfall for the
    next day. To keep things simple and consistent, let's frame the problem statement
    using the frameworks we studied previously, in *Chapter 2*, *Exploratory Analysis
    of Data*. This will help us distill the end goal we want to solve in a business-first
    approach while keeping the machine learning perspective at the forefront.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure creates a simple visual for the **Situation** - **Complication**
    - **Question** (**SCQ**) framework for the previously defined use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1: SCQ for the classification use case'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.1: SCQ for the classification use case'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can clearly answer the question from the SCQ: we would need a predictive
    model to predict the chances of rain for the next day as a solution to the problem.
    Let''s move on to the next step – gathering data to build a predictive model that
    will help us solve the business problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Gathering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `rattle.data` package provides us with the data for the use case, which
    can be accessed using the internal dataset methods of R. In case you have not
    already installed the packages, you can easily install them using the `install.packages("rattle.data")`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 63: Exploring Data for the Use Case'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will perform the initial exploration of the dataset we
    have gathered for the use case. We will explore the shape of the data, that is,
    the number of rows and columns, and study the content within each column.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore the shape (rows x columns) and content of the data, perform the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the `rattle` package using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data for our use case, which is available from the `rattle` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The `weatherAUS` dataset is a DataFrame containing more than 1,40,000 daily
    observations from over 45 Australian weather stations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, load the weather data directly into a DataFrame called `df`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Explore the DataFrame''s content using the `str` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.2: Final output'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.2: Final output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have almost 1,50,000 rows of data and 24 variables. We would need to drop
    the `RISK_MM` variable, as it will be the target variable for the regression use
    case (that is, predicting how much it will rain the next day). Therefore, we are
    left with 22 independent variables and 1 dependent variable, `RainTomorrow`, for
    our use case. We can also see a good mix of continuous and categorical variables.
    The `Location`, `WindDir`, `RainToday`, and many more variables are categorical,
    and the remainder are continuous.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vwgu8Q.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will calculate the total percentage of the null values
    in each column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 64: Calculating the Null Value Percentage in All Columns'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dataset we explored in *Exercise 1*, *Exploring Data for the Use Case* has
    quite a few null values. In this exercise, we will write a script to calculate
    the percentage of null values within each column.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the presence of null values in a few variables. Let's check the percentage
    of null values in each column within the `df` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to calculate the percentage of null values in each
    column of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, remove the column named `RISK_MM`, since it is supposed to be used as
    a target variable for regression use. (Adding this to our model will result in
    data leakage.):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `temp_df` DataFrame object and store the value in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the `print` function to display the percentage null values in each
    column using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the last four variables have more than *30%* missing or null
    values. This is a significantly huge drop. It would be best to drop these variables
    from our analysis. Also, we can see that there are a few other variables that
    have roughly *1%*-*2%*, and in some cases, up to *10%* missing or null values.
    We can treat these variables using various missing value treatment techniques,
    such as replacing them with mean or mode. In some important cases, we can also
    use additional techniques, such as clustering-based mean and mode replacement,
    for improved treatment. Additionally, in very critical scenarios, we can use a
    regression model to estimate the remainder of the missing values by defining a
    model where the column with the required missing value is treated as a function
    of the remaining variables.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2ViZEp1.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will remove null values. We will revisit data
    if we do not have a good model in place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 65: Removing Null Values from the Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: John is working on the newly created dataset, and while doing analysis, he has
    found out that the dataset contains significant null values. To make the dataset
    useful for further analysis, he must remove the null values from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to remove the null values from the `df` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, select the last four columns to drop that have more than *30%* null
    values using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove all the rows from the DataFrame that will have one or more columns with
    null values using the `na.omit` command, which removes all of the null rows from
    the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the newly formatted data using the following `print` commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the following command, verify whether the newly created dataset contains
    null values or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, print the dataset using the following `print` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now double check and see that the new dataset has no more missing values
    and the overall number of rows in the dataset also reduced to 112,000, which is
    around a *20%* loss of training data. We should use missing value treatment techniques
    such as replacing missing values with the mean, mode, or median to combat such
    high losses due to the omission of missing values. A rule of thumb would be to
    safely ignore anything less than a *5%* loss. Since, we have more than 1,00,000
    records (a reasonably high number of records for a simple use case), we are ignoring
    this rule of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q3HIgT.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we can also engineer date- and time-related features using the
    `Date` column. The following exercise creates numeric features such as day, month,
    day of the week, and quarter of the year as additional time-related features and
    drops the original `Date` variable.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `lubridate` library in R to work with date and time-related
    features. It provides us with extremely easy-to-use functions to perform date
    and time operations. If you have not already installed the package, please install
    the library using the `install.packages('lubridate')` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 66: Engineer Time-Based Features from the Date Variable'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time- and date-related attributes cannot be directly used in a supervised classification
    model. To extract meaningful properties from date- and time-related variables,
    it is a common practice to create month, year, week, and quarter from the date
    as features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to work with the data and time function in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `lubridate` library into RStudio using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The `lubridate` library provides handy date- and time-related functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract `day`, `month`, `dayofweek`, and `quarter` as new features from the
    `Date` variable using the `lubridate` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the newly created variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have created all of the date- and time-related features, we won''t
    need the actual `Date` variable. Therefore, delete the previous `Date` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this exercise, we have extracted meaningful features from date- and time-related
    attributes from the data and removed the actual date-related columns.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2E4hOEU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to process or clean another feature within the DataFrame: `location`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 67: Exploring the Location Frequency'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Location` variable defines the actual location where the weather data was
    captured for the specified time. Let's do a quick check on the number of distinct
    values that are captured within this variable and see whether there are any interesting
    patterns that might be of importance.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will be using the `Location` variable to define
    the actual location where the weather data was captured for the specified time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Calculate the frequency of rain across each location using the grouping functions
    from the `dplyr` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the number of distinct locations for sanity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print `summary` to examine the aggregation performed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that there are 44 distinct locations in the data. The `cnt` variable,
    which defines the number of records (in the previous transformed data) for each
    location, has an average 2,566 records. The similar number distribution between
    the first quartile, median, and third quartile denote that the locations are evenly
    distributed in the data. However, if we investigate the percentage of records
    where rain was recorded (`pct`), we see an interesting trend. Here, we have locations
    with around a *6%* chance of rain and some with around a *36%* chance of rain.
    There is a huge difference in the possibility of rain, based on the location.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30aKUMx.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have around 44 distinct locations, it is difficult to utilize this
    variable directly as a categorical feature. In R, most supervised learning algorithms
    internally convert the categorical column into a numerical form that can be interpreted
    by the model. However, with an increased number of classes within the categorical
    variable, the complexity of the model increases with no additional value. To keep
    things simple, we can transform the `Location` variable as a new variable with
    a reduced number of levels. We will select the top five and the bottom five locations
    with chances of rain and tag all other locations as `Others`. This will reduce
    the number of distinct levels in the variable as 10+1 and will be more suitable
    for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 68: Engineering the New Location with Reduced Levels'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `location` variable has too many distinct values (44 locations), and machine
    learning models in general do not perform well with categorical variables with
    a high frequency of distinct classes. We therefore need to prune the variable
    by reducing the number of distinct classes within it. We will select the top five
    and the bottom five locations with chances of rain and tag all other locations
    as `Others`. This will reduce the number of distinct levels in the variable as
    10+1 and will be more suitable for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to engineer a new variable for location with a
    reduced number of distinct levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the `location` variable from a factor into a character:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a list with the top five and the bottom five locations with respect
    to the chances of rain. We can do this by using the `head` command for the top
    five and the `tail` command for the bottom five locations after ordering the DataFrame
    in ascending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the list to double-check that we have the locations correctly stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the `Location` variable in the main `df_new` DataFrame into a `character`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reduce the number of distinct locations in the variable. This can be done by
    tagging all the locations that are not a part of the `location_list` list as `Others`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Delete the old `Location` variable using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To ensure that the fifth step was correctly performed, we can create a temporary
    DataFrame and summarize the frequency of records against the new `location` variable
    we created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the temporary test DataFrame and observe the results. We should see only
    11 distinct location values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We first convert the `Location` variable from a factor to a character to ease
    the string operation's tasks. The DataFrame is sorted in descending order according
    to the percentage chance of rain. The `head` and the `tail` commands are used
    to extract the top and bottom five locations in a list. This list is then used
    as a reference check to reduce the number of levels in the new feature. Finally,
    after engineering the new feature with the reduced levels, we do a simple check
    to ensure that our feature has been engineered in the way we expect.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30fnR31.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now get into the most interesting topic of the chapter and explore classification
    techniques for supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Classification Techniques for Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To approach a **supervised classification algorithm**, we first need to understand
    the basic functioning of the algorithm, explore a bit of the math in an abstract
    way, and then develop the algorithm using readily available packages in R. We
    will cover a few basic algorithms, such as white-box algorithms such as Logistic
    Regression and Decision Trees, and then we will move on to advanced modeling techniques,
    such as black-box models such as Random Forest, XGBoost, and neural networks.
    The list of algorithms we plan to cover is not exhaustive, but these five algorithms
    will help you gain a broad understanding of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Logistic regression** is the most favorable white-box model used for binary
    classification. White-box models are defined as models where we have visibility
    of the entire reasoning used for the prediction. For each prediction made, we
    can leverage the model''s mathematical equation and decode the reasons for the
    prediction made. There are also a set of classification models that are completely
    black-box, that is, by no means can we understand the reasoning for the prediction
    leveraged by the model. In situations where we want to focus on only the end outcome,
    we should prefer black-box models, as they are more powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: Though the name ends with *regression*, logistic regression is a technique used
    to predict binary categorical outcomes. We would need a different approach to
    model for a categorical outcome. This can be done by transforming the outcome
    into a log of odds ratio or the probability of the event happening.
  prefs: []
  type: TYPE_NORMAL
- en: Let's distill this approach into simpler constructs. Assume the probability
    of success for an event is 0.8\. Then, the probability of failure for the same
    event would be defined as *(1-0.8) = 0.2*. The odds of success are defined as
    the ratio of the probability of success over the probability of failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, the odds of success would be *(0.8/0.2) = 4*. That
    is, the odds of success are four to one. If the probability of success is 0.5,
    that is, a 50-50 percent chance, then the odds of success are 0.5 to 1\. The logistic
    regression model can be mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where, ![](img/C12624_05_11.png) is the log of odds ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solving the math further, we can deduce the probability of the outcome as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C12624_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Discussing the mathematical background and derivation of the equations is beyond
    the scope of the chapter. To summarize, the `logit` function, that is, the link
    function, helps logistic regression reframe the problem (predicted outcome) intuitively
    as the log of odds ratio. When solved, it helps us predict the probability of
    a binary dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Logistic Regression Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like linear regression, where the beta coefficients for the variables are
    estimated using the **Ordinary Least Squares** (**OLS**) method, a logistic regression
    model leverages the **maximum-likelihood estimation** (**MLE**). The MLE function
    estimates the best set of values of the model parameters or beta coefficients
    such that it maximizes the likelihood function, that is, the probability estimates,
    which can be also defined as the *agreement* of the selected model with the observed
    data. When the best set of parameter values are estimated, plugging these values
    or beta coefficients into the model equation as previously defined would help
    in estimating the probability of the outcome for a given sample. Akin to OLS,
    MLE is also an iterative process.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see a logistic regression model in action on our dataset. To get started,
    we will use only a small subset of variables for the model. Ideally, it is recommended
    to start with the most important variables based on the EDA exercise and then
    incrementally add remainder variables. For now, we will start with a temperature-related
    variable for the maximum and minimum values, a wind speed-related variable, pressure
    and humidity at 3 P.M., and the rainfall for the current day.
  prefs: []
  type: TYPE_NORMAL
- en: We will divide the entire dataset into train (70%) and test (30%). While fitting
    the data to the model, we will only use the train dataset and will later evaluate
    the performance of the model on the train, as well as the unseen test data. This
    approach will help us understand whether our model is overfitting and provide
    a more realistic model performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 69: Build a Logistic Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will build a binary classification model using logistic regression and the
    dataset we explored in the Exercises 1-6\. We will divide the data into train
    and test (70% and 30%, respectively) and leverage the training data to fit the
    model and the test data to evaluate the model's performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, set `seed` for reproducibility using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create a list of indexes for the training dataset (70%):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, split the data into test and train datasets using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the logistic regression model with `RainTomorrow` as the dependent variable
    and a few independent variables (we selected `MinTemp`, `Rainfall`, `WindGustSpeed`,
    `WindSpeed3pm`, `Humidity3pm`. `Pressure3pm`, `RainToday`, `Temp3pm`, and `Temp9am`).
    We can add all the available independent variables in the DataFrame too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the dataset using the `summary` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `set.seed` command ensures that the random selections used for the train
    and test data split can be reproduced. We divide the data into 70% train and 30%
    test. The set seed function ensures that, for the same seed, we get the same split
    every time. The `glm` function is used in R to build generalized linear models.
    Logistic regression is defined in the model using the `family` parameter value
    set to `binomial(link ='logit')`. The `glm` function can be used to build several
    other models too (such as gamma, Poisson, and binomial). The formula defines the
    dependent, as well as the set of independent, variables. It takes the general
    form *Var1 ~ Var2 + Var3 + …*, which denotes `Var1` as the dependent or target
    variable and the remainder as the independent variables. If we want to use all
    of the variables in the DataFrame as independent variables, we can instead use
    `formula = Var1 ~ .`, which would indicate that `Var1` is the dependent variable
    and the rest are all independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2HwwUUX.'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting the Results of Logistic Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We previously had a glimpse of logistic regression in *Chapter 2*, *Exploratory
    Analysis of Data*, but we didn't get into the specifics of the model results.
    The results demonstrated in the previous output snippet will look like what you
    observed in linear regression, but with some differences. Let's explore and interpret
    the results part by part.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we have the `glm` function calculates two types of residuals, that
    is, **Null Deviance** and **Residual Deviance**. The difference between the two
    is that one reports the goodness of fit when only the intercept (that is, no dependent
    variables) is used and the other reports when all the provided independent variables
    are used. The reduction in deviance between null and residual deviance helps us
    understand the quantified value added by the independent variables in defining
    the variance or the predictive correctness. The distribution of deviance residuals
    is reported right after the formula.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have the **beta coefficients** and the associated **standard error**,
    the *z-value* and the *p-value*, which is the probability of significance. For
    each variable provided, R internally calculates the coefficients and, along with
    the parameter value, it also reports additional test results to help us interpret
    how effective these coefficients are. The absolute value of the coefficient is
    a simple way to understand how important that variable is to the final predictive
    power, that is, how impactful the variable is in determining the end outcome of
    the prediction. We can see that all variables have a low value for the coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the standard error helps us quantify how stable the value will be. A lower
    value for the standard error would indicate more consistent or stable values for
    the beta coefficients. The standard errors for all the variables in our exercise
    are low. The *z-value* and the probability of significance together help us take
    a call as to whether the results are statistically significant or just appear
    as they are due to random chance. This idea follows on from the same principle
    we learned about the null and alternate hypothesis in *Chapter 2*, *Exploratory
    Analysis of Data*, and is akin to linear regression parameter significance, which
    we learned about in *Chapter 4*, *Regression*.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to interpret the significance would be to study the *asterix*
    besides each independent variable, that is, `*`. The number of `*` is defined
    by the actual probability value, as defined below the parameter values. In our
    exercise, notice that the `MinTemp` variable is not statistically significant,
    that is, *p-value > 0.05*. The rest are all statistically significant variables.
  prefs: []
  type: TYPE_NORMAL
- en: The **Akaike Information Criterion** (**AIC**) is again a metric reported by
    R to assess the goodness of fit of the model or the quality of the model. This
    number comes in handy to compare different models for the same use case. Say you
    fit several models using a combination of independent variables but the same dependent
    variable, the AIC can be used to study the best model by way of a simple comparison
    of the value in all models. The calculation of the metric is derived from the
    deviance between the model's prediction and the actual labels, but factors in
    the presence of variables that are not adding any value. Therefore, akin to **R
    Squared** and **adjusted R Squared** in linear regression, the AIC helps us to
    avoid building complicated models. To select the best model from a list of candidate
    models, we should select the model with the lowest AIC.
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end of the previous output, we can see the results from **Fisher's
    Scoring** algorithm, which is a derivative of Newton's method for solving maximum
    likelihood problems numerically. We see that it required five iterations to fit
    the data to the model, but beyond that, this information is not of much value
    to us. It is a simple indication for us to conclude that the model did converge.
  prefs: []
  type: TYPE_NORMAL
- en: We now understand how logistic regression works and have interpreted the results
    reported by the model in R. However, we still need to evaluate the model results
    using our train and test dataset and ensure that the model performs well on unseen
    data. To study the performance of a classification model, we would need to leverage
    various metrics, such as accuracy, precision, and recall. Though we already explored
    them in *Chapter 4*, *Regression*, let's now study them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Classification Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification models require a bunch of different metrics to be thoroughly
    evaluated, unlike regression models. Here, we don't have something as intuitive
    as **R Squared**. Moreover, the performance requirements completely change based
    on a specific use case. Let's take a brief look at the various metrics that we
    already studied in *Chapter 3*, *Introduction to Supervised Learning*, for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion Matrix and Its Derived Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first basis for studying model performance for classification algorithms
    starts with a **confusion matrix**. A confusion matrix is a simple representation
    of the distribution of predictions of each class across the actuals of each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3: Confusion matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.3: Confusion matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The previous table is a simple representation of a confusion matrix. Here, we
    assume that the `1`) and `0`); when the result is correctly predicted, then we
    assign `1` correctly predicted as `1` and so on for the remaining outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the confusion matrix and the values defined from it, we can further
    define a couple of metrics that will help us better understand the model''s performance.
    We will now use the abbreviations **TP** for **True Positive**, **FP** for **False
    Positive**, **TN** for **True Negative**, and **FN** for **False Negative** going
    forward:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall accuracy**: Overall accuracy is defined as the ratio of total correct
    predictions to the total number of predictions in the entire test sample. So,
    this would be simply the sum of **True Positives** and **True Negatives** divided
    by all the metrics in the confusion matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Precision** or **Positive Predictive Value** (**PPV**): Precision is defined
    as the ratio of correctly predicted positive labels to the total number of positively
    predicted labels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Recall** or **Sensitivity**: Recall measures how sensitive your model is
    by representing the ratio of the number of correctly predicted positive labels
    to the total number of actual positive labels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Specificity** or **True Negative Rate** (**TNR**): Specificity defines the
    ratio of correctly predicted negative labels to the total number of actual negative
    labels:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**F1 Score**: The F1 score is the harmonic mean between precision and recall.
    It is a better metric to consider than overall accuracy for most cases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/C12624_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What Metric Should You Choose?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important aspect to consider on a serious note, is which metric we should
    consider while evaluating a model. There is no straightforward answer, as the
    best combination of metrics completely depend on the type of classification use
    case we are dealing with. One situation that commonly arises in classification
    use cases is imbalanced classes. It is not necessary for us to always have an
    equal distribution of positive and negative labels in data. In fact, in most cases,
    we would be dealing with a scenario where the positive class would be less than
    *30%* of the data. In such cases, the overall accuracy would not be the ideal
    metric to consider.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a simple example to understand this better. Consider the example
    of predicting fraud in credit card transactions. In a realistic scenario, for
    every 100 transactions there may be just one or two fraud transactions. Now, if
    we use overall accuracy as the only metric to evaluate a model, even if we predict
    all the labels as **No**, that is, **Not Fraud**, we would have approximately
    *99%* accuracy, *0%* precision, and *0%* recall. The *99%* accuracy might seem
    a great number for model performance; however, in this case, it would not be the
    ideal metric to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: To deal with such a situation, there is often additional business context required
    to make a tangible call, but in most cases (for this type of a scenario), the
    business would want a higher recall with a bit of compromise on the overall accuracy
    and precision. The rationale to use high recall as the metric for model evaluation
    is that it would still be fine to predict a transaction as fraud even if it is
    authentic; however, it would be a mistake to predict a fraud transaction as authentic;
    the business losses would be colossal.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the evaluation of a model is taken with a combination of metrics based
    on business demands. The biggest decision maker would be the trade-off between
    precision and recall. As indicated by the confusion matrix, whenever we try to
    improve precision, it hurts recall and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some business situations in which we prioritize different metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Predicting a rare event with catastrophic consequences**: When predicting
    whether a patient has cancer or not, whether a transaction is fraud, and so on,
    it is OK to predict a person without cancer as having cancer, but the other way
    around would result in the loss of life. Such scenarios demand high recall by
    compromising *precision* and *overall accuracy*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicting a rare event with not such catastrophic consequences**: When predicting
    whether a customer will churn or whether a customer will positively respond to
    a marketing campaign, the business outcome is not jeopardized by an incorrect
    prediction, but would be the campaign. In such cases, based on the situation,
    it would make sense to have high precision with a bit of compromise on recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicting a regular (non-rare) event with not such catastrophic consequences**:
    This would deal with most classification use cases, where the cost of correctly
    predicting a class is almost equal to the cost of incorrectly predicting the class.
    In such cases, we can use the F1 score, which represents a harmonic mean between
    precision and recall. It would be ideal to use overall accuracy in conjunction
    with the F1 score, as accuracy is more easily interpretable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now evaluate the logistic regression model that we built previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 70: Evaluate a Logistic Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine learning models fitted on a training dataset cannot be evaluated using
    the same dataset. We would need to leverage a separate test dataset and compare
    the model's performance on a train as well as a test dataset. The `caret` package
    has some handy functions to compute the model evaluation metrics previously discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to evaluate the logistic regression model we built
    in *Exercise 7*, *Build a Logistic Regression Model*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the distribution of records for the `RainTomorrow` target variable
    in the `df_new` DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the `RainTomorrow` target variable on the train data using the `predict`
    function and cast observations with values (probability >0.5) as `Yes`, else `No`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the confusion matrix and print the results for the train data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the results on the test data, similar to the second step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a confusion matrix for the test data predictions and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We first load the necessary `caret` library, which will provide the functions
    to compute the desired metrics, as discussed. We then use the `predict` function
    in R to predict the results using the previously fitted model on the train as
    well as the test data (separately). The `predict` function for logistic regression
    returns the value of the `link` function, by default. Using the `type= 'response'`
    parameter, we can override the function to return probabilities for the target.
    For simplicity, we use `0.5` as a threshold on the predictions. Therefore, anything
    above 0.5 would be `confusionMatrix` function from the `caret` library provides
    us with a simple way to construct the confusion matrix and calculate an exhaustive
    list of metrics. We would need to pass the actual, as well the predicted labels,
    to the function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q6mYW0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The distribution of the target label is imbalanced: *77%* no and *23%* yes.
    In such a scenario, we cannot rely only on the overall accuracy as a metric to
    evaluate the model''s performance. Also, the confusion matrix, as shown in the
    output for steps 3 and 5, is inverted when compared to the illustration shown
    in the previous section, *Confusion Matrix and Its Derived Metrics*. We have the
    predictions as rows and actual values as columns. However, the interpretation
    and results will remain the same. The next set of output reports the metrics of
    interest, along with a few others we have not explored. We have covered the most
    important ones (sensitivity and precision, that is, positive predictive value);
    however, it is recommended to explore the remaining metrics, such as negative
    predicted value, prevalence and detection rate. We can see that we are getting
    precision of around *73%* and *50%* recall and overall accuracy of *85%*. The
    results are similar on the train and test datasets; therefore, we can conclude
    that the model doesn''t overfit.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The results are not bad overall. Please don't be surprised to see the low recall
    rate; in scenarios where we have imbalanced datasets, the metrics that are used
    to assess model performance are business-driven.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that we would correctly predict at least half of the time whenever
    there is a possibility of rain, and whenever we predict, we are *73%* correct.
    From a business perspective, if we try to contemplate whether we should strive
    for high recall or precision, we would need to estimate the cost of misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, whenever we predict that there is rainfall predicted for the
    next day, the operations management team would prepare the team with a higher
    number of agents to deliver faster. Since there isn't a pre-existing technique
    to combat rainfall-related problems, we have an opportunity to cover even if we
    recall only 50% of the times when there is rain. In this problem, since the cost
    of incorrectly predicting rain will be more expensive for the business, that is,
    if the chances of rainfall are predicted, the team would invest in pooling more
    agents for delivery, which comes at an additional cost. Therefore, we would want
    higher precision, while we are OK to compromise on recall.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ideal scenario is to have high precision and high recall. However, there
    is always a trade-off in achieving one over the other. In most real-life machine
    learning use cases, a business-driven decision finalizes the priority to choose
    either precision or recall.
  prefs: []
  type: TYPE_NORMAL
- en: The previous model developed in *Exercise 8*, *Evaluate a Logistic Regression
    Model*, was developed only using a few variables that were available in the `df_new`
    dataset. Let's build an improved model with all the available variables in the
    dataset and check the performance on the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to iterate for model improvements would be with feature selection
    and hyperparameter tuning. Feature selection involves selecting the best set of
    features from the available list through various validation approaches and finalizing
    a model with the best performance and the least number of features. Hyperparameter
    tuning deals with building generalized models that will not overfit, that is,
    a model that performs well on training as well as unseen test data. These topics
    will be covered in detail in *Chapter 6*, *Feature Selection and Dimensionality
    Reduction*, and *Chapter 7*, *Model Improvements*. For now, the scope of the chapter
    will be restricted to demonstrate model evaluation only. We will touch on the
    same use case for hyperparameter tuning and feature selection in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 71: Develop a Logistic Regression Model with All of the Independent
    Variables Available in Our Use Case'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous exercise, we limited the number of independent variables to
    only a few. In this example, we will use all the available independent variables
    in our `df_new` dataset and create an improved model. We will again use the train
    dataset to fit the model and test to evaluate the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build a logistic regression model with all of
    the independent variables available within the use case:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the logistic regression model with all the available independent variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict on the train dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the results on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We leverage all the variables within the dataset to create a logistic regression
    model using the `glm` function. We then use the **fitted** model to predict the
    outcomes for the train and the test datasets; akin to the previous exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2HgwjaU.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the overall accuracy, precision, and recall has improved a bit (though
    marginally). The results are fair and we can iterate with logistic regression
    to improving them further. For now, let's explore a few other classification techniques
    and study the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this exercise, we have not printed the model's summary statistics, akin to
    the first model, with a few variables. If printed, the results would consume less
    than two pages of the chapter. For now, we will ignore that since we are not exploring
    the model characteristics that are reported by R; instead, we are evaluating a
    model purely from the accuracy, precision, and recall metrics on the train and
    test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The ideal way to get the best model would be to eliminate all statistically
    insignificant variables, remove multicollinearity, and treat the data for outliers,
    and so on. All these steps have been ignored for now, given the scope of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Building a Logistic Regression Model with Additional Features'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We built a simple model with few features in *Exercise 8*, *Evaluate a Logistic
    Regression Model*, and then with all the features in *Exercise 9*, *Develop a
    Logistic Regression Model with All of the Independent Variables Available in Our
    Use Case*. In this activity, we will build a logistic regression model with additional
    features that we can generate using simple mathematical transformations. It is
    good practice to add additional transformations of numeric features with log transformations,
    square and cube power transformations, square root transformations, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to develop a logistic regression model with additional
    features engineered:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a copy of the `df_new` dataset in `df_copy` for the activity and select
    any three numeric features (for example, `MaxTemp`, `Rainfall` and `Humidity3pm`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Engineer new features with square and cube power and square root transformations
    for each of the selected features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the `df_copy` dataset into train and test in a 70:30 ratio.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the model with the new train data, evaluate it on test data, and finally,
    compare the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 451.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like logistic regression, there is another popular classification technique
    that is very popular due to its simplicity and white-box nature. A decision tree
    is a simple flowchart that is represented in the form of a tree (an inverted tree).
    It starts with a root node and branches into several nodes, which can be traversed
    based on a decision, and ends with a leaf node where the *final outcome* is determined.
    Decision trees can be used for regression, as well as classification use cases.
    There are several variations of decision trees implemented in machine learning.
    A few popular choices are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative Dichotomiser 3** (**ID3**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Successor to ID3** (**C4.5**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification and Regression Tree** (**CART**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CHi-squared Automatic Interaction Detector** (**CHAID**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional Inference Trees** (**C Trees**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding list is not exhaustive. There are other alternatives, and each
    of them has small variations in how they approach the tree creation process. In
    this chapter, we will limit our exploration to **CART Decision Trees**, which
    are the most widely used. R provides a few packages that house the implementation
    of the CART algorithm. Before we delve into the implementation, let's explore
    a few important aspects of decision trees in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: How Do Decision Trees Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each variation of decision trees has a slightly different approach. Overall,
    if we try to simplify the pseudocode for a generic decision tree, it can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the root node (the node corresponds to a variable).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition the data into groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each group from the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a decision node or leaf node (based on the splitting criteria).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Repeat until node size <= threshold or features = empty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Variations between different forms of tree implementations include the way categorical
    and numerical variables are handled, the approach used to select the root node
    and consecutive nodes in the tree, the rules to branch each decision node, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following visual is a sample decision tree. The root node and the decision
    nodes are the independent variables we provide to the algorithm. The leaf nodes
    denote the final outcome, whereas the root node and the intermediate decision
    nodes help in traversing the data to the leaf node. The simplicity of a decision
    tree is what makes it so effective and easy to interpret. This helps in easily
    identifying rules for a prediction task. Often, many research and business initiatives
    leverage decision trees to design a set of rules for a simple classification system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4: Sample decision tree'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.4: Sample decision tree'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In a general sense, given a combination of dependent and several independent
    variables, the decision tree algorithm calculates a metric that represents the
    goodness of fit between the dependent target variable and all independent variables.
    For classification use cases, entropy and information gain are commonly used metrics
    in CART decision trees. The variable with the best fit for the metric is chosen
    as the root node and the next best is used as the decision nodes in the descending
    order of fit. The nodes are terminated into leaf nodes based on a defined threshold.
    The tree keeps growing till it exhausts the number of variables for decision nodes
    or when a predefined threshold for the number of nodes is reached.
  prefs: []
  type: TYPE_NORMAL
- en: To improve tree performance and reduce overfitting, a few strategies, such as
    restricting the depth or breadth of the tree or additional rules for leaf nodes
    or decision nodes help in generalizing a tree for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement the same use case using CART decision trees in R. The CART model
    is available through the `rpart` package in R. This algorithm was developed by
    Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone in 1984 and has
    been widely adopted in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 72: Create a Decision Tree Model in R'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will create a decision tree model in R using the same data
    and the use case we leveraged in *Exercise 9*, *Develop a Logistic Regression
    Model with All of the Independent Variables Available in Our Use Case*. We will
    try to study whether there are any differences in the performance of a decision
    tree model over a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create a decision tree model in R:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `rpart` and `rpart.plot` packages using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the CART model with all of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the cost parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.5: Decision tree model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_05_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.5: Decision tree model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot the tree using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.6: Predicting rainfall'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C12624_05_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 5.6: Predicting rainfall'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Make predictions on the train data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make predictions on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `rpart` library provides us with the CART implementation of decision trees.
    There are additional libraries that help us visualize the decision tree in R.
    We have used `rpart.plot` here. If the package is not already installed, please
    install it using the `install.packages` command. We use the `rpart` function to
    create the tree model and we use all the available independent variables. We then
    use the `plotcp` function to visualize the complexity parameter's corresponding
    validation error on different iterations. We also use the `plot.rpart` function
    to plot the decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we make predictions on the train as well as the test data and build
    the confusion matrix and calculate the metrics of interest using the `confusionMatrix`
    function for the train and test datasets individually.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2WECLgZ.'
  prefs: []
  type: TYPE_NORMAL
- en: The CART decision tree implemented in R has several optimizations already in
    place. The function, by default, sets a ton of parameters for optimum results.
    In a decision tree, there are several parameters that we can manually set to tune
    the performance based on our requirements. However, the R implementation does
    a great job of setting a wide number of parameters with a relatively good value
    by default. These additional settings can be added to the `rpart` tree with the
    `control` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add the following parameter to the tree model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: One parameter of interest would be the `0.01`. We can further change this to
    a lower number that would make the tree grow deeper and become more complicated.
    The `plotcp` function visualizes the relative validation error for different values
    of `cp`, that is, the complexity parameter. The most ideal value for `cp` is the
    leftmost value below the dotted line in the plot in *Figure 5.4*. In this case
    (as shown in the plot), the best value is 0.017\. Since this value is not very
    different from the default value, we don't change it further.
  prefs: []
  type: TYPE_NORMAL
- en: The next plot in *Figure 5.5* helps us visualize the actual decision tree constructed
    by the algorithm. We can see the simple set of rules being constructed using the
    available data. As you can see, only two independent variables, that is, `Humidity3pm`
    and `WindGustSpeed`, have been selected for the tree. If we change the complexity
    parameter to *0.001* instead of *0.01*, we can see a much deeper tree (which could
    overfit the model) would have been constructed. Finally, we can see the results
    from the confusion matrix (step 6) along with additional metrics of interest for
    the train and test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the results are similar for the train and test dataset. We can
    therefore conclude that the model doesn't overfit. However, there is a significant
    drop in accuracy (*83%*) and recall (*35%*), while the precision has increased
    to a slightly higher value (*77%*).
  prefs: []
  type: TYPE_NORMAL
- en: We have now worked with a few white-box modeling techniques. Given the simplicity
    and ease of interpretation of white-box models, they are the most preferred technique
    for classification use cases in business, where reasoning and driver analysis
    is of paramount importance. However, there are a few scenarios where a business
    might be more interested in the *net outcome* of the model rather than the entire
    interpretation of the outcome. In such cases, the end model performance is of
    more interest. In our use case, we want to achieve high precision. Let's explore
    a few black-box models that are superior (in most cases) to white-box models in
    terms of model performance and that can be achieved with far less effort and more
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Create a Decision Tree Model with Additional Control Parameters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The decision tree model we created in *Exercise 10*, *Create a Decision Tree
    Model in R*, used the default control parameters for the tree. In this activity,
    we will override a few control parameters and study its impact on the overall
    tree-fitting process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create a decision tree model with additional
    control parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `rpart` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the control object for the decision tree with new values: `minsplit
    =15` and `cp = 0.00`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the tree model with the train data and pass the control object to the `rpart`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the complexity parameter plot to see how the tree performs at different
    values of `CP`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fitted model to make predictions on the train data and create the confusion
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fitted model to make predictions on the test data and create the confusion
    matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 454.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ensemble Modelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ensemble modeling is one of the most popular approaches used in classification
    and regression modeling techniques when there is a need for improved performance
    with a larger training sample. In simple words, ensemble modeling can be defined
    by breaking down the name into individual terms: **ensemble** and **modeling**.
    We have already studied modeling in this book; an ensemble in simple terms is
    a **group**. Therefore, the process of building several models for the same task
    instead of just one model and then combining the results into a single outcome
    through any means, such as averaging or voting, and many others, is called **ensemble
    modeling**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build ensembles of any models, such as linear models or tree models,
    and in fact can even build an ensemble of ensemble models. However, the most popular
    approach is using tree models as the base for ensembles. There are two broad types
    of ensemble models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging**: Here, each model is built in parallel with some randomization
    introduced within each model, and the results of all models are combined using
    a simple voting mechanism. Say we built 100 tree models and 60 models predicted
    the outcome as *Yes* and 40 predicted it as *No*. The end result would be a *Yes*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Boosting**: Here, models are built sequentially and the results of the first
    model are used to tune the next model. Each model iteratively learns from errors
    made by the previous model and tries to improve with successive iterations. The
    result is usually a weighted average of all the individual outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several implementations available in bagging as well as boosting.
    **Bagging** itself is an ensemble model available in R. By far the most popular
    bagging technique used is random forest. Another bagging technique along similar
    lines as random forest is **extra trees**. Similarly, a few examples of boosting
    techniques are AdaBoost, Stochastic Gradient Boosting, BrownBoost, and many others.
    However, the most popular boosting technique is **XGBoost**, which is derived
    from the name **EXtreme Gradient Boosting**. In most cases, for classification
    as well as regression use cases, data scientists prefer using random forests or
    XGBoost models. A recent survey on Kaggle (an online data science community) revealed
    the most popular technique used for most machine learning competitions were always
    random forest and XGBoost. In this chapter, we will take a closer look at both
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random forest** is the most popular bagging technique used in machine learning.
    It was developed by Leo Brieman, the author of CART. This simple technique is
    so effective that it is almost always the first choice of algorithm for a data
    scientist given a supervised use case. Random forest is a good choice for classification
    as well as regression use cases. It is a highly effective method for reducing
    overfitting with a bare minimum amount of effort. Let''s have a deeper understanding
    of how random forests work.'
  prefs: []
  type: TYPE_NORMAL
- en: As we already know, random forest is an ensemble modeling technique, where we
    build several models and combine their results using a simple voting technique.
    In random forests, we use decision trees as the base model. The inner workings
    of the algorithm can be fairly guessed from the name itself, that is, random (since
    it induces a layer of randomization in every model that is built) and forest (since
    there are several *tree* models we build). Before we get into the actual workings
    of the algorithm, we first need to understand the story of its predecessor, **bagging**,
    and study why we need ensembles.
  prefs: []
  type: TYPE_NORMAL
- en: Why Are Ensemble Models Used?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first question that would have surfaced in your thoughts may be, why do
    we need to build several models for the same task in the first place? Is it necessary?
    Well, yes! When we build ensembles, we don't build the exact same model several
    times; instead, every model we build will be different from the others in some
    way. The intuition behind this can be understood using a simple example from our
    day-to-day lives. It is built on the principle that several weak learners combined
    together build a stronger and more robust model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this idea using a simple example. Say you reach a new city
    and want to know the chances of there being rain in the city the next day. Assuming
    technology is not an available option, the easiest way you could find this out
    would be to ask someone in the neighborhood who has been a dweller of the place
    for a while. Maybe the answer would not always be correct; if someone said that
    there was a very high chance of rain the next day, it doesn't necessarily mean
    that it would certainly rain. Therefore, to make an improved guess, you ask several
    people in the neighborhood. Now, if 7 out of the 10 people you asked mentioned
    that there was a high chance of rain the next day, then it almost certainly would
    rain the very next day. The reason this works effectively is because every person
    you reached out to would have some understanding about rain patterns and also
    every person's understanding about those patterns would be a bit different. Though
    the differences are not miles apart, some level of randomness among the people's
    understanding when aggregated for a collective answer would yield a better answer.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging – Predecessor to Random Forest
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ensemble modeling works on the same principle. Here, in each model, we induce
    some level of randomness. The bagging algorithm brings in this randomness for
    each model on the training data. The name bagging is derived from **Bootstrap
    Aggregation**; a process where we sample two-thirds of the available data with
    replacement data for training and the remainder for testing and validation. Here,
    each model, that is, a decision tree model, trains on a slightly different dataset
    and therefore might have a slightly different outcome for the same test sample.
    Bagging, in a way, mimics the real-world example that we discussed and therefore
    combines several weak learners (decision tree models) into a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Random Forest Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Random forest** is basically a successor to bagging. Here, apart from the
    randomness in the training data, random forest adds an additional layer of randomness
    with the feature set. Therefore, each decision tree not only has bootstrap aggregation,
    that is, two thirds of the training data with replacement, but also a subset of
    features randomly selected from the available list. Thus, each individual decision
    tree in the ensemble has a slightly different training dataset and a slightly
    different set of features to train. This additional layer of randomness works
    effectively in generalizing the model and reduces variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 73: Building a Random Forest Model in R'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will build a random forest model on the same dataset we
    leveraged in Exercises 8, 9, and 10\. We will leverage ensemble modelling and
    test whether the overall model performance improves compared to decision trees
    and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To get started, we can quickly build a random forest model using the same dataset
    we used earlier. The `randomForest` package in R provides the implementation for
    the model, along with a few additional functions to optimize the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a basic random forest model. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the `randomForest` library using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build a random forest model with all of the independent features available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the feature importance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 5.7: Random Forest model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.7: Random Forest model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Q2xKwd.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Build a Random Forest Model with a Greater Number of Trees'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 11*, *Building a Random Forest Model in R*, we created a random
    forest model with just 100 trees; we can build a more robust model with a higher
    number of trees. In this activity, we will create a random forest model with 500
    trees and study the impact of the model having only 100 trees. In general, we
    expect the model's performance to improve (at least marginally with an increased
    number of trees). This comes with higher computational time for the model to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to build a random forest model with 500 trees:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a random forest model with a higher number of trees; say, 500\. Readers
    are encouraged to try higher numbers such as 1,000, 2,000, and so on, and study
    the incremental improvements in each version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leverage the fitted model to predict estimates on the train-and-test data and
    study whether there was any improvement compared to the model with 100 trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can find the solution for this activity on page 457.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**XGBoost** is the most popular boosting technique in recent times. Although
    there have been various new versions that have been developed by large corporations,
    XGBoost still remains the undisputed king. Let''s look at a brief history of boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Boosting Process Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Boosting differs from bagging in its core principles; the learning process is,
    in fact, sequential. Every model built in an ensemble is ideally an improved version
    of the previous model. To understand boosting in simple terms, imagine you are
    playing a game where you must remember all the objects placed on the table that
    you are shown just once for 30 seconds. The moderator of the game arranges around
    50-100 different objects on a table, such as a bat, ball, clock, die, coins, and
    so on, and covers them with a large piece of cloth. When the game begins, he withdraws
    the cloth from the table and gives you exactly 30 seconds to see them and puts
    the curtain back. You now must recollect all the objects you can remember. The
    participant who can recollect the most, aces the game.
  prefs: []
  type: TYPE_NORMAL
- en: In this game, let's add one new dimension. Assume you are a team and the players
    take turns one by one to announce all the objects they can recollect, while the
    others listen to them. Say there are 10 participants; each participant steps forward
    and announces out loud the objects they can recollect from the table. By the time
    the second player steps forward, they have heard all the objects called out by
    the first player. They would have mentioned a few objects that the second player
    might not have recollected. To improve on the first player, the second player
    learns a few new objects from the first player, adds them to his list, and then
    announces them out loud. By the time the last player steps forward, they have
    already learned several objects that other players recollected, which they failed
    to recollect themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Putting those together, that player creates the most exhaustive list and aces
    the competition. The fact that each player announces the list sequentially helps
    the next player learn from their mistakes and improvise on it.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting works in the same way. Each model trained sequentially is imparted
    with additional knowledge, such that the errors of the first model are learned
    better in the second model. Say the first model learns to classify well for most
    cases of a specific independent variable; however, it fails to correctly predict
    for just one specific category. The next model is imparted with a different training
    sample, such that the model learns better for the category where the previous
    model fails. A simple example would be oversampling based on the variable or category
    of interest. Boosting effectively reduces bias and therefore improves the model's
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Some Popular Boosting Techniques?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The boosting techniques introduced earlier were not very popular, because they
    were easily overfit and often required, relatively, a lot of effort in tuning
    to achieve great performance. AdaBoost, BrownBoost, Gradient Boosting, and Stochastic
    Gradient Boosting are all boosting techniques that were popular for a long time.
    However, in 2014, when T Chen and others introduced XGBoost (**Extreme Gradient
    Boosting**), it ushered in a new height in the boosting performance.
  prefs: []
  type: TYPE_NORMAL
- en: How Does XGBoost Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XGBoost natively introduced regularization, which helps models combat overfitting
    and thus delivered high performance. Compared to other available boosting techniques
    at the time, XGBoost reduced the overfitting problem significantly and with the
    least amount of effort. With current implementations of the model in R or any
    other language, XGBoost almost always performs great with the default parameter
    setting. (Though, this is not always true; in many cases, random forest outperforms
    XGBoost). XGBoost has been among the most popular choice of algorithms used in
    data science hackathons and enterprise projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, XGBoost has regularization introduced in the objective function,
    which penalizes the model when it gets more complicated in a training iteration.
    Discussing the depth of mathematical constructs that goes into XGBoosting is beyond
    the scope of this chapter. You can refer to T Chen''s paper here (https://arxiv.org/abs/1603.02754)
    for further notes. Also, this blog will help you to understand the mathematical
    differences between GBM and XGBoost in a simple way: https://towardsdatascience.com/boosting-algorithm-xgboost-4d9ec0207d.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing XGBoost in R
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can leverage the XGBoost package, which provides a neat implementation of
    the algorithm. There are a few differences in the implementation approach that
    we will need to take care of before getting started. Unlike other implementations
    of algorithms in R, XGBoost does not handle categorical data (others take care
    of converting it into numeric data internally). The internal functioning of XGBoost
    in R doesn't handle the automatic conversion of categorical columns into numeric
    columns. Therefore, we manually convert categorical columns into numeric or one-hot
    encoded form.
  prefs: []
  type: TYPE_NORMAL
- en: A one-hot encoded form basically represents a single categorical column as a
    binary encoded form. Say we have a categorical column with values such as **Yes**/**No**/**Maybe**;
    then, we transform this single variable, where we have an individual variable
    for each value of the categorical variable indicating its value as **0** or **1**.
    So, the values for the columns **Yes**, **No**, and **Maybe** will take **0**
    and **1** based on the original value.
  prefs: []
  type: TYPE_NORMAL
- en: 'One-hot encoding is demonstrated in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8: One-hot encoding'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C12624_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5.8: One-hot encoding'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let's transform the data into the required form and build an XGBoost model on
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 74: Building an XGBoost Model in R'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we did in *Exercise 11*, *Building a Random Forest Model in R*, we will
    try to improve the performance of the classification model by building an XGBoost
    model for the same use case and dataset as in *Exercise 11*, *Building a Random
    Forest Model in R*.
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to build an XGBoost model in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create list placeholders for the target, categorical, and numeric variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the categorical factor variables into character. This will be useful
    for converting them into one-hot-encoded forms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the categorical variables into one-hot encoded forms using the `dummyVars`
    function from the `caret` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Combine numeric variables and the one-hot encoded variables from the third
    step into a single DataFrame named `df_final`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the target variable into numeric form, as the XGBoost implementation
    in R doesn''t accept factor or character forms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the `df_final` dataset into train (70%) and test (30%) datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build an XGBoost model using the `xgboost` function. Pass the train data and
    `y_train` target variable and define the `eta = 0.01`, `max_depth = 6`, `nrounds
    = 200`, and `colsample_bytree = 1` hyperparameters, define the evaluation metric
    as `logloss,` and the `objective` function as `binary:logistic`, since we are
    dealing with binary classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make a prediction using the fitted model on the train dataset and create the
    confusion matrix to evaluate the model''s performance on the train data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, as in the previous step, make predictions using the fitted model on the
    test dataset and create the confusion matrix to evaluate the model''s performance
    on the test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If we take a closer look at the results from the model, we can see a slight
    improvement in the performance compared to random forest model results. The `0.54`
    instead of `0.5`, we can increase the precision (to match random forest) while
    still having slightly higher recall than random forest. The increase in recall
    for XGBoost is significantly higher than the decrease in precision. The threshold
    value for the probability cutoff is not a defined, hard cutoff. We can tweak the
    threshold based on our use case. The best number can be studied with empirical
    experiments or by studying the sensitivity, specificity distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30gzSW0.'
  prefs: []
  type: TYPE_NORMAL
- en: The following exercise uses 0.54 instead of 0.5 as the probability cutoff to
    study the improvement in precision at the cost of recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 75: Improving the XGBoost Model''s Performance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can tweak the model performance of binary classification models by adjusting
    the threshold value of the output. By default, we select 0.5 as the default probability
    cutoff. So, all responses above 0.5 are tagged as `Yes`, else `No`. Adjusting
    the threshold can help us achieve more sensitive or more precise models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to improve the XGBoost model''s performance by
    adjusting the threshold for the probability cutoff:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Increase the probability cutoff for the prediction on the train dataset from
    0.5 to 0.53 and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Increase the probability cutoff for the prediction on the test dataset from
    0.5 to 0.53 and print the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We see that, at 44% recall, we have 80% precision on the test dataset, and the
    difference in performance between the train and test datasets is also negligible.
    We can therefore conclude that the model performance of XGBoost is a bit better
    than random forest, though only a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/30c5DQ9.'
  prefs: []
  type: TYPE_NORMAL
- en: Before wrapping up our chapter, let's experiment with the last supervised technique
    for classification, that is, deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last type of technique that we will be discussing before wrapping up our
    chapter is deep neural networks or deep learning. This is a long and complicated
    topic, which by no means will we be able to do justice in a short section of this
    chapter. A complete book may not even suffice to cover the surface of the topic!
    We will explore the topic from 100 feet and quickly study an easy implementation
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks, which are primarily used in the field of computer vision
    and natural language processing, have also found significance in machine learning
    use cases for regression and classification on tabular cross-sectional data. With
    large amounts of data, deep neural networks have been proved to be very effective
    at learning latent patterns and thus training models with better performance.
  prefs: []
  type: TYPE_NORMAL
- en: A Deeper Look into Deep Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep neural networks were inspired by the neural structure of the human brain.
    The field of deep learning became popular for solving computer vision problems,
    that is, the area of problems that were easily solved by humans, but computers
    struggled with for a long time. The motivation for designing deep neural networks
    akin to a miniature and highly simplified human brain was to solve problems that
    were specifically easy for humans. Later, with the success of deep learning in
    the field of computer vision, it was embraced in several other fields, including
    traditional machine learning supervised use cases.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is organized as a hierarchy of neurons, just like the neurons
    in the human brain. Each neuron is connected to other neurons, which enables communication
    between them that traverses as a signal to other neurons and results in a large
    complex network that can learn with a feedback mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure demonstrates a simple neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9: Simple neural network](img/C12624_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Simple neural network'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The input data forms the 0th layer in the network. This layer then connects
    to the neurons within the next layer, which is hidden. It is called **hidden**
    as the network can be perceived as a black box where we provide input to the network
    and directly see the output. The intermediate layers are hidden. In a neural network,
    a layer can have any number of neurons and each network can have any number of
    layers. The larger the number of layers, the 'deeper' the network will be. Hence
    the name deep learning and deep neural networks. Every neuron in each hidden layer
    computes a mathematical function, which is called the activation function in deep
    learning. This function helps in mimicking the signal between two neurons. If
    the function (activation) computes a value greater than a threshold, it sends
    a signal to the immediate connected neuron in the next layer. The connection between
    these two neurons is moderated by a weight. The weight decides how important the
    incoming neuron's signal is for the receiving neuron. The learning method in the
    deep learning model updates the weights between neurons such that the end prediction,
    akin to machine learning models, is the most accurate one.
  prefs: []
  type: TYPE_NORMAL
- en: How Does the Deep Learning Model Work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand how a neural network works and learns to make predictions on data,
    let's consider a simple task that is, relatively, very easy for humans. Consider
    the task of learning to identify different people by their faces. Most of us meet
    a few different people every day; say, at work, school, or on the street. Every
    person we meet is different from each other in some dimension. Though everyone
    would have a ton of similar features, such as two eyes, two ears, lips, two hands,
    and so on, our brain easily distinguishes between two individuals. The second
    time we meet a person, we would most probably recognize them and distinguish them
    as someone we met previously. Given the scale at which this happens and the fact
    that our brain effectively works to solve this mammoth problem with ease, it makes
    us wonder how exactly this happens.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this and appreciate the beauty of our brain, we need to understand
    how the brain fundamentally learns. The brain is a large, complex structure of
    interconnected neurons. Each neuron gets activated when it senses something essential
    and passes a message or signal to other neurons it is connected to. The connection
    between neurons is strengthened by constant learning from the feedback they receive.
    Here, when we see a new face, rather than learning the structure of the face to
    identify people, the brain learns how different the given face is from a generic
    baseline face. This can be further simplified as calculating the difference between
    important facial features, such as eye shape, nose, lips, ears, and lip structure,
    color deviations of the skin and hair, and other attributes. These differences,
    which are quantified by different neurons, are then orchestrated in a systematic
    fashion for the brain to distinguish one face from another and recall a face from
    memory. This entire computation happens subconsciously, and we barely realize
    this as the results are instant for us to notice anything specific.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network essentially tries to mimic the learning functionality of the
    brain in an extremely simplified form. Neurons are connected to each other in
    a layer-wise fashion and initialized with random weights. A mathematical calculation
    across the network combines the inputs from all neurons layer-wise and finally
    reaches the end outcome. The deviation of the end outcome (the predicted value)
    is then quantified as an error and is given as feedback to the network. Based
    on the error, the network tries updating the weights of the connections and tries
    to reduce the error in the prediction iteratively. With several iterations, the
    network updates its weights in an ordered fashion and thus learns to recognize
    patterns to make a correct prediction.
  prefs: []
  type: TYPE_NORMAL
- en: What Framework Do We Use for Deep Learning Models?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For now, we will experiment with deep neural networks for our classification
    use case, using Keras for R. For deep learning model development, we would need
    to write a ton of code, which would render the building blocks for the network.
    To speed up our process, we can leverage Keras, a deep learning framework that
    provides neat abstraction for deep learning components. Keras has an R interface
    and works on top of a low-level deep learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning frameworks available in today's AI community are either low-level
    or high-level. Frameworks such as TensorFlow, Theano, PyTorch, PaddlePaddle, and
    mxnet are low-level frameworks that provide the basic building blocks for deep
    learning models. Using low-level frameworks offers a ton of flexibility and customization
    to the end network design. However, we would still need to write quite a lot of
    code to get a relatively large network working. To simplify this further, there
    are a few high-level frameworks available that work on top of the low-level frameworks
    and provide a second layer of abstraction in the process of building deep learning
    models. Keras, Gluon, and Lasagne are a few frameworks that leverage the aforementioned
    low-level framework as a backend and provide a new API that makes the overall
    development process far easier. This reduces the flexibility when compared to
    directly using a low-level framework such as TensorFlow, and offers a robust solution
    for most networks. For our use case, we can directly leverage Keras with the R
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `install.packages('keras')` command would install the R interface
    to Keras and would also automatically install TensorFlow as the low-level backend
    for Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deep Neural Network in Keras
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To leverage Keras in R, we would need additional data augmentations to our existing
    training dataset. In most machine learning functions available under R, we can
    pass the categorical column directly coded as a factor. However, we saw that XGBoost
    had a mandate that the data needs to be rendered into one-hot encoded form, as
    it does not internally transform the data into the required format. We therefore
    used the `dummyVars` function in R to transform the training and test dataset
    into a one-hot encoded version, such that we have only numerical data in the dataset.
    In Keras, we would need to feed a matrix instead of a DataFrame as the training
    dataset. Therefore, in addition to transforming the data into a one-hot encoded
    form, we would also need to convert the dataset into a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it is also recommended that we standardize, normalize, or scale all
    our input dimensions. The process of normalization rescales data values into the
    range 0 to 1\. Similarly, standardization rescales data to have a mean (*μ*) of
    0 and standard deviation (*σ*) of 1 (unit variance). This transformation is a
    good feature to have in machine learning, as some algorithms tend to benefit and
    learn better. However, in deep learning, this transformation becomes crucial,
    as the model learning process suffers if we provide an input training dataset
    such that all dimensions are in a different range or scale. The reason behind
    this issue is the type of activation function used in neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet implements a basic neural network in Keras. Here,
    we use an architecture that has three layers with 250 neurons each. Finding the
    right architecture is an empirical process and does not have a definitive guide.
    The deeper network is designed, the more computation it will need to fit the data.
    The dataset used in the following snippet is the same as was used for XGBoost
    and already has the one-hot encoded forms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 76: Build a Deep Neural Network in R using R Keras'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will leverage deep neural networks to build a classification
    model for the same use case as *Exercise 13*, *Improving XGBoost Model Performance*,
    and try to improve the performance. Deep neural networks will not always perform
    better than ensemble models. They are usually a preferred choice when we have
    a very high number of training samples, say 10 million. However, we will experiment
    and check whether we can achieve any better performance than the models we built
    in exercises 10-13.
  prefs: []
  type: TYPE_NORMAL
- en: Perform the following steps to build a deep neural network in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale the input dataset in the range 0 to 1\. We would first need to initiate
    a `preProcess` object on the training data. This will be later used to scale the
    train as well as the test data. Neural networks perform better with scaled data.
    The train data alone is used for creating the object to scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `standardizer` object created in the previous step to scale the train
    and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the number of predictor variables in a variable called **predictors**.
    We will use this information to construct the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the structure for a deep neural network. We will use the `keras_model_sequential`
    method. We will create a network with three hidden layers, having 250 neurons
    each and `relu` as the activation function. The output layer will have one neuron
    with the `sigmoid` activation function (since we are developing a binary classification
    mode):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the model optimizer as `adam`, loss function, and the metrics to capture
    for the model''s training iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model structure we created in steps 4-5 with the training and test
    data from steps 1-2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the responses using the fitted model on the train dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the responses using the fitted model on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can find the complete code on GitHub: http://bit.ly/2Vz8Omb.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preprocessor function helps to transform the data into the required scale
    or range. Here, we scale the data to a 0 to 1 scale. We should only consider using
    the train data as the input to the function generator and use the fitted method
    to scale the test data. This is essential, as we won't have access to the test
    data in a real-time scenario. Once the `preProcess` method is fit, we use it to
    transform the train and test data. We then define the architecture for the deep
    neural network model. R provides the easy to extend pipe operator with `%>%`,
    which enables the easy concatenation of the operators. We design a network with
    three layers and 250 neurons each. The input data will form the 0th layer and
    the last layer will be the predicted outcome. The activation function used in
    the network for the hidden layers is `relu`, the most recommended activation function
    for any deep learning use case. The final layer has the `sigmoid` activation function,
    as we have a binary classification use case. There are a ton of activation functions
    to choose from in Keras, such as `prelu`, `tanh`, `swish`, and so on. Once, the
    model architecture is defined, we define the loss function, `binary_crossentropy`,
    which is analogous to binary `logloss` (akin to XGBoost), the optimizer, that
    is, technique, used by the model to learn and backpropagate. The errors in the
    prediction are backpropagated to the network so that it can adjust the weights
    in the right direction and iteratively reduce the error.
  prefs: []
  type: TYPE_NORMAL
- en: The mathematical intuitiveness of this functionality can take various approaches.
    Adam optimization, which is based on adaptive estimates of lower-order moments,
    is the most popular choice, which we can almost blindly experiment with for most
    use cases in deep learning. Some of the other options are `rmsprop`, stochastic
    gradient descent, and `Adagrad`. We also define the metrics to calculate on the
    validation dataset after each epoch, that is, one complete presentation of training
    samples to the network. The `summary` function displays the resultant architecture
    we defined in the preceding section using the Keras constructs. The `summary`
    function gives us a brief idea of the number of parameters in each layer and additionally
    represents the network in a hierarchical structure to help us visualize the model
    architecture. Lastly, we use the `fit` function which trains or 'fits' the data
    to the network. We also define the number of epochs the model should iterate;
    the higher the number of epochs, the longer the training process will take to
    compute.
  prefs: []
  type: TYPE_NORMAL
- en: The batch size indicates the number of training samples the network consumes
    in one single pass before updating the weights of the network; a lower number
    for the batch indicates more frequent weight updates and helps the RAM memory
    to be effectively utilized. The validation split defines the percentage of training
    samples to be used for validation at the end of each epoch. Finally, we validate
    the model's performance on the train and test data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This explanation in the code snippet will by no means be a justification for
    the topic. A deep neural network is an extremely vast and complex topic that might
    need a complete book for a basic introduction. We have wrapped the context into
    a short paragraph for you to understand the constructs used in the model development
    process. Exploring the depth of the topic would be beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the results, we can see similar results as for the previous models.
    The results are almost comparable with the XGBoost model we developed previously.
    We have around 48% recall and 75% precision on the test dataset. The results can
    be further tweaked to reduce recall and enhance precision (if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can therefore conclude that we got fairly good results from our simple logistic
    regression model, XGBoost, and the deep neural network model. The differences
    between all three models were relatively slight. This might bring important questions
    into your mind: Is it worth iterating for various models on the same use case?
    Which model will ideally give the best results? Though there are no straightforward
    answers to these questions, we can say that, overall, simple models always do
    great; ensemble models perform better with lots of data; and deep learning models
    perform better with a ton of data. In the use case that we experimented with in
    this chapter, we will get improved results from all the models with hyperparameter
    tuning and; most importantly; feature engineering. We will explore hyperparameter
    tuning in *Chapter 7*, *Model Improvements*, and feature engineering on a light
    node in *Chapter 6*, *Feature Selection and Dimensionality Reduction*. The process
    of feature engineering is very domain-specific and can only be generalized to
    a certain extent. We will have a look at this in more detail in the next chapter.
    The primary agenda for this chapter was to introduce the range of modeling techniques
    that cover a substantial area in the field and can help you build the foundations
    for any machine learning technique to be developed for a classification use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Model for Your Use Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have explored a set of white-box models and a couple of black-box
    machine learning models for the same classification use case. We also extended
    the same use case with a deep neural network in Keras and studied its performance.
    With the results from several models and various iterations, we need to decide
    which model would be the best for a classification use case. There isn't a simple
    and straightforward answer to this. In a more general sense, we can say that the
    best model would be a Random Forest or XGBoost for most use cases. However, this
    is not true for all types of data. There will be numerous scenarios where ensemble
    modeling may not be the right fit and a linear model would outperform it and vice
    versa. In most experiments conducted by data scientists for classification use
    cases, the approach would be an exploratory and iterative one. There is no one-size-fits-all
    model in machine learning. The process of designing and training a machine learning
    model is arduous and extremely iterative and will always depend on the type of
    data used to train it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best approach to proceed, given the task of building a supervised machine
    learning model, would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 0**: **EDA, Data Treatment and Feature Engineering**: Study the data
    extensively using a combination of visualization techniques and then treat the
    data for missing values, remove outliers, engineer new features, and build the
    train and test datasets. (If necessary, create a validation dataset too.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 1**: **Start with a simple white-box model such as logistic regression**:
    The best starting point in the modeling iterations is a simple white-box model
    that helps us study the impact of each predictor on the dependent variable in
    an easy-to-quantify way. A couple of model iterations will help with feature selection
    and getting a clear understanding of the best predictors and a model benchmark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 2**: **Repeat the modeling experiments with a decision tree model**:
    Leveraging decision tree models will always help us get a new perspective on the
    model and feature patterns. It might give us simple rules and thereby new ideas
    to engineer features for an improved model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step 3**: If there is enough data, experiment with ensemble modeling; otherwise,
    try alternative approaches, such as support vector machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensemble modeling with Random Forest and XGBoost is almost always a safe option
    to experiment with. But in cases where there is a scarcity of data to train, ensemble
    modeling might not be an effective approach to proceed. In such cases, a black
    box kernel-based model would be more effective at learning data patterns and,
    thus, would improve model performance. We have not covered **Support Vector Machines**
    (**SVM**) in this chapter, given the scope. However, with the wide range of topics
    covered in the chapter, getting started with SVMs would be a straightforward task
    for you. This blog provides a simple and easy to understand guide to SVMs: https://eight2late.wordpress.com/2017/02/07/a-gentle-introduction-to-support-vector-machines-using-r/.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, to understand whether the number of training samples is less or
    more, you can use a simple rule of thumb. If there are at least 100 rows of training
    samples for every feature in the dataset, then there is enough data for ensemble
    models; if the number of samples is lower than that, then ensemble models might
    not always be effective. It is still worth a try, though. For example, if there
    are 15 features (independent variables) and 1 dependent variable, and then if
    we have *15 x 100 = 1500* training samples, ensemble models might have better
    performance on a white-box model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 4**: If there is more than enough data, try deep neural networks. If
    there are at least 10,000 samples for every feature in the dataset, experimenting
    with deep neural networks might be a good idea. The problem with neural networks
    is mainly the huge training data and large number of iterations required to get
    good performance. In most generic cases for classification using tabular cross-sectional
    data (the type of use case we solved in this book), deep neural networks are just
    as effective as ensemble models but require significantly more effort in training
    and tuning to achieve the same results. They do outperform ensemble models when
    there is a significantly large number of samples to train. Investing the effort
    in deep neural networks only returns favorable results when there is a significantly
    higher number of training samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we explored different types of classification algorithms for
    supervised machine learning. We leveraged the Australian weather data, designed
    a business problem around it, and explored various machine learning techniques
    on the same use case. We studied how to develop these models in R and studied
    the functioning of these algorithms in depth with mathematical abstractions. We
    summarized the results from each technique and studied a generalized approach
    to tackle common classification use cases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will study feature selection, dimensionality reduction,
    and feature engineering for machine learning models.
  prefs: []
  type: TYPE_NORMAL
