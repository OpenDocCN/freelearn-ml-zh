["```py\nimport numpy as np\n\nfrom keras.datasets import fashion_mnist\n\nnb_samples = 5000\n\n(X_train, _), (_, _) = fashion_mnist.load_data()\nX_train = X_train.astype(np.float32)[0:nb_samples] / 255.0\nX_train = (2.0 * X_train) - 1.0\n\nwidth = X_train.shape[1]\nheight = X_train.shape[2] \n```", "```py\nimport tensorflow as tf\n\ndef generator(z, is_training=True):\n    with tf.variable_scope('generator'):\n        conv_0 = tf.layers.conv2d_transpose(inputs=z,\n                                            filters=1024,\n                                            kernel_size=(4, 4),\n                                            padding='valid')\n\n        b_conv_0 = tf.layers.batch_normalization(inputs=conv_0, training=is_training)\n\n        conv_1 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_0),\n                                            filters=512,\n                                            kernel_size=(4, 4),\n                                            strides=(2, 2),\n                                            padding='same')\n\n        b_conv_1 = tf.layers.batch_normalization(inputs=conv_1, training=is_training)\n\n        conv_2 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_1),\n                                            filters=256,\n                                            kernel_size=(4, 4),\n                                            strides=(2, 2),\n                                            padding='same')\n\n        b_conv_2 = tf.layers.batch_normalization(inputs=conv_2, training=is_training)\n\n        conv_3 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_2),\n                                            filters=128,\n                                            kernel_size=(4, 4),\n                                            strides=(2, 2),\n                                            padding='same')\n\n        b_conv_3 = tf.layers.batch_normalization(inputs=conv_3, training=is_training)\n\n        conv_4 = tf.layers.conv2d_transpose(inputs=tf.nn.leaky_relu(b_conv_3),\n                                            filters=1,\n                                            kernel_size=(4, 4),\n                                            strides=(2, 2),\n                                            padding='same')\n\n        return tf.nn.tanh(conv_4)\n```", "```py\nimport tensorflow as tf\n\ndef discriminator(x, is_training=True, reuse_variables=True):\n    with tf.variable_scope('discriminator', reuse=reuse_variables):\n        conv_0 = tf.layers.conv2d(inputs=x,\n                                  filters=128,\n                                  kernel_size=(4, 4),\n                                  strides=(2, 2),\n                                  padding='same')\n\n        conv_1 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(conv_0),\n                                  filters=256,\n                                  kernel_size=(4, 4),\n                                  strides=(2, 2),\n                                  padding='same')\n\n        b_conv_1 = tf.layers.batch_normalization(inputs=conv_1, training=is_training)\n\n        conv_2 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_1),\n                                  filters=512,\n                                  kernel_size=(4, 4),\n                                  strides=(2, 2),\n                                  padding='same')\n\n        b_conv_2 = tf.layers.batch_normalization(inputs=conv_2, training=is_training)\n\n        conv_3 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_2),\n                                  filters=1024,\n                                  kernel_size=(4, 4),\n                                  strides=(2, 2),\n                                  padding='same')\n\n        b_conv_3 = tf.layers.batch_normalization(inputs=conv_3, training=is_training)\n\n        conv_4 = tf.layers.conv2d(inputs=tf.nn.leaky_relu(b_conv_3),\n                                  filters=1,\n                                  kernel_size=(4, 4),\n                                  padding='valid')\n\n        return conv_4\n```", "```py\nimport tensorflow as tf\n\ncode_length = 100\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    input_x = tf.placeholder(tf.float32, shape=(None, width, height, 1))\n    input_z = tf.placeholder(tf.float32, shape=(None, code_length))\n    is_training = tf.placeholder(tf.bool)\n\n    gen = generator(z=tf.reshape(input_z, (-1, 1, 1, code_length)), is_training=is_training)\n\n    r_input_x = tf.image.resize_images(images=input_x, size=(64, 64))\n\n    discr_1_l = discriminator(x=r_input_x, is_training=is_training, reuse_variables=False)\n    discr_2_l = discriminator(x=gen, is_training=is_training, reuse_variables=True)\n\n    loss_d_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(discr_1_l), logits=discr_1_l))\n    loss_d_2 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discr_2_l), logits=discr_2_l))\n    loss_d = loss_d_1 + loss_d_2\n\n    loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(discr_2_l), logits=discr_2_l))\n\n    variables_g = [variable for variable in tf.trainable_variables() if variable.name.startswith('generator')]\n    variables_d = [variable for variable in tf.trainable_variables() if variable.name.startswith('discriminator')]\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        training_step_d = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(loss=loss_d, var_list=variables_d)\n        training_step_g = tf.train.AdamOptimizer(0.0002, beta1=0.5).minimize(loss=loss_g, var_list=variables_g) \n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nnb_epochs = 200\nbatch_size = 128\nnb_iterations = int(nb_samples / batch_size)\n\nsession = tf.InteractiveSession(graph=graph)\ntf.global_variables_initializer().run()\n\nsamples_range = np.arange(nb_samples)\n\nfor e in range(nb_epochs * 5):\n    d_losses = []\n    g_losses = []\n\n    for i in range(nb_iterations):\n        Xi = np.random.choice(samples_range, size=batch_size)\n        X = np.expand_dims(X_train[Xi], axis=3)\n        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)\n\n        _, d_loss = session.run([training_step_d, loss_d], \n                                feed_dict={\n                                    input_x: X,\n                                    input_z: Z,\n                                    is_training: True\n                                })\n        d_losses.append(d_loss)\n\n        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)\n\n        _, g_loss = session.run([training_step_g, loss_g], \n                                feed_dict={\n                                    input_x: X,\n                                    input_z: Z,\n                                    is_training: True\n                                })\n\n        g_losses.append(g_loss)\n\n    print('Epoch {}) Avg. discriminator loss: {} - Avg. generator loss: {}'.format(e + 1, np.mean(d_losses), np.mean(g_losses)))\n```", "```py\nZ = np.random.uniform(-1.0, 1.0, size=(50, code_length)).astype(np.float32)\n\nYs = session.run([gen], \n                 feed_dict={\n                     input_z: Z,\n                     is_training: False\n                })\n\nYs = np.squeeze((Ys[0] + 1.0) * 0.5 * 255.0).astype(np.uint8)\n```", "```py\nimport tensorflow as tf\n\ndef critic(x, is_training=True, reuse_variables=True):\n    with tf.variable_scope('critic', reuse=reuse_variables):\n...\n```", "```py\nimport tensorflow as tf\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    input_x = tf.placeholder(tf.float32, shape=(None, width, height, 1))\n    input_z = tf.placeholder(tf.float32, shape=(None, code_length))\n    is_training = tf.placeholder(tf.bool)\n\n    gen = generator(z=tf.reshape(input_z, (-1, 1, 1, code_length)), is_training=is_training)\n\n    r_input_x = tf.image.resize_images(images=input_x, size=(64, 64))\n\n    crit_1_l = critic(x=r_input_x, is_training=is_training, reuse_variables=False)\n    crit_2_l = critic(x=gen, is_training=is_training, reuse_variables=True)\n\n    loss_c = tf.reduce_mean(crit_2_l - crit_1_l)\n    loss_g = tf.reduce_mean(-crit_2_l)\n\n    variables_g = [variable for variable in tf.trainable_variables() if variable.name.startswith('generator')]\n    variables_c = [variable for variable in tf.trainable_variables() if variable.name.startswith('critic')]\n\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        optimizer_c = tf.train.AdamOptimizer(0.00005, beta1=0.5, beta2=0.9).minimize(loss=loss_c, var_list=variables_c)\n\n        with tf.control_dependencies([optimizer_c]):\n            training_step_c = tf.tuple(tensors=[tf.assign(variable, tf.clip_by_value(variable, -0.01, 0.01)) \n                                                for variable in variables_c])\n\n        training_step_g = tf.train.AdamOptimizer(0.00005, beta1=0.5, beta2=0.9).minimize(loss=loss_g, var_list=variables_g)\n```", "```py\nimport numpy as np\nimport tensorflow as tf\n\nnb_epochs = 200\nnb_critic = 5\nbatch_size = 64\nnb_iterations = int(nb_samples / batch_size)\n\nsession = tf.InteractiveSession(graph=graph)\ntf.global_variables_initializer().run()\n\nsamples_range = np.arange(nb_samples)\n\nfor e in range(nb_epochs):\n    c_losses = []\n    g_losses = []\n\n    for i in range(nb_iterations):\n        for j in range(nb_critic):\n            Xi = np.random.choice(samples_range, size=batch_size)\n            X = np.expand_dims(X_train[Xi], axis=3)\n            Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)\n\n            _, c_loss = session.run([training_step_c, loss_c], \n                                    feed_dict={\n                                        input_x: X,\n                                        input_z: Z,\n                                        is_training: True\n                                    })\n            c_losses.append(c_loss)\n\n        Z = np.random.uniform(-1.0, 1.0, size=(batch_size, code_length)).astype(np.float32)\n\n        _, g_loss = session.run([training_step_g, loss_g], \n                                feed_dict={\n                                    input_x: np.zeros(shape=(batch_size, width, height, 1)),\n                                    input_z: Z,\n                                    is_training: True\n                                })\n\n        g_losses.append(g_loss)\n\n    print('Epoch {}) Avg. critic loss: {} - Avg. generator loss: {}'.format(e + 1, np.mean(c_losses), np.mean(g_losses)))\n```"]