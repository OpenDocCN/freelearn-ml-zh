<html><head></head><body>
		<div id="_idContainer077">
			<h1 id="_idParaDest-92"><a id="_idTextAnchor117"/>Chapter 6: Training and Tuning at Scale</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) practitioners face multiple challenges when training and tuning models at scale. <strong class="bold">Scale challenges</strong> come in the form of high volumes of training data and increased model size and model architecture complexity. Additional challenges come from having to run a large number of tuning jobs to identify the right set of hyperparameters and keeping track of multiple experiments conducted with varying algorithms for a specific ML objective. Scale challenges lead to long training times, resource constraints, and increased costs. This can reduce the productivity of teams, and potentially create a bottleneck for ML projects.</p>
			<p><strong class="bold">Amazon SageMaker</strong> provides managed distributed training and tuning capabilities to improve training efficiency, and capabilities to organize and track ML experiments at scale. SageMaker enables techniques such as streaming data into algorithms by using pipe mode for training with data at scale and Managed Spot Training for reduced training costs. Pipe mode and managed spot training are discussed in detail in <em class="italic">Learn Amazon SageMaker: A guide to building, training, and deploying machine learning models for developers and data scientists</em>, by Julien Simon. </p>
			<p>In this chapter, we will discuss advanced topics of distributed training, best practices for hyperparameter tuning, and how to organize ML experiments at scale. By the end of this chapter, you will be able to use Amazon SageMaker's managed capabilities to train and tune at scale in a cost-effective manner and keep track of a large number of training experiments.</p>
			<p>In this chapter, we will cover the following main topics:</p>
			<ul>
				<li>ML training at scale with SageMaker distributed libraries</li>
				<li>Automated model tuning with SageMaker hyperparameter tuning </li>
				<li>Organizing and tracking training jobs with SageMaker Experiments</li>
			</ul>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor118"/>Technical requirements</h1>
			<p>You will need an <strong class="bold">AWS</strong> account to run the examples included in this chapter. If you have not set up the data science environment yet, please refer to <a href="B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Data Science Environments</em>, which walks you through the setup process.</p>
			<p>Code examples included in the book are available on GitHub at https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter06. You will need to install a Git client to access them (<a href="https://git-scm.com/">https://git-scm.com/</a>). </p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor119"/>ML training at scale with SageMaker distributed libraries</h1>
			<p>Two <a id="_idIndexMarker210"/>common scale challenges with <a id="_idIndexMarker211"/>ML projects are scaling training data and scaling model size. While increased training data volume, model size, and complexity can potentially result in a more accurate model, there is a limit to the data volume and the model size that you can use with a single compute node, CPU, or GPU. Increased training data volumes and model sizes typically result in more computations, and therefore training jobs take longer to finish, even when using powerful compute instances <a id="_idIndexMarker212"/>such as <strong class="bold">Amazon Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) <strong class="source-inline">p3</strong> and <strong class="source-inline">p4</strong> instances. </p>
			<p><strong class="bold">Distributed training</strong> is a <a id="_idIndexMarker213"/>commonly used technique to speed up training when dealing with scale challenges. Training load can be distributed either across multiple compute instances (nodes), or across multiple CPUs and GPUs (devices) on a single compute instance. There are two strategies for distributed training – <strong class="bold">data parallelism</strong> and <strong class="bold">model parallelism</strong>. Their names are a good indication of what is involved with each <a id="_idIndexMarker214"/>strategy. With data parallelism, the training data is<a id="_idIndexMarker215"/> split up across multiple nodes (or devices). With model parallelism, the model is split up across the nodes (or devices). </p>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="bold">Mixed-precision training</strong> is a <a id="_idIndexMarker216"/>popular technique to handle training at scale and reduce training time. Typically used on compute instances equipped with NVIDIA GPUs, mixed-precision training converts network weights from FP32 representation to FP16, calculates the gradients, converts weights back to FP32, multiplies by the learning rate, and finally updates the optimizer weights.</p>
			<p>In the <a id="_idIndexMarker217"/>data parallelism distribution <a id="_idIndexMarker218"/>strategy, the ML algorithm or the neural network-based model is replicated on all devices, and each device processes a batch of data. Results from all devices are then combined. In the model parallelism distribution strategy, the model (which is the neural network) is split up across the devices. Batches of training data are sent to all devices so that the data can be processed by all parts of the model. The following diagram shows an overview of data and model parallelism:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B17249_06_01.jpg" alt="Figure 6.1 – Distribution strategies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Distribution strategies</p>
			<p>Both data and model parallelism distribution strategies come with their own complexities. With data parallelism, each node (or device) is trained on a subset of data (called a mini-batch), and a mini-gradient is calculated. However, within each node, a mini-gradient average, with gradients coming from other nodes, should be calculated and communicated to all other nodes. This step is called <strong class="bold">all reduce</strong>, which is a communication overhead that grows as the<a id="_idIndexMarker219"/> training cluster is scaled up.</p>
			<p>While model parallelism addresses the requirements of a model not fitting in a single device's memory by splitting it across devices, partitioning the model across multiple GPUs may lead to under-utilization. This is because training on GPUs is sequential in nature, where only one GPU is actively processing data while the other GPUs are<a id="_idIndexMarker220"/> waiting to be activated. To be<a id="_idIndexMarker221"/> effective, model parallelism should be coupled with a pipeline execution schedule to train the model across multiple nodes, and in turn, maximize GPU utilization. Now that you know two different distribution strategies, how do you choose between data and model parallelism?</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor120"/>Choosing between data and model parallelism</h2>
			<p>When choosing a <a id="_idIndexMarker222"/>distributed strategy to implement, keep in mind the following:</p>
			<ul>
				<li>Training on multiple nodes inherently causes inter-node communication overhead.</li>
				<li>Additionally, to meet security and regulatory requirements, you may choose to protect the data transmitted between the nodes by enabling inter-container encryption. </li>
				<li>Enabling inter-container encryption will further increase the training time.</li>
			</ul>
			<p>Due to these reasons, use data parallelism if the trained model can fit in the memory of a single device or node. In situations where the model does not fit in the memory due to its size or complexity, you should experiment further with data parallelism before deciding on model parallelism. </p>
			<p>You can experiment with the following to improve data parallelism<a id="_idIndexMarker223"/> performance: </p>
			<ul>
				<li><strong class="bold">Tuning the model's hyperparameters</strong>: Tuning parameters such as the number of layers of a neural network, or the optimizer to use, affects the model's size considerably. </li>
				<li><strong class="bold">Reducing the batch size</strong>: Experiment by incrementally reducing the batch size until the model fits in the memory. This experiment should balance out the model's memory needs with optimal batch size. Make sure you do not end up with a suboptimal small batch size just because training with a large batch size takes up most of the device memory.</li>
				<li><strong class="bold">Reducing the model input size</strong>: If the model input is tabular, consider embedding vectors of reduced dimensions. Similarly, for <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models, reduce the input NLP sequence length, and if the input is an image, reduce image resolution.</li>
				<li><strong class="bold">Using mixed-point precision</strong>: Experiment <a id="_idIndexMarker224"/>with mixed-precision training, which uses FP16 representation of weights during gradient calculation, to reduce memory consumption.</li>
			</ul>
			<p>The following flowchart shows the sequence of decisions and experiments to follow when choosing a distribution strategy<a id="_idIndexMarker225"/> to implement:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B17249_06_02.jpg" alt="Figure 6.2 – Choose a distribution strategy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Choose a distribution strategy</p>
			<p>While<a id="_idIndexMarker226"/> data parallelism<a id="_idIndexMarker227"/> addresses the challenge of training data scale, model parallelism<a id="_idIndexMarker228"/> addresses the challenge of increased model size and complexity. A hybrid distribution strategy<a id="_idIndexMarker229"/> can also be implemented to include both data and model parallelism. <em class="italic">Figure 6.3</em> walks you through a hybrid distribution strategy with two-way data parallelism and four-way model parallelism:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17249_06_03.jpg" alt="Figure 6.3 – Hybrid distribution strategy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Hybrid distribution strategy</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor121"/>Scaling the compute resources</h2>
			<p>Both the distributed<a id="_idIndexMarker230"/> training strategies depend on a cluster of compute resources to spread the training load. When scaling the distributed cluster to meet the training demands, the recommended best practices are as follows:</p>
			<ul>
				<li>First, scale vertically. That is, scale from a single GPU to multiple GPUs on a single instance. For example, let's say you started with the instance type <strong class="source-inline">p3.2xlarge</strong>, which has a single GPU for training your model, and you find yourself needing a greater number of GPUs to increase the training time. Change the instance type to <strong class="source-inline">p3.16xlarge</strong>, which has eight GPUs. This will result in a nearly eight-times decrease in the training, a near-linear speedup. Keeping the training job on a single scaled-up instance results in better performance than using multiple instances while keeping the cost low. </li>
				<li>Next, scale from a <a id="_idIndexMarker231"/>single instance to multiple instances. When you reach limits of the instance types offered and still need to scale your training even further, then use multiple instances of the same type, that is, scale from a single <strong class="source-inline">p3.16xlarge</strong> to two <strong class="source-inline">p3.16xlarge</strong> instances. This will give you double the compute capacity, going from 8 GPUs on a single instance, to 16 GPUs across two instances. Keep in mind that when you use multiple instances in the training cluster, all instances should be in the same <strong class="bold">Availability Zone</strong>. For example, instances in <strong class="source-inline">us-west-2</strong> must all be in <strong class="source-inline">us-west-2a</strong> or all in <strong class="source-inline">us-west-2b</strong>. Your training data should also be in the same region, <strong class="source-inline">us-west-2</strong>.</li>
			</ul>
			<p>When moving from a single instance to multiple instances, it is recommended that you observe the model convergence and increase the batch size as necessary. Since the batch size you use is split across GPUs, each GPU is processing a lower batch size, which could lead to a high error rate and disrupt the model convergence. </p>
			<p>For example, let's say you start with a single GPU on a <strong class="source-inline">p3.2xlarge</strong> instance using a batch size of 64, then scale up to four <strong class="source-inline">p3dn.24xlarge</strong>, which gives you 32 GPUs. After this move, each GPU only processes a batch size of two, which is very likely to break the model convergence you observed with the original training.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor122"/>SageMaker distributed libraries</h2>
			<p>For easy<a id="_idIndexMarker232"/> implementation of data and model parallelism in your training jobs, SageMaker provides two different distributed training libraries. The libraries address the issues of inter-node and inter-GPU communications overhead using a combination of software and hardware technologies. To implement the distributed libraries and take advantage of data and model parallelism, you will need to make minor code changes to your training scripts.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">At the time of the book publication, the <a id="_idIndexMarker233"/>SageMaker distributed libraries support two <a id="_idIndexMarker234"/>frameworks—<strong class="bold">TensorFlow</strong> and <strong class="bold">PyTorch</strong>.</p>
			<p class="callout">While in this chapter we are focusing on the SageMaker native libraries for distributed training, you can also choose to <a id="_idIndexMarker235"/>use <strong class="bold">Horovod</strong>, the most popular open source distributed training framework, or the native distributed training strategies in frameworks such as TensorFlow and PyTorch. Please see the blog link in the references section for details on using Horovod with TensorFlow on SageMaker.</p>
			<h3>SageMaker distributed data parallel library</h3>
			<p>Let's first dive into the<a id="_idIndexMarker236"/> SageMaker distributed data parallel library.</p>
			<p>The SageMaker distributed data parallel library provides the capabilities to achieve near-linear scaling efficiency and fast training times on deep learning models. The library addresses the challenge of communications overhead in a distributed cluster using two approaches: </p>
			<ul>
				<li>It automatically performs the <strong class="source-inline">AllReduce</strong> operation responsible for the overhead. </li>
				<li>It optimizes node-to-communication by utilizing AWS's network infrastructure and Amazon EC2 instance topology.</li>
			</ul>
			<p>SageMaker data parallelism can be used with both single-node, multi-device setup, and with multi-node setup. However, its value is more apparent in training clusters with two or more nodes. In this multi-node cluster, the <strong class="source-inline">AllReduce</strong> operation implemented as part of the library gives you significant performance improvement.</p>
			<p>To use the distributed libraries with the SageMaker training jobs, first enable the strategy you want when you construct the <strong class="source-inline">estimator</strong> object. The following code bl<a id="_idTextAnchor123"/>ock shows how to create an <strong class="source-inline">estimator</strong> object using a <strong class="source-inline">PyTorch</strong> container with the data parallel strategy enabled:</p>
			<p class="source-code">from sagemaker.pytorch import PyTorch</p>
			<p class="source-code"> </p>
			<p class="source-code">pt_dist_estimator = PyTorch(</p>
			<p class="source-code">                entry_point="train_pytorch_dist.py",</p>
			<p class="source-code">               … </p>
			<p class="source-code">              distribution={</p>
			<p class="source-code">                    "smdistributed": {"dataparallel": {"enabled": True}}</p>
			<p class="source-code">              }</p>
			<p class="source-code">)</p>
			<p>Additionally, there are a few<a id="_idIndexMarker237"/> changes that are needed to the training script, <strong class="source-inline">train_pytorch_dist</strong>, in this example. The next few code blocks show the changes required to the training script:</p>
			<ol>
				<li>First, import and initialize the SageMaker distributed library:<p class="source-code">import smdistributed.dataparallel.torch.distributed as dist</p><p class="source-code">from smdistributed.dataparallel.torch.parallel.distributed import DistributedDataParallel as DDP</p><p class="source-code">dist.init_process_group()</p></li>
				<li>Next, pin each GPU to a single SageMaker data parallel library process with <strong class="source-inline">local_rank</strong>, which is a relative rank of the process within a given node:<p class="source-code">torch.cuda.set_device(dist_get_local_rank())</p></li>
				<li>Next, resize the batch size to be handled by each worker:<p class="source-code">batch_size //= dist.get_world_size()</p><p class="source-code">batch_size = max(batch_size, 1)</p></li>
				<li>Next, wrap the trained model artifact with the <strong class="source-inline">DDP</strong> class from the distributed library:<p class="source-code">model = DDP(model)</p></li>
				<li>Finally, once all of the changes are in place, simply call the <strong class="source-inline">fit()</strong> method on the estimator to kick off training with the training script:<p class="source-code">pt_dist_estimator.fit()</p></li>
			</ol>
			<p>To observe the benefits <a id="_idIndexMarker238"/>of the distributed training, we ran two different training jobs on the same dataset. Both the jobs were run on a single <strong class="source-inline">ml.p3.16xlarge</strong>, the first job without distributed training, and the second job with <strong class="source-inline">smdistributed dataparallel</strong> enabled. In this experiment, the first job was completed in 12041 seconds, and the second job was completed in 4179 seconds, resulting in a 65.29% improvement in the training time. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Comparison of the two training jobs with and without <strong class="source-inline">smdistributed dataparallel</strong> enabled is captured in the notebook in the GitHub repo: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/train-distributed.ipynb</a>. </p>
			<h3>SageMaker distributed model parallel library</h3>
			<p>Next, let's look into the<a id="_idIndexMarker239"/> SageMaker distributed model parallel library. This provides the capability to train large, complex deep learning models that can potentially increase prediction accuracy. The library automatically and efficiently splits a model across multiple GPUs, providing an option for both manual and automatic partitioning. It further coordinates training through a pipelined execution by building an efficient computation schedule where different nodes can simultaneously work on forward and backward passes for different data samples.</p>
			<p>The following code block shows creating an <strong class="source-inline">estimator</strong> object using a <strong class="source-inline">PyTorch</strong> container with the model parallel strategy enabled:</p>
			<p class="source-code">mpi_options = {</p>
			<p class="source-code">    "enabled": True,</p>
			<p class="source-code">   "processes_per_host": 4</p>
			<p class="source-code">  }</p>
			<p class="source-code">  </p>
			<p class="source-code">dist_options = {</p>
			<p class="source-code">    "modelparallel":{</p>
			<p class="source-code">       "enabled": True,</p>
			<p class="source-code">       "parameters": {</p>
			<p class="source-code">           "partitions": 4,  # we'll partition the model among the 4 GPUs </p>
			<p class="source-code">           "microbatches": 8,  # Mini-batchs are split in micro-batch to increase parallelism</p>
			<p class="source-code">           "optimize": "memory" # The automatic model partitioning can optimize speed or memory</p>
			<p class="source-code">           }</p>
			<p class="source-code">       }</p>
			<p class="source-code">}</p>
			<p class="source-code">pt_model_dist_estimator = PyTorch(</p>
			<p class="source-code">    entry_point="train_pytorch_model_dist.py",</p>
			<p class="source-code">    ...</p>
			<p class="source-code">    distribution={"mpi": mpi_options, "smdistributed": dist_options}</p>
			<p class="source-code">)</p>
			<p>As with the data<a id="_idIndexMarker240"/> parallel strategy, there are a few code changes necessary to the training script. Important changes are discussed in the next few code blocks:</p>
			<ol>
				<li value="1">First, import and initialize the SageMaker distributed library:<p class="source-code">import smdistributed.modelparallel.torch as smp</p><p class="source-code">smp.init()</p></li>
				<li>Next, wrap the model artifact in the <strong class="source-inline">DistributedModel</strong> class from the distributed library, and wrap the optimizer in the <strong class="source-inline">DistributedOptimizer</strong> class:<p class="source-code">model = smp.DistributedModel(model)</p><p class="source-code">optimizer = smp.DistributedOptimizer(optimizer)</p></li>
				<li>Next, add the <a id="_idIndexMarker241"/>forward and backward logic to a function and decorate it with <strong class="source-inline">smp.step</strong>:<p class="source-code">@smp.step</p><p class="source-code">def train_step(model, data, target):</p><p class="source-code">    output = model(data)</p><p class="source-code">    long_target = target.long()</p><p class="source-code">    loss = F.nll_loss(output, long_target, reduction="mean")</p><p class="source-code">    model.backward(loss)</p><p class="source-code">    return output, loss</p></li>
				<li>Finally, call the <strong class="source-inline">fit()</strong> method on the <strong class="source-inline">estimator</strong> object to kick off training:<p class="source-code">pt_dist_estimator.fit()</p><p class="callout-heading">Important Note</p><p class="callout">An example notebook that provides a complete walk-through of using the <strong class="source-inline">ModelParallel</strong> distribution strategy with a PyTorch container is provided in the GitHub repository: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH06/train.ipynb</a>.</p></li>
			</ol>
			<p>While the <a id="_idIndexMarker242"/>SageMaker distributed model parallel library makes it easy to implement model parallel distributed training, for optimal training results consider the following best practices:</p>
			<ul>
				<li><strong class="bold">Using manual versus auto-partitioning</strong>: You can partition the model onto multiple nodes (or devices) using either manual or auto-partitioning. While both of the options are supported, you should choose auto-partitioning over the manual approach. With auto-partitioning, training operations and modules that share the same parameters will automatically be placed on the same device for correctness. With a manual approach, you will have to take care of the details on how to split up the model parts, and which part should be placed on which device. This is a time-consuming and error-prone process.</li>
				<li><strong class="bold">Choosing the batch size</strong>: The model parallel library is most efficient with large batch sizes. In case you start with a smaller batch size to fit the model into a single node, then decide to implement model parallelism across multiple nodes, you should increase the batch size accordingly. Model parallelism saves memory for large models, allowing training with large batch sizes.</li>
				<li><strong class="bold">Choosing the number and size of micro-batches</strong>: The model parallel library executes each micro-batch sequentially in each node or device. So, the micro-batch size should be large enough to fully utilize each GPU. At the same time, pipeline efficiency increases with the number of micro-batches, so balancing the two is important. </li>
			</ul>
			<p>It is best practice to start with two or four micro-batches and increase the batch size according to the available memory of the node/device. Then experiment with larger batch sizes and increase the number of micro-batches. As the number of micro-batches is increased, larger batch sizes might become feasible if an interleaved pipeline is used.</p>
			<h3>Incremental training</h3>
			<p>When huge volumes of data are available upfront before training your model, distributed training strategies should be used. But what happens when a trained model is deployed and then you collect new data that might improve the model predictions? In this situation, you can incrementally train a new model starting with artifacts from an existing model and using an expanded dataset.</p>
			<p>Incremental training <a id="_idIndexMarker243"/>can save training time, resources, and costs in the following situations:</p>
			<ul>
				<li>An existing model is under-performing and new data becomes available that can potentially improve model performance.</li>
				<li>You want to use publicly available models as a starting point for your model without having to train from scratch.</li>
				<li>You want to train multiple versions of a model, with either different hyperparameters or using different datasets.</li>
				<li>You want to restart a previously stopped training job, without having to start from scratch again.</li>
			</ul>
			<p>Additionally, to complement or substitute for loading existing model weights and incrementally training, you can retrain on a sliding window on the most recent data.</p>
			<p>In this section, you learned how to use SageMaker capabilities to train with large volumes of data and complex model architectures. Besides the training data and model architecture, a critical part of ML training is tuning hyperparameters of the ML algorithm. In the next section, you will learn the best practices for using SageMaker to handle model tuning at scale.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor124"/>Automated model tuning with SageMaker hyperparameter tuning</h1>
			<p><strong class="bold">Hyperparameter tuning</strong> (<strong class="bold">HPT</strong>) helps you<a id="_idIndexMarker244"/> find the right parameters to use with your ML algorithm or the neural network to find an optimal version of the model. Amazon SageMaker supports managed hyperparameter tuning, also <a id="_idIndexMarker245"/>called <strong class="bold">automatic model tuning</strong>. In this section, we discuss the best practices to consider while configuring hyperparameter jobs on Amazon SageMaker. </p>
			<p>To execute a SageMaker hyperparameter tuning job, you specify a set of hyperparameters, a range of values to explore for each hyperparameter, and an objective metric to measure the model's performance. Automatic tuning executes multiple training jobs on your training dataset with the ML algorithm and the hyperparameter values to find the best-performing model as measured by the objective metric.</p>
			<p>In the<a id="_idIndexMarker246"/> following<a id="_idIndexMarker247"/> code blocks, we will see how to create an HPT job on SageMaker:</p>
			<ol>
				<li value="1">First, initialize the hyperparameter names and range of values for each hyperparameter you want to explore:<p class="source-code">from sagemaker.tuner import (</p><p class="source-code"> IntegerParameter,</p><p class="source-code"> CategoricalParameter,</p><p class="source-code"> ContinuousParameter,</p><p class="source-code"> HyperparameterTuner, </p><p class="source-code">) </p><p class="source-code">hyperparameter_ranges = { </p><p class="source-code"> "eta": ContinuousParameter(0, 1),</p><p class="source-code"> "min_child_weight": ContinuousParameter(1, 10),</p><p class="source-code"> "alpha": ContinuousParameter(0, 2), </p><p class="source-code"> "max_depth": IntegerParameter(1, 10)</p><p class="source-code">}</p></li>
				<li>Next, configure the SageMaker <strong class="source-inline">estimator</strong> object:<p class="source-code">estimator_hpo = \ sagemaker.estimator.Estimator( </p><p class="source-code">image_uri=xgboost_container, </p><p class="source-code">hyperparameters=hyperparameters, </p><p class="source-code">role=sagemaker.get_execution_role(), </p><p class="source-code">instance_count=1, </p><p class="source-code">instance_type='ml.m5.12xlarge', </p><p class="source-code">volume_size=200, # 5 GB </p><p class="source-code">output_path=output_path </p><p class="source-code">) </p></li>
				<li>Next, configure <a id="_idIndexMarker248"/>the <strong class="source-inline">HyperparameterTuner</strong> object:<p class="source-code">tuner = HyperparameterTuner(</p><p class="source-code">             estimator_hpo, </p><p class="source-code">     objective_metric_name,</p><p class="source-code">     hyperparameter_ranges, </p><p class="source-code">     max_jobs=10,</p><p class="source-code">     max_parallel_jobs=2,</p><p class="source-code">     objective_type = 'Minimize'</p><p class="source-code">)</p></li>
				<li>Finally, call <a id="_idIndexMarker249"/>the <strong class="source-inline">fit()</strong> method on the <strong class="source-inline">tuner</strong> object:<p class="source-code">tuner.fit({'train': train_input, </p><p class="source-code">           'validation': validation_input})</p></li>
			</ol>
			<p>Once the hyperparameter job is completed, you can view the different training jobs executed by SageMaker, along with the objective metric for each job, in <em class="italic">Figure 6.4</em>:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17249_06_04.jpg" alt="Figure 6.4 – SageMaker HPT results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – SageMaker HPT results</p>
			<p>You can dive further<a id="_idIndexMarker250"/> into each of the <a id="_idIndexMarker251"/>training jobs to view the exact values of the hyperparameters used, as shown in <em class="italic">Figure 6.5</em>:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B17249_06_05.jpg" alt="Figure 6.5 – Hyperparameter values for a specific training job &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Hyperparameter values for a specific training job </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">An example notebook that provides a complete walk-through of using SageMaker HPT, along with analysis of results, is provided in the GitHub repository: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/HPO.ipynb</a>. </p>
			<p>Now that you know <a id="_idIndexMarker252"/>the basics, let's discuss some of the best practices to consider while configuring hyperparameter jobs on Amazon SageMaker:</p>
			<ul>
				<li><strong class="bold">Selecting a small number of hyperparameters</strong>: HPT is a computationally intensive task, the computational complexity being proportional to the number of hyperparameters you want to tune. SageMaker allows you to specify up to 20 hyperparameters to optimize for a tuning job but limiting your search to a smaller number is likely to give you better results.</li>
				<li><strong class="bold">Selecting a small range for hyperparameters</strong>: Along the same lines, the range of values for hyperparameters can significantly affect the success of hyperparameter optimization. Intuitively, you may want to specify a very large range to explore all possible values for a hyperparameter, but you will in fact get better results by limiting your search to a small range of values.</li>
				<li><strong class="bold">Specifying hyperparameter type</strong>: For the hyperparameters you want to explore, select the right type from the three types supported—categorical, integer, and continuous. Use the categorical type to test different categorical values for a hyperparameter, such as different optimizers for a neural network. Additionally, you can also use the categorical type when you want to test specific values.<p>For example, for the <strong class="source-inline">train_batch_size</strong> hyperparameter, instead of exploring a range in a linear fashion, you might want only to evaluate the two values–128 and 256. In this case, you treat the parameter as a categorical value. In contrast, if you want to explore the values for the <strong class="source-inline">train_batch_size</strong> hyperparameter in a range from a minimum threshold value of 128 to a maximum threshold value of 256, you will use the <strong class="source-inline">Integer</strong> type. The <strong class="source-inline">Integer</strong> type allows for greater exploration of the range. </p><p>If you search a range that spans several orders of magnitude, you can optimize the search by choosing a logarithmic scale for <strong class="source-inline">Integer</strong> hyperparameters. Finally, choose a continuous parameter if the range of all values to explore, from the lowest to the highest, is relativ<a id="_idTextAnchor125"/>ely small. For example, exploring the <strong class="source-inline">learning_rate</strong> hyperparameter in the range of <strong class="source-inline">0.0001</strong> and <strong class="source-inline">0.0005</strong> at a linear scale. </p></li>
				<li><strong class="bold">Enabling warm start</strong>: SageMaker HPT supports warm start, which reuses results from one or more prior tuning jobs as a starting point. Configure your HPT job to use warm start to limit the combinations of hyperparameters to search over in the new tuning job. This results in a faster tuning job. Warm<a id="_idIndexMarker253"/> start is particularly useful when you want to change the HPT ranges from the previous job or add new hyperparameters.</li>
				<li><strong class="bold">Enabling early stop to save tuning time and costs</strong>: With early stop enabled, the individual training jobs launched by the HPT job will terminate early when the objective metric is not improving significantly. After each epoch of training, a running average of the objective metric for all the previous training jobs up to the same epoch is determined and the median of running averages is calculated. If the value of the objective metric for the current training job is worse than the median value, SageMaker stops the current training job.<p>Stopping jobs early reduces the overall compute time and thereby the cost of the job. An additional benefit is that early stopping helps prevent overfitting.</p></li>
				<li><strong class="bold">Selecting a small number of concurrent training jobs</strong>: SageMaker allows you to execute multiple training jobs concurrently as part of the overall tuning job using the <strong class="source-inline">MaxParallelTrainingJobs</strong> parameter. On one hand, running more HPT jobs concurrently completes the tuning job quickly. On the other, a tuning job can only find better combinations of hyperparameters through successive rounds of experiments. In the long run, executing a single training job at a time gives the best results with minimum computation time.<p>This is the case when the default <strong class="bold">Bayesian</strong> optimization tuning strategy<a id="_idIndexMarker254"/> is used by SageMaker HPO. However, if you have experience with your algorithm and dataset, you can also use the random search strategy natively supported by <a id="_idIndexMarker255"/>SageMaker, since it enables concurrency but doesn't require serial rounds of experiments.</p></li>
			</ul>
			<p>While in this section we focused on a single algorithm for best practice. The <strong class="source-inline">CreateHyperParameterTuningJob</strong> API can also be used to tune multiple algorithms by providing multiple training job definitions pointing to the different algorithms. For a detailed explanation of this API, see the following article: <a href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html">https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateHyperParameterTuningJob.html</a>.</p>
			<p>In the next section, you will learn how to keep track of all your ML experiments related to solving a specific problem.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor126"/>Organizing and tracking training jobs with SageMaker Experiments</h1>
			<p>A key challenge ML practitioners face is keeping track of the myriad ML experiments that need to be executed before a model achieves desired results. For a single ML project, it is not uncommon for data scientists to routinely train several different models looking for improved accuracy. HPT adds more training jobs to these experiments. Typically, there are many details to track for experiments such as hyperparameters, model architectures, training algorithms, custom scripts, metrics, result artifacts, and more.</p>
			<p>In this section, we will discuss <strong class="bold">Amazon SageMaker Experiments</strong>, which allows you to <a id="_idIndexMarker256"/>organize, track, visualize, and <a id="_idIndexMarker257"/>compare ML models across all phases of the ML lifecycle, including feature engineering, model training, model tuning, and model deploying. SageMaker Experiments' capability tracks model lineage, allowing you to troubleshoot production issues and audit your models to meet compliance requirements.</p>
			<p>Basic <a id="_idIndexMarker258"/>components that make up Amazon <a id="_idIndexMarker259"/>SageMaker Experiments include an experiment, a trial, a trial component, and a tracker, as shown in <em class="italic">Figure 6.6</em>:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B17249_06_06.jpg" alt="Figure 6.6 – Amazon SageMaker Experiments overview&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Amazon SageMaker Experiments overview</p>
			<p>Let's look at each component:</p>
			<ul>
				<li><strong class="bold">Experiment</strong>: An experiment<a id="_idIndexMarker260"/> encapsulates all related components that represent the <a id="_idIndexMarker261"/>ML problem you are attempting to solve. Each experiment is a collection of trials, with the goal of determining the trial that produces the best model.</li>
				<li><strong class="bold">Trial</strong>: A trial<a id="_idIndexMarker262"/> represents a single attempt at solving the ML problem that captures<a id="_idIndexMarker263"/> the end-to-end ML process within an experiment. Each trial is a collection consisting of several trial components.</li>
				<li><strong class="bold">Trial Component</strong>: A trial component<a id="_idIndexMarker264"/> represents a specific step within a given <a id="_idIndexMarker265"/>trial. For example, the data preprocessing step could be one trial component, and model training could be another trial component.</li>
				<li><strong class="bold">Tracker</strong>: A tracker<a id="_idIndexMarker266"/> is used to track metadata of individual trial components, including<a id="_idIndexMarker267"/> all parameters, inputs, outputs, artifacts, and metrics. Since this metadata is tracked and persisted, you can link the final model artifact to its origin.</li>
			</ul>
			<p>In the following code <a id="_idIndexMarker268"/>blocks, we will see how to<a id="_idIndexMarker269"/> create a SageMaker experiment:</p>
			<ol>
				<li value="1">First, create an experiment: <p class="source-code">weather_experiment = Experiment.create(</p><p class="source-code">    experiment_name=f"weather-experiment-{int(time.time())}",  </p><p class="source-code">    description="Weather Data Predi<a id="_idTextAnchor127"/>ction", </p><p class="source-code">    sagemaker_boto_client=sm)</p></li>
				<li>Next, create a <strong class="source-inline">Tracker</strong> instance to track the <strong class="source-inline">Training</strong> stage:<p class="source-code">with Tracker.create(display_name="Training", sagemaker_boto_client=sm) as tracker:</p><p class="source-code">    # Log the location of the training dataset</p><p class="source-code">    tracker.log_input(name="weather-training-dataset", </p><p class="source-code">  media_type="s3/uri", </p><p class="source-code"> value="s3://{}/{}/{}/".format(s3_bucket, s3_prefix, 'train')) </p><p>Next, define experiment variables to define what you want to change to see how your objective is affected. In this example, we will experiment with several values for the number of the <strong class="source-inline">max_depth</strong> hyperparameter of <strong class="source-inline">XGBoostmodel</strong>. We will create a trial to track each training job run.</p><p>W<a id="_idTextAnchor128"/>e will also create a <strong class="source-inline">TrialComponent</strong> instance from the <strong class="source-inline">Tracker</strong> instance we created earlier and add this to the <strong class="source-inline">Trial</strong> instance. This will allow you <a id="_idIndexMarker270"/>to capture metrics from the <a id="_idIndexMarker271"/>training step as follows:</p><p class="source-code">for i, max_depth in enumerate([2, 5]):</p><p class="source-code">    # create trial</p><p class="source-code">    trial_name = f"xgboost-training-job-trial-{max_depth}-max-depth-{int(time.time())}"</p><p class="source-code">    xgboost_trial = Trial.create(</p><p class="source-code">        trial_name=trial_name, </p><p class="source-code">        experiment_name=weather_experiment.experiment_name,</p><p class="source-code">        sagemaker_boto_client=sm,</p><p class="source-code">    )</p><p class="source-code">    max_depth_trial_name_map[max_depth] = trial_name</p><p class="source-code"> </p><p class="source-code">    xgboost_training_job_name = "xgboost-training-job-{}".format(int(time.time()))</p><p class="source-code"> </p></li>
				<li>When running the training job with the <strong class="source-inline">fit()</strong> method, associate <strong class="source-inline">estimator</strong> with the experiment and trial:<p class="source-code"># Now associate the estimator with the Experiment and Trial</p><p class="source-code">    estimator.fit(</p><p class="source-code">        inputs={'training': train_input}, </p><p class="source-code">        job_name=xgboost_training_job_name,</p><p class="source-code">        experiment_config={</p><p class="source-code">            "TrialName": xgboost_trial.trial_name,</p><p class="source-code">            "TrialComponentDisplayName": "Training",</p><p class="source-code">        },</p><p class="source-code">        wait=False,</p><p class="source-code">    )</p></li>
				<li>Finally, after<a id="_idIndexMarker272"/> the experiment is<a id="_idIndexMarker273"/> completed, let's analyze the experiment results:<p class="source-code">trial_component_analytics = \ ExperimentAnalytics(sagemaker_session=sagemaker_session, experiment_name=experiment_name ) </p><p class="source-code">trial_component_analytics.dataframe()</p></li>
			</ol>
			<p><em class="italic">Figure 6.7</em> shows a list of all the trial components that were created as part of the experiment:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17249_06_07.jpg" alt="Figure 6.7 – Trial components from the experiment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Trial components from the experiment</p>
			<p>As you can <a id="_idIndexMarker274"/>see from this section, a SageMaker <a id="_idIndexMarker275"/>experiment gives you a way to organize your efforts toward an ML goal and allows visibility into several important aspects of those efforts. A best practice we recommend is that any time you launch a training or tuning job, wrap it in an experiment. This allows you to gain visibility into the training and tuning jobs without any additional cost.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">An example notebook that provides a complete walk-through of using SageMaker Experiments is provided in the GitHub repository: <a href="https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb">https://gitlab.com/randydefauw/packt_book/-/blob/main/CH05/Experiments.ipynb</a>. </p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor129"/>Summary</h1>
			<p>In this chapter, you learned the advanced techniques required to train models at scale using different distribution strategies. You further reviewed best practices for hyperparameter tuning to find the best version of the model to meet your objectives. You learned how to organize and track multiple experiments conducted in a typical ML workflow and create comparison reports. </p>
			<p>Using the SageMaker capabilities and best practices discussed in this chapter, you can tackle ML at scale, allowing your organization to move out of the experimentation phase. You can take advantage of large datasets collected over years, and move toward realizing the full benefits of ML. In the next chapter, you will continue to enhance ML training by profiling training jobs using <strong class="bold">Amazon SageMaker</strong> <strong class="bold">Debugger</strong>.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor130"/>References</h1>
			<p>For ad<a id="_idTextAnchor131"/>ditional reading material, please review these references:</p>
			<ul>
				<li><em class="italic">Learn Amazon SageMaker: A guide to building, training, and deploying ML models for developers and data scientists</em>:<p><strong class="source-inline">https://www.amazon.com/Learn-Amazon-SageMaker-developers-scientists/dp/180020891X/ref=sr_1_1?dchild=1&amp;keywords =Learn+Amazon+SageMaker+%3A+A+guide+to+building%2C+training%2C+and+deploying+machine+learning+model<a id="_idTextAnchor132"/>s+for +developers+and+data+scientists&amp;qid=1624801601&amp;sr=8-1</strong></p></li>
				<li><em class="italic">Multi-GPU and distributed training using Horovod in Amazon SageMaker Pipe mode</em>:<p><a href="https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/">https://aws.amazon.com/blogs/machine-learning/multi-gpu-and-distributed-training-using-horovod-in-amazon-sagemaker-pipe-mode/</a></p></li>
				<li><em class="italic">Streamline modeling with Amazon SageMaker Studio and the Amazon Experiments SDK</em>:<p>https://aws.amazon.com/blogs/machine-learning/streamline-modeling-with-amazon-sagemaker-studio-and-amazon-experiments-sdk</p></li>
			</ul>
		</div>
	</body></html>