<html><head></head><body>
		<div id="_idContainer134">
			<h1 class="chapter-number"><a id="_idTextAnchor179"/>15</h1>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor180"/>Ethics in Machine  Learning Systems</h1>
			<p>Ethics involves data acquisition and management and focuses on collecting data, with a particular focus on protecting individuals and organizations from any harm that could be inflicted upon them. However, data is not the only source of bias in <strong class="bold">machine learning</strong> (<span class="No-Break"><strong class="bold">ML</strong></span><span class="No-Break">) systems.</span></p>
			<p>Algorithms and ways of data processing are also prone to introducing bias to the data. Despite our best efforts, some of the steps in data processing may even emphasize the bias and let it spread beyond algorithms and toward other parts of ML-based systems, such as user interfaces or <span class="No-Break">decision-making components.</span></p>
			<p>Therefore, in this chapter, we’ll focus on the bias in ML systems. We’ll start by exploring sources of bias and briefly discussing these sources. Then, we’ll explore ways to spot biases, how to minimize them, and finally how to communicate potential bias to the users of <span class="No-Break">our system.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Bias and ML – is it possible to have an <span class="No-Break">objective AI?</span></li>
				<li>Measuring and monitoring <span class="No-Break">for bias</span></li>
				<li><span class="No-Break">Reducing bias</span></li>
				<li>Developing mechanisms to prevent ML bias from spreading in the <span class="No-Break">entire system</span></li>
			</ul>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor181"/>Bias and ML – is it possible to have an objective AI?</h1>
			<p>In the intertwined domains of ML and software engineering, the allure of data-driven decision-making and predictive modeling is undeniable. These fields, which once operated largely in silos, now converge in numerous applications, from software development tools to automated testing frameworks. However, as we increasingly rely on data and algorithms, a pressing concern emerges: the issue of bias. Bias, in this context, refers to systematic and unfair discrepancies that can manifest in the decisions and predictions of ML models, often stemming from the very data used in software <span class="No-Break">engineering processes.</span></p>
			<p>The sources of bias in<a id="_idIndexMarker578"/> software engineering data are multifaceted. They can arise from historical project data, user feedback loops, or even the design and objectives of the software itself. For instance, if a software tool is predominantly tested and refined using feedback from a specific demographic, it might inadvertently underperform or misbehave for users outside that group. Similarly, a defect prediction model might be skewed if trained on data from projects that lack diversity in team composition or <span class="No-Break">coding practices.</span></p>
			<p>The implications of such biases extend beyond mere technical inaccuracies. They can lead to software products that alienate or disadvantage certain user groups, perpetuating and amplifying existing societal inequalities. For example, a development environment might offer suggestions that resonate more with one cultural context than another, or a software recommendation system might favor applications from well-known developers, <span class="No-Break">sidelining newcomers.</span></p>
			<p>Generally, bias is defined to be an inclination or prejudice for, or against, one person or group. In ML, bias is when a model systematically produces prejudiced results. There are several types of <a id="_idIndexMarker579"/>bias <span class="No-Break">in ML:</span></p>
			<ul>
				<li><strong class="bold">Prejudicial bias</strong>: This <a id="_idIndexMarker580"/>is a type of bias that is present in the empirical world and made its way into ML models and algorithms – both knowingly and unknowingly. An example is racial bias or <span class="No-Break">gender bias.</span></li>
				<li><strong class="bold">Measurement bias</strong>: This <a id="_idIndexMarker581"/>is a type of bias that is introduced through a systematic error in our measurement instruments. For example, we measure the McCabe complexity of software modules by counting the if/for statements, excluding the <span class="No-Break">while loops.</span></li>
				<li><strong class="bold">Sampling bias</strong>: This is a <a id="_idIndexMarker582"/>type of bias that occurs when our sample does not reflect the real distribution of data. It can be the case that we sample too often or too seldom from a specific class – such a bias in our data will <span class="No-Break">affect inference.</span></li>
				<li><strong class="bold">Algorithm bias</strong>: This is a type<a id="_idIndexMarker583"/> of bias that occurs when we use the wrong algorithm for the task at hand. A wrong algorithm may not generalize well and therefore it may introduce bias into <span class="No-Break">the inference.</span></li>
				<li><strong class="bold">Confirmation bias</strong>: This is a<a id="_idIndexMarker584"/> type of bias that is introduced when we remove/select data points that are aligned with the theoretical notions that we want to capture. By doing this, we introduce the bias that confirms our theory, rather than reflecting the <span class="No-Break">empirical world.</span></li>
			</ul>
			<p>This list is, by no means, exclusive. Bias can be introduced in many ways and through many ways, but it is always our responsibility to identify it, monitor it, and <span class="No-Break">reduce it.</span></p>
			<p>Luckily, there are a few frameworks that can allow us to identify bias – Fair ML, IBM AI Fairness 360, and Microsoft Fairlearn, just to name a few. These frameworks allow us to scrutinize our algorithms and datasets in search of the most <span class="No-Break">common biases.</span></p>
			<p>Donald et al. present a recent overview of methods and tools for reducing bias in software engineering, which includes ML. The important part of that article is that it focuses on use cases, which is important for understanding bias; bias is not something universal but depends on the dataset and the use case of that data. In addition to the sources of bias presented previously, they also recognize that bias is something that can change over time, just as our society changes and just as our data changes. Although Donald et al.’s work is generic, it tends to focus on one of the data types – natural language – and how bias can be present. They provide an overview of tools and techniques that can help identify such phenomena as <span class="No-Break">hateful language.</span></p>
			<p>In this chapter, however, we’ll focus on one of the frameworks that is a bit more generic to illustrate how to work with bias <span class="No-Break">in general.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor182"/>Measuring and monitoring for bias</h1>
			<p>Let’s look at one of these frameworks – IBM AI Fairness 360 (<a href="https://github.com/Trusted-AI/AIF360">https://github.com/Trusted-AI/AIF360</a>). The basis for this framework is the ability to set variables that can be linked to bias and then calculate how different the other variables <a id="_idIndexMarker585"/>are. So, let’s dive into an example of how to calculate bias for a dataset. Since bias is often associated with gender or similar attributes, we need to use a <a id="_idIndexMarker586"/>dataset that contains it. So far in this book, we have not used any dataset that contained this kind of attribute, so we need to find <span class="No-Break">another one.</span></p>
			<p>Let’s take the Titanic survival dataset to check if there was any bias in terms of survivability between male and female passengers. First, we need to install the IBM AI Fairness <span class="No-Break">360 framework:</span></p>
			<pre class="console">
pip install aif360</pre>			<p>Then, we can start creating a program that will check for bias. We need to import the appropriate libraries and create the data. In this example, we’ll create the data of salaries, which is biased <span class="No-Break">toward men:</span></p>
			<pre class="source-code">
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
t i
data = {
    'Age': [25, 45, 35, 50, 23, 30, 40, 28, 38, 48, 27, 37, 47, 26, 36, 46],
    'Income': [50000, 100000, 75000, 120000, 45000, 55000, 95000, 65000, 85000, 110000, 48000, 58000, 98000, 68000, 88000, 105000],
    'Gender': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1],  # 1: Male, 0: Female
    'Hired': [1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]   # 1: Hired, 0: Not Hired
}
df = pd.DataFrame(data)</pre>			<p>This<a id="_idIndexMarker587"/> data <a id="_idIndexMarker588"/>contains four different attributes – age, income, gender, and whether the person is recommended to be hired or not. It is difficult to spot whether there is a bias between the genders, but let’s apply the IBM fairness algorithm to check <span class="No-Break">for that:</span></p>
			<pre class="source-code">
# Split data into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=42)
# Convert dataframes into BinaryLabelDataset format
train_bld = BinaryLabelDataset(df=train, label_names=['Hired'], protected_attribute_names=['Gender'])
test_bld = BinaryLabelDataset(df=test, label_names=['Hired'], protected_attribute_names=['Gender'])
# Compute fairness metric on original training dataset
metric_train_bld = BinaryLabelDatasetMetric(train_bld, unprivileged_groups=[{'Gender': 1}], privileged_groups=[{'Gender': 0}])
print(f'Original training dataset disparity: {metric_train_bld.mean_difference():.2f}')
# Mitigate bias by reweighing the dataset
RW = Reweighing(unprivileged_groups=[{'Gender': 1}], privileged_groups=[{'Gender': 0}])
train_bld_transformed = RW.fit_transform(train_bld)
# Compute fairness metric on transformed training dataset
metric_train_bld_transformed = BinaryLabelDatasetMetric(train_bld_transformed, unprivileged_groups=[{'Gender': 1}], privileged_groups=[{'Gender': 0}])
print(f'Transformed training dataset disparity: {metric_train_bld_transformed.mean_difference():.2f}')</pre>			<p>The <a id="_idIndexMarker589"/>preceding code creates a data split and calculates the fairness metric – dataset disparity. The important part of the algorithm is where we set the protected attribute – gender (<strong class="source-inline">protected_attribute_names=['Gender'])</strong>). We manually set the attribute that we <a id="_idIndexMarker590"/>think could be prone to bias, which is an important observation. The fairness framework does not set any attributes automatically. Then, we set which values of this attribute indicate the privileged and unprivileged groups – <strong class="source-inline">unprivileged_groups=[{'Gender': 1}]</strong>. Once the code executes, we get an understanding of whether there is bias in <span class="No-Break">the dataset:</span></p>
			<pre class="console">
Original training dataset disparity: 0.86
Transformed training dataset disparity: 0.50</pre>			<p>This means that the algorithm could reduce the disparity but did not remove it completely. The disparity value of 0.86 means that there is a bias toward the privileged group (in this case males). The value of 0.5 means that the bias is reduced, but it is still far from 0.0, which would indicate the lack of bias. The fact that the bias was reduced and not removed can indicate that there is just too little data to be able to reduce the <span class="No-Break">bias completely.</span></p>
			<p>Therefore, let’s take a look at the real dataset, which can contain a bias – the Titanic dataset. The dataset contains protected attributes such as gender and it is significantly larger so that we <a id="_idIndexMarker591"/>have a better chance to reduce the bias <span class="No-Break">even more:</span></p>
			<pre class="source-code">
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
# Load Titanic dataset
url = "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"
df = pd.read_csv(url)</pre>			<p>Now that <a id="_idIndexMarker592"/>we have the dataset in place, we can write the script that will calculate the disparity metrics, which quantifies how much difference there is in the data based on the <span class="No-Break">controlled variable:</span></p>
			<pre class="source-code">
# Preprocess the data
df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})  # Convert 'Sex' to binary: 1 for male, 0 for female
df.drop(['Name'], axis=1, inplace=True)  # Drop the 'Name' column
# Split data into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=42)
# Convert dataframes into BinaryLabelDataset format
train_bld = BinaryLabelDataset(df=train, label_names=['Survived'], protected_attribute_names=['Sex'])
test_bld = BinaryLabelDataset(df=test, label_names=['Survived'], protected_attribute_names=['Sex'])
# Compute fairness metric on the original training dataset
metric_train_bld = BinaryLabelDatasetMetric(train_bld, unprivileged_groups=[{'Sex': 0}], privileged_groups=[{'Sex': 1}])
print(f'Original training dataset disparity: {metric_train_bld.mean_difference():.2f}')
# Mitigate bias by reweighing the dataset
RW = Reweighing(unprivileged_groups=[{'Sex': 0}], privileged_groups=[{'Sex': 1}])
train_bld_transformed = RW.fit_transform(train_bld)
# Compute fairness metric on the transformed training dataset
metric_train_bld_transformed = BinaryLabelDatasetMetric(train_bld_transformed, unprivileged_groups=[{'Sex': 0}], privileged_groups=[{'Sex': 1}])
print(f'Transformed training dataset disparity: {metric_train_bld_transformed.mean_difference():.2f}')</pre>			<p>First, we <a id="_idIndexMarker593"/>need <a id="_idIndexMarker594"/>to convert the <strong class="source-inline">'Sex'</strong> column of the DataFrame, <strong class="source-inline">df</strong>, into a binary format: <strong class="source-inline">1</strong> for male and <strong class="source-inline">0</strong> for female. Then, we need to drop the <strong class="source-inline">'Name'</strong> column from the DataFrame as it could be confused with the index. Then, the data is split into training and testing sets using the <strong class="source-inline">train_test_split</strong> function. 20% of the data (<strong class="source-inline">test_size=0.2</strong>) is reserved for testing, and the rest is used for training. <strong class="source-inline">random_state=42</strong> ensures the reproducibility of <span class="No-Break">the split.</span></p>
			<p>Next, we<a id="_idIndexMarker595"/> convert the training and testing DataFrames into a <strong class="source-inline">BinaryLabelDataset</strong> format, which is a specific format used by the fairness framework. The<a id="_idIndexMarker596"/> target variable (or label) is <strong class="source-inline">'Survived'</strong>, and the protected attribute (that is, the attribute we’re concerned about in terms of fairness) is <strong class="source-inline">'Sex'</strong>. The framework considers females (<strong class="source-inline">'Sex': 0</strong>) as the unprivileged group and males (<strong class="source-inline">'Sex': 1</strong>) as the <span class="No-Break">privileged group.</span></p>
			<p>The <strong class="source-inline">mean_difference</strong> method computes the difference in mean outcomes between the privileged and unprivileged groups. A value of 0 indicates perfect fairness, while a non-zero value indicates some disparity. Then, the code uses the <strong class="source-inline">Reweighing</strong> method to mitigate bias in the training dataset. This method assigns weights to the instances in the dataset to ensure fairness. The transformed dataset (<strong class="source-inline">train_bld_transformed</strong>) has these new weights. Then, we calculate the same metric on the transformed dataset. This results in the <span class="No-Break">following output:</span></p>
			<pre class="console">
Original training dataset disparity: 0.57
Transformed training dataset disparity: 0.00</pre>			<p>This means that the algorithm has balanced the datasets so that the survival rate is the same for male and female passengers. We can now use this dataset to train <span class="No-Break">a model:</span></p>
			<pre class="source-code">
# Train a classifier (e.g., logistic regression) on the transformed dataset
scaler = StandardScaler()
X_train = scaler.fit_transform(train_bld_transformed.features)
y_train = train_bld_transformed.labels.ravel()
clf = LogisticRegression().fit(X_train, y_train)
# Test the classifier
X_test = scaler.transform(test_bld.features)
y_test = test_bld.labels.ravel()
y_pred = clf.predict(X_test)
# Evaluate the classifier's performance
from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
report = classification_report(y_test, y_pred, target_names=["Not Survived", "Survived"])
print(report)</pre>			<p>First, we<a id="_idIndexMarker597"/> initialize <strong class="source-inline">StandardScaler</strong>. This scaler standardizes<a id="_idIndexMarker598"/> features by removing the mean and scaling to unit variance. Then, we transform and standardize the features of the training dataset (<strong class="source-inline">train_bld_transformed.features</strong>) using the <strong class="source-inline">fit_transform</strong> method of the scaler. The standardized features are stored in <strong class="source-inline">X_train</strong>. Then, we extract the labels of the transformed training dataset using the <strong class="source-inline">ravel()</strong> method, resulting in <strong class="source-inline">y_train</strong>. After, we train the logistic regression classifier (<strong class="source-inline">clf</strong>) using the standardized features (<strong class="source-inline">X_train</strong>) and <span class="No-Break">labels (</span><span class="No-Break"><strong class="source-inline">y_train</strong></span><span class="No-Break">).</span></p>
			<p>Then, we standardize the features of the test dataset (<strong class="source-inline">test_bld.features</strong>) using the transform method of the scaler to obtain <strong class="source-inline">X_test</strong>. We do the same with the <strong class="source-inline">y_test</strong> data. We use the trained classifier (<strong class="source-inline">clf</strong>) to make predictions on the standardized test features and store them <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">y_pred</strong></span><span class="No-Break">.</span></p>
			<p>Finally, we calculate the evaluation scores for the dataset and print the report with accuracy, precision, <span class="No-Break">and recall.</span></p>
			<p>With that, we’ve come to my best practice related <span class="No-Break">to bias.</span></p>
			<p class="callout-heading">Best practice #73</p>
			<p class="callout">If a dataset contains variables that can be prone to bias, use the disparity metric to get a quick orientation of <span class="No-Break">the data.</span></p>
			<p>It is important to check for bias, although we do not always have access to the variables that we need for its calculations, such as gender or age. If we do not, we should look for attributes that can be correlated with them and check for bias against <span class="No-Break">these attributes.</span></p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor183"/>Other metrics of bias</h2>
			<p>The dataset<a id="_idIndexMarker599"/> disparity metrics that we’ve used so far are only some of the metrics related to bias. Some of the other metrics that are available in the IBM AI Fairness 360 framework are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">True positive rate</strong>: The ratio <a id="_idIndexMarker600"/>of true positives conditioned on the protected attribute. This is usually used <span class="No-Break">for classification.</span></li>
				<li><strong class="bold">False discovery rate</strong>: The <a id="_idIndexMarker601"/>difference between the false discovery ratio between the privileged and unprivileged groups in <span class="No-Break">classification tasks.</span></li>
				<li><strong class="bold">Generalize binary confusion matrix</strong>: The confusion matrix conditioner on the <a id="_idIndexMarker602"/>protected attributes in the <span class="No-Break">classification tasks.</span></li>
				<li>The ratio between privileged and unprivileged instances, which can be used for all kinds <span class="No-Break">of tasks.</span></li>
			</ul>
			<p>There are several metrics in addition to these, but the ones we’ve covered here illustrate the most important point – or two points. First of all, we can see that there needs to be an attribute, called the protected attribute, which can help us understand the bias. Without such an attribute, the framework cannot do any calculations and therefore it cannot provide any useful feedback for the developers. The second point is the fact that the metrics are based on the unbalance between different groups – privileged and unprivileged – which we define ourselves. We cannot use this framework to discover bias that <span class="No-Break">is hidden.</span></p>
			<p>Hidden biases are biases that are not directly represented by attributes. For example, there are differences in the occupations that men and women have, and therefore the occupation can be such an attribute that is correlated to the gender, but not equal to it. This means that we cannot treat this as a protected attribute, but we need to consider it – basically, there are no occupations that are purely male or purely female occupations, but different occupations have different proportions of men <span class="No-Break">and women.</span></p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor184"/>Developing mechanisms to prevent ML bias from spreading throughout the system</h1>
			<p>Unfortunately, it is <a id="_idIndexMarker603"/>generally not possible to completely remove bias from ML as we often do not have access to the attributes needed to reduce the bias. However, we can reduce the bias and reduce the risk that the bias spreads to the <span class="No-Break">entire system.</span></p>
			<p>Awareness and education are some of the most important measures that we can use to manage bias in software systems. We need to understand the potential sources of bias and their implications. We also need to identify biases related to protected attributes (for example, gender) and identify whether other attributes can be correlated with them (for example, occupation and address). Then, we need to educate our team about the ethical implications of <span class="No-Break">biased models.</span></p>
			<p>Then, we need to diversify our data collection. We must ensure that the data we collect is representative of the population we’re to model. To avoid over-representing or under-representing certain groups, we need to ensure that our data collection procedures are scrutinized before they are applied. We also need to monitor for biases in the collected data and reduce them. For example, if we identify a bias in credit scores, we can introduce the data that will prevent this bias from being strengthened by <span class="No-Break">our model.</span></p>
			<p>During data preprocessing, we need to ensure that we handle missing data correctly. Instead of just removing the data points or imputing them with mean values, we should use the right imputation, which takes care of the differences between the privileged and <span class="No-Break">unprivileged groups.</span></p>
			<p>We also need to actively work with the detection of bias. We should use statistical tests to check whether the data distribution is biased toward certain groups, at which point we need to visualize the distributions and identify potential bias. We’ve already discussed visualization techniques; at this point, we can add that we need to use different symbols for privileged and unprivileged groups to visualize two distributions on the same diagram, <span class="No-Break">for example.</span></p>
			<p>In addition to working with the data, we also need to work with algorithmic fairness, which is when we design the models. We need to set fairness constraints and we need to introduce the attributes that can help us to identify privileged and unprivileged groups. For <a id="_idIndexMarker604"/>example, if we know that different occupations have a certain bias toward genders, we need to introduce superficial gender-bias attributes that can help us to create a model that will take that into account and prevent the bias from spreading to other parts of our system. We can also make post-hoc adjustments to the model after training. For example, when predicting a salary, we can adjust that salary based on pre-defined rules after the prediction. That can help to reduce the biases inherent in <span class="No-Break">the model.</span></p>
			<p>We can also use fairness-enhancing interventions, such as the IBM Fairness tools and techniques, which include debiasing, reweighing, and disparate impact removal. This can help us to achieve interpretable models or allow us to use model interpretation tools to understand how decisions are being made. This can help in identifying and <span class="No-Break">rectifying bias.</span></p>
			<p>Finally, we can regularly audit our models for bias and fairness. This includes both automated checks and human reviews. This helps us understand whether there are biases that cannot be captured automatically and that we need to <span class="No-Break">react to.</span></p>
			<p>With that, we have come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #74</p>
			<p class="callout">Complement automated bias management with <span class="No-Break">regular audits.</span></p>
			<p>We need to accept the fact that bias is inherent in data, so we need to act accordingly. Instead of relying on algorithms to detect bias, we need to manually monitor for bias and understand it. Therefore, I recommend making regular checks for bias manually. Make classifications and predictions and check whether they strengthen or reduce bias by comparing them to the expected data <span class="No-Break">without bias.</span></p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor185"/>Summary</h1>
			<p>One of our responsibilities as software engineers is to ensure that we develop software systems that contribute to the greater good of society. We love working with technology development, but the technology needs to be developed responsibly. In this chapter, we looked at the concept of bias in ML and how to work with it. We looked at the IBM Fairness framework, which can assist us in identifying bias. We also learned that automated bias detection is too limited to be able to remove bias from the <span class="No-Break">data completely.</span></p>
			<p>There are more frameworks to explore and more studies and tools are available every day. These frameworks are more specific and provide a means to capture more domain-specific bias – in medicine and advertising. Therefore, my final recommendation in this chapter is to explore the bias frameworks that are specific to the task at hand and for the domain <span class="No-Break">at hand.</span></p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor186"/>References</h1>
			<ul>
				<li><em class="italic">Donald, A., et al., Bias Detection for Customer Interaction Data: A Survey on Datasets, Methods, and Tools. IEEE </em><span class="No-Break"><em class="italic">Access, 2023.</em></span></li>
				<li><em class="italic">Bellamy, R.K., et al., AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. arXiv preprint </em><span class="No-Break"><em class="italic">arXiv:1810.01943, 2018.</em></span></li>
				<li><em class="italic">Zhang, Y., et al. Introduction to AI fairness. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing </em><span class="No-Break"><em class="italic">Systems. 2020.</em></span></li>
				<li><em class="italic">Alves, G., et al. Reducing unintended bias of ml models on tabular and textual data. In 2021 IEEE 8th International Conference on Data Science and Advanced Analytics (DSAA). </em><span class="No-Break"><em class="italic">2021. IEEE.</em></span></li>
				<li><em class="italic">Raza, S., D.J. Reji, and C. Ding, Dbias: detecting biases and ensuring fairness in news articles. International Journal of Data Science and Analytics, 2022: </em><span class="No-Break"><em class="italic">p. 1-21.</em></span></li>
			</ul>
		</div>
	</body></html>