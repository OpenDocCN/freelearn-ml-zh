- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Preprocessing – Stemming, Tagging, and Parsing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理——词干提取、标记和解析
- en: Before we can start assigning emotions to texts, we have to carry out a range
    of preprocessing tasks to get to the elements that carry the information we want.
    In [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*, we briefly
    covered the various components of a generic NLP system, but without looking in
    detail at how any of these components might be implemented. In this chapter, we
    will provide sketches and partial implementations of the tools that are most likely
    to be useful for sentiment mining – where we give a partial implementation or
    a code fragment for something, the full implementation is available in the code
    repository.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始为文本分配情感之前，我们必须执行一系列预处理任务，以获取携带我们所需信息的元素。在[*第1章*](B18714_01.xhtml#_idTextAnchor015)“基础”中，我们简要介绍了通用NLP系统的各个组成部分，但没有详细探讨这些组件可能如何实现。在本章中，我们将提供对情感挖掘最有用的工具的草图和部分实现——当我们给出某物的部分实现或代码片段时，完整的实现可以在代码仓库中找到。
- en: 'We will look at the earlier stages of the language processing pipeline in detail.
    The texts that are most often used for sentiment mining tend to be very informal
    – tweets, product reviews, and so on. This material is often ungrammatical and
    contains made-up words, misspellings, and non-text items such as emoticons, images,
    and emojis. Standard parsing algorithms cannot deal with this kind of material,
    and even if they could, the analyses that they produce would be very hard to work
    with: what would the parse tree for *@MarthatheCat Look at that toof! #growl*
    look like? We will include material relating to tagging (which is generally only
    useful as a precursor to parsing) and parsing, but the focus will be largely on
    the lowest level steps – reading the text (not as straightforward as it seems),
    decomposing words into parts, and identifying compound words.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将详细探讨语言处理管道的早期阶段。用于情感挖掘的最常用的文本往往非常非正式——推文、产品评论等等。这种材料通常语法不规范，包含虚构的词语、拼写错误以及非文本项目，如表情符号、图像和表情符号。标准的解析算法无法处理这类材料，即使它们能够处理，产生的分析结果也非常难以处理：对于“@MarthatheCat
    Look at that toof! #growl”这样的文本，其解析树会是什么样子呢？我们将包括与标记（这通常仅作为解析的先导有用）和解析相关的材料，但重点将主要放在最低级步骤上——阅读文本（并不像看起来那么简单）、将单词分解成部分，以及识别复合词。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Readers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读者
- en: Word parts and compound words
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词素和复合词
- en: Tokenizing, morphology, and stemming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词、形态学和词干提取
- en: Compound words
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复合词
- en: Tagging and parsing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记和解析
- en: Readers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读者
- en: 'Before we can do anything, we need to be able to read the documents that contain
    the texts – in particular, the training data that will be used by the preprocessing
    algorithms and the sentiment mining algorithms themselves. These documents come
    in two classes:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够做任何事情之前，我们需要能够阅读包含文本的文档——特别是预处理算法和情感挖掘算法将使用的训练数据。这些文档分为两类：
- en: '**Training data for the preprocessing algorithms**: A number of the algorithms
    that we use for finding words, decomposing them into smaller units, and combining
    them into larger groups require training data. This can be raw text or it can
    be annotated with suitable labels. In either case, we need a lot of it (for some
    tasks, you need hundreds of millions of words, maybe more), and it is often more
    convenient to use data from external sources than to try to compile it yourself.
    Unfortunately, externally supplied data does not always come in a single agreed
    format, so you need **readers** to abstract away from the details of the format
    and organization of this data. To take a simple example, data for training a program
    to assign part-of-speech tags to text needs to be given text that has been labeled
    with such tags. We will carry out some experiments here using two well-known corpora:
    the **British National Corpus (BNC)** and the **Universal Dependency Tre ebanks
    (UDT)**. The BNC provides text with complex XML-like annotations, such as the
    following:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理算法的训练数据**：我们用于查找单词、将它们分解成更小的单元以及将它们组合成更大组别的算法中，有一些需要训练数据。这可以是原始文本，也可以是带有适当标签的注释文本。在两种情况下，我们都需要大量的数据（对于某些任务，你可能需要数亿个单词，甚至更多），而且使用外部来源的数据通常比自行编译更方便。不幸的是，外部提供的数据并不总是以单一同意的格式出现，因此你需要**读者**来抽象出这些数据的格式和组织细节。以一个简单的例子来说，用于训练程序为文本分配词性的训练数据需要提供已经标记了这些标签的文本。在这里，我们将使用两个著名的语料库进行一些实验：**英国国家语料库（BNC**）和**通用依存关系语料库（UDT**）。BNC提供具有复杂XML类似注释的文本，如下所示：'
- en: '[PRE0]'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This says that *factsheet* is an NN1 and *what* is a pronoun.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明*fact sheet*是一个NN1，而*what*是一个代词。
- en: 'The UDT provides text as tab-separated files where each line represents information
    about a word:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: UDT提供以制表符分隔的文件，其中每一行代表一个单词的信息：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This says that *what* is a pronoun and *is* is an auxiliary. To use these to
    train a tagger, we have to dig out the information that we want and convert it
    into a uniform format.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明*what*是一个代词，而*is*是一个助动词。为了使用这些来训练一个标记器，我们必须挖掘我们想要的信息，并将其转换为统一格式。
- en: '**Training data for the sentiment analysis algorithms**: Almost all approaches
    to assigning emotions to texts employ machine learning algorithms, and hence again
    require training data. As noted in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061),
    *Building and Using a Dataset*, it is often convenient to use externally supplied
    data, and as with the data used for the preprocessing algorithms, this data can
    come in a variety of formats.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析算法的训练数据**：几乎所有的将情感分配给文本的方法都使用机器学习算法，因此又需要训练数据。正如在[*第二章*](B18714_02.xhtml#_idTextAnchor061)“构建和使用数据集”中所述，使用外部提供的数据通常很方便，并且与用于预处理算法的数据一样，这些数据可以以各种格式出现。'
- en: 'Training data may be supplied as text files, as directory trees with text files
    as leaves, or as SQL or other databases. To make matters more complex, there can
    be very large amounts of data (hundreds of millions of items, even billions of
    items), to the point where it is not convenient to have all the data in memory
    at once. Therefore, we start by providing a reader generator function that will
    traverse a directory tree until it reaches a leaf file whose name matches an optional
    pattern and then uses an appropriate reader to return items one at a time from
    this file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据可以以文本文件、以文本文件为叶子的目录树或SQL或其他数据库的形式提供。更复杂的是，可能会有非常大量的数据（数亿个条目，甚至数十亿个条目），以至于一次将所有数据放入内存中并不方便。因此，我们首先提供一个读者生成函数，该函数将遍历目录树，直到达到一个名称匹配可选模式的叶子文件，然后使用适当的读者一次返回一个文件中的项目：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`reader` will return a generator that walks down through the directory tree
    specified by `path` until it finds leaf files whose names match `pattern` and
    then uses `dataFileReader` to iterate through the data in the given files. We
    use a generator rather than a simple function because corpora can be very large
    and reading all the data contained in a corpus into memory at once can become
    unmanageable. The disadvantage of using a generator is that you can only iterate
    through it once – if you want to solidify the results of using a reader, you can
    use `list` to store them as a list:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`reader`将返回一个生成器，它会遍历由`path`指定的目录树，直到找到名称与`pattern`匹配的叶文件，然后使用`dataFileReader`遍历给定文件中的数据。我们使用生成器而不是简单的函数，因为语料库可能非常大，一次性将语料库中包含的所有数据读入内存可能变得难以管理。使用生成器的缺点是您只能迭代它一次
    – 如果您想巩固使用读者得到的结果，可以使用`list`将它们存储为列表：'
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will create a generator, `r`, for reading words from the **BNC**. The BNC
    is a widely used collection of documents, though its status as a resource for
    training, and especially testing, taggers is slightly unclear since the tags for
    the vast majority of the material were assigned programmatically by the CLAWS4
    tagger (Leech et al., 1994). This means that any tagger trained on it will be
    learning the tags assigned by CLAWS4, so unless these are 100% accurate (which
    they are not), then it will not be learning the “real” tags. Nonetheless, it is
    an extremely useful resource, and can certainly be used as a resource for training
    usable taggers. It can be downloaded from [https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个用于从**BNC**读取单词的生成器，`r`。BNC是一个广泛使用的文档集合，尽管它作为训练资源，尤其是测试标记器的资源地位略不清楚，因为绝大多数材料的标签都是由CLAWS4标记器程序分配的（Leech等，1994）。这意味着任何在它上面训练的标记器都将学习CLAWS4分配的标签，所以除非这些标签是100%准确的（它们不是），那么它将不会学习“真实”的标签。尽管如此，它是一个非常有用的资源，并且当然可以用作训练可用的标记器的资源。可以从[https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554)下载。
- en: Then, we solidify this generator to a list, `l`, for ease of use. The BNC contains
    about 110 million words, which is a manageable amount of data on a modern computer,
    so storing them as a single list makes sense. However, for larger corpora, this
    may not be feasible, so having the option of using a generator can be useful.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这个生成器固化为一个列表，`l`，以便于使用。BNC包含大约1.1亿个单词，这在现代计算机上是一个可管理的数据量，因此将它们存储为单个列表是有意义的。然而，对于更大的语料库，这可能不可行，因此有使用生成器的选项可能是有用的。
- en: 'The BNC is supplied as a directory tree with subdirectories, `A`, `B`, …, `K`,
    which contain `A0`, `A1`, …, `B0`, `B1`, …, which, in turn, contain `A00.xml`,
    `A01.xml`, …:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: BNC以目录树的形式提供，包含子目录`A`、`B`、…、`K`，它们包含`A0`、`A1`、…、`B0`、`B1`、…，这些目录反过来又包含`A00.xml`、`A01.xml`、…：
- en: '![Figure 4.1 – Structure of the BNC directory tree](img/B18714_04_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – BNC目录树结构](img/B18714_04_01.jpg)'
- en: Figure 4.1 – Structure of the BNC directory tree
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – BNC目录树结构
- en: 'The leaf files contain header information followed by sentences demarcated
    by `<s n=???>...</s>`, with the words that make up a sentence marked by `<w c5=???
    hw=??? pos=???>???</w>`. Here’s an example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 叶文件包含标题信息，随后是句子，由`<s n=???>...</s>`分隔，组成句子的单词由`<w c5=??? hw=??? pos=???>???</w>`标记。以下是一个示例：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To read all the words in the BNC, we need something to dig out the items between
    `<w ...>` and `</w>`. The easiest way to do this is by using a regular expression:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取BNC中的所有单词，我们需要某种东西来挖掘`<w ...>`和`</w>`之间的项目。最简单的方法是使用正则表达式：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The pattern looks for instances of either `<w ...>` or `<c ...>` and then looks
    for the appropriate closing bracket since the BNC marks words with `<w ...>` and
    punctuation marks with `<c ...>`. We have to find both and make sure that we get
    the right closing bracket.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 模式查找`<w ...>`或`<c ...>`的实例，然后查找适当的闭合括号，因为BNC用`<w ...>`标记单词，用`<c ...>`标记标点符号。我们必须找到两者并确保我们得到正确的闭合括号。
- en: 'Given this definition of `BNCWordReader`, we can, as we did previously, create
    a reader to extract all the raw text from the BNC. Other corpora require different
    patterns for extracting the text – for example, the **Penn Arabic Tree Bank**
    (**PATB**) (this is a useful resource for training and testing Arabic NLP tools.
    Unfortunately, it is not free – see the Linguistic Data Consortium ([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/))
    for information about how to obtain it – however, we will use it for illustration
    when appropriate) contains leaf files that look like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对`BNCWordReader`的定义，我们可以像之前一样创建一个读取器，从BNC中提取所有原始文本。其他语料库需要不同的模式来提取文本——例如，**宾州阿拉伯语树库**（**PATB**）（这是一个用于训练和测试阿拉伯语NLP工具的有用资源。不幸的是，它不是免费的——有关如何获取它的信息，请参阅语言数据联盟([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/))——然而，在适当的时候，我们将用它来举例说明）包含看起来像这样的叶文件：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To extract words from this, we would need a pattern like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要从这些内容中提取单词，我们需要一个类似这样的模式：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will return the following when applied to the PATB:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于PATB时，这将返回以下内容：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Given the similarity between `BNCWordReader` and `PATBWordReader`, we could
    have simply defined a single function called `WordReader` that takes a path and
    a pattern and bound the pattern as required:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`BNCWordReader`和`PATBWordReader`之间的相似性，我们本可以简单地定义一个名为`WordReader`的单个函数，它接受一个路径和一个模式，并将模式绑定到所需的形式：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The same technique can be applied to extract the raw text from a wide range
    of corpora, such as the UDT ([https://universaldependencies.org/#download](https://universaldependencies.org/#download)),
    which provides free access to moderate amounts of tagged and parsed data for a
    large number of languages (currently 130 languages, with about 200K words per
    language for the commoner languages but rather less for others).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的技术可以应用于从广泛的语料库中提取原始文本，例如UDT ([https://universaldependencies.org/#download](https://universaldependencies.org/#download))，它为大量语言（目前有130种语言，对于常见语言大约有20万个单词，而对于其他语言则较少）提供了免费访问标记和解析数据的途径。
- en: 'Similarly, the training data for sentiment assignment algorithms comes in a
    variety of formats. As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015),
    *Foundations*, Python already provides a module, `pandas`, for managing training
    sets for generic sets of training data. `pandas` is useful if your training data
    consists of sets of data points, where a data point is a set of `feature:value`
    pairs that describe the properties of the data point, along with a label that
    says what class it belongs to. The basic object in `pandas` is a DataFrame, which
    is a collection of objects where each object is described by a set of `feature:value`
    pairs. As such, a DataFrame is very much like a SQL table, where the names of
    the columns are the feature names and an object corresponds to a row in the table;
    it is also extremely like a nested Python dictionary, where the keys at the top
    level are the feature names and the values associated with those names are index-value
    pairs. And it’s also very like a spreadsheet, where the top row is the feature
    names and the remaining rows are the data points. DataFrames can be read directly
    from all these formats and more (including from a table in a SQL database), and
    can be written directly to any of them. Consider the following extract from a
    set of annotated tweets stored as a MySQL database:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，情感分配算法的训练数据以各种格式提供。如[*第一章*](B18714_01.xhtml#_idTextAnchor015)中所述，*基础*，Python已经提供了一个名为`pandas`的模块，用于管理通用训练数据集的训练集。如果您的训练数据由数据点集合组成，其中数据点是一组`feature:value`对，描述了数据点的属性，以及一个标签说明它属于哪个类别，那么`pandas`非常有用。`pandas`中的基本对象是DataFrame，它是一组对象的集合，其中每个对象由一组`feature:value`对描述。因此，DataFrame非常类似于SQL表，其中列名是特征名称，一个对象对应于表中的一行；它也非常类似于嵌套的Python字典，其中顶层键是特征名称，与这些名称相关联的值是索引-值对。它还非常类似于电子表格，其中顶部行是特征名称，其余行是数据点。DataFrame可以直接从所有这些格式以及更多格式（包括SQL数据库中的表）中读取，并且可以直接写入其中。以下是从存储为MySQL数据库的标注推文集合中提取的示例：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `sentiments` table contains rows representing the ID of the annotator who
    annotated a given tweet, the ID of the tweet itself, and the set of emotions that
    the given annotator assigned to it (for example, annotator 8 assigned anger and
    dissatisfaction to tweet 1). This table can be read directly as a DataFrame and
    can be transformed into a dictionary, a JSON object (very similar to a dictionary),
    a string in CSV format, and more:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`sentiments` 表包含代表注释者ID的行，该注释者注释了特定的推文，推文本身的ID，以及给定注释者分配给它的情绪集合（例如，注释者8将愤怒和不满分配给推文1）。此表可以直接作为DataFrame读取，并可以转换为字典、JSON对象（与字典非常相似），CSV格式的字符串等：'
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We therefore do not have to worry too much about actually reading and writing
    the data to be used to train the emotion classification algorithms – DataFrames
    can be read from, and written to, in almost any reasonable format. Nonetheless,
    we still have to be careful about what features we are using and what values they
    can have. The preceding MySQL database, for instance, refers to the IDs of the
    tweets and the annotators, with the text of each tweet being kept in a separate
    table, and it stores each annotator’s assignment of emotions as a single compound
    value (for example, love+joy+optimism). It would have been perfectly possible
    to store the text of the tweet in the table and to have each emotion as a column
    that could be set to 1 if the annotator had assigned this emotion to the tweet
    and 0 otherwise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不必过于担心实际读取和写入用于训练情绪分类算法的数据——DataFrame可以从几乎任何合理的格式中读取和写入。尽管如此，我们仍然必须小心我们使用哪些特征以及它们可以有哪些值。例如，前面的MySQL数据库引用了推文和注释者的ID，每个推文的文本保存在单独的表中，并且将每个注释者分配的情绪作为单个复合值存储（例如，爱+快乐+乐观）。完全有可能将推文的文本存储在表中，并且将每个情绪作为一列，如果注释者将此情绪分配给推文，则设置为1，否则为0：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, each tweet has an explicit ID, as well as a position in the DataFrame;
    the tweet itself is included, and each emotion is a separate column. The data
    here was supplied as a CSV file, so it was read directly as a DataFrame without
    any trouble, but the way it is presented is completely different from the previous
    set. Therefore, we will need preprocessing algorithms to make sure that the data
    we are using is organized the way the machine learning algorithms want it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个推文都有一个明确的ID，以及在DataFrame中的位置；推文本身被包含在内，每个情绪都是单独的列。这里的数据是以CSV文件的形式提供的，因此可以直接作为DataFrame读取，没有任何问题，但它的呈现方式与之前的一组完全不同。因此，我们需要预处理算法来确保我们使用的数据是以机器学习算法想要的方式组织的。
- en: Fortunately, DataFrames have database-like options for selecting data and merging
    tables, so converting from one way of presenting the data into another is reasonably
    straightforward, but it does have to be carried out. There are, for instance,
    advantages to having a single column per emotion and advantages to having a single
    column for all emotions but allowing compound emotions – having a single column
    per emotion makes it easy to allow for cases where a single object may have more
    than one emotion associated with it; having just one column for all emotions makes
    it easy to search for tweets that have the same emotions. Some resources will
    provide one and some the other, but almost any learning algorithm will require
    one or the other, so it is necessary to be able to convert between them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，DataFrame具有类似数据库的选项，可以用于选择数据和合并表，因此将数据的一种表示方式转换为另一种方式相对直接，但确实需要执行。例如，对于每种情绪只有一个列有优势，而对于所有情绪只有一个列但允许复合情绪也有优势——对于每种情绪只有一个列使得允许单个对象可能具有多种与之相关的情绪变得容易；对于所有情绪只有一个列使得搜索具有相同情绪的推文变得容易。一些资源会提供一种方式，而另一些会提供另一种方式，但几乎任何学习算法都会需要其中一种，因此能够在这两者之间进行转换是必要的。
- en: Word parts and compound words
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词的部分和复合词
- en: The key to identifying emotions in texts lies with the words that make up those
    texts. It may turn out to be useful to classify words, find similarities between
    words, and find out how the words in a given text are related, but the most important
    things are the words themselves. If a text contains the words *love* and *happy*,
    then it’s very likely to be positive, whereas if it contains the words *hate*
    and *horrible*, it’s very likely to be negative.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中识别情绪的关键在于构成这些文本的词语。可能对分类词语、找出词语之间的相似性以及了解给定文本中的词语关系很有用，但最重要的是词语本身。如果一个文本包含*爱*和*快乐*这样的词语，那么它很可能具有积极的含义，而如果一个文本包含*恨*和*可怕*这样的词语，那么它很可能具有消极的含义。
- en: 'As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*,
    however, it can be difficult to specify exactly what counts as a word, and hence
    difficult to find the words that make up a text. While the writing systems of
    many languages use white space to split up text, there are languages where this
    does not happen (for example, written Chinese). But even where the written form
    of a language does use white space, finding the units that we are interested in
    is not always straightforward. There are two basic problems:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第一章*](B18714_01.xhtml#_idTextAnchor015)中所述，“基础”，然而，确切地指定什么算作一个词可能很困难，因此很难找到构成文本的词语。虽然许多语言的书写系统使用空格来分隔文本，但也有一些语言不会这样做（例如，书面汉语）。即使在语言的书面形式使用空格的情况下，找到我们感兴趣的单元也不总是直接的。存在两个基本问题：
- en: Words are typically made up of a core lexeme and several affixes that add to
    or alter the meaning of the core. *love*, *loves*, *loved*, *loving*, *lover*,
    and *lovable* are all clearly related to a single concept, though they all look
    slightly different. So, do we want to treat them as different words or as variations
    on a single word? Do we want to treat *steal*, *stole*, and *stolen* as different
    words or as variations of the same word? The answer is that sometimes we want
    to do one and sometimes the other, but when we do want to treat them as variations
    of the same word, we need machinery to do so.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语通常由一个核心词素和几个词缀组成，这些词缀增加了或改变了核心词素的意义。*爱*、*loves*、*loved*、*loving*、*lover*和*lovable*都与一个单一的概念明显相关，尽管它们看起来略有不同。那么，我们是想将它们视为不同的词语，还是视为单一词语的变体？我们是想将*steal*、*stole*和*stolen*视为不同的词语，还是视为同一词语的变体？答案是，有时我们想这样做，有时不想这样做，但当我们想将它们视为同一词语的变体时，我们需要相应的机制来实现。
- en: 'Some items that are separated by white space look as though they are made out
    of several components: *anything*, *anyone*, and *anybody* look very much as though
    they are made out of *any* plus *thing*, *one*, or *body* – it is hard to imagine
    that the underlying structures of *anyone could do that* and *any fool could do
    that* are different. It is worth noting that in English, the stress patterns of
    the spoken language match up with the presence or absence of white space in text
    – *anyone* is pronounced with a single stress on the first syllable, /en/, while
    *any fool* has stress on /en/ and /fool/, so there is a difference, but they also
    have the same structure.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些由空格分隔的项看起来像是由几个组件组成的：*anything*、*anyone*和*anybody*看起来非常像是由*any*加上*thing*、*one*或*body*组成的——很难想象*anyone
    could do that*和*any fool could do that*的潜在结构是不同的。值得注意的是，在英语中，口语的重音模式与文本中空格的存在或缺失相匹配——*anyone*在第一个音节/en/上有一个重音，而*any
    fool*在/en/和/fool/上有重音，所以它们之间有区别，但它们也有相同的结构。
- en: 'It is tempting to try to deal with this by looking at each White space-separated
    item to see whether it is made out of two (or more) other known units. *Stock
    market* and *stockmarket* seem to be the same word, as do *battle ground* and
    *battleground*, and looking at the contexts in which they occur bears this out:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易想要通过查看每个由空格分隔的项来处理这个问题，看看它是否由两个（或更多）其他已知单位组成。*股市*和*stockmarket*看起来是同一个词，同样*战场*和*battleground*也是如此，查看它们出现的上下文可以证实这一点：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'However, there are also clear examples where the compound word is not the same
    as the two words put next to one another. A *heavy weight* is something (anything)
    that weighs a lot, whereas a *heavyweight* is nearly always a boxer; if something
    is *under study*, then someone is studying it, whereas an *understudy* is someone
    who will step in to fill a role when the person who normally performs it is unavailable:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些明显的例子表明复合词并不等同于相邻的两个单词。*重量级*是指重量很大的任何东西，而*重量级*几乎总是指拳击手；如果某物正在*研究*，那么有人在研究它，而*
    understudy*是指当通常表演的人不可用时将填补角色的人：
- en: '[PRE15]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is particularly important for languages such as Chinese, which are written
    without white space, where almost any character can be a standalone word but sequences
    of two or more characters can also be words, often with very little connection
    to the words that correspond to the individual characters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于像中文这样的语言尤为重要，中文没有空格书写，几乎任何字符都可以作为一个独立的单词，但两个或更多字符的序列也可以是单词，通常与单个字符对应的单词联系很少。
- en: These phenomena occur in all languages. Some languages have very complex rules
    for breaking words into smaller parts, some make a great deal of use of compounds,
    and some do both. These examples gave a rough idea of the issues as they arise
    in English, but in the following discussion of using algorithms to deal with these
    issues, we will look at examples from other languages. In the next section, *Tokenizing,
    morphology, and stemming*, we will look at algorithms for splitting words into
    parts – that is, at ways of recognizing that the word *recognizing* is made up
    of two parts, *recognize* and *-ing*. In the section, *Compound words*, we will
    look at ways of spotting compounds in languages where they are extremely common.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现象在所有语言中都存在。有些语言有非常复杂的规则来将单词分解成更小的部分，有些大量使用复合词，还有些两者都做。这些例子给出了在英语中这些问题出现的大致情况，但在以下关于使用算法处理这些问题的讨论中，我们将查看其他语言的例子。在下一节“分词、形态学和词干提取”，我们将探讨将单词分解成部分的方法——也就是说，识别单词*recognizing*由两部分组成，*recognize*和*-ing*。在“复合词”这一节中，我们将探讨在复合词非常普遍的语言中识别复合词的方法。
- en: Tokenizing, morphology, and stemming
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词、形态学和词干提取
- en: The very first thing we have to do is split the input text into **tokens** –
    units that make an identifiable contribution to the message carried by the whole
    text. Tokens include words, as roughly characterized previously, but also punctuation
    marks, numbers, currency symbols, emojis, and so on. Consider the text *Mr. Jones
    bought it for £5.3K!* The first token is *Mr.*, which is a word pronounced /m
    i s t uh/, while the next few are *Jones*, *bought*, *it*, and *for*, then the
    currency symbol, *£*, followed by the number *5.3K* and the punctuation mark,
    *!*. Exactly what gets treated as a token depends on what you are going to do
    next (is *5.3K* a single number or is it two tokens, *5.3* and *K*?), but there
    is very little you can do with a piece of text without splitting it into units
    along these lines.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做的第一件事是将输入文本分割成**标记**——这些标记对整个文本所传达的信息有可识别的贡献。标记包括单词，如之前大致描述的那样，但也包括标点符号、数字、货币符号、表情符号等等。考虑以下文本*Mr.
    Jones bought it for £5.3K!*第一个标记是*Mr.*，这是一个发音为/m i s t uh/的单词，而接下来的几个标记是*Jones*、*bought*、*it*和*for*，然后是货币符号*£*，接着是数字*5.3K*和标点符号*!*。确切地说，哪些被当作标记处理取决于你接下来要做什么（*5.3K*是一个单独的数字，还是两个标记，*5.3*和*K*？），但如果没有将这些文本分割成这样的单位，你几乎无法对文本进行任何操作。
- en: 'The easiest way to do this is by defining a regular expression where the pattern
    specifies the way the text is to be split. Consider the preceding sentence: we
    need something for picking out numbers, something for abbreviations, something
    for currency symbols, and something for punctuation marks. This suggests the following
    pattern:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事的最简单方法是通过定义一个正则表达式，其中模式指定了文本应该被分割的方式。考虑前面的句子：我们需要一些用于挑选数字的，一些用于缩写的，一些用于货币符号的，以及一些用于标点符号的。这建议以下模式：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The first part of this pattern says that a number can consist of some digits,
    possibly followed by a comma (to capture cases such as 120,459 for one hundred
    and twenty thousand four hundred and fifty-nine), followed by a point and some
    more digits, and then finally possibly followed by the letter K; the second part
    lists several abbreviations that will normally be followed by a full stop; the
    next two, `n't` and `([A-Za-z](?!n't))*[A-Za-z]`, are fairly complex; `n't` recognizes
    *n’t* as a token, while `([A-Za-z](?!n't))*[A-Za-z]` picks out sequences of alphabetic
    characters not ending with *n’t* so that *hasn’t* and *didn’t* are each recognized
    as two tokens, *has + n’t* and *did + n’t*. The next few just recognize punctuation
    marks, currency symbols, and similar; the last one recognizes sequences of non-alphabetic
    symbols, which is useful for treating sequences of emojis as tokens.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式的第一个部分表明，一个数字可以由一些数字组成，可能后面跟着一个逗号（以捕获像120,459这样的一个百万和二十万四千五百九），然后是一个点和一些更多的数字，最后可能跟着字母K；第二部分列出了几个通常后面跟着句号的缩写；接下来的两个，`n't`和`([A-Za-z](?!n't))*[A-Za-z]`相当复杂；`n't`识别*n’t*作为一个标记，而`([A-Za-z](?!n't))*[A-Za-z]`挑选出不以*n’t*结尾的字母字符序列，因此*hasn’t*和*didn’t*都被识别为两个标记，*has
    + n’t*和*did + n’t*。接下来的几个只是识别标点符号、货币符号和类似的东西；最后一个识别非字母符号的序列，这对于将表情符号序列作为标记处理很有用。
- en: 'Looking for instances of this pattern in English texts produces results like
    this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在英文文本中寻找这种模式的实例会产生如下结果：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Using regular expressions for tokenization has two advantages: regular expressions
    can be applied extremely fast, so large amounts of text can be tokenized very
    quickly (about three times as fast as the NLTK’s built-in `word_tokenize`: the
    only major difference between the output of the two is that `tokenise` treats
    words such as *built-in* as being made of three components, *built*, *-*, and
    *in*, whereas the NLTK treats them as single compounds, *built-in*); and the patterns
    are completely self-contained, so they can be changed easily (if, for instance,
    you would rather treat each emoji as a separate token, just remove `+` from `[^a-zA-Z\s]+`,
    and if you would rather treat *built-in* as a single compound unit, just remove
    `–` from the list of options) and can also be easily adapted to other languages,
    such as by replacing the character range, `[a-z]`, with the Unicode range of characters
    used by the required language:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式进行标记化有两个优点：正则表达式可以非常快地应用，因此大量文本可以非常快地进行标记化（大约是NLTK内置的`word_tokenize`的三倍快：这两个输出的唯一主要区别是`tokenise`将诸如*built-in*这样的词视为由三个部分组成，*built*、*-*和*in*，而NLTK将它们视为单个复合词，*built-in*）；并且模式是完全自包含的，因此可以很容易地进行更改（例如，如果你更愿意将每个表情符号视为一个单独的标记，只需从`[^a-zA-Z\s]+`中移除`+`，如果你更愿意将*built-in*视为一个单一的复合单位，只需从选项列表中移除`–`），并且也可以很容易地适应其他语言，例如通过替换所需的语言的字符范围，将`[a-z]`替换为该语言使用的Unicode字符范围：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Once we have tokenized our text, we are likely to have tokens that are minor
    variants of the same lexeme – *hating* and *hated* are both versions of the same
    lexeme, *hate*, and will tend to carry the same emotional charge. The importance
    (and difficulty) of this step will vary from language to language, but virtually
    all languages make words out of a core lexeme and a set of affixes, and finding
    the core lexeme will generally contribute to tasks such as emotion detection.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们标记了我们的文本，我们很可能会得到一些是同一词根的微小变体——*hating*和*hated*都是*hate*这个词根的版本，并且倾向于携带相同的情感电荷。这一步骤的重要性（和难度）会因语言而异，但几乎所有语言都是由一个词根和一组词缀构成的，找到词根通常有助于诸如情感检测等任务。
- en: 'The obvious starting point is to produce a list of affixes and try to chop
    them off the start and end of a token until a known core lexeme is found. This
    requires us to have a set of core lexemes, which can be quite difficult to get.
    For English, we can simply use the list of words from WordNet. This gives us 150K
    words, which will cover most cases:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的起点是生成一个词缀列表，并尝试从标记的开始和结束处切掉它们，直到找到一个已知的词根。这要求我们拥有一组词根，这可能是相当困难的。对于英语，我们可以简单地使用WordNet中的单词列表。这给我们提供了15万个单词，将覆盖大多数情况：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This will look at the first few characters of the token to see whether they
    are prefixes (allowing for empty prefixes), and then at the next few to see whether
    they are known words, and then at the remainder to see whether it’s a suffix.
    Writing it as a generator makes it easy to produce multiple answers – for example,
    if `WORDS` contains both *construct* and *reconstruct*, then `stem1` will return
    `['-reconstruct+ing', 're-construct+ing']` to form *reconstructing*. `stem1` takes
    around 2*10-06 seconds for a short word such as *cut* and 7*10-06 seconds for
    a longer and more complex case such as *reconstructing* – fast enough for most
    purposes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将检查标记的前几个字符，以查看它们是否是前缀（允许空前缀），然后查看接下来的几个字符，以查看它们是否是已知单词，然后查看剩余部分，以查看它是否是后缀。将其编写为生成器使得产生多个答案变得容易——例如，如果
    `WORDS` 包含 *construct* 和 *reconstruct*，那么 `stem1` 将返回 `['-reconstruct+ing', 're-construct+ing']`
    以形成 *reconstructing*。`stem1` 对短词如 *cut* 大约需要 2*10-06 秒，对更长更复杂的案例如 *reconstructing*
    大约需要 7*10-06 秒——对于大多数用途来说足够快。
- en: 'To use `stem1`, use the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `stem1`，请使用以下代码：
- en: '[PRE20]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`stem1.stem` is a generator because there might be several ways to decompose
    a word. For *unexpected*, we get three analyses because *expect*, *expected*,
    and *unexpected* are all in the base lexicon:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`stem1.stem` 是一个生成器，因为分解一个词可能有几种方式。对于 *unexpected*，我们得到三种分析，因为 *expect*、*expected*
    和 *unexpected* 都在基本词典中：'
- en: '[PRE21]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For *uneaten*, on the other hand, we only get *un-eat+en*, because the lexicon
    does not contain *eaten* and *uneaten* as entries:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于 *uneaten*，我们只得到 *un-eat+en*，因为词典中没有将 *eaten* 和 *uneaten* 作为条目：
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'That’s a bit awkward because it is hard to predict which derived forms will
    be listed in the lexicon. What we want is the root and its affixes, and it is
    clear that *expected* and *unexpected* are not the root forms. The more affixes
    you remove, the closer you get to the root. So, we might decide to use the output
    with the shortest root as the best:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点尴尬，因为很难预测词典中会列出哪些派生形式。我们想要的是词根及其词缀，很明显，*expected* 和 *unexpected* 都不是词根形式。你移除的词缀越多，就越接近词根。因此，我们可能会决定使用具有最短词根的输出作为最佳选择：
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The quality of the output depends very heavily on the quality of the lexicon:
    if it contains forms that are themselves derived from smaller items, we will get
    analyses like the three for *unexpected*, and if it doesn’t contain some form,
    then it won’t return it (the WordNet lexicon contains about 150K entries, so this
    won’t happen all that often if we use it!).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的质量在很大程度上取决于词典的质量：如果它包含自身由更小项目派生的形式，我们将得到如 *unexpected* 的三种分析，如果它不包含某些形式，则不会返回它（WordNet
    词典包含大约 150K 个条目，所以如果我们使用它，这种情况不会经常发生！）。
- en: '`stem1.stem` is the basis for several well-known tools – for example, the `morphy`
    function from the NLTK for analyzing English forms and the **Standard Arabic Morphological
    Analyzer** (**SAMA**), (Buckwalter, T., 2007). There are, as ever, some complications,
    notably that the spelling of a word can change when you add a prefix or a suffix
    (for instance, when you add the English negative prefix *in-* to a word that begins
    with a *p*, it becomes *im-*, so *in-* + *perfect* becomes *imperfect* and *in-*
    + *possible* becomes *impossible*), and that words can have multiple affixes (for
    instance, French nouns and adjectives can have two suffixes, one to mark gender
    and one to mark number). We will look at these in the next two sections.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`stem1.stem` 是几个知名工具的基础——例如，NLTK 中的 `morphy` 函数用于分析英语形式和 **标准阿拉伯形态分析器**（**SAMA**）（Buckwalter,
    T., 2007）。一如既往，有一些复杂性，特别是当添加前缀或后缀时，单词的拼写可能会改变（例如，当你在以 *p* 开头的单词前添加英语否定前缀 *in-*
    时，它变成 *im-*，所以 *in-* + *perfect* 变成 *imperfect*，*in-* + *possible* 变成 *impossible*），以及单词可以有多个词缀（例如，法语名词和形容词可以有后缀，一个用于标记性别，一个用于标记数）。我们将在下一两节中探讨这些内容。'
- en: Spelling changes
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在拼写变化方面
- en: 'In many languages (for example, English), the relationship between spelling
    and pronunciation is quite subtle. In particular, it can encode facts about the
    stress of a word, and it can do so in ways that change when you add prefixes and
    suffixes. The “magic e,” for instance, is used to mark words where the final vowel
    is long – for example, *site* versus *sit*. However, when a suffix that begins
    with a vowel is added to such a word, the *e* is dropped from the version with
    a long vowel, and the final consonant of the version with a short vowel is doubled:
    *siting* versus *sitting* (this only happens when the final vowel of the root
    is both short and stressed, with *enter* and *infer* becoming *entering* and *inferring*).
    Such rules tend to reflect the way that spelling encodes pronunciation (for example,
    the magic e marks the preceding vowel as being long, while the doubled consonant
    in *inferring* marks the previous vowel as being short and stressed) or to arise
    from actual changes in pronunciation (the *im-* in *impossible* is the *in-* prefix,
    but it’s difficult to say *inpossible* (try it!), so English speakers have lazily
    changed it to *im-*. See (Chomsky & Halle, 1968) for a detailed discussion of
    the relationship between spelling and pronunciation in English).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多语言（例如，英语）中，拼写和发音之间的关系相当微妙。特别是，它可以编码关于单词重音的事实，并且当添加前缀和后缀时，它可以以改变的方式做到这一点。例如，“魔法e”用于标记以长元音结尾的单词——例如，*site*
    与 *sit*。然而，当以元音开头的后缀添加到这样的单词时，*e* 会从长元音版本中省略，而短元音版本的最后辅音会被双写：*siting* 与 *sitting*（这仅在根词的最后一个元音既短又重音时发生，*enter*
    和 *infer* 分别变为 *entering* 和 *inferring*）。这样的规则往往反映了拼写编码发音的方式（例如，魔法e标记前面的元音为长音，而*inferring*中的双辅音标记前面的元音为短音和重音）或者源于发音的实际变化（*im-*
    在 *impossible* 中是 *in-* 前缀，但很难说成 *inpossible*（试试看！），所以英语使用者们懒惰地将其改为 *im-*。参见（Chomsky
    & Halle, 1968）以了解英语中拼写和发音之间关系的详细讨论）。
- en: '`morphy` deals with this by including all possible versions of the affixes
    and stopping as soon as one that matches is found:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`morphy` 通过包含所有可能的后缀版本，并在找到匹配项时停止来处理这个问题：'
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This table says, for instance, that if you see a word that ends with *s*, it
    might be a noun if you delete the *s*, and that if you see a word that ends with
    *ches*, then it might be a form of a noun that ends *ch*. Making these substitutions
    will work fine a lot of the time, but it does not deal with cases such as *hitting*
    and *slipped*. Due to this, `morphy` includes a list of exceptions (quite a long
    list: 2K for nouns and 2.4K for verbs) that includes forms like these. This will
    work, of course, but it does take a lot of maintenance and it does mean that words
    that obey the rules but are not in the exception list will not be recognized (for
    example, the basic word list includes *kit* as a verb, but does not including
    *kitting* and *kitted* as exceptions and hence will not recognize that *kitted*
    in *he was kitted out with all the latest gear* is a form of *kit*).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这个表格说明，如果你看到一个以 *s* 结尾的单词，如果你删除 *s*，它可能是一个名词；如果你看到一个以 *ches* 结尾的单词，那么它可能是以
    *ch* 结尾的名词形式。这些替换在很多情况下都会有效，但它并不处理像 *hitting* 和 *slipped* 这样的情况。因此，`morphy` 包含一个例外情况的列表（相当长的列表：名词有2K个，动词有2.4K个），包括这些形式。当然，这会起作用，但它确实需要大量的维护，并且这意味着遵守规则但不在例外列表中的单词将不会被识别（例如，基本单词列表包括
    *kit* 作为动词，但不包括 *kitting* 和 *kitted* 作为例外，因此不会识别 *kitted* 在 *he was kitted out
    with all the latest gear* 中的形式是 *kit*）。
- en: 'Instead of providing multiple versions of the affixes and long lists of exceptions,
    we can provide a set of spelling changes that are to be applied as the word is
    split:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供一组在单词拆分时应用的拼写变化，而不是提供多个词缀版本和长列表的例外情况：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In these rules, the left-hand side is a pattern that is to be matched somewhere
    in the current form and the right-hand side is how it might be rewritten. `C,
    C0, C1`, … will match any consonant, `V, V0, V1`, … will match any vowel, `X,
    X0, X1`, … will match any character, and `X:(d|n)` will match `d` or `n` and fix
    the value of `X` to be whichever one was matched. Thus, the first rule will match
    *seen* and *freed* and suggest rewriting them as *see+en* or *free+ed*, and the
    second last one, which looks for a consonant, a vowel, a repeated consonant, and
    any of *ed*, *en*, *er*, or *ing* will match *slipping* and *kitted* and suggest
    rewriting them as *slip+ing* and *kit+ed*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些规则中，左侧是一个要在当前形式中某处匹配的模式，右侧是它可能被重写的方式。`C, C0, C1`, … 将匹配任何辅音，`V, V0, V1`,
    … 将匹配任何元音，`X, X0, X1`, … 将匹配任何字符，而 `X:(d|n)` 将匹配 `d` 或 `n` 并将 `X` 的值固定为所匹配的任何一个。因此，第一条规则将匹配
    *seen* 和 *freed* 并建议将它们重写为 *see+en* 或 *free+ed*，而倒数第二条规则，它寻找一个辅音、一个元音、一个重复的辅音以及
    *ed*, *en*, *er* 或 *ing* 中的任何一个，将匹配 *slipping* 和 *kitted* 并建议将它们重写为 *slip+ing*
    和 *kit+ed*。
- en: 'If we use rules like these, we can find roots of forms where the ending has
    been changed without explicitly listing them:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用这样的规则，我们可以找到那些结尾已更改但未明确列出它们的词的词根：
- en: '[PRE26]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As before, if we use it with a word where the derived form is explicitly listed
    as an exception, then we will get multiple versions, but again, using the one
    with the shortest root will give us the most basic version of the root:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果我们用它与一个派生形式明确列出为例外情况的词一起使用，那么我们会得到多个版本，但再次强调，使用具有最短根的版本将给我们提供最基础的根版本：
- en: '[PRE27]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The implementation of `allstems` in `chapter4.stem2` in this book’s code repository
    also allows multiple affixes, so we can analyze words such as *unreconstructed*
    and *derestrictions*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本书代码库中 `chapter4.stem2` 中 `allstems` 的实现也允许多个词缀，因此我们可以分析像 *unreconstructed*
    和 *derestrictions* 这样的词：
- en: '[PRE28]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These rules can be compiled into a single regular expression, and hence can
    be applied very fast and will cover the vast majority of spelling changes at the
    boundaries between the morphemes that make up an English word (the use of regular
    expressions for this task was pioneered by (Koskiennemi, 1985)). Rules only apply
    at the junctions between morphemes, so it is possible to immediately check whether
    the first part of the rewritten form is a prefix (if no root has been found) or
    a root (if no root has been found so far), so they will not lead to multiple unjustified
    splits. This kind of approach leads to tools that can be much more easily maintained
    since you do not have to add all the forms that cannot be obtained just by splitting
    off some versions of the affixes as exceptions, so it may be worth considering
    if you are working with a language where there are large numbers of spelling changes
    of this sort.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则可以编译成一个单一的正规表达式，因此可以非常快速地应用，并将覆盖构成英语单词的语素之间的拼写变化的大多数情况（这种任务中使用正规表达式的方法是由
    (Koskiennemi, 1985) 领先提出的）。规则仅在语素的接合处应用，因此可以立即检查重写形式的第一部分是否是一个前缀（如果没有找到根）或一个根（如果到目前为止还没有找到根），因此它们不会导致不合理的多次分割。这种方法导致可以更容易维护的工具，因为你不需要添加所有不能仅通过分割词缀的一些版本来获得的形式作为例外，因此如果你正在处理有大量此类拼写变化的语言，这可能值得考虑。
- en: Multiple and contextual affixes
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个和上下文词缀
- en: 'The preceding discussion suggests that there is a fixed set of affixes, each
    of which can be attached to a suitable root. Even in English, the situation is
    not that simple. Firstly, there are several alternative past endings, some of
    which attach to some verbs and some to others. Most verbs take *–ed* for their
    past participles, but some, such as *take*, require *–en*: `morphy` accepts *taked*
    as well as *taken* as forms of *take*, and likewise for other *–en* verbs and
    completely irregular cases such as *thinked* and *bringed*. Secondly, there are
    cases where a word may take a sequence of affixes – for example, *unexpectedly*
    looks as though it is made out of a prefix, *un-*, the root, *expect*, and two
    suffixes, *-ed* and *-ly*. Both these issues become more significant in other
    languages. It probably doesn’t matter that `morphy` returns *steal* as the root
    of *stealed* since it is very unlikely that anyone would ever write *stealed*
    (likewise, it doesn’t matter that it accepts *sliping* as the present participle
    of *slip* since no one would ever write *sliping*). In other languages, failing
    to spot that some affix is the wrong kind of thing to attach to a given root can
    lead to incorrect readings. Likewise, there are not all that many cases in English
    of multiple affixes, certainly not of multiple inflectional affixes (in the preceding
    example, *un-*, *-ed*, and *-ly* are all derivational affixes – *-un* obtains
    a new adjective from an old one, *-ed* in this case obtains an adjective from
    a verb root, and *-ly* obtains an adverb from an adjective).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的讨论表明，存在一组固定的词缀，每个词缀都可以附加到一个合适的词根上。即使在英语中，情况也并不那么简单。首先，存在几种不同的过去时结尾，其中一些附加到某些动词上，而另一些则附加到其他动词上。大多数动词使用
    *–ed* 作为它们的过去分词，但有些动词，如 *take*，则需要 *–en*：`morphy` 既可以接受 *taked* 也可以接受 *taken*
    作为 *take* 的形式，其他 *–en* 动词和完全不规则的例子，如 *thinked* 和 *bringed*，也是如此。其次，有些情况下，一个词可能需要一系列词缀——例如，*unexpectedly*
    看起来是由一个前缀 *un-*，一个词根 *expect* 和两个后缀 *-ed* 和 *-ly* 组成的。这两个问题在其他语言中变得更加重要。`morphy`
    将 *steal* 作为 *stealed* 的词根返回可能并不重要，因为几乎没有人会写出 *stealed*（同样，它接受 *sliping* 作为 *slip*
    的现在分词也不重要，因为没有人会写出 *sliping*）。在其他语言中，未能注意到某些词缀不适合附加到给定的词根上可能会导致错误的解读。同样，英语中多词缀的情况并不多见，当然，多屈折词缀的情况更少（在前面的例子中，*un-*，*-ed*
    和 *-ly* 都是派生词缀——*-un* 从一个形容词获得一个新的形容词，*-ed* 在这种情况下从一个动词词根获得一个形容词，*-ly* 从一个形容词获得一个副词）。
- en: Again, this can matter more in languages such as French (and other Romance languages),
    where a noun is expected to take a gender marker and a number marker (*noir*,
    *noire*, *noirs*, and *noires*), and a verb is expected to take a tense marker
    and an appropriate person marker (*achetais* as the first person singular imperfect,
    *acheterais* as the first person singular conditional); and in languages such
    as Arabic, where a word may have varying numbers of inflectional affixes (for
    example, a present tense verb will have a tense prefix and a present tense person
    marker, while a past tense one will just have the past tense number marker) and
    also a number of clitic affixes (closed class words that are directly attached
    to the main word) – for example, the form ويكتبون (wyktbwn) consists of a conjunction,
    وَ/PART (wa), a present tense prefix, يَ/IV3MP, a present tense form of the verb,
    (كْتُب/VERB_IMP), a present tense suffix, ُونَ/IVSUFF_SUBJ:MP_MOOD:I, and a third
    person plural pronoun, هُم/IVSUFF_DO:3MP, with the whole thing meaning *and they
    are writing them*. The permitted constituents, the order they are allowed to appear
    in, and the form of the root, which can vary with different affixes and between
    different classes of nouns and verbs, are complex and crucial.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这在像法语（以及其他罗曼语族语言）这样的语言中可能更为重要，在这些语言中，名词需要接受一个性别标记和一个数量标记（*noir*，*noire*，*noirs*
    和 *noires*），动词需要接受一个时态标记和一个适当的人称标记（*achetais* 作为第一人称单数的过去不定式，*acheterais* 作为第一人称单数的条件式）；以及在像阿拉伯语这样的语言中，一个词可能有可变数量的屈折词缀（例如，现在时态的动词将有一个时态前缀和一个现在时态的人称标记，而过去时态的动词则只有一个过去时态的数量标记）以及许多粘着词缀（直接附加到主要词上的封闭类词）——例如，形式
    ويكتبون (wyktbwn) 由一个连词，和/PART (wa)，一个现在时态前缀，ي/IV3MP，一个动词的现在时态形式（ك/VERB_IMP），一个现在时态后缀，ُون/IVSUFF_SUBJ:MP_MOOD:I，以及一个第三人称复数代词，هُم/IVSUFF_DO:3MP，整个短语的意思是
    *他们正在写它们*。允许的成分，它们允许出现的顺序，以及词根的形式，这些形式可以随着不同的词缀以及不同类别的名词和动词而变化，都是复杂且至关重要的。
- en: 'To capture these phenomena, we need to make a further change to the algorithm
    given previously. We need to say what each affix can combine with, and we need
    to assign words to lexical classes. To capture the first part of this, we must
    assume that a root is typically incomplete without certain affixes – that an English
    verb is incomplete without a tense marker, a French adjective is incomplete without
    a gender marker and a number marker, and so on. We will write `A->B` to denote
    an `A` character that is missing a following `B` – for example, an English verb
    root is of the type `V->TNS`, and `A <-B` denotes an `A` that is missing a preceding
    `B`. For example, *-ly* is an adverb missing a preceding adjective, so it is of
    the type `ADV<-ADJ`. Given this notion, we can require that items have to combine
    as they are found – for example, *sadly*, which is made of the adjective *sad*
    and the derivational affix *-ly* can be combined, but *dogly* is not a word because
    the noun *dog* is not what *-ly* requires. Thus, the standard set of English affixes
    becomes as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉这些现象，我们需要对之前给出的算法进行进一步的修改。我们需要说明每个前缀可以与什么结合，并且我们需要将单词分配到词汇类别中。为了捕捉这一部分，我们必须假设一个词根通常在没有某些前缀的情况下是不完整的——例如，一个英语动词没有时态标记是不完整的，一个法语形容词没有性别标记和数量标记是不完整的，等等。我们将用
    `A->B` 来表示缺少后续 `B` 的 `A` 字符——例如，一个英语动词词根是 `V->TNS` 类型，而 `A <-B` 表示缺少前导 `B` 的 `A`。例如，*-ly*
    是一个缺少前导形容词的副词，因此它是 `ADV<-ADJ` 类型。基于这个概念，我们可以要求项目必须按照它们被发现的方式进行组合——例如，*sadly*，它由形容词
    *sad* 和派生前缀 *-ly* 组成，可以组合，但 *dogly* 不是一个词，因为名词 *dog* 不是 *-ly* 所需要的。因此，标准的英语前缀集合如下：
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, the root forms of nouns, verbs, and adjectives are assigned the types
    `n->num`, `v->tns`, and `a->cmp`. Now, analyzing a word such as *smaller* involves
    combining *small* (`adj->cmp`) and the suffix, *–er* (`cmp`), while the analysis
    of *redevelopments* involves combining *re-* (`(v->tns)->(v->tns)`) and *develop*
    (`v->tns`) to produce a new untensed verb, *redevelop*, also of the `(v->tns)`
    type. Now, we can combine this with *–ment* (`(n->num)<-(v->tns)`) to produce
    a new noun root, *redevelopment* (`(n->num)`), and finally combine that with *-s*
    (`(num)`) to produce the plural noun *redevelopments*. If we pick the best analysis
    in each case, we will get the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，名词、动词和形容词的词根被分配了类型 `n->num`、`v->tns` 和 `a->cmp`。现在，分析像 *smaller* 这样的词涉及到将
    *small* (`adj->cmp`) 和后缀 *–er* (`cmp`) 结合起来，而 *redevelopments* 的分析则涉及到将 *re-*
    (`(v->tns)->(v->tns)`) 和 *develop* (`v->tns`) 结合起来，以产生一个新的不带时态的动词 *redevelop*，它也是
    `(v->tns)` 类型。现在，我们可以将这个与 *–ment* (`(n->num)<-(v->tns)`) 结合起来，以产生一个新的名词词根 *redevelopment*
    (`(n->num)`), 最后再与 *-s* (`(num)`) 结合，以产生复数名词 *redevelopments*。如果我们为每种情况选择最佳分析，我们将得到以下结果：
- en: '[PRE30]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that *unreconstructions* leads to an error because the *un-*, *re-*, and
    *-ion* affixes don’t go together – *un-* produces an adjective from a verb, so
    *un-re-construct* is an adjective and -*ion* has to be attached to a verb root.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*unreconstructions* 会导致错误，因为 *un-*、*re-* 和 *-ion* 前缀不能一起使用——*un-* 从动词产生形容词，所以
    *un-re-construct* 是一个形容词，而 *-ion* 必须附加到一个动词词根上。
- en: The more elements you can remove from a complex word, the more likely you are
    to arrive at a root that is known to carry an emotional charge. If you can work
    out that *disastrously* is *disaster+ous+ly*, then you will be able to make use
    of the fact that *disaster* is a highly negative word to detect the negative overtones
    of *disastrously*; if you can spot that *enthusiastic* and *enthusiastically*
    are *enthusiast+ic* and *enthusiast+ic+al+ly*, then these three words can be treated
    as though they were the same when learning and subsequently applying rules for
    detecting emotions. It is worth noting that some affixes reverse the meaning of
    the words to which they apply – for example, an unexpected event is one that was
    not expected. This has to be taken into account when understanding that *undesirable*,
    for instance, is *un+desire+able*, where *desire* is a generally positive term
    but the prefix reverses its meaning and hence suggests that a text that contains
    it will be negative.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你能从一个复杂词中移除的元素越多，你越有可能到达一个已知带有情感电荷的词根。如果你能分析出 *disastrously* 是由 *disaster+ous+ly*
    组成的，那么你将能够利用 *disaster* 是一个高度负面词汇的事实来检测 *disastrously* 的负面含义；如果你能发现 *enthusiastic*
    和 *enthusiastically* 是由 *enthusiast+ic* 和 *enthusiast+ic+al+ly* 组成的，那么这三个词在学习时以及随后应用检测情感的规则时可以被视为相同的。值得注意的是，一些前缀会反转它们所应用词汇的含义——例如，一个出乎意料的事件是一个没有被预期的事件。在理解例如
    *undesirable* 是由 *un+desire+able* 组成时，必须考虑到这一点，其中 *desire* 是一个通常具有积极含义的术语，但前缀反转了它的含义，因此暗示包含它的文本将是负面的。
- en: Similar phenomena occur in many other languages, with affixes either adding
    information to the base word or changing its meaning and/or class. In many cases,
    such as in Romance languages, the root will require multiple affixes to complete
    itself. In the English cases mentioned previously, we saw several examples where
    a word was made out of multiple components, but all such cases involved at most
    one inflectional affix plus one or more derivational ones.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他语言中也会出现类似的现象，词缀要么向基词添加信息，要么改变其意义和/或类别。在许多情况下，例如在罗曼语系中，词根需要多个词缀才能完整。在之前提到的英语例子中，我们看到了一些由多个成分组成的词的例子，但所有这些情况最多只涉及一个屈折词缀和一个或多个派生词缀。
- en: 'Consider, for instance, the adjective *noir*: this, like most French adjectives,
    has four forms – *noir*, *noire*, *noirs*, and *noires*. We can easily capture
    this pattern by saying that a French adjectival root is of the type`(a->num)->gen`
    (note the bracketing – the first thing that has to be found is the gender marker,
    and only once that has been found do we have `a->num` – that is, an adjective
    looking for a number marker). Now, let’s say we have a set of affixes, like so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以*noir*这个形容词为例，这就像大多数法语形容词一样，有四种形式——*noir*，*noire*，*noirs*和*noires*。我们可以通过以下方式轻松捕捉这种模式：一个法语形容词词根的类型为`(a->num)->gen`（注意括号——必须首先找到性别标记，只有找到性别标记后，我们才有`a->num`——即形容词寻找数量标记）。现在，假设我们有一组词缀，如下所示：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With this, we can easily decompose the various forms of *noir*. We will also
    need a set of spelling change rules since some adjectives change their form when
    we add the various suffixes – for example, adjectives that end with *-if* (*sportif*,
    *objectif*) change their feminine endings to *–ive* (singular) and *–ives* (plural),
    so we need spelling rules such as the following, which says that the *–ive* sequence
    could arise from adding *-e* to the end of a word that ends with *if*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以轻松地分解*noir*的各种形式。我们还需要一套拼写变化规则，因为一些形容词在添加各种后缀时，其形式会发生变化——例如，以*-if*结尾的形容词（如*sportif*，*objectif*）在添加各种后缀后，其阴性形式会变为*–ive*（单数）和*–ives*（复数），因此我们需要如下的拼写规则，该规则说明*–ive*序列可能是由在以*if*结尾的词的末尾添加*-e*而产生的：
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This rule will account for the four forms (*sportif*, *sportive*, *sportifs*,
    and *sportives*), with the *e* marking the fact that *sportif* and *sportifs*
    are pronounced with an unvoiced following consonant and *sportive* and *sportives*
    are pronounced with the voiced version of the consonant.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 此规则将解释四种形式（*sportif*，*sportive*，*sportifs*和*sportives*），其中*e*标记表明*sportif*和*sportifs*发音时跟随着一个清辅音，而*sportive*和*sportives*发音时跟随着一个浊辅音。
- en: 'The situation becomes considerably more complicated when we come to deal with
    verbs. Consider the following conjugation table for the regular verb *regarder*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理动词时，情况会变得相当复杂。以下为规则动词*regarder*的屈折变化表：
- en: '|  | **Present** | **Imperfect** | **Future** | **Conditional** | **Subjunctive**
    | **Imperfect subj.** |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | **现在时** | **过去时** | **将来时** | **条件时** | **虚拟时** | **虚拟过去时** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| je | regarde | regardais | regarderai | regarderais | regarde | regardasse
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| je | regarde | regardais | regarderai | regarderais | regarde | regardasse
    |'
- en: '| tu | regardes | regardais | regarderas | regarderais | regardes | regardasses
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| tu | regardes | regardais | regarderas | regarderais | regardes | regardasses
    |'
- en: '| il | regarde | regardait | regardera | regarderait | regarde | regardât |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| il | regarde | regardait | regardera | regarderait | regarde | regardât |'
- en: '| nous | regardons | regardions | regarderons | regarderions | regardions |
    regardassions |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| nous | regardons | regardions | regarderons | regarderions | regardions |
    regardassions |'
- en: '| vous | regardez | regardiez | regarderez | regarderiez | regardiez | regardassiez
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| vous | regardez | regardiez | regarderez | regarderiez | regardiez | regardassiez
    |'
- en: '| ils | regardent | regardaient | regarderont | regarderaient | regardent |
    regardassent |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ils | regardent | regardaient | regarderont | regarderaient | regardent |
    regardassent |'
- en: Figure 4.2 – Conjugation table for “regarder”
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – “regarder”的屈折变化表
- en: 'There are some easy-to-spot regularities in this table – for example, that
    the future and conditional forms all contain the *-er* suffix, and that the imperfect
    and conditional forms have the same set of person affixes. There are quite a lot
    of semi-regularities that don’t carry over completely – for example, the subjunctive
    and imperfect subjunctive have very similar (but not identical) sets of person
    endings. It is very difficult to do anything useful with the semi-regularities,
    so the best that we can easily do is specify that a French verb requires a mood
    marker and a person marker – that is, that *regard* is of the `(v->person)->mood`
    type (as with the type for adjectives, this says that you have to supply the mood
    marker first to get something of the `(v->person)` type and then look for the
    person marker). Now, we can supply the collection of affixes, which can then be
    used to analyze input text:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中存在一些容易发现的规律——例如，未来和条件形式都包含 *-er* 后缀，以及不完美和条件形式具有相同的人称词缀集。存在相当多的半规律性，它们并不完全适用——例如，虚拟式和不完美虚拟式具有非常相似（但不完全相同）的人称词尾。处理这些半规律性非常困难，所以我们能轻易做到的最好的事情是指定法语动词需要一个情态标记和一个人称标记——也就是说，*regard*
    是 `(v->person)->mood` 类型（与形容词的类型一样，这意味着你必须首先提供情态标记，以获得 `(v->person)` 类型的某种东西，然后寻找人称标记）。现在，我们可以提供词缀集合，然后可以使用这些词缀来分析输入文本：
- en: '[PRE33]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These affixes can then be used to reduce verbs to their base forms – *regardions*
    and *regarderions*, for instance, become *regard+ions* and *regard+er+ions*, respectively
    – so that different variants of the same word can be recognized.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词缀可以用来将动词还原到其基本形式——例如，*regardions* 和 *regarderions* 分别变为 *regard+ions* 和 *regard+er+ions*——这样就可以识别同一单词的不同变体。
- en: 'Simply using this table will overgenerate, incorrectly recognizing, for instance,
    *regardere* as *regard+er+e*. This may not matter too much since people don’t
    generally write incorrect forms (and maybe if they do, it is helpful to recognize
    them anyway, as with the English examples of *stealed* and *sliping* mentioned
    previously). More significantly, different verbs have substantially different
    conjugation tables that require different sets of affixes:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地使用这个表格会导致过度生成，错误地识别，例如，将 *regardere* 识别为 *regard+er+e*。这可能不是很重要，因为人们通常不会写错形式（也许如果他们真的写了，识别它们也是有帮助的，就像之前提到的英语例子中的
    *stealed* 和 *sliping*）。更重要的是，不同的动词有不同的词形变化表，需要不同的词缀集：
- en: '|  | **Present** | **Imperfect** | **Future** | **Conditional** | **Subjunctive**
    | **Imperfect subj.** |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | **现在时** | **过去时** | **将来时** | **条件时** | **虚拟式** | **不完美虚拟式** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| je | faiblis | faiblissais | faiblirai | faiblirais | faiblisse | faiblisse
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| je | faiblis | faiblissais | faiblirai | faiblirais | faiblisse | faiblisse
    |'
- en: '| tu | faiblis | faiblissais | faibliras | faiblirais | faiblisses | faiblisses
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| tu | faiblis | faiblissais | faibliras | faiblirais | faiblisses | faiblisses
    |'
- en: '| il | faiblit | faiblissait | faiblira | faiblirait | faiblisse | faiblît
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| il | faiblit | faiblissait | faiblira | faiblirait | faiblisse | faiblît
    |'
- en: '| nous | faiblissons | faiblissions | faiblirons | faiblirions | faiblissions
    | faiblissions |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| nous | faiblissons | faiblissions | faiblirons | faiblirions | faiblissions
    | faiblissions |'
- en: '| vous | faiblissez | faiblissiez | faiblirez | faibliriez | faiblissiez |
    faiblissiez |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| vous | faiblissez | faiblissiez | faiblirez | faibliriez | faiblissiez |
    faiblissiez |'
- en: '| ils | faiblissent | faiblissaient | faibliront | faibliraient | faiblissent
    | faiblissent |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ils | faiblissent | faiblissaient | faibliront | faibliraient | faiblissent
    | faiblissent |'
- en: Figure 4.3 – Conjugation table for “faiblir”
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 - “faiblir”的词形变化表
- en: Here, several (but not all) of the tense markers that were empty for *regard*
    are now *-iss*, the marker for the future and conditional tenses is *ir*, and
    some of the person markers for the present tense are different. We can add (and
    indeed will have to) these to our table, but we would also like to make sure that
    the right affixes get applied to the right verb.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于 *regard* 而言为空的几个（但不是全部）时态标记现在是 *-iss*，表示未来和条件时态的标记是 *ir*，而一些表示现在时的人称标记是不同的。我们可以将这些添加到我们的表格中（实际上我们必须这样做），但我们还想确保正确的词缀被应用到正确的动词上。
- en: To do this, we have to be able to say more about words and affixes than simply
    assigning them a single atomic label. In English, we want to be able to say that
    *–ly* attaches to participles but not to tensed forms (for example, that *unexpectedly*
    and *unsurprisingly* are *un+expect+ed+ly* and *un+surprise+ing+ly*, but that
    *unexpectly* and *unsurprisesly* are not words). We want to be able to say that
    *regard* is an *er* verb and *faibl* is an *ir* verb, with an empty imperfect
    marker for *er* verbs and *iss* as the imperfect marker for *ir* verbs. In general,
    we want to be able to say quite detailed things about words and their affixes
    and to be able to copy information from one to the other (for example, that the
    verb root will get its tense and form from the tense affix).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '要做到这一点，我们必须能够对单词和词缀说更多，而不仅仅是给它们分配一个单一的原子标签。在英语中，我们希望能够说 *–ly* 附加到分词上，但不附加到时态形式上（例如，*unexpectedly*
    和 *unsurprisingly* 是 *un+expect+ed+ly* 和 *un+surprise+ing+ly*，但 *unexpectly* 和
    *unsurprisesly* 不是单词）。我们希望能够说 *regard* 是一个 *er* 动词，*faibl* 是一个 *ir* 动词，*er* 动词有一个空的不定式标记，而
    *ir* 动词的不定式标记是 *iss*。一般来说，我们希望能够对单词及其词缀说相当详细的事情，并且能够从一方复制信息到另一方（例如，动词词根将从时态词缀那里获得时态和形式）。 '
- en: 'We can do this by extending our notation to allow for features – that is, properties
    that distinguish one instance of a word from another. For example, we can say
    that *sleeps* is `[hd=v, tense=present, finite=tensed, number=singular, person=third]`
    and *sleeping* is `[hd=v, tense=present, finite=participle]`. We can, for instance,
    change the description of the base form of a verb from `v->tns` to `v[tense=T,
    finite=F, number=N, person=P]->tense[tense=T, finite=F, number=N, person=P],`
    – i.e., a base verb isn’t just something that needs a tense marker; it will also
    inherit the values of the features for tense, finiteness, number, and person from
    that affix. The verbal suffixes then become as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过扩展我们的符号来允许特征——也就是说，区分一个单词实例与其他实例的性质。例如，我们可以说 *sleeps* 是 `[hd=v, tense=present,
    finite=tensed, number=singular, person=third]`，而 *sleeping* 是 `[hd=v, tense=present,
    finite=participle]`。例如，我们可以将动词的基本形式的描述从 `v->tns` 改为 `v[tense=T, finite=F, number=N,
    person=P]->tense[tense=T, finite=F, number=N, person=P]` —— 即，一个基本动词不仅仅是需要时态标记的东西；它还将从那个词缀那里继承时态、限定性、数和人称的特征值。那么，动词后缀将如下所示：
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This code block says that adding an empty suffix to a verbal root will give
    you the infinitive form or the present tense, adding *-ing* will give you the
    present participle, and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码块说明，给动词词根添加一个空后缀将得到不定式形式或现在时，添加 *-ing* 将得到现在分词，依此类推。
- en: 'This general approach can be used to assign French verbs to the various classes
    of *-er*, *-ir*, *-re*, and irregular cases, to ensure that tense and agreement
    markers on Arabic verbs match each other, as well as to ensure that complex sequences
    of derivational and inflectional affixes are handled properly. If you want to
    get at the root of a surface form and see exactly what properties it has, you
    will have to do something like this. It does, however, come at a price:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种一般方法可以用来将法语动词分配到各种类别的 *-er*、*-ir*、*-re* 和不规则形式，以确保阿拉伯语动词的时态和一致标记相互匹配，以及确保处理复杂的派生和屈折词缀序列得当。如果你想找到表面形式的根源并确切地看到它具有哪些属性，你必须做类似的事情。然而，这确实需要付出代价：
- en: 'You have to say more about the words in your lexicon and a lot more about the
    affixes themselves. To realize that *kissed* can be the past tense or the past
    participle or the passive participle of *kiss* but that *walked* can only be the
    past tense or past participle of *walk*, you have to know what the *-ed* suffix
    does and you also have to know that *walk* is an intransitive verb and hence does
    not have a passive form. The more you want to know about the properties of a surface
    form, the more you have to say about its root and about the affixes that are attached
    to it. This is hard work and can make it very hard to maintain the lexicon. This
    can be seen in an extreme form in the lexicon for the most widely used morphological
    analyzer for Arabic, namely the SAMA lexicon (Buckwalter, T., 2007). A typical
    entry in the SAMA lexicon looks like this:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This entry contains six distinct variants for a verb meaning something such
    as desire. The first part of a variant is what the stem looks like with diacritics
    omitted (diacritics are things such as short vowels and other marks that affect
    the pronunciation of the word, and are generally omitted in written Arabic), the
    second part is what the stem would look like if the diacritics were written, the
    third is a label that specifies what affixes the stem will combine with, and the
    last is the English gloss. To add a single entry to the lexicon, you have to know
    what all the surface forms look like and which class they belong to – for example,
    the stem &b (ؤب) is the `IV_Vd` form of this word. To do that, you have to know
    what it means to say that something is the `IV_Vd` form of the word. And then,
    there are over 14K prefixes and nearly 15k suffixes, each with a complex label
    saying what stems it attaches to.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an extreme case: we need five inflectional affixes for English verbs
    and maybe another 10 derivational ones, and around 250 inflectional affixes for
    French verbs. Nonetheless, the point is clear: if you want to get complete and
    correct decompositions of complex words, you need to provide a lot of information
    about words and suffixes. (See Hoeksema, 1985, for more on describing words in
    terms that specify what they need to complete themselves.)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting this information requires more work than just splitting the surface
    form into pieces, and can markedly slow things down. `morphy` runs at about 150K
    words/second, but it does very little with compound words such as *unexpectedly*
    – if a word like this is in the set of exceptions, then it is returned without
    being decomposed; if it is not (for example, *unwarrantedly*), then it will simply
    return nothing at all. The analyzer provided in the code repository runs at 27K
    words/second if we use simple labels and no spelling rules, 17.2K words/second
    with simple labels and spelling rules, 21.4K words/second with complex labels
    and no spelling rules, and 14.7K words/second with complex labels and spelling
    rules, and the SAMA lexicon runs at about 9K words/second. The analyzer from the
    code repository and the SAMA lexicon also provide all the alternative analyses
    of a given form, whereas `morphy` just returns the first match it finds.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The lesson is clear: if you want words that have been stripped right down to
    their roots, you will have to provide a substantial amount of clear information
    about word classes and about the effects that the various affixes have. If you
    take a simple-minded approach and are not too worried about getting right to the
    heart of each form, and about finding out its exact properties, then you can do
    the task substantially faster, but even at 14.7K words/second, morphological analysis
    not going to be a major bottleneck.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Compound words
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at how to find the root element of a complex
    word. This is important for our overall task since a large part of the emotional
    content of a text is determined simply by the choice of words. A tweet such as
    *My joy at finding that you loved me as much as I love you has filled my heart
    with contentment* is overwhelmingly likely to be tagged as expressing **joy**
    and **love**, and the form *loved* will contribute as much to this as the form
    *love*. It can also happen, however, that a group of words expresses something
    quite different from what they express individually – that a tweet containing
    the phrases *greenhouse gases* and *climate change* is much more likely to be
    negative than one that just contains *greenhouse* and *change*, and that one that
    contains the phrase *crime prevention* is much more likely to be positive than
    one that just contains *crime* or *prevention*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a fairly marginal effect in languages that use white space to separate
    tokens because compound words of this kind tend to be written either with no separator
    or with a dash: a *blackbird* is not just a bird that is black, and a *greenhouse*
    is neither a house nor green. In some languages, however, each token is potentially
    a word and each sequence of tokens is also potentially a word, with no white space
    to mark the boundaries around sequences. In Chinese, for instance, the words 酒
    and 店 mean *wine* and *shop*, but the sequence 酒店 means *hotel*. Similarly, the
    word 奄 means *suddenly* but the sequence 奄奄 means *dying*. While it is easy enough
    to see the connection between *wine shop* and *hotel*, there is more to a hotel
    than just somewhere that sells wine; and it is all but impossible to see why *suddenly-*'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*suddenly* would mean *dying*. Similarly, the four characters 新, 冠, 疫, and
    情, which individually mean *new crown epidemic feeling*, meaning COVID-19, when
    taken as a group, are hard to predict. Also, a tweet about COVID-19 is much more
    likely to be negative than one about *new crown epidemic feeling*. Therefore,
    it is important to be able to detect such compounds even when there is no typographical
    evidence for them, particularly since they are fluid, with new ones being created
    all the time (新冠疫情 would not have meant COVID-19 in 2018!).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The key to finding such compounds is observing that the elements of a compound
    will occur next to each much more frequently than you would expect just by chance.
    The standard way to detect this is by using **pointwise mutual information** (**PMI**)
    (Fano, r. M., 1961). The idea here is that if two events, E1 and E2, are unconnected,
    then the likelihood of E2 occurring immediately after E1 should be the same as
    the likelihood of E2 occurring immediately after some other event. If E1 and E2
    have nothing to do with each other, then the likelihood of E2 occurring immediately
    after E1 is *prob(E1)*prob(E2)*. If we find that they are occurring together more
    often than that, we can conclude that they have some connection. If E1 and E2
    are words, we can hypothesize that if they co-occur much more often than expected,
    then they may be a compound. Therefore, we can calculate the PMI of two words
    as *log(prob(W1+W2)/(prob(W1)*prob(W2))* (taking logs smooths out values that
    are returned but this is not crucial to the approach).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The machinery for doing this is implemented in `chapter4.compounds`. If we
    apply it to a collection of 10 million words from the BNC, we will see that the
    top 20 pairs that occur at least 300 times are largely fixed phrases, often Latin
    (*inter-alia*, *vice-versa*, *ad-hoc*, *status-quo*, and *de-facto*) or technical/medical
    terms (*ulcerative-colitis*, and *amino-acids*). The following scores are of the
    `(<PMI-score>, <pair>, <``freq>)` form:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Even in English, pairs such as `crime-prevention` and `greenhouse-gases`, which
    have high PMI scores (the median pair in our set is `(``5.48, needs-help, 121)`,
    and `crime-prevention` and `greenhouse-gases` are both in the top 2% of the entire
    set), can carry an emotional weight that is different from the emotions associated
    with the components:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: So, it may be worth looking at the emotional weights associated with particularly
    frequent compound terms even in English. For other languages, this may be even
    more important.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Tagging and parsing
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have spent quite a long time looking at individual words – finding tokens
    in text, decomposing those into smaller elements, looking at the way that spelling
    changes happen at the boundaries between word parts, and considering the problems
    that arise, particularly in languages that do not use white space to separate
    tokens when words come together to form compounds. A huge part of the task of
    emotion detection relies on identifying words that carry emotions, so it makes
    sense to be careful when looking at words.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*,
    for most NLP tasks, finding the relationships between words is just as important
    as finding the words themselves. For our current task, which involves finding
    the general emotional overtones of a short informal text, this may not be the
    case. There are two major questions to be answered here:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Does assigning a set of relations between words help with emotion detection?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it possible to assign relations to elements of informal text?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normal texts are divided into sentences – that is, groups of words separated
    by punctuation marks that describe the words (or query a description of the words).
    A properly formed sentence has a main verb that denotes an event or a state and
    a set of satellite phrases that either describe the participants in this event
    or state or say something about where, when, how, or why it took place. Consider
    the second question: if we use a rule-based parser we get something similar to
    the following tree (the exact form of the tree will depend on the nature of the
    rules being used; we are using a parser that was designed to deal cleanly with
    out-of-position items (Ramsay, A. M., 1999), but any rule-based parser will produce
    something like this):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Rule-based parse of “Is it possible to assign relations to elements
    of normal text”](img/B18714_04_04.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Rule-based parse of “Is it possible to assign relations to elements
    of normal text”
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: This tree says that the given sentence is a query about the existence of a particular
    kind of possibility, namely the possibility of assigning relations to elements
    of normal text. To understand this sentence properly, we have to find these relations.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: The preceding tree was generated by a rule-based parser (Ramsay, A. M., 1999).
    As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*, rule-based
    parsers can be fragile when confronted with texts that do not obey the rules,
    and they can be slow. Given that informal texts are, more or less by definition,
    likely not to obey the rules of normal language, we will consider using a data-driven
    parser to analyze them.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by looking at two tweets that were chosen fairly randomly from
    the SEMEVAL training data:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: These tweets do not obey the rules of normal well-formed text. They contain
    elements that simply do not appear in normal language (usernames, hashtags, emojis,
    emoticons), they contain non-standard uses of punctuation, they very often have
    no main verb, they contain deliberate misspellings and words made out of repeated
    elements (*hahahahahahahahaha*), and so on. Our rule-based parser will just fail
    if we try to use it to analyze them. What happens if we were to use a data-driven
    one (we use the NLTK pre-trained version of the MALT parser (Nivre, 2006) with
    the NLTK recommended tagger, but very little changes if we choose another data-driven
    parser or a different tagger)?
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Just using MALT with the standard tagger, we get the following trees for *@PM
    @KF Very misleading heading.* and *#anxious don’t know why ................. #worry
    (: slowly going #**mad hahahahahahahahaha*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – MALT parses of informal texts](img/B18714_04_05.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – MALT parses of informal texts
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: There are two problems here. The first is that the tagger and parser are data-driven
    – that is, the decisions they make are learned from a labeled corpus, and the
    corpus they have been trained on does not contain the kind of informal text that
    is found in tweets. Secondly, and more significantly, informal texts often contain
    fragments jumbled up together, so it is not possible to assign relationships to
    such a text in a way that makes a single coherent tree.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: The first of these problems could be overcome by marking up a corpus of tweets.
    This would, of course, be tedious, but no more tedious than doing so for a corpus
    of standard texts. The second problem raises its head again here because to mark
    up a piece of text, you have to have an underlying theory of what POS tags to
    use and what kinds of relationships exist between the elements of the text. If
    you assume that the only POS tags that you can use are the standard NN, VV, JJ,
    DET, IN, CC, and PR, then you cannot assign the correct tags to tweet elements,
    since these are new and are not of the standard types. And if you assume that
    only the standard relations between words can be used, then you cannot assign
    the correct roles to tweet items since they do not tend to occupy these roles
    – the emoticon *(:* and the word *hahahahahahahahaha* are not the kinds of things
    that can play these roles. So, if we are going to mark up a collection of tweets
    to train a tagger and parser, we are going to have to come up with a theory of
    the structure of such texts. Constructing a treebank is not a theory-free activity.
    The guidelines given to the annotators are, by definition, an informal specification
    of a grammar, so unless you have a clear idea of what roles things such as hashtags
    and emojis can play, and a clear understanding of when, for instance, an emoji
    should be seen as a comment on the tweet as a whole and when it should be seen
    as a comment on a particular element, it is just not possible to construct a treebank.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweets often contain well-formed fragments, so maybe we can get some benefit
    from finding these:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '*Never regret anything that once made you smile :) #**positive*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '*Literally hanging on by a thread need some taylor ray tonight loving a bad
    dog sucks #taylorrayholbrook #**hurting @TRT*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '*was one moron driving his oversize tonka truck with the big flag in the bed
    back and forth blaring country music.* 😐 *#**disappointment*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '*#ThingsIveLearned The wise #shepherd never trusts his flock to a #smiling
    wolf. #TeamFollowBack #**fact #wisewords*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of things that are worth doing to start with. No existing
    parser, rule-based or data-driven, is going to do anything sensible with tags,
    usernames, emojis, or emoticons at the beginning or end of a sentence, so we may
    as well strip those off before attempting to find parseable fragments. Hashtags
    in the middle of a tweet are often attached to meaningful words, so we may as
    well remove those too. This will give us the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '*Never regret anything that once made* *you smile*'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '*was one moron driving his oversize tonka truck with the big flag in the bed
    back and forth blaring* *country music.*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '*Literally hanging on by a thread need some taylor ray tonight loving a bad*
    *dog sucks*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '*The wise shepherd never trusts his flock to a* *smiling wolf*.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'These all contain well-formed fragments: the first and fourth are, indeed,
    well-formed sentences, and the other two contain well-formed fragments. What happens
    if we try to parse them using our rule-based tagger and then use MALT?'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'The two parsers give essentially the same answers for the first and fourth
    of these (rule-based analysis on the left, MALT analysis on the right), save that
    the rule-based parser gives the wrong attachment to *to a smiling wolf*. No parser
    can be expected to get the attachment of such phrases right every time, and apart
    from that, the two behave perfectly sensibly given the rules that either explicitly
    or implicitly underly them:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Rule-based and MALT parses for “The wise shepherd never trusts
    his flock to a smiling wolf”](img/B18714_04_06.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Rule-based and MALT parses for “The wise shepherd never trusts
    his flock to a smiling wolf”
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Rule-based and MALT parses for “Never regret anything that once
    made you smile”](img/B18714_04_07.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Rule-based and MALT parses for “Never regret anything that once
    made you smile”
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'So, either approach will suffice for these examples. When we consider the other
    cases, the situation becomes more difficult. The rule-based parser fails to produce
    any overall analysis for *was one moron driving his oversize tonka truck with
    the big flag in the bed back and forth blaring country music* or *Literally hanging
    on by a thread need some taylor ray tonight loving a bad dog sucks*. Both these
    sentences are simply too long for it to handle because there are too many options
    to be explored. MALT produces analyses for both cases:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – MALT parse for “was one moron driving his oversize tonka truck
    with the big flag in the bed back and forth blaring country music”](img/B18714_04_08.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – MALT parse for “was one moron driving his oversize tonka truck
    with the big flag in the bed back and forth blaring country music”
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – MALT parse for “Literally hanging on by a thread need some taylor
    ray tonight loving a bad dog sucks”](img/B18714_04_09.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – MALT parse for “Literally hanging on by a thread need some taylor
    ray tonight loving a bad dog sucks”
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is reasonable – the analysis of *was* as a copula is questionable,
    and the attachment of *back and forth* is wrong, but by and large, it captures
    most of the relevant relationships. The second is just a mess. The problem with
    the second is that the text contains several disjoint elements – *Literally hanging
    on by a thread, need some taylor ray tonight* and *loving a bad dog sucks* – but
    the parser has been told to analyze the whole thing and hence it has analyzed
    the whole thing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing to be done about this. Data-driven parsers are generally designed
    to be robust, so even if the text they are given is entirely ungrammatical or
    contains grammatical fragments but is not a single coherent whole, they will return
    a single tree **and there is no way of telling that what they return is problematic**.
    This holds pretty much by definition. If a text doesn’t have a sensible structure
    – that is, it can’t be assigned a sensible parse tree – then a robust parser will
    assign it a tree that is not sensible. A rule-based parser, on the other hand,
    will just fail if the text does not obey the rules it expects it to obey or is
    just too long and complex to handle.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: So, there seems little point in including a parser in the preprocessing steps
    for sentiment mining. Rule-based parsers will frequently just fail when confronted
    with informal texts, even if they are preprocessed to strip non-textual items
    off the front and back and to do various other simple steps. Data-driven parsers
    will always produce an answer, but for text that does not obey the rules of normal
    language, this answer will often be nonsensical and **there is no easy way of
    telling which analyses are reasonable and which are not**. And if there is no
    point in including a parser, then there is also no point in including a tagger,
    since the sole function of a tagger is to preprocess the text for a parser. It
    might be possible to use a rule-based parser to check the output of a data-driven
    parser – if the output of the data-driven one is reasonable, then using it to
    guide the rule-based one will enable the rule-based one to verify that it is indeed
    acceptable without exploring vast numbers of dead-ends.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: However, it is very unclear how a typical machine learning algorithm would be
    able to make use of such trees, even if they could be reliably found. The code
    repository for this book includes code for tagging tweets and running a data-driven
    parser on the results, and some examples can be explored further there, but since
    these are not generally useful steps for our overall goal, we will not discuss
    them further here.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at the issues that arise when you try to split
    a piece of text into words by looking at how to split text into tokens, how to
    find the basic components of words, and how to identify compound words. These
    are all useful steps for assigning emotions to informal texts. We also looked
    at what happens when we try to take the next step and assign grammatical relations
    to the words that make up an informal text, concluding that this is an extremely
    difficult task that provides comparatively little benefit for our overall task.
    We had to look quite carefully at this step, even though we believe it is not
    all that useful, since we need to understand why it is so hard and why the results
    of even the best parsers cannot be relied on.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Buckwalter, T. (2007). *Issues in Arabic Morphological Analysis*. Arabic Computational
    Morphology, 23-42.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chomsky, N., & Halle, M. (1968). *The Sound Pattern of English*. MIT Press.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fano, R. M. (1961). *Transmission of Information: A Statistical Theory of Communications*.
    MIT Press.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoeksema, J. (1985). *Categorial Morphology*. Garland Publishing.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koskiennemi, K. (1985). *A General Two-level Computational Model for Word-form
    Recognition and Production*. COLING-84, 178-181.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leech, G., Garside, R., & Bryant, M. (1994, August). *CLAWS4: The Tagging of
    the British National Corpus*. COLING 1994 Volume 1: The 15th International Conference
    on Computational Linguistics. [https://aclanthology.org/C94-1103](https://aclanthology.org/C94-1103)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nivre, J., Hall, J., & Nilsson, J. (2006). *MaltParser: A language-independent
    system for data-driven dependency parsing*. Proceedings of the International Conference
    on Language Resources (LREC), 6, 2216-2219.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramsay, A. M. (1999). *Direct parsing with discontinuous phrases*. Natural Language
    Engineering, 5(3), 271-300.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3:Approaches
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you’ll learn how we go about the task of EA. We will discuss various
    models, explain how they work, and evaluate the results.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and Vector
    Space Models*'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naïve Bayes*'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18714_07.xhtml#_idTextAnchor144), *Support Vector Machines*'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18714_08.xhtml#_idTextAnchor157), *Neural Networks and Deep
    Neural Networks*'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18714_09.xhtml#_idTextAnchor172), *Exploring Transformers*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
