- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preprocessing – Stemming, Tagging, and Parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can start assigning emotions to texts, we have to carry out a range
    of preprocessing tasks to get to the elements that carry the information we want.
    In [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*, we briefly
    covered the various components of a generic NLP system, but without looking in
    detail at how any of these components might be implemented. In this chapter, we
    will provide sketches and partial implementations of the tools that are most likely
    to be useful for sentiment mining – where we give a partial implementation or
    a code fragment for something, the full implementation is available in the code
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the earlier stages of the language processing pipeline in detail.
    The texts that are most often used for sentiment mining tend to be very informal
    – tweets, product reviews, and so on. This material is often ungrammatical and
    contains made-up words, misspellings, and non-text items such as emoticons, images,
    and emojis. Standard parsing algorithms cannot deal with this kind of material,
    and even if they could, the analyses that they produce would be very hard to work
    with: what would the parse tree for *@MarthatheCat Look at that toof! #growl*
    look like? We will include material relating to tagging (which is generally only
    useful as a precursor to parsing) and parsing, but the focus will be largely on
    the lowest level steps – reading the text (not as straightforward as it seems),
    decomposing words into parts, and identifying compound words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Readers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word parts and compound words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing, morphology, and stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compound words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging and parsing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Readers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we can do anything, we need to be able to read the documents that contain
    the texts – in particular, the training data that will be used by the preprocessing
    algorithms and the sentiment mining algorithms themselves. These documents come
    in two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data for the preprocessing algorithms**: A number of the algorithms
    that we use for finding words, decomposing them into smaller units, and combining
    them into larger groups require training data. This can be raw text or it can
    be annotated with suitable labels. In either case, we need a lot of it (for some
    tasks, you need hundreds of millions of words, maybe more), and it is often more
    convenient to use data from external sources than to try to compile it yourself.
    Unfortunately, externally supplied data does not always come in a single agreed
    format, so you need **readers** to abstract away from the details of the format
    and organization of this data. To take a simple example, data for training a program
    to assign part-of-speech tags to text needs to be given text that has been labeled
    with such tags. We will carry out some experiments here using two well-known corpora:
    the **British National Corpus (BNC)** and the **Universal Dependency Tre ebanks
    (UDT)**. The BNC provides text with complex XML-like annotations, such as the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This says that *factsheet* is an NN1 and *what* is a pronoun.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UDT provides text as tab-separated files where each line represents information
    about a word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This says that *what* is a pronoun and *is* is an auxiliary. To use these to
    train a tagger, we have to dig out the information that we want and convert it
    into a uniform format.
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data for the sentiment analysis algorithms**: Almost all approaches
    to assigning emotions to texts employ machine learning algorithms, and hence again
    require training data. As noted in [*Chapter 2*](B18714_02.xhtml#_idTextAnchor061),
    *Building and Using a Dataset*, it is often convenient to use externally supplied
    data, and as with the data used for the preprocessing algorithms, this data can
    come in a variety of formats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Training data may be supplied as text files, as directory trees with text files
    as leaves, or as SQL or other databases. To make matters more complex, there can
    be very large amounts of data (hundreds of millions of items, even billions of
    items), to the point where it is not convenient to have all the data in memory
    at once. Therefore, we start by providing a reader generator function that will
    traverse a directory tree until it reaches a leaf file whose name matches an optional
    pattern and then uses an appropriate reader to return items one at a time from
    this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`reader` will return a generator that walks down through the directory tree
    specified by `path` until it finds leaf files whose names match `pattern` and
    then uses `dataFileReader` to iterate through the data in the given files. We
    use a generator rather than a simple function because corpora can be very large
    and reading all the data contained in a corpus into memory at once can become
    unmanageable. The disadvantage of using a generator is that you can only iterate
    through it once – if you want to solidify the results of using a reader, you can
    use `list` to store them as a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will create a generator, `r`, for reading words from the **BNC**. The BNC
    is a widely used collection of documents, though its status as a resource for
    training, and especially testing, taggers is slightly unclear since the tags for
    the vast majority of the material were assigned programmatically by the CLAWS4
    tagger (Leech et al., 1994). This means that any tagger trained on it will be
    learning the tags assigned by CLAWS4, so unless these are 100% accurate (which
    they are not), then it will not be learning the “real” tags. Nonetheless, it is
    an extremely useful resource, and can certainly be used as a resource for training
    usable taggers. It can be downloaded from [https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we solidify this generator to a list, `l`, for ease of use. The BNC contains
    about 110 million words, which is a manageable amount of data on a modern computer,
    so storing them as a single list makes sense. However, for larger corpora, this
    may not be feasible, so having the option of using a generator can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The BNC is supplied as a directory tree with subdirectories, `A`, `B`, …, `K`,
    which contain `A0`, `A1`, …, `B0`, `B1`, …, which, in turn, contain `A00.xml`,
    `A01.xml`, …:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Structure of the BNC directory tree](img/B18714_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Structure of the BNC directory tree
  prefs: []
  type: TYPE_NORMAL
- en: 'The leaf files contain header information followed by sentences demarcated
    by `<s n=???>...</s>`, with the words that make up a sentence marked by `<w c5=???
    hw=??? pos=???>???</w>`. Here’s an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To read all the words in the BNC, we need something to dig out the items between
    `<w ...>` and `</w>`. The easiest way to do this is by using a regular expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The pattern looks for instances of either `<w ...>` or `<c ...>` and then looks
    for the appropriate closing bracket since the BNC marks words with `<w ...>` and
    punctuation marks with `<c ...>`. We have to find both and make sure that we get
    the right closing bracket.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given this definition of `BNCWordReader`, we can, as we did previously, create
    a reader to extract all the raw text from the BNC. Other corpora require different
    patterns for extracting the text – for example, the **Penn Arabic Tree Bank**
    (**PATB**) (this is a useful resource for training and testing Arabic NLP tools.
    Unfortunately, it is not free – see the Linguistic Data Consortium ([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/))
    for information about how to obtain it – however, we will use it for illustration
    when appropriate) contains leaf files that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract words from this, we would need a pattern like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the following when applied to the PATB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the similarity between `BNCWordReader` and `PATBWordReader`, we could
    have simply defined a single function called `WordReader` that takes a path and
    a pattern and bound the pattern as required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The same technique can be applied to extract the raw text from a wide range
    of corpora, such as the UDT ([https://universaldependencies.org/#download](https://universaldependencies.org/#download)),
    which provides free access to moderate amounts of tagged and parsed data for a
    large number of languages (currently 130 languages, with about 200K words per
    language for the commoner languages but rather less for others).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the training data for sentiment assignment algorithms comes in a
    variety of formats. As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015),
    *Foundations*, Python already provides a module, `pandas`, for managing training
    sets for generic sets of training data. `pandas` is useful if your training data
    consists of sets of data points, where a data point is a set of `feature:value`
    pairs that describe the properties of the data point, along with a label that
    says what class it belongs to. The basic object in `pandas` is a DataFrame, which
    is a collection of objects where each object is described by a set of `feature:value`
    pairs. As such, a DataFrame is very much like a SQL table, where the names of
    the columns are the feature names and an object corresponds to a row in the table;
    it is also extremely like a nested Python dictionary, where the keys at the top
    level are the feature names and the values associated with those names are index-value
    pairs. And it’s also very like a spreadsheet, where the top row is the feature
    names and the remaining rows are the data points. DataFrames can be read directly
    from all these formats and more (including from a table in a SQL database), and
    can be written directly to any of them. Consider the following extract from a
    set of annotated tweets stored as a MySQL database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `sentiments` table contains rows representing the ID of the annotator who
    annotated a given tweet, the ID of the tweet itself, and the set of emotions that
    the given annotator assigned to it (for example, annotator 8 assigned anger and
    dissatisfaction to tweet 1). This table can be read directly as a DataFrame and
    can be transformed into a dictionary, a JSON object (very similar to a dictionary),
    a string in CSV format, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We therefore do not have to worry too much about actually reading and writing
    the data to be used to train the emotion classification algorithms – DataFrames
    can be read from, and written to, in almost any reasonable format. Nonetheless,
    we still have to be careful about what features we are using and what values they
    can have. The preceding MySQL database, for instance, refers to the IDs of the
    tweets and the annotators, with the text of each tweet being kept in a separate
    table, and it stores each annotator’s assignment of emotions as a single compound
    value (for example, love+joy+optimism). It would have been perfectly possible
    to store the text of the tweet in the table and to have each emotion as a column
    that could be set to 1 if the annotator had assigned this emotion to the tweet
    and 0 otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, each tweet has an explicit ID, as well as a position in the DataFrame;
    the tweet itself is included, and each emotion is a separate column. The data
    here was supplied as a CSV file, so it was read directly as a DataFrame without
    any trouble, but the way it is presented is completely different from the previous
    set. Therefore, we will need preprocessing algorithms to make sure that the data
    we are using is organized the way the machine learning algorithms want it.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, DataFrames have database-like options for selecting data and merging
    tables, so converting from one way of presenting the data into another is reasonably
    straightforward, but it does have to be carried out. There are, for instance,
    advantages to having a single column per emotion and advantages to having a single
    column for all emotions but allowing compound emotions – having a single column
    per emotion makes it easy to allow for cases where a single object may have more
    than one emotion associated with it; having just one column for all emotions makes
    it easy to search for tweets that have the same emotions. Some resources will
    provide one and some the other, but almost any learning algorithm will require
    one or the other, so it is necessary to be able to convert between them.
  prefs: []
  type: TYPE_NORMAL
- en: Word parts and compound words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key to identifying emotions in texts lies with the words that make up those
    texts. It may turn out to be useful to classify words, find similarities between
    words, and find out how the words in a given text are related, but the most important
    things are the words themselves. If a text contains the words *love* and *happy*,
    then it’s very likely to be positive, whereas if it contains the words *hate*
    and *horrible*, it’s very likely to be negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*,
    however, it can be difficult to specify exactly what counts as a word, and hence
    difficult to find the words that make up a text. While the writing systems of
    many languages use white space to split up text, there are languages where this
    does not happen (for example, written Chinese). But even where the written form
    of a language does use white space, finding the units that we are interested in
    is not always straightforward. There are two basic problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Words are typically made up of a core lexeme and several affixes that add to
    or alter the meaning of the core. *love*, *loves*, *loved*, *loving*, *lover*,
    and *lovable* are all clearly related to a single concept, though they all look
    slightly different. So, do we want to treat them as different words or as variations
    on a single word? Do we want to treat *steal*, *stole*, and *stolen* as different
    words or as variations of the same word? The answer is that sometimes we want
    to do one and sometimes the other, but when we do want to treat them as variations
    of the same word, we need machinery to do so.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some items that are separated by white space look as though they are made out
    of several components: *anything*, *anyone*, and *anybody* look very much as though
    they are made out of *any* plus *thing*, *one*, or *body* – it is hard to imagine
    that the underlying structures of *anyone could do that* and *any fool could do
    that* are different. It is worth noting that in English, the stress patterns of
    the spoken language match up with the presence or absence of white space in text
    – *anyone* is pronounced with a single stress on the first syllable, /en/, while
    *any fool* has stress on /en/ and /fool/, so there is a difference, but they also
    have the same structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is tempting to try to deal with this by looking at each White space-separated
    item to see whether it is made out of two (or more) other known units. *Stock
    market* and *stockmarket* seem to be the same word, as do *battle ground* and
    *battleground*, and looking at the contexts in which they occur bears this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there are also clear examples where the compound word is not the same
    as the two words put next to one another. A *heavy weight* is something (anything)
    that weighs a lot, whereas a *heavyweight* is nearly always a boxer; if something
    is *under study*, then someone is studying it, whereas an *understudy* is someone
    who will step in to fill a role when the person who normally performs it is unavailable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is particularly important for languages such as Chinese, which are written
    without white space, where almost any character can be a standalone word but sequences
    of two or more characters can also be words, often with very little connection
    to the words that correspond to the individual characters.
  prefs: []
  type: TYPE_NORMAL
- en: These phenomena occur in all languages. Some languages have very complex rules
    for breaking words into smaller parts, some make a great deal of use of compounds,
    and some do both. These examples gave a rough idea of the issues as they arise
    in English, but in the following discussion of using algorithms to deal with these
    issues, we will look at examples from other languages. In the next section, *Tokenizing,
    morphology, and stemming*, we will look at algorithms for splitting words into
    parts – that is, at ways of recognizing that the word *recognizing* is made up
    of two parts, *recognize* and *-ing*. In the section, *Compound words*, we will
    look at ways of spotting compounds in languages where they are extremely common.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing, morphology, and stemming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The very first thing we have to do is split the input text into **tokens** –
    units that make an identifiable contribution to the message carried by the whole
    text. Tokens include words, as roughly characterized previously, but also punctuation
    marks, numbers, currency symbols, emojis, and so on. Consider the text *Mr. Jones
    bought it for £5.3K!* The first token is *Mr.*, which is a word pronounced /m
    i s t uh/, while the next few are *Jones*, *bought*, *it*, and *for*, then the
    currency symbol, *£*, followed by the number *5.3K* and the punctuation mark,
    *!*. Exactly what gets treated as a token depends on what you are going to do
    next (is *5.3K* a single number or is it two tokens, *5.3* and *K*?), but there
    is very little you can do with a piece of text without splitting it into units
    along these lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to do this is by defining a regular expression where the pattern
    specifies the way the text is to be split. Consider the preceding sentence: we
    need something for picking out numbers, something for abbreviations, something
    for currency symbols, and something for punctuation marks. This suggests the following
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The first part of this pattern says that a number can consist of some digits,
    possibly followed by a comma (to capture cases such as 120,459 for one hundred
    and twenty thousand four hundred and fifty-nine), followed by a point and some
    more digits, and then finally possibly followed by the letter K; the second part
    lists several abbreviations that will normally be followed by a full stop; the
    next two, `n't` and `([A-Za-z](?!n't))*[A-Za-z]`, are fairly complex; `n't` recognizes
    *n’t* as a token, while `([A-Za-z](?!n't))*[A-Za-z]` picks out sequences of alphabetic
    characters not ending with *n’t* so that *hasn’t* and *didn’t* are each recognized
    as two tokens, *has + n’t* and *did + n’t*. The next few just recognize punctuation
    marks, currency symbols, and similar; the last one recognizes sequences of non-alphabetic
    symbols, which is useful for treating sequences of emojis as tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking for instances of this pattern in English texts produces results like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Using regular expressions for tokenization has two advantages: regular expressions
    can be applied extremely fast, so large amounts of text can be tokenized very
    quickly (about three times as fast as the NLTK’s built-in `word_tokenize`: the
    only major difference between the output of the two is that `tokenise` treats
    words such as *built-in* as being made of three components, *built*, *-*, and
    *in*, whereas the NLTK treats them as single compounds, *built-in*); and the patterns
    are completely self-contained, so they can be changed easily (if, for instance,
    you would rather treat each emoji as a separate token, just remove `+` from `[^a-zA-Z\s]+`,
    and if you would rather treat *built-in* as a single compound unit, just remove
    `–` from the list of options) and can also be easily adapted to other languages,
    such as by replacing the character range, `[a-z]`, with the Unicode range of characters
    used by the required language:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Once we have tokenized our text, we are likely to have tokens that are minor
    variants of the same lexeme – *hating* and *hated* are both versions of the same
    lexeme, *hate*, and will tend to carry the same emotional charge. The importance
    (and difficulty) of this step will vary from language to language, but virtually
    all languages make words out of a core lexeme and a set of affixes, and finding
    the core lexeme will generally contribute to tasks such as emotion detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The obvious starting point is to produce a list of affixes and try to chop
    them off the start and end of a token until a known core lexeme is found. This
    requires us to have a set of core lexemes, which can be quite difficult to get.
    For English, we can simply use the list of words from WordNet. This gives us 150K
    words, which will cover most cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will look at the first few characters of the token to see whether they
    are prefixes (allowing for empty prefixes), and then at the next few to see whether
    they are known words, and then at the remainder to see whether it’s a suffix.
    Writing it as a generator makes it easy to produce multiple answers – for example,
    if `WORDS` contains both *construct* and *reconstruct*, then `stem1` will return
    `['-reconstruct+ing', 're-construct+ing']` to form *reconstructing*. `stem1` takes
    around 2*10-06 seconds for a short word such as *cut* and 7*10-06 seconds for
    a longer and more complex case such as *reconstructing* – fast enough for most
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `stem1`, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`stem1.stem` is a generator because there might be several ways to decompose
    a word. For *unexpected*, we get three analyses because *expect*, *expected*,
    and *unexpected* are all in the base lexicon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For *uneaten*, on the other hand, we only get *un-eat+en*, because the lexicon
    does not contain *eaten* and *uneaten* as entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s a bit awkward because it is hard to predict which derived forms will
    be listed in the lexicon. What we want is the root and its affixes, and it is
    clear that *expected* and *unexpected* are not the root forms. The more affixes
    you remove, the closer you get to the root. So, we might decide to use the output
    with the shortest root as the best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The quality of the output depends very heavily on the quality of the lexicon:
    if it contains forms that are themselves derived from smaller items, we will get
    analyses like the three for *unexpected*, and if it doesn’t contain some form,
    then it won’t return it (the WordNet lexicon contains about 150K entries, so this
    won’t happen all that often if we use it!).'
  prefs: []
  type: TYPE_NORMAL
- en: '`stem1.stem` is the basis for several well-known tools – for example, the `morphy`
    function from the NLTK for analyzing English forms and the **Standard Arabic Morphological
    Analyzer** (**SAMA**), (Buckwalter, T., 2007). There are, as ever, some complications,
    notably that the spelling of a word can change when you add a prefix or a suffix
    (for instance, when you add the English negative prefix *in-* to a word that begins
    with a *p*, it becomes *im-*, so *in-* + *perfect* becomes *imperfect* and *in-*
    + *possible* becomes *impossible*), and that words can have multiple affixes (for
    instance, French nouns and adjectives can have two suffixes, one to mark gender
    and one to mark number). We will look at these in the next two sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Spelling changes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In many languages (for example, English), the relationship between spelling
    and pronunciation is quite subtle. In particular, it can encode facts about the
    stress of a word, and it can do so in ways that change when you add prefixes and
    suffixes. The “magic e,” for instance, is used to mark words where the final vowel
    is long – for example, *site* versus *sit*. However, when a suffix that begins
    with a vowel is added to such a word, the *e* is dropped from the version with
    a long vowel, and the final consonant of the version with a short vowel is doubled:
    *siting* versus *sitting* (this only happens when the final vowel of the root
    is both short and stressed, with *enter* and *infer* becoming *entering* and *inferring*).
    Such rules tend to reflect the way that spelling encodes pronunciation (for example,
    the magic e marks the preceding vowel as being long, while the doubled consonant
    in *inferring* marks the previous vowel as being short and stressed) or to arise
    from actual changes in pronunciation (the *im-* in *impossible* is the *in-* prefix,
    but it’s difficult to say *inpossible* (try it!), so English speakers have lazily
    changed it to *im-*. See (Chomsky & Halle, 1968) for a detailed discussion of
    the relationship between spelling and pronunciation in English).'
  prefs: []
  type: TYPE_NORMAL
- en: '`morphy` deals with this by including all possible versions of the affixes
    and stopping as soon as one that matches is found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This table says, for instance, that if you see a word that ends with *s*, it
    might be a noun if you delete the *s*, and that if you see a word that ends with
    *ches*, then it might be a form of a noun that ends *ch*. Making these substitutions
    will work fine a lot of the time, but it does not deal with cases such as *hitting*
    and *slipped*. Due to this, `morphy` includes a list of exceptions (quite a long
    list: 2K for nouns and 2.4K for verbs) that includes forms like these. This will
    work, of course, but it does take a lot of maintenance and it does mean that words
    that obey the rules but are not in the exception list will not be recognized (for
    example, the basic word list includes *kit* as a verb, but does not including
    *kitting* and *kitted* as exceptions and hence will not recognize that *kitted*
    in *he was kitted out with all the latest gear* is a form of *kit*).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of providing multiple versions of the affixes and long lists of exceptions,
    we can provide a set of spelling changes that are to be applied as the word is
    split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In these rules, the left-hand side is a pattern that is to be matched somewhere
    in the current form and the right-hand side is how it might be rewritten. `C,
    C0, C1`, … will match any consonant, `V, V0, V1`, … will match any vowel, `X,
    X0, X1`, … will match any character, and `X:(d|n)` will match `d` or `n` and fix
    the value of `X` to be whichever one was matched. Thus, the first rule will match
    *seen* and *freed* and suggest rewriting them as *see+en* or *free+ed*, and the
    second last one, which looks for a consonant, a vowel, a repeated consonant, and
    any of *ed*, *en*, *er*, or *ing* will match *slipping* and *kitted* and suggest
    rewriting them as *slip+ing* and *kit+ed*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use rules like these, we can find roots of forms where the ending has
    been changed without explicitly listing them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, if we use it with a word where the derived form is explicitly listed
    as an exception, then we will get multiple versions, but again, using the one
    with the shortest root will give us the most basic version of the root:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The implementation of `allstems` in `chapter4.stem2` in this book’s code repository
    also allows multiple affixes, so we can analyze words such as *unreconstructed*
    and *derestrictions*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These rules can be compiled into a single regular expression, and hence can
    be applied very fast and will cover the vast majority of spelling changes at the
    boundaries between the morphemes that make up an English word (the use of regular
    expressions for this task was pioneered by (Koskiennemi, 1985)). Rules only apply
    at the junctions between morphemes, so it is possible to immediately check whether
    the first part of the rewritten form is a prefix (if no root has been found) or
    a root (if no root has been found so far), so they will not lead to multiple unjustified
    splits. This kind of approach leads to tools that can be much more easily maintained
    since you do not have to add all the forms that cannot be obtained just by splitting
    off some versions of the affixes as exceptions, so it may be worth considering
    if you are working with a language where there are large numbers of spelling changes
    of this sort.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple and contextual affixes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding discussion suggests that there is a fixed set of affixes, each
    of which can be attached to a suitable root. Even in English, the situation is
    not that simple. Firstly, there are several alternative past endings, some of
    which attach to some verbs and some to others. Most verbs take *–ed* for their
    past participles, but some, such as *take*, require *–en*: `morphy` accepts *taked*
    as well as *taken* as forms of *take*, and likewise for other *–en* verbs and
    completely irregular cases such as *thinked* and *bringed*. Secondly, there are
    cases where a word may take a sequence of affixes – for example, *unexpectedly*
    looks as though it is made out of a prefix, *un-*, the root, *expect*, and two
    suffixes, *-ed* and *-ly*. Both these issues become more significant in other
    languages. It probably doesn’t matter that `morphy` returns *steal* as the root
    of *stealed* since it is very unlikely that anyone would ever write *stealed*
    (likewise, it doesn’t matter that it accepts *sliping* as the present participle
    of *slip* since no one would ever write *sliping*). In other languages, failing
    to spot that some affix is the wrong kind of thing to attach to a given root can
    lead to incorrect readings. Likewise, there are not all that many cases in English
    of multiple affixes, certainly not of multiple inflectional affixes (in the preceding
    example, *un-*, *-ed*, and *-ly* are all derivational affixes – *-un* obtains
    a new adjective from an old one, *-ed* in this case obtains an adjective from
    a verb root, and *-ly* obtains an adverb from an adjective).'
  prefs: []
  type: TYPE_NORMAL
- en: Again, this can matter more in languages such as French (and other Romance languages),
    where a noun is expected to take a gender marker and a number marker (*noir*,
    *noire*, *noirs*, and *noires*), and a verb is expected to take a tense marker
    and an appropriate person marker (*achetais* as the first person singular imperfect,
    *acheterais* as the first person singular conditional); and in languages such
    as Arabic, where a word may have varying numbers of inflectional affixes (for
    example, a present tense verb will have a tense prefix and a present tense person
    marker, while a past tense one will just have the past tense number marker) and
    also a number of clitic affixes (closed class words that are directly attached
    to the main word) – for example, the form ويكتبون (wyktbwn) consists of a conjunction,
    وَ/PART (wa), a present tense prefix, يَ/IV3MP, a present tense form of the verb,
    (كْتُب/VERB_IMP), a present tense suffix, ُونَ/IVSUFF_SUBJ:MP_MOOD:I, and a third
    person plural pronoun, هُم/IVSUFF_DO:3MP, with the whole thing meaning *and they
    are writing them*. The permitted constituents, the order they are allowed to appear
    in, and the form of the root, which can vary with different affixes and between
    different classes of nouns and verbs, are complex and crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'To capture these phenomena, we need to make a further change to the algorithm
    given previously. We need to say what each affix can combine with, and we need
    to assign words to lexical classes. To capture the first part of this, we must
    assume that a root is typically incomplete without certain affixes – that an English
    verb is incomplete without a tense marker, a French adjective is incomplete without
    a gender marker and a number marker, and so on. We will write `A->B` to denote
    an `A` character that is missing a following `B` – for example, an English verb
    root is of the type `V->TNS`, and `A <-B` denotes an `A` that is missing a preceding
    `B`. For example, *-ly* is an adverb missing a preceding adjective, so it is of
    the type `ADV<-ADJ`. Given this notion, we can require that items have to combine
    as they are found – for example, *sadly*, which is made of the adjective *sad*
    and the derivational affix *-ly* can be combined, but *dogly* is not a word because
    the noun *dog* is not what *-ly* requires. Thus, the standard set of English affixes
    becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the root forms of nouns, verbs, and adjectives are assigned the types
    `n->num`, `v->tns`, and `a->cmp`. Now, analyzing a word such as *smaller* involves
    combining *small* (`adj->cmp`) and the suffix, *–er* (`cmp`), while the analysis
    of *redevelopments* involves combining *re-* (`(v->tns)->(v->tns)`) and *develop*
    (`v->tns`) to produce a new untensed verb, *redevelop*, also of the `(v->tns)`
    type. Now, we can combine this with *–ment* (`(n->num)<-(v->tns)`) to produce
    a new noun root, *redevelopment* (`(n->num)`), and finally combine that with *-s*
    (`(num)`) to produce the plural noun *redevelopments*. If we pick the best analysis
    in each case, we will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that *unreconstructions* leads to an error because the *un-*, *re-*, and
    *-ion* affixes don’t go together – *un-* produces an adjective from a verb, so
    *un-re-construct* is an adjective and -*ion* has to be attached to a verb root.
  prefs: []
  type: TYPE_NORMAL
- en: The more elements you can remove from a complex word, the more likely you are
    to arrive at a root that is known to carry an emotional charge. If you can work
    out that *disastrously* is *disaster+ous+ly*, then you will be able to make use
    of the fact that *disaster* is a highly negative word to detect the negative overtones
    of *disastrously*; if you can spot that *enthusiastic* and *enthusiastically*
    are *enthusiast+ic* and *enthusiast+ic+al+ly*, then these three words can be treated
    as though they were the same when learning and subsequently applying rules for
    detecting emotions. It is worth noting that some affixes reverse the meaning of
    the words to which they apply – for example, an unexpected event is one that was
    not expected. This has to be taken into account when understanding that *undesirable*,
    for instance, is *un+desire+able*, where *desire* is a generally positive term
    but the prefix reverses its meaning and hence suggests that a text that contains
    it will be negative.
  prefs: []
  type: TYPE_NORMAL
- en: Similar phenomena occur in many other languages, with affixes either adding
    information to the base word or changing its meaning and/or class. In many cases,
    such as in Romance languages, the root will require multiple affixes to complete
    itself. In the English cases mentioned previously, we saw several examples where
    a word was made out of multiple components, but all such cases involved at most
    one inflectional affix plus one or more derivational ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider, for instance, the adjective *noir*: this, like most French adjectives,
    has four forms – *noir*, *noire*, *noirs*, and *noires*. We can easily capture
    this pattern by saying that a French adjectival root is of the type`(a->num)->gen`
    (note the bracketing – the first thing that has to be found is the gender marker,
    and only once that has been found do we have `a->num` – that is, an adjective
    looking for a number marker). Now, let’s say we have a set of affixes, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we can easily decompose the various forms of *noir*. We will also
    need a set of spelling change rules since some adjectives change their form when
    we add the various suffixes – for example, adjectives that end with *-if* (*sportif*,
    *objectif*) change their feminine endings to *–ive* (singular) and *–ives* (plural),
    so we need spelling rules such as the following, which says that the *–ive* sequence
    could arise from adding *-e* to the end of a word that ends with *if*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This rule will account for the four forms (*sportif*, *sportive*, *sportifs*,
    and *sportives*), with the *e* marking the fact that *sportif* and *sportifs*
    are pronounced with an unvoiced following consonant and *sportive* and *sportives*
    are pronounced with the voiced version of the consonant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The situation becomes considerably more complicated when we come to deal with
    verbs. Consider the following conjugation table for the regular verb *regarder*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Present** | **Imperfect** | **Future** | **Conditional** | **Subjunctive**
    | **Imperfect subj.** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| je | regarde | regardais | regarderai | regarderais | regarde | regardasse
    |'
  prefs: []
  type: TYPE_TB
- en: '| tu | regardes | regardais | regarderas | regarderais | regardes | regardasses
    |'
  prefs: []
  type: TYPE_TB
- en: '| il | regarde | regardait | regardera | regarderait | regarde | regardât |'
  prefs: []
  type: TYPE_TB
- en: '| nous | regardons | regardions | regarderons | regarderions | regardions |
    regardassions |'
  prefs: []
  type: TYPE_TB
- en: '| vous | regardez | regardiez | regarderez | regarderiez | regardiez | regardassiez
    |'
  prefs: []
  type: TYPE_TB
- en: '| ils | regardent | regardaient | regarderont | regarderaient | regardent |
    regardassent |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.2 – Conjugation table for “regarder”
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some easy-to-spot regularities in this table – for example, that
    the future and conditional forms all contain the *-er* suffix, and that the imperfect
    and conditional forms have the same set of person affixes. There are quite a lot
    of semi-regularities that don’t carry over completely – for example, the subjunctive
    and imperfect subjunctive have very similar (but not identical) sets of person
    endings. It is very difficult to do anything useful with the semi-regularities,
    so the best that we can easily do is specify that a French verb requires a mood
    marker and a person marker – that is, that *regard* is of the `(v->person)->mood`
    type (as with the type for adjectives, this says that you have to supply the mood
    marker first to get something of the `(v->person)` type and then look for the
    person marker). Now, we can supply the collection of affixes, which can then be
    used to analyze input text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: These affixes can then be used to reduce verbs to their base forms – *regardions*
    and *regarderions*, for instance, become *regard+ions* and *regard+er+ions*, respectively
    – so that different variants of the same word can be recognized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply using this table will overgenerate, incorrectly recognizing, for instance,
    *regardere* as *regard+er+e*. This may not matter too much since people don’t
    generally write incorrect forms (and maybe if they do, it is helpful to recognize
    them anyway, as with the English examples of *stealed* and *sliping* mentioned
    previously). More significantly, different verbs have substantially different
    conjugation tables that require different sets of affixes:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Present** | **Imperfect** | **Future** | **Conditional** | **Subjunctive**
    | **Imperfect subj.** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| je | faiblis | faiblissais | faiblirai | faiblirais | faiblisse | faiblisse
    |'
  prefs: []
  type: TYPE_TB
- en: '| tu | faiblis | faiblissais | faibliras | faiblirais | faiblisses | faiblisses
    |'
  prefs: []
  type: TYPE_TB
- en: '| il | faiblit | faiblissait | faiblira | faiblirait | faiblisse | faiblît
    |'
  prefs: []
  type: TYPE_TB
- en: '| nous | faiblissons | faiblissions | faiblirons | faiblirions | faiblissions
    | faiblissions |'
  prefs: []
  type: TYPE_TB
- en: '| vous | faiblissez | faiblissiez | faiblirez | faibliriez | faiblissiez |
    faiblissiez |'
  prefs: []
  type: TYPE_TB
- en: '| ils | faiblissent | faiblissaient | faibliront | faibliraient | faiblissent
    | faiblissent |'
  prefs: []
  type: TYPE_TB
- en: Figure 4.3 – Conjugation table for “faiblir”
  prefs: []
  type: TYPE_NORMAL
- en: Here, several (but not all) of the tense markers that were empty for *regard*
    are now *-iss*, the marker for the future and conditional tenses is *ir*, and
    some of the person markers for the present tense are different. We can add (and
    indeed will have to) these to our table, but we would also like to make sure that
    the right affixes get applied to the right verb.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we have to be able to say more about words and affixes than simply
    assigning them a single atomic label. In English, we want to be able to say that
    *–ly* attaches to participles but not to tensed forms (for example, that *unexpectedly*
    and *unsurprisingly* are *un+expect+ed+ly* and *un+surprise+ing+ly*, but that
    *unexpectly* and *unsurprisesly* are not words). We want to be able to say that
    *regard* is an *er* verb and *faibl* is an *ir* verb, with an empty imperfect
    marker for *er* verbs and *iss* as the imperfect marker for *ir* verbs. In general,
    we want to be able to say quite detailed things about words and their affixes
    and to be able to copy information from one to the other (for example, that the
    verb root will get its tense and form from the tense affix).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do this by extending our notation to allow for features – that is, properties
    that distinguish one instance of a word from another. For example, we can say
    that *sleeps* is `[hd=v, tense=present, finite=tensed, number=singular, person=third]`
    and *sleeping* is `[hd=v, tense=present, finite=participle]`. We can, for instance,
    change the description of the base form of a verb from `v->tns` to `v[tense=T,
    finite=F, number=N, person=P]->tense[tense=T, finite=F, number=N, person=P],`
    – i.e., a base verb isn’t just something that needs a tense marker; it will also
    inherit the values of the features for tense, finiteness, number, and person from
    that affix. The verbal suffixes then become as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This code block says that adding an empty suffix to a verbal root will give
    you the infinitive form or the present tense, adding *-ing* will give you the
    present participle, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This general approach can be used to assign French verbs to the various classes
    of *-er*, *-ir*, *-re*, and irregular cases, to ensure that tense and agreement
    markers on Arabic verbs match each other, as well as to ensure that complex sequences
    of derivational and inflectional affixes are handled properly. If you want to
    get at the root of a surface form and see exactly what properties it has, you
    will have to do something like this. It does, however, come at a price:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have to say more about the words in your lexicon and a lot more about the
    affixes themselves. To realize that *kissed* can be the past tense or the past
    participle or the passive participle of *kiss* but that *walked* can only be the
    past tense or past participle of *walk*, you have to know what the *-ed* suffix
    does and you also have to know that *walk* is an intransitive verb and hence does
    not have a passive form. The more you want to know about the properties of a surface
    form, the more you have to say about its root and about the affixes that are attached
    to it. This is hard work and can make it very hard to maintain the lexicon. This
    can be seen in an extreme form in the lexicon for the most widely used morphological
    analyzer for Arabic, namely the SAMA lexicon (Buckwalter, T., 2007). A typical
    entry in the SAMA lexicon looks like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This entry contains six distinct variants for a verb meaning something such
    as desire. The first part of a variant is what the stem looks like with diacritics
    omitted (diacritics are things such as short vowels and other marks that affect
    the pronunciation of the word, and are generally omitted in written Arabic), the
    second part is what the stem would look like if the diacritics were written, the
    third is a label that specifies what affixes the stem will combine with, and the
    last is the English gloss. To add a single entry to the lexicon, you have to know
    what all the surface forms look like and which class they belong to – for example,
    the stem &b (ؤب) is the `IV_Vd` form of this word. To do that, you have to know
    what it means to say that something is the `IV_Vd` form of the word. And then,
    there are over 14K prefixes and nearly 15k suffixes, each with a complex label
    saying what stems it attaches to.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an extreme case: we need five inflectional affixes for English verbs
    and maybe another 10 derivational ones, and around 250 inflectional affixes for
    French verbs. Nonetheless, the point is clear: if you want to get complete and
    correct decompositions of complex words, you need to provide a lot of information
    about words and suffixes. (See Hoeksema, 1985, for more on describing words in
    terms that specify what they need to complete themselves.)'
  prefs: []
  type: TYPE_NORMAL
- en: Exploiting this information requires more work than just splitting the surface
    form into pieces, and can markedly slow things down. `morphy` runs at about 150K
    words/second, but it does very little with compound words such as *unexpectedly*
    – if a word like this is in the set of exceptions, then it is returned without
    being decomposed; if it is not (for example, *unwarrantedly*), then it will simply
    return nothing at all. The analyzer provided in the code repository runs at 27K
    words/second if we use simple labels and no spelling rules, 17.2K words/second
    with simple labels and spelling rules, 21.4K words/second with complex labels
    and no spelling rules, and 14.7K words/second with complex labels and spelling
    rules, and the SAMA lexicon runs at about 9K words/second. The analyzer from the
    code repository and the SAMA lexicon also provide all the alternative analyses
    of a given form, whereas `morphy` just returns the first match it finds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The lesson is clear: if you want words that have been stripped right down to
    their roots, you will have to provide a substantial amount of clear information
    about word classes and about the effects that the various affixes have. If you
    take a simple-minded approach and are not too worried about getting right to the
    heart of each form, and about finding out its exact properties, then you can do
    the task substantially faster, but even at 14.7K words/second, morphological analysis
    not going to be a major bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Compound words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we looked at how to find the root element of a complex
    word. This is important for our overall task since a large part of the emotional
    content of a text is determined simply by the choice of words. A tweet such as
    *My joy at finding that you loved me as much as I love you has filled my heart
    with contentment* is overwhelmingly likely to be tagged as expressing **joy**
    and **love**, and the form *loved* will contribute as much to this as the form
    *love*. It can also happen, however, that a group of words expresses something
    quite different from what they express individually – that a tweet containing
    the phrases *greenhouse gases* and *climate change* is much more likely to be
    negative than one that just contains *greenhouse* and *change*, and that one that
    contains the phrase *crime prevention* is much more likely to be positive than
    one that just contains *crime* or *prevention*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a fairly marginal effect in languages that use white space to separate
    tokens because compound words of this kind tend to be written either with no separator
    or with a dash: a *blackbird* is not just a bird that is black, and a *greenhouse*
    is neither a house nor green. In some languages, however, each token is potentially
    a word and each sequence of tokens is also potentially a word, with no white space
    to mark the boundaries around sequences. In Chinese, for instance, the words 酒
    and 店 mean *wine* and *shop*, but the sequence 酒店 means *hotel*. Similarly, the
    word 奄 means *suddenly* but the sequence 奄奄 means *dying*. While it is easy enough
    to see the connection between *wine shop* and *hotel*, there is more to a hotel
    than just somewhere that sells wine; and it is all but impossible to see why *suddenly-*'
  prefs: []
  type: TYPE_NORMAL
- en: '*suddenly* would mean *dying*. Similarly, the four characters 新, 冠, 疫, and
    情, which individually mean *new crown epidemic feeling*, meaning COVID-19, when
    taken as a group, are hard to predict. Also, a tweet about COVID-19 is much more
    likely to be negative than one about *new crown epidemic feeling*. Therefore,
    it is important to be able to detect such compounds even when there is no typographical
    evidence for them, particularly since they are fluid, with new ones being created
    all the time (新冠疫情 would not have meant COVID-19 in 2018!).'
  prefs: []
  type: TYPE_NORMAL
- en: The key to finding such compounds is observing that the elements of a compound
    will occur next to each much more frequently than you would expect just by chance.
    The standard way to detect this is by using **pointwise mutual information** (**PMI**)
    (Fano, r. M., 1961). The idea here is that if two events, E1 and E2, are unconnected,
    then the likelihood of E2 occurring immediately after E1 should be the same as
    the likelihood of E2 occurring immediately after some other event. If E1 and E2
    have nothing to do with each other, then the likelihood of E2 occurring immediately
    after E1 is *prob(E1)*prob(E2)*. If we find that they are occurring together more
    often than that, we can conclude that they have some connection. If E1 and E2
    are words, we can hypothesize that if they co-occur much more often than expected,
    then they may be a compound. Therefore, we can calculate the PMI of two words
    as *log(prob(W1+W2)/(prob(W1)*prob(W2))* (taking logs smooths out values that
    are returned but this is not crucial to the approach).
  prefs: []
  type: TYPE_NORMAL
- en: 'The machinery for doing this is implemented in `chapter4.compounds`. If we
    apply it to a collection of 10 million words from the BNC, we will see that the
    top 20 pairs that occur at least 300 times are largely fixed phrases, often Latin
    (*inter-alia*, *vice-versa*, *ad-hoc*, *status-quo*, and *de-facto*) or technical/medical
    terms (*ulcerative-colitis*, and *amino-acids*). The following scores are of the
    `(<PMI-score>, <pair>, <``freq>)` form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Even in English, pairs such as `crime-prevention` and `greenhouse-gases`, which
    have high PMI scores (the median pair in our set is `(``5.48, needs-help, 121)`,
    and `crime-prevention` and `greenhouse-gases` are both in the top 2% of the entire
    set), can carry an emotional weight that is different from the emotions associated
    with the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: So, it may be worth looking at the emotional weights associated with particularly
    frequent compound terms even in English. For other languages, this may be even
    more important.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging and parsing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have spent quite a long time looking at individual words – finding tokens
    in text, decomposing those into smaller elements, looking at the way that spelling
    changes happen at the boundaries between word parts, and considering the problems
    that arise, particularly in languages that do not use white space to separate
    tokens when words come together to form compounds. A huge part of the task of
    emotion detection relies on identifying words that carry emotions, so it makes
    sense to be careful when looking at words.
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*,
    for most NLP tasks, finding the relationships between words is just as important
    as finding the words themselves. For our current task, which involves finding
    the general emotional overtones of a short informal text, this may not be the
    case. There are two major questions to be answered here:'
  prefs: []
  type: TYPE_NORMAL
- en: Does assigning a set of relations between words help with emotion detection?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is it possible to assign relations to elements of informal text?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normal texts are divided into sentences – that is, groups of words separated
    by punctuation marks that describe the words (or query a description of the words).
    A properly formed sentence has a main verb that denotes an event or a state and
    a set of satellite phrases that either describe the participants in this event
    or state or say something about where, when, how, or why it took place. Consider
    the second question: if we use a rule-based parser we get something similar to
    the following tree (the exact form of the tree will depend on the nature of the
    rules being used; we are using a parser that was designed to deal cleanly with
    out-of-position items (Ramsay, A. M., 1999), but any rule-based parser will produce
    something like this):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Rule-based parse of “Is it possible to assign relations to elements
    of normal text”](img/B18714_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Rule-based parse of “Is it possible to assign relations to elements
    of normal text”
  prefs: []
  type: TYPE_NORMAL
- en: This tree says that the given sentence is a query about the existence of a particular
    kind of possibility, namely the possibility of assigning relations to elements
    of normal text. To understand this sentence properly, we have to find these relations.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding tree was generated by a rule-based parser (Ramsay, A. M., 1999).
    As noted in [*Chapter 1*](B18714_01.xhtml#_idTextAnchor015), *Foundations*, rule-based
    parsers can be fragile when confronted with texts that do not obey the rules,
    and they can be slow. Given that informal texts are, more or less by definition,
    likely not to obey the rules of normal language, we will consider using a data-driven
    parser to analyze them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by looking at two tweets that were chosen fairly randomly from
    the SEMEVAL training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: These tweets do not obey the rules of normal well-formed text. They contain
    elements that simply do not appear in normal language (usernames, hashtags, emojis,
    emoticons), they contain non-standard uses of punctuation, they very often have
    no main verb, they contain deliberate misspellings and words made out of repeated
    elements (*hahahahahahahahaha*), and so on. Our rule-based parser will just fail
    if we try to use it to analyze them. What happens if we were to use a data-driven
    one (we use the NLTK pre-trained version of the MALT parser (Nivre, 2006) with
    the NLTK recommended tagger, but very little changes if we choose another data-driven
    parser or a different tagger)?
  prefs: []
  type: TYPE_NORMAL
- en: 'Just using MALT with the standard tagger, we get the following trees for *@PM
    @KF Very misleading heading.* and *#anxious don’t know why ................. #worry
    (: slowly going #**mad hahahahahahahahaha*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – MALT parses of informal texts](img/B18714_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – MALT parses of informal texts
  prefs: []
  type: TYPE_NORMAL
- en: There are two problems here. The first is that the tagger and parser are data-driven
    – that is, the decisions they make are learned from a labeled corpus, and the
    corpus they have been trained on does not contain the kind of informal text that
    is found in tweets. Secondly, and more significantly, informal texts often contain
    fragments jumbled up together, so it is not possible to assign relationships to
    such a text in a way that makes a single coherent tree.
  prefs: []
  type: TYPE_NORMAL
- en: The first of these problems could be overcome by marking up a corpus of tweets.
    This would, of course, be tedious, but no more tedious than doing so for a corpus
    of standard texts. The second problem raises its head again here because to mark
    up a piece of text, you have to have an underlying theory of what POS tags to
    use and what kinds of relationships exist between the elements of the text. If
    you assume that the only POS tags that you can use are the standard NN, VV, JJ,
    DET, IN, CC, and PR, then you cannot assign the correct tags to tweet elements,
    since these are new and are not of the standard types. And if you assume that
    only the standard relations between words can be used, then you cannot assign
    the correct roles to tweet items since they do not tend to occupy these roles
    – the emoticon *(:* and the word *hahahahahahahahaha* are not the kinds of things
    that can play these roles. So, if we are going to mark up a collection of tweets
    to train a tagger and parser, we are going to have to come up with a theory of
    the structure of such texts. Constructing a treebank is not a theory-free activity.
    The guidelines given to the annotators are, by definition, an informal specification
    of a grammar, so unless you have a clear idea of what roles things such as hashtags
    and emojis can play, and a clear understanding of when, for instance, an emoji
    should be seen as a comment on the tweet as a whole and when it should be seen
    as a comment on a particular element, it is just not possible to construct a treebank.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tweets often contain well-formed fragments, so maybe we can get some benefit
    from finding these:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Never regret anything that once made you smile :) #**positive*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Literally hanging on by a thread need some taylor ray tonight loving a bad
    dog sucks #taylorrayholbrook #**hurting @TRT*'
  prefs: []
  type: TYPE_NORMAL
- en: '*was one moron driving his oversize tonka truck with the big flag in the bed
    back and forth blaring country music.* 😐 *#**disappointment*'
  prefs: []
  type: TYPE_NORMAL
- en: '*#ThingsIveLearned The wise #shepherd never trusts his flock to a #smiling
    wolf. #TeamFollowBack #**fact #wisewords*'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of things that are worth doing to start with. No existing
    parser, rule-based or data-driven, is going to do anything sensible with tags,
    usernames, emojis, or emoticons at the beginning or end of a sentence, so we may
    as well strip those off before attempting to find parseable fragments. Hashtags
    in the middle of a tweet are often attached to meaningful words, so we may as
    well remove those too. This will give us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Never regret anything that once made* *you smile*'
  prefs: []
  type: TYPE_NORMAL
- en: '*was one moron driving his oversize tonka truck with the big flag in the bed
    back and forth blaring* *country music.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Literally hanging on by a thread need some taylor ray tonight loving a bad*
    *dog sucks*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The wise shepherd never trusts his flock to a* *smiling wolf*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These all contain well-formed fragments: the first and fourth are, indeed,
    well-formed sentences, and the other two contain well-formed fragments. What happens
    if we try to parse them using our rule-based tagger and then use MALT?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two parsers give essentially the same answers for the first and fourth
    of these (rule-based analysis on the left, MALT analysis on the right), save that
    the rule-based parser gives the wrong attachment to *to a smiling wolf*. No parser
    can be expected to get the attachment of such phrases right every time, and apart
    from that, the two behave perfectly sensibly given the rules that either explicitly
    or implicitly underly them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Rule-based and MALT parses for “The wise shepherd never trusts
    his flock to a smiling wolf”](img/B18714_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Rule-based and MALT parses for “The wise shepherd never trusts
    his flock to a smiling wolf”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Rule-based and MALT parses for “Never regret anything that once
    made you smile”](img/B18714_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Rule-based and MALT parses for “Never regret anything that once
    made you smile”
  prefs: []
  type: TYPE_NORMAL
- en: 'So, either approach will suffice for these examples. When we consider the other
    cases, the situation becomes more difficult. The rule-based parser fails to produce
    any overall analysis for *was one moron driving his oversize tonka truck with
    the big flag in the bed back and forth blaring country music* or *Literally hanging
    on by a thread need some taylor ray tonight loving a bad dog sucks*. Both these
    sentences are simply too long for it to handle because there are too many options
    to be explored. MALT produces analyses for both cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – MALT parse for “was one moron driving his oversize tonka truck
    with the big flag in the bed back and forth blaring country music”](img/B18714_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – MALT parse for “was one moron driving his oversize tonka truck
    with the big flag in the bed back and forth blaring country music”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – MALT parse for “Literally hanging on by a thread need some taylor
    ray tonight loving a bad dog sucks”](img/B18714_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – MALT parse for “Literally hanging on by a thread need some taylor
    ray tonight loving a bad dog sucks”
  prefs: []
  type: TYPE_NORMAL
- en: The first of these is reasonable – the analysis of *was* as a copula is questionable,
    and the attachment of *back and forth* is wrong, but by and large, it captures
    most of the relevant relationships. The second is just a mess. The problem with
    the second is that the text contains several disjoint elements – *Literally hanging
    on by a thread, need some taylor ray tonight* and *loving a bad dog sucks* – but
    the parser has been told to analyze the whole thing and hence it has analyzed
    the whole thing.
  prefs: []
  type: TYPE_NORMAL
- en: There is nothing to be done about this. Data-driven parsers are generally designed
    to be robust, so even if the text they are given is entirely ungrammatical or
    contains grammatical fragments but is not a single coherent whole, they will return
    a single tree **and there is no way of telling that what they return is problematic**.
    This holds pretty much by definition. If a text doesn’t have a sensible structure
    – that is, it can’t be assigned a sensible parse tree – then a robust parser will
    assign it a tree that is not sensible. A rule-based parser, on the other hand,
    will just fail if the text does not obey the rules it expects it to obey or is
    just too long and complex to handle.
  prefs: []
  type: TYPE_NORMAL
- en: So, there seems little point in including a parser in the preprocessing steps
    for sentiment mining. Rule-based parsers will frequently just fail when confronted
    with informal texts, even if they are preprocessed to strip non-textual items
    off the front and back and to do various other simple steps. Data-driven parsers
    will always produce an answer, but for text that does not obey the rules of normal
    language, this answer will often be nonsensical and **there is no easy way of
    telling which analyses are reasonable and which are not**. And if there is no
    point in including a parser, then there is also no point in including a tagger,
    since the sole function of a tagger is to preprocess the text for a parser. It
    might be possible to use a rule-based parser to check the output of a data-driven
    parser – if the output of the data-driven one is reasonable, then using it to
    guide the rule-based one will enable the rule-based one to verify that it is indeed
    acceptable without exploring vast numbers of dead-ends.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is very unclear how a typical machine learning algorithm would be
    able to make use of such trees, even if they could be reliably found. The code
    repository for this book includes code for tagging tweets and running a data-driven
    parser on the results, and some examples can be explored further there, but since
    these are not generally useful steps for our overall goal, we will not discuss
    them further here.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at the issues that arise when you try to split
    a piece of text into words by looking at how to split text into tokens, how to
    find the basic components of words, and how to identify compound words. These
    are all useful steps for assigning emotions to informal texts. We also looked
    at what happens when we try to take the next step and assign grammatical relations
    to the words that make up an informal text, concluding that this is an extremely
    difficult task that provides comparatively little benefit for our overall task.
    We had to look quite carefully at this step, even though we believe it is not
    all that useful, since we need to understand why it is so hard and why the results
    of even the best parsers cannot be relied on.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Buckwalter, T. (2007). *Issues in Arabic Morphological Analysis*. Arabic Computational
    Morphology, 23-42.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chomsky, N., & Halle, M. (1968). *The Sound Pattern of English*. MIT Press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fano, R. M. (1961). *Transmission of Information: A Statistical Theory of Communications*.
    MIT Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoeksema, J. (1985). *Categorial Morphology*. Garland Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koskiennemi, K. (1985). *A General Two-level Computational Model for Word-form
    Recognition and Production*. COLING-84, 178-181.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leech, G., Garside, R., & Bryant, M. (1994, August). *CLAWS4: The Tagging of
    the British National Corpus*. COLING 1994 Volume 1: The 15th International Conference
    on Computational Linguistics. [https://aclanthology.org/C94-1103](https://aclanthology.org/C94-1103)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nivre, J., Hall, J., & Nilsson, J. (2006). *MaltParser: A language-independent
    system for data-driven dependency parsing*. Proceedings of the International Conference
    on Language Resources (LREC), 6, 2216-2219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramsay, A. M. (1999). *Direct parsing with discontinuous phrases*. Natural Language
    Engineering, 5(3), 271-300.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3:Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you’ll learn how we go about the task of EA. We will discuss various
    models, explain how they work, and evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and Vector
    Space Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B18714_06.xhtml#_idTextAnchor134), *Naïve Bayes*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18714_07.xhtml#_idTextAnchor144), *Support Vector Machines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18714_08.xhtml#_idTextAnchor157), *Neural Networks and Deep
    Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18714_09.xhtml#_idTextAnchor172), *Exploring Transformers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
