<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Customer Segmentation Using Clustering Algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter will introduce the main clustering algorithms by exploring how to apply them to customer segmentation based on their behavioral patterns. In particular, we will demonstrate how Apache Spark and Amazon SageMaker can seamlessly interoperate to perform clustering. Throughout this chapter, we will be using the <strong>Kaggle Dataset E-Commerce</strong> data from <strong>Fabien Daniel</strong>,<strong> </strong>which can be downloaded from <a href="https://www.kaggle.com/fabiendaniel/customer-segmentation/data">https://www.kaggle.com/fabiendaniel/customer-segmentation/data</a>. </p>
<p>Let's take a look at the topics we will be  covering:</p>
<ul>
<li>Understanding how clustering algorithms work</li>
<li>Clustering with <strong>Apache Spark</strong> on <strong>Elastic MapReduce</strong> (<strong>EMR</strong>)</li>
<li>Clustering using <strong>SageMaker</strong> through Spark integration</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding How Clustering Algorithms Work</h1>
                </header>
            
            <article>
                
<p><strong>Cluster analysis</strong>, or clustering, is a process of grouping a set of observations based on their similarities. The idea is that the observations in a cluster are more similar to one another than the observations from other clusters. Hence, the outcome of this algorithm is a set of clusters that can identify the patterns in the dataset and arrange the data into different clusters.</p>
<p>Clustering algorithms are referred to as <strong>unsupervised learning algorithms</strong>. Unsupervised learning does not depend on predicting ground truth and is designed to discover the natural patterns in the data. Since there is no ground truth provided, it is difficult to compare different unsupervised learning models. Unsupervised learning is generally used for exploratory analysis and dimensionality reduction. Clustering is an example of exploratory analysis. In this task, you are looking for patterns and structure in the dataset.</p>
<p>This is different than the algorithms we have been studying so far in the book. <strong>Naive Bayes</strong>, <strong>linear regression</strong>, and <strong>decision trees</strong> algorithms are all examples of supervised learning. There is an assumption that each dataset has a set of observations and an event class associated with those observations. Hence, the data is already grouped based on the actual outcome event for each observation. However, not every dataset has labeled outcomes associated with each event. For example, consider a dataset where you have information regarding each transaction on an e-commerce website: </p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><strong>SKU</strong></td>
<td><strong>Item name</strong></td>
<td><strong>Customer ID</strong></td>
<td><strong>Country</strong></td>
</tr>
<tr>
<td>12423</td>
<td>iPhone</td>
<td>10</td>
<td>USA</td>
</tr>
<tr>
<td><span>12423</span></td>
<td><span>iPhone</span></td>
<td>11</td>
<td><span>USA</span></td>
</tr>
<tr>
<td><span>12423</span></td>
<td><span>iPhone</span></td>
<td>12</td>
<td>USA</td>
</tr>
<tr>
<td>11011</td>
<td>Samsung S9</td>
<td>13</td>
<td>UK</td>
</tr>
<tr>
<td><span>11011</span></td>
<td><span>Samsung S9</span></td>
<td>10</td>
<td>USA</td>
</tr>
<tr>
<td><span>11011</span></td>
<td><span>Samsung S9</span></td>
<td>14</td>
<td>UK</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This dataset is a list of transactions but does not have any class variable that informs us regarding what kind of users buy specific products. Hence, if the task is to identify patterns from this dataset, we cannot use any algorithms that can predict a specific event. That is where clustering algorithms come into the picture. We want to explore whether we can find trends in the transactions on the website based on the dataset. Let's look at a simple example. Consider that the dataset only had one feature: <strong>Item name</strong>. We will discover that the data can be arranged in three clusters, namely, iPhone, Samsung S9, and Pixel 2. Similarly, if we consider that the only feature to cluster on is <strong>Country</strong>, the data can be clustered into two clusters: USA and UK. Once you generate the clusters, you can analyze the statistics in each cluster to understand the type of audience buying certain things. </p>
<p>However, in most of your experiences, you will have to cluster the data based on more than one feature. There are many clustering algorithms that you can deploy in clustering the data into clusters. The following diagram shows an example of how clusters would look for the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-742 image-border" src="assets/6cece5e6-2a84-46dc-ad32-95548553028c.png" style="width:23.42em;height:10.83em;"/></p>
<p>Clustering helps us get an outcome where we can group the data into two clusters and understand the patterns in each cluster. We may be able to cluster the customers into users who buy a certain kind of phone. By analyzing the clusters, we can learn the patterns of which users buy an iPhone or Samsung S9 phone. </p>
<p> In this chapter, we will study two common clustering approaches: </p>
<ul>
<li>k-means clustering</li>
<li>Hierarchical clustering</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">k-means clustering </h1>
                </header>
            
            <article>
                
<p>The <strong>k-means clustering</strong> algorithm aims to cluster a dataset into k clusters by selecting k centroids in the dataset. Each record is evaluated based on its distance to the centroid and assigned a cluster. Centroids are observations that are at the center of each cluster. To define k-means clustering formally, we are optimizing the <strong>Within-Cluster Sum of the Square</strong> <em>(</em><strong>WCSS</strong><em>)</em> distance of observations. Hence, the most optimal clustering would ensure that each cluster has all the observations close to its centroid, and as far away from the other centroids as possible. </p>
<p>There are two important parameters in k-means clustering. Firstly, we need to discover the centroids in our dataset. One of the popular methodologies for selecting centroids is called <strong>Random partitioning</strong>. This methodology uses a technique called <strong>Expectation Maximization</strong> (<strong>EM</strong>) to achieve high-quality clusters. In the first step, we randomly assign a cluster to each observation. Once each observation is assigned to a cluster, we calculate the quality of the cluster using the WCSS methodology:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ecf2864c-bc47-445c-9291-635fe6cba82a.png" style="width:16.92em;height:4.33em;"/></p>
<p><em>J</em> represents the <span>WCSS score for the clusters that are generated where we have <em>M</em> observations and have generated <em>K</em> clusters. <img class="fm-editor-equation" src="assets/08a4dad4-65e2-404e-b49d-ab177ab1d566.png" style="width:2.25em;height:1.17em;"/> is 1 if the observation <em>i</em> belongs to cluster <em>k</em>, and 0 if the observation <em>i</em> does not belong to cluster <em>k</em>. <img class="fm-editor-equation" src="assets/97a87758-272e-4eda-802e-7872dbd08ff4.png" style="width:1.42em;height:1.17em;"/> is the observation, while <img class="fm-editor-equation" src="assets/9b7475ed-080c-42b9-b83d-4782273d2d79.png" style="width:1.67em;height:1.25em;"/> is the centroid of cluster <em>k</em>. The difference between <img class="fm-editor-equation" src="assets/6f0c9f95-2408-4a18-8f67-99485a2df395.png" style="width:1.42em;height:1.17em;"/> and <img class="fm-editor-equation" src="assets/a6cc06fe-37a0-4392-ad0a-dac23cacbee4.png" style="width:1.67em;height:1.25em;"/> represents the distance between the observation and the centroid. Our aim is to minimize the value of <em>J</em>.</span></p>
<p><span>In the next step, we calculate new centroids again based on the current clusters in the first step. This is the maximization step in the EM algorithm, where we try to step toward more optimal cluster assignments for records. The new centroid values are calculated using the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f889985e-9683-4c62-aa3d-83ff060f2eec.png" style="width:11.08em;height:4.50em;"/></p>
<p>This represents the fact that we recalculate the centroids based on the mean of the clusters created in the previous steps. Based on the new centroids, <span>we assign each observation in the dataset to a centroid based on their distance to the centroids. </span><span>Distance is the measure of similarity between two observations. </span><span>We will discuss the concept of how to calculate distance later in this section. W</span>e recalculate the <span>WCSS score for the new clusters and repeat the minimization step again. We repeat these steps until the assignments of the observations in the cluster do not change. </span></p>
<p>Although the random partition algorithm allows the k-means algorithm to discover centroids with a low <span>WCSS score, they do not guarantee a global optimum solution. This is because the EM algorithm may greedily find a local optimum solution and stop exploring, for a more optimum solution. Also, selecting different random centroids in the first step may lead to different optimal solutions at the end of this algorithm. </span></p>
<p>To address this issue, there are other algorithms such as the <strong><span>Forgy </span>algorithm</strong>, where we choose random observations from the dataset as centroids in the first step. This leads to more spread out centroids in the first step, compared to the random partition algorithm. </p>
<p>As we discussed before, we have to calculate the distance between the observations and the centroid of the cluster. There are various methodologies to calculate this distance. The two popular methodologies are the <strong>Euclidean distance</strong> and the <strong>Manhattan distance</strong>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Euclidean distance</h1>
                </header>
            
            <article>
                
<p>The Euclidean distance between two <span>points is the length of the line connecting them. For the n-dimensional points <em>P</em> and <em>Q</em>, where both the vectors have <em>n</em> values, the Euclidean distance is calculated using this formula: </span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bd0b051c-0341-4fc1-a596-833eb3bf5e88.png" style="width:10.08em;height:2.92em;"/></p>
<p>If the values of the data points are categorical values, then <img class="fm-editor-equation" src="assets/02482353-46c5-4319-a68f-32cd65412314.png" style="width:3.08em;height:1.00em;"/> is 1 if both the observations have the same values for a feature and 0 if the observations have different values. For continuous variables, we can calculate the normalized distance between the values of the attributions. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manhattan distance</h1>
                </header>
            
            <article>
                
<p>The <strong>Manhattan distance</strong> is the sum of absolute differences between two data points. <span>For the n-dimensional points <em>P</em> and <em>Q</em>, where both the vectors have <em>n</em> values, we calculate the Manhattan distance using the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d7af374d-c24a-4cc7-9975-521e9b808d0a.png" style="width:9.67em;height:2.75em;"/></p>
<p>The Manhattan distance reduces the effects of outliers in the data, and hence, should be used when we have noisy data with a lot of outliers. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hierarchical clustering</h1>
                </header>
            
            <article>
                
<p><strong>Hierarchical clustering</strong> aims to build a hierarchical structure of clusters from the observations. There are two strategies for generating the hierarchy:</p>
<ul>
<li><strong>Agglomerative clustering</strong>: In this approach, we use a bottom-up methodology, where each observation starts as its own cluster and clusters are merged at each stage of generating a hierarchy. </li>
<li><strong>Divisive clustering</strong>: In this approach, we use a top-down methodology, where we divide the observations into smaller clusters as we move down the stages of the hierarchy. </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Agglomerative clustering</h1>
                </header>
            
            <article>
                
<p>In <strong>agglomerative clustering</strong>, we start with each observation as its own cluster and combine these clusters based on certain criteria so that we end up with one cluster that contains all the observations. Similar to k-means clustering, we use distance metrics such as the Manhattan distance and the Euclidean distance in order to calculate the distance between two observations. We also use <strong>linkage criteria</strong>, which can represent the distance between two clusters. In this section, we study three linkage criteria, namely, <strong>complete-linkage clustering</strong>, <strong>single-linkage clustering</strong>, and <strong>average-linkage clustering</strong>. </p>
<p>Complete-linkage clustering calculates the distance between two clusters as the maximum distance between observations from two clusters and is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f6e4cb11-684a-4bb2-a529-8b0a410a4bf8.png" style="width:17.75em;height:1.42em;"/></p>
<p>Single-linkage clustering calculates the distance between two clusters as the minimum distance between observations from two clusters and is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/579ab169-d590-4bb8-8ca2-3b17d2c5911c.png" style="width:16.58em;height:1.33em;"/></p>
<p>Average-linkage clustering calculates the distance between each observation from cluster A with cluster B and normalizes it based on the observations in cluster A and B. This is represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c3561735-c411-4586-93f0-5a95df3b2c4c.png" style="width:15.42em;height:3.00em;"/></p>
<p>Thus, in the first step of agglomerative clustering, we use distance methodology to calculate the distance between each observation and merge observations with the smallest distance. For the second step, we calculate the distances between each cluster using linkage criteria based on the methodologies just presented. We run the necessary iterations until we only have one cluster left with all observations in it.</p>
<p>The following diagram shows how agglomerative clustering would work for observations with only one continuous variable:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-743 image-border" src="assets/0ea5a2bb-81df-45e0-a62b-76365fedbabe.png" style="width:23.08em;height:18.83em;"/></p>
<p>In this example, we have five observations with one continuous feature. In the first iteration, we look at the Euclidean distance between each observation and can deduce that records 1 and 2 are closest to each other. Hence, in the first iteration, we merge the observations 1 and 2. In the second iteration, we discover that the observations 10 and 15 are the closest records and create a new cluster from it. In the third iteration, we observe that the distance between the <span>(1,2) </span>cluster and the <span>(</span>10,15<span>) </span>cluster is smaller than any of those clusters and observation 90<strong>.</strong> Hence, we create a cluster of (1,2,10,15) in the third iteration. Finally, in the last iteration, we add element 90 to the cluster and terminate the process of agglomerative clustering. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Divisive clustering</h1>
                </header>
            
            <article>
                
<p><strong>Divisive clustering</strong> is a top-bottom approach where we first start with a large cluster with all observations and, at iteration, we split the clusters into smaller clusters. The process is similar to using distances and linkage criteria such as agglomerative clustering. The aim is to find an observation or cluster in the larger cluster that has the furthest distance from the rest of the cluster. In each iteration, we look at a cluster and recursively split the larger clusters by finding clusters that have the farthest distance from one another. Finally, the process is stopped when each observation is its own cluster. Divisive clustering uses an exhaustive search to find the perfect split in each cluster, which may be computationally very expensive. </p>
<p class="mce-root"/>
<p>Hierarchical clustering approaches are computationally more expensive than a k-means approach. Hence, even on medium-sized datasets, hierarchical cluster approaches may struggle to generate results compared to a k-means approach. However, since we do not need to start with a random partition at the start of hierarchical clustering, they remove the risks in the k-means approach where a bad random partition may hurt the clustering process. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering with Apache Spark on EMR</h1>
                </header>
            
            <article>
                
<p>In this section, we step through the creation of a clustering model capable of grouping consumer patterns in three distinct clusters. The first step will be to launch an EMR notebook along with a small cluster (a single <kbd>m5.xlarge</kbd> node works fine as the dataset we selected is not very large). Simply follow these steps:</p>
<ol>
<li>The first step is to load the dataframe and inspect the dataset:</li>
</ol>
<pre style="padding-left: 60px">df = spark.read.csv(SRC_PATH + 'data.csv', <br/>                    header=True, <br/>                    inferSchema=True)</pre>
<p style="padding-left: 60px">The following screenshot shows the first few lines of our <kbd>df</kbd> dataframe:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-744 image-border" src="assets/0ac39800-71d9-4400-a9e8-e0fe9d0518be.png" style="width:140.00em;height:29.83em;"/></p>
<p style="padding-left: 60px">As you see, the dataset involves transactions of products bought by different customers at different times and in different locations. We attempt to cluster these customer transactions using k-means by looking at three factors:</p>
<ul>
<li style="font-weight: 400">The product (represented by the <kbd>StockCode</kbd> column)</li>
<li style="font-weight: 400">The country where the product was bought</li>
<li style="font-weight: 400">The total amount spent by the customer across all products</li>
</ul>
<p style="padding-left: 60px">Note that this last factor is not directly available in the dataset, but it seems like an intuitively valuable feature (whether the client is a big spender or not). Oftentimes, during our feature preparation, we need to find aggregate values and plug them into our dataset.</p>
<ol start="2">
<li>On this occasion, we first find the total amount spent by each customer by multiplying the <kbd>Quantity</kbd> and <kbd>UnitPrice</kbd> columns on a new column:</li>
</ol>
<pre style="padding-left: 60px">df = df.selectExpr("*",<br/>                   "Quantity * UnitPrice as TotalBought")</pre>
<p style="padding-left: 60px">The following screenshot shows the first few lines of our modified <kbd>df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-745 image-border" src="assets/ce250198-23ec-4819-b866-11e0ccfa48c2.png" style="width:153.83em;height:29.75em;"/></p>
<ol start="3">
<li>Then, we proceed to aggregate the <kbd>TotalBought</kbd> column by a customer:</li>
</ol>
<pre style="padding-left: 60px">customer_df = df.select("CustomerID","TotalBought")<br/>   .groupBy("CustomerID")<br/>   .sum("TotalBought")<br/>   .withColumnRenamed('sum(TotalBought)','SumTotalBought')</pre>
<p style="padding-left: 60px">The following screenshot shows the first few lines of the <kbd>customer_df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-746 image-border" src="assets/29eae442-050e-48aa-9bb0-59c99d1f00e6.png" style="width:13.67em;height:10.50em;"/></p>
<p class="mce-root"/>
<ol start="4">
<li>We can then join back this new column back to our original dataset based on the customer:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.sql.functions import *<br/>joined_df = df.join(customer_df, 'CustomerId')</pre>
<p style="padding-left: 60px">The following screenshot shows the first few lines of the <kbd>joined_df</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-835 image-border" src="assets/cbc94201-5161-4703-8c1a-cc5aa0687792.png" style="width:166.58em;height:33.25em;"/></p>
<p>Note that two of the features that we are interested in using for clustering (<strong>Country</strong> and <strong>StockCode</strong>) are categorical. Hence, we need to find a way to encode those two numbers, similar to what we did in the previous chapter. String indexing these features would not be suitable in this case, as k-means works by computing distances between data points. Distances between artificial indices assigned to string values do not convey a lot of information. Instead, we apply one hot encoding to these features so that the vector distances represent something meaningful (note that two data points coinciding on most vector components have a cosine or Euclidean distance closer to 0).</p>
<p>Our pipeline will consist of two one hot encoding steps (for <strong>Country</strong> and <strong>Product</strong>), and a column that represents whether a customer is a big, normal, or small spender. To determine this, we discretize the <kbd>SumTotalBought</kbd> column into three values using a <kbd>QuantileDiscretizer</kbd>, which will result in three buckets depending on the quantile each customer falls into. We use the vector assembler to compile a vector of features. Given that the k-means algorithm works by computing distances, we normalize the feature vector so that the third feature (spender bucker) does not have a higher influence on the distance, as it has larger absolute values in the vector component. Finally, our pipeline will run the k-means estimator.</p>
<ol>
<li><span><span>In the following code block, we define the stages of our pipeline and fit a model:</span></span></li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml import Pipeline<br/>from pyspark.ml.clustering import KMeans<br/>from pyspark.ml.feature import Normalizer<br/>from pyspark.ml.feature import OneHotEncoder<br/>from pyspark.ml.feature import QuantileDiscretizer<br/>from pyspark.ml.feature import StringIndexer<br/>from pyspark.ml.feature import VectorAssembler<br/><br/><br/>stages = [ <br/>   StringIndexer(inputCol='StockCode', <br/>                 outputCol="stock_code_index", <br/>                 handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='stock_code_index', <br/>                 outputCol='stock_code_encoded'),<br/>   StringIndexer(inputCol='Country', <br/>                 outputCol='country_index', <br/>                 handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='country_index', <br/>                 outputCol='country_encoded'),<br/>   QuantileDiscretizer(numBuckets=3,<br/>                       inputCol='SumTotalBought',<br/>                       outputCol='total_bought_index'),<br/>   VectorAssembler(inputCols=['stock_code_encoded', <br/>                              'country_encoded', <br/>                              'total_bought_index'],<br/>                   outputCol='features_raw'),<br/>   Normalizer(inputCol="features_raw",         <br/>              outputCol="features", p=1.0),<br/>   KMeans(featuresCol='features').setK(3).setSeed(42) ]<br/><br/>pipeline = Pipeline(stages=stages)<br/>model = pipeline.fit(joined_df)</pre>
<ol start="2">
<li>Once we have a model, we apply that model to our dataset to obtain the clusters each transaction falls into:</li>
</ol>
<pre style="padding-left: 60px">df_with_clusters = model.transform(joined_df).cache()</pre>
<p style="padding-left: 60px">The following screenshot shows the first lines of the <kbd>df_with_clusters</kbd> dataframe:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-747 image-border" src="assets/32b5eb56-16ff-4907-b74a-8ff01df740c5.png" style="width:159.83em;height:48.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Note the new <strong>prediction</strong> column, which holds the value of cluster each row belongs to. We evaluate how well the clusters were formed by using the silhouette metric, which measures how similar data points are within their cluster compared to other clusters:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">from pyspark.ml.evaluation import ClusteringEvaluator<br/><br/>evaluator = ClusteringEvaluator()<br/>silhouette = evaluator.evaluate(df_with_clusters)</pre>
<p style="padding-left: 60px">In this example, we got a value of 0.35, which is average as a clustering score (ideally it's near 1.0, but at least it's positive). One main reason for not having a larger value is because we did not reduce the dimensionality of our vectors. Typically, before clustering, we apply some transformation for reducing the cardinality of the feature vector, such as <strong>principal component analysis</strong> (<strong>PCA</strong>). We didn't include such a step in this example for simplicity.</p>
<ol start="4">
<li>We can now examine each cluster to have a sense of how the data was clustered. The first thing to look at is the size of each cluster. As we can see in the following, the clusters vary in size, where one cluster captures more than half of the data points:</li>
</ol>
<pre style="padding-left: 60px">df_with_clusters<br/>.groupBy("prediction")<br/>.count()<br/>.toPandas()<br/>.plot(kind='pie',x='prediction', y='count')</pre>
<p style="padding-left: 60px">The following diagram shows the relative sizes of the different clusters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-748 image-border" src="assets/9599bbf9-7d2f-4c94-89c5-5c9ca363c0bd.png" style="width:20.50em;height:18.42em;"/></p>
<p class="mce-root"/>
<ol start="5">
<li>If we look at the countries contained on each cluster, we can see that two clusters just contain data points from the UK, and the third cluster only contains points from the rest of the countries<span>. We first inspect the counts for cluster 0:</span></li>
</ol>
<pre style="padding-left: 60px">df_with_clusters \<br/>.where(df_with_clusters.prediction==0) \<br/>.groupBy("Country") \<br/>.count() \<br/>.orderBy("count", ascending=False) \<br/>.show()<br/><br/>+--------------+------+<br/>| Country      | count|<br/>+--------------+------+<br/>|United Kingdom|234097|<br/>+--------------+------+</pre>
<p> </p>
<p style="padding-left: 60px">Similarly, the count for cluster 1 is displayed:</p>
<pre style="padding-left: 60px">df_with_clusters \<br/>.where(df_with_clusters.prediction==1) \<br/>.groupBy("Country") \<br/>.count() \<br/>.orderBy("count", ascending=False) \<br/>.show()<br/><br/>+--------------+------+<br/>| Country .    | count|<br/>+--------------+------+<br/>|United Kingdom|127781|<br/>+--------------+------+</pre>
<p> </p>
<p style="padding-left: 60px">Finally, the count for cluster 2 is shown:</p>
<pre style="padding-left: 60px">df_with_clusters \<br/>.where(df_with_clusters.prediction==2) \<br/>.groupBy("Country") \<br/>.count() \<br/>.orderBy("count", ascending=False) \<br/>.show()<br/><br/>+---------------+-----+<br/>| Country       |count|<br/>+---------------+-----+<br/>| Germany       | 9495|<br/>| France        | 8491|<br/>.<br/>.<br/>| USA           | 291 |<br/>+---------------+-----+</pre>
<ol start="6">
<li>An interesting<span> </span>observation<span> </span>is that the different clusters seem to<span> </span>have<span> </span>very different spending profiles:</li>
</ol>
<pre style="padding-left: 60px">pandas_df = df_with_clusters \<br/>   .limit(5000) \<br/>   .select('CustomerID','InvoiceNo','StockCode',<br/>           'Description','Quantity','InvoiceDate',<br/>           'UnitPrice','Country','TotalBought',<br/>            'SumTotalBought','prediction') \<br/>   .toPandas()<br/><br/>pandas_df.groupby('prediction') \<br/>.describe()['SumTotalBought']['mean'] \<br/>.plot(kind='bar', <br/>      title = 'Mean total amount bought per cluster')</pre>
<p style="padding-left: 60px">The preceding <kbd>plot()</kbd> command produces the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-749 image-border" src="assets/9a7332a4-f25c-4dcf-889d-a2ee487523c1.png" style="width:153.50em;height:76.42em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>To have a sense of how each cluster classifies the products, we look at the product description field of the different clusters. A nice visual representation is to use a word cloud with the words that appear on the product descriptions of each cluster. Using the Python <kbd>wordcloud</kbd> library, we can create a function that strips out the words of the products and constructs a wordcloud:</li>
</ol>
<pre style="padding-left: 60px">import itertools<br/>import re<br/>from functools import reduce<br/>import matplotlib.pyplot as plt<br/>from wordcloud import WordCloud, STOPWORDS<br/><br/>def plot_word_cloud(description_column):<br/>   list_of_word_sets = description_column \<br/>        .apply(str.split) \<br/>        .tolist()<br/>   text = list(itertools.chain(*list_of_word_sets))<br/>   text = map(lambda x: re.sub(r'[^A-Z]', r'', x), text)<br/>   text = reduce(lambda x, y: x + ' ' + y, text)<br/>   wordcloud = WordCloud(<br/>       width=3000,<br/>       height=2000,<br/>       background_color='black',<br/>       stopwords=STOPWORDS,<br/>       collocations=False).generate(str(text))<br/>   fig = plt.figure(<br/>       figsize=(10, 5),<br/>       facecolor='k',<br/>       edgecolor='k')<br/>   plt.imshow(wordcloud, interpolation='bilinear')<br/>   plt.axis('off')<br/>   plt.tight_layout(pad=0)<br/>   plt.show()</pre>
<ol start="8">
<li>We call this function on each cluster and obtain the following:</li>
</ol>
<pre>plot_word_cloud(pandas_df[pandas_df.prediction==0].Description)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">The resulting word cloud for cluster 0 is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-750 image-border" src="assets/c7bf2242-eafa-4777-9c1d-76175feba753.png" style="width:32.17em;height:21.50em;"/></p>
<ol start="9">
<li class="mce-root">Take a look at the following code:</li>
</ol>
<pre style="padding-left: 60px">plot_word_cloud(pandas_df[pandas_df.prediction==1].Description)</pre>
<p style="padding-left: 60px"><span>The resulting word cloud for cluster 1 is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-751 image-border" src="assets/80f586a8-d947-4a86-93aa-c6b130d6e63d.png" style="width:32.75em;height:21.83em;"/></p>
<p>Take <span>a look at the following code:</span></p>
<pre style="padding-left: 60px">plot_word_cloud(pandas_df[pandas_df.prediction==2].Description)</pre>
<p style="padding-left: 60px"><span>The resulting word cloud for cluster 2 is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-752 image-border" src="assets/80b490b1-b5bb-47f2-ad51-3207ef270632.png" style="width:30.42em;height:20.33em;"/></p>
<p>We can see in the word clouds that, despite some very common words, the relative importance of a few words such as C<em>hristmas</em> or <em>retrospot</em> comes out with a higher weight on one of the clusters. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering with Spark and SageMaker on EMR</h1>
                </header>
            
            <article>
                
<p><span>In this section, we will show how </span><strong>Spark</strong><span> and </span><strong>SageMaker</strong><span> can work together seamlessly through code integration. </span></p>
<p>In the previous chapter, regarding decision trees, we performed the data preparation in Apache Spark through <strong>EMR</strong> and uploaded the prepared data in S3 to then open a SageMaker notebook instance using the <kbd>SageMaker</kbd> Python library to perform the training. There is an alternative way of doing the same thing, which, on many occasions, is more convenient, using the <kbd>sagemaker_pyspark</kbd> library. This library allows us to perform the training stage through SageMaker services just by adding a special stage to our pipeline. To do this, we will define a pipeline identical to the one we wrote in the previous section, with the difference being the last stage.</p>
<p class="mce-root"/>
<p>Instead of including Apache Spark's implementation of <kbd>KMeans</kbd>, we will use  <kbd>KMeansSageMakerEstimator</kbd>:</p>
<ol>
<li>Firstly, we will import all the necessary dependencies:</li>
</ol>
<pre style="padding-left: 60px">from pyspark.ml import Pipeline<br/>from pyspark.ml.clustering import KMeans<br/>from pyspark.ml.feature import Normalizer<br/>from pyspark.ml.feature import OneHotEncoder<br/>from pyspark.ml.feature import QuantileDiscretizer<br/>from pyspark.ml.feature import StringIndexer<br/>from pyspark.ml.feature import VectorAssembler<br/>from sagemaker_pyspark import IAMRole<br/>from sagemaker_pyspark.algorithms import KMeansSageMakerEstimator</pre>
<ol start="2">
<li>Next, we start by defining the IAM role to use and the full pipeline:</li>
</ol>
<pre style="padding-left: 60px">role = 'arn:aws:iam::095585830284:role/EMR_EC2_DefaultRole'<br/><br/>kmeans = KMeansSageMakerEstimator(<br/>  sagemakerRole=IAMRole(role),<br/>  trainingInstanceType="ml.m4.xlarge",<br/>  trainingInstanceCount=1,<br/>  endpointInstanceType="ml.m4.xlarge",<br/>  endpointInitialInstanceCount=1)<br/>kmeans.setK(3)<br/>kmeans.setFeatureDim(3722)<br/><br/>stages = [<br/>   StringIndexer(inputCol='StockCode', outputCol="stock_code_index", handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='stock_code_index', outputCol='stock_code_encoded'),<br/>   StringIndexer(inputCol='Country', outputCol='country_index', handleInvalid='keep'),<br/>   OneHotEncoder(inputCol='country_index', outputCol='country_encoded'),<br/>   QuantileDiscretizer(numBuckets=3, inputCol='SumTotalBought',outputCol='total_bought_index'),<br/>   VectorAssembler(inputCols=['stock_code_encoded', 'country_encoded', 'total_bought_index'],<br/>                   outputCol='features_raw'),<br/>   Normalizer(inputCol="features_raw", <br/>              outputCol="features", p=1.0),<br/>              kmeans ]<br/><br/>pipeline = Pipeline(stages=stages)<br/>model = pipeline.fit(joined_df)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p><kbd>KMeansSageMakerEstimator</kbd> implements the estimator interface from Apache Spark, so it is included as any other estimator or transformer on our pipelines. Through the <kbd>KMeansSageMakerEstimator</kbd> constructor, we define the amount and type of machines to use as well as the IAM role. We explain the purpose of the role in the next subsection, <em>Understanding the purpose of the IAM Role</em>. Additionally, we set the number of clusters we want to create (value of <em>k</em>) as well as the length of the vectors we'll be using for training (which we find by examining the output rows from the last section).</p>
<p><span>Let's look at what happens under the hood when we call <kbd>fit()</kbd> on the pipeline. </span>The first part of the pipeline works exactly the same as before, whereby the different stages launch Spark jobs that run a series of transformations to the dataset by appending columns (for example, the encoded vectors or discretized features). The last stage, being a SageMaker estimator, works in a slightly different way. Instead of using the EMR cluster resources to compute and train the clustering model, it saves the data in S3 and makes an API call to the SageMaker KMeans service pointing to that S3 temporary location. The SageMaker service, in turn, spins up EC2 servers to perform the training and creates both a SageMaker model and endpoint. Once the training is complete, the <kbd>KMeansSageMakerEstimator</kbd> stores a reference to the newly created endpoint that is used each time the model's <kbd>transfom()</kbd> method is called.</p>
<p>You can find the models and endopoints created by  <kbd>KMeansSageMakerEstimator</kbd> by inspecting the SageMaker AWS console at <a href="https://console.aws.amazon.com/sagemaker/">https://console.aws.amazon.com/sagemaker/</a>.</p>
<p>Now, follow these steps:</p>
<ol>
<li>Let's examine what happens when we call the <kbd>transform()</kbd> method of the pipeline:</li>
</ol>
<pre><span>df_with_clusters = model.transform(joined_df)</span></pre>
<p>The first series of transformations (the data preparation stages) will run on the EMR cluster through Spark jobs. As the final transformation is a SageMaker model, it relies on the SageMaker endpoint to obtain the predictions (in our case, the cluster assignment).</p>
<div class="packt_tip"><span>You should remember to delete the endpoint (for example using the console) once it's no longer required.</span></div>
<p class="mce-root"/>
<ol start="2">
<li>Then, run the following code:</li>
</ol>
<pre style="padding-left: 60px">df_with_clusters.show(5)</pre>
<p>Take a look at the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-753 image-border" src="assets/98afac8f-020c-4fe6-89d6-cd9834d34d98.png" style="width:137.08em;height:24.08em;"/></p>
<p style="padding-left: 60px"><em>(</em>The image has been truncated to show just the last few columns.<em>)</em></p>
<p style="padding-left: 60px">Note how the two columns were added by the  <kbd>distance_to_cluster</kbd> and <kbd>closest_cluster</kbd> pipelines.</p>
<ol start="3">
<li>By instructing the cluster evaluator to use this column, we can evaluate the clustering ability:</li>
</ol>
<pre style="padding-left: 60px">evaluator = ClusteringEvaluator()<br/>evaluator.setPredictionCol("closest_cluster")<br/>silhouette = evaluator.evaluate(df_with_clusters)</pre>
<p class="mce-root"><span>The silhouette value we get is almost the same as the one using Spark's algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the purpose of the IAM role</h1>
                </header>
            
            <article>
                
<p>SageMaker is a managed service on AWS that manages the hardware responsible for training and inference. In order for SageMaker to perform such tasks on your behalf, you need to allow it through IAM configuration. For example, if you're running on EMR, the EC2 instances (that is, the computers) in the cluster are running with a specific role. This role can be found by going to the cluster page on the EMR console: <a href="https://console.aws.amazon.com/elasticmapreduce/">https://console.aws.amazon.com/elasticmapreduce/.</a></p>
<p><a href="https://console.aws.amazon.com/elasticmapreduce/"/></p>
<p><span><span>The following screenshot shows the cluster details, including the security and access information:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-754 image-border" src="assets/784faccc-3f9a-42a9-a96b-af2e924b2207.png" style="width:150.33em;height:83.33em;"/></p>
<p>The role under <span class="packt_screen">EC2 instance profile</span> in the previous screenshot is the one we are using, that is, <kbd>EMR_EC2_DefaultRole</kbd>.</p>
<p class="mce-root"/>
<p>We then go to the IAM console at <a href="https://console.aws.amazon.com/iam/home">https://console.aws.amazon.com/iam/home</a> to edit the permissions of that role to grant access to SageMaker resources, as well as allow the role to be assumed:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-755 image-border" src="assets/08002147-dc8f-4698-bb68-17ffa7ea963f.png" style="width:119.25em;height:99.83em;"/></p>
<p>In the <span class="packt_screen">Trust relationships</span> section, we click on the <span class="packt_screen">Edit trust relationship</span> button to open the dialog that will allow us to add the settings:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-756 image-border" src="assets/54eee129-2ba9-4315-9eb4-e3067ebcbf48.png" style="width:123.58em;height:40.92em;"/></p>
<p>You can edit and allow the role to be assumed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-757 image-border" src="assets/0fa43a28-a41b-4805-8db5-4e66534142aa.png" style="width:30.42em;height:29.00em;"/></p>
<p>The previous changes are required to allow our EMR cluster to talk to SageMaker and enable the kind of integration described in this section.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we studied the difference between supervised and unsupervised learning and looked at situations when unsupervised learning is applied. We studied the exploratory analysis application of unsupervised learning, where clustering approaches are used. We studied the k-means clustering and hierarchical clustering approaches in detail and looked at examples of how they are applied. </p>
<p>We also looked at how clustering approaches can be implemented on Apache Spark on AWS clusters. In our experience, clustering tasks are generally done on larger datasets, and, hence, taking the setup of the cluster into account for such tasks is important. We discussed these nuances in this chapter. </p>
<p><span>As a data scientist, there have been many situations where we analyze data with the sole purpose of extracting value from the data. You should consider clustering approaches in these cases as it will help you to understand the inherent structure in your data. Once you discover the patterns in your data, you can identify events and categories by which your data is arranged. Once you have established your clusters, you can also evaluate any new observation based on which cluster it may belong to and predict that the observation will exhibit similar behavior to other observations in the cluster. </span></p>
<p>In the next chapter, we will cover a very interesting problem: how to make recommendations through machine learning by finding products that similar users find relevant.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the situations where you would apply the k-means algorithm compared to hierarchical clustering?</li>
<li>What is the difference between a regular Spark estimator and an estimator that calls SageMaker?</li>
<li>For a dataset that takes too long to train, why would it not be a good idea to launch such a job using a SageMaker estimator?</li>
<li>Research and establish other alternative metrics for cluster evaluation. </li>
<li>Why is string indexing not a good idea when encoding features for k-means?</li>
</ol>


            </article>

            
        </section>
    </body></html>