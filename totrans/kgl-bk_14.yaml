- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Simulation and Optimization Competitions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟和优化竞赛
- en: '**Reinforcement learning** (**RL**) is an interesting case among the different
    branches of machine learning. On the one hand, it is quite demanding from a technical
    standpoint: various intuitions from supervised learning do not hold, and the associated
    mathematical apparatus is quite a bit more advanced; on the other hand, it is
    the easiest one to explain to an outsider or layperson. A simple analogy is teaching
    your pet (I am very intentionally trying to steer clear of the dogs versus cats
    debate) to perform tricks: you provide a treat for a trick well done, and refuse
    it otherwise.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）在机器学习的不同分支中是一个有趣的案例。一方面，它在技术层面上要求很高：来自监督学习的各种直觉并不适用，相关的数学工具也更为复杂；另一方面，它对于外行或非专业人士来说最容易解释。一个简单的类比是教你的宠物（我非常有意地避免狗与猫的争论）表演特技：你为做得好的特技提供奖励，否则拒绝。'
- en: Reinforcement learning was a latecomer to the competition party on Kaggle, but
    the situation has changed in the last few years with the introduction of simulation
    competitions. In this chapter, we will describe this new and exciting part of
    the Kaggle universe. So far – at the time of writing – there have been four **Featured**
    competitions and two **Playground** ones; this list, while admittedly not extensive,
    allows us to give a broad overview.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在Kaggle的竞赛派对中是一个后来者，但近年来，随着模拟竞赛的引入，情况发生了变化。在本章中，我们将描述Kaggle宇宙中这个新而令人兴奋的部分。到目前为止——在撰写本文时——已经举办了四个**特色**竞赛和两个**游乐场**竞赛；虽然这个列表并不广泛，但它允许我们给出一个广泛的概述。
- en: 'In this chapter, we will demonstrate solutions to the problems presented in
    several simulation competitions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示在几个模拟竞赛中提出的问题的解决方案：
- en: We begin with *Connect X*.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从*Connect X*开始。
- en: We follow with *Rock, Paper, Scissors*, where a dual approach to building a
    competitive agent is shown.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们接着展示*剪刀石头布*，其中展示了构建竞争性代理的双重方法。
- en: Next, we demonstrate a solution based on multi-armed bandits to the *Santa*
    competition.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们展示了一个基于多臂老虎机的解决方案，用于*Santa*竞赛。
- en: We conclude with an overview of the remaining competitions, which are slightly
    outside the scope of this chapter.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们以对剩余竞赛的概述结束，这些竞赛略超出了本章的范围。
- en: If reinforcement learning is a completely new concept for you, it is probably
    a good idea to get some basic understanding first. A very good way to start on
    the RL adventure is the Kaggle Learning course dedicated to this very topic in
    the context of Game AI ([https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)).
    The course introduces basic concepts such as agents and policies, also providing
    a (crash) introduction to deep reinforcement learning. All the examples in the
    course use the data from the Playground competition *Connect X*, in which the
    objective is to train an agent capable of playing a game of connecting checkers
    in a line ([https://www.kaggle.com/c/connectx/overview](https://www.kaggle.com/c/connectx/overview)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果强化学习对你来说是一个全新的概念，那么首先获得一些基本理解可能是个好主意。开始RL冒险的一个非常好的方式是Kaggle学习课程，该课程专门针对游戏AI背景下的这个主题([https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning))。该课程介绍了诸如代理和策略等基本概念，同时也提供了一个（快速）介绍深度强化学习。课程中的所有示例都使用了来自游乐场竞赛*Connect
    X*的数据，其目标是训练一个能够在线连接跳棋的代理([https://www.kaggle.com/c/connectx/overview](https://www.kaggle.com/c/connectx/overview))。
- en: 'On a more general level, it is worth pointing out that an important aspect
    of simulation and optimization competitions is the **environment**: due to the
    very nature of the problem, your solution needs to exhibit more dynamic characteristics
    than just submitting a set of numbers (as would be the case for “regular” supervised
    learning contests). A very informative and detailed description of the environment
    used in the simulation competitions can be found at [https://github.com/Kaggle/kaggle-environments/blob/master/README.md](https://github.com/Kaggle/kaggle-environments/blob/master/README.md).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的意义上，值得指出的是，模拟和优化竞赛的一个重要方面是**环境**：由于问题的本质，你的解决方案需要表现出比仅仅提交一组数字（如“常规”监督学习竞赛的情况）更动态的特性。关于模拟竞赛中使用的环境的非常详尽和有信息量的描述可以在[https://github.com/Kaggle/kaggle-environments/blob/master/README.md](https://github.com/Kaggle/kaggle-environments/blob/master/README.md)找到。
- en: Connect X
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Connect X
- en: In this section, we demonstrate how to approach the simple problem of playing
    checkers using heuristics. While not a deep learning solution, it is our view
    that this bare-bones presentation of the concepts is much more useful for people
    without significant prior exposure to RL.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示如何使用启发式方法解决玩跳棋的简单问题。虽然这不是深度学习解决方案，但我们认为这种概念的基本展示对那些没有显著先前接触RL的人来说更有用。
- en: 'If you are new to the concept of using AI for board games, the presentation
    by *Tom van de Wiele* ([https://www.kaggle.com/tvdwiele](https://www.kaggle.com/tvdwiele))
    is a resource worth exploring: [https://tinyurl.com/36rdv5sa](https://tinyurl.com/36rdv5sa).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触使用AI玩棋类游戏的概念，*汤姆·范·德·维勒*（[https://www.kaggle.com/tvdwiele](https://www.kaggle.com/tvdwiele)）的演示是一个值得探索的资源：[https://tinyurl.com/36rdv5sa](https://tinyurl.com/36rdv5sa)。
- en: The objective of *Connect X* is to get a number (*X*) of your checkers in a
    row – horizontally, vertically, or diagonally – on the game board before your
    opponent. Players take turns dropping their checkers into one of the columns at
    the top of the board. This means each move may have the purpose of trying to win
    for you or trying to stop your opponent from winning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*Connect X*的目标是在游戏棋盘上先于对手将你的棋子排成一行（水平、垂直或对角线）的*数量*（*X*）。玩家轮流将棋子放入棋盘顶部的某一列。这意味着每一步的目的可能是为了赢得比赛，或者是为了阻止对手赢得比赛。'
- en: '![](img/B17574_12_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_12_01.png)'
- en: 'Figure 12.1: Connect X board'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1：Connect X棋盘
- en: '*Connect X* was the first competition that introduced **agents**: instead of
    a static submission (or a Notebook that was evaluated against an unseen dataset),
    participants had to submit agents capable of playing the game against others.
    The evaluation proceeded in steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*Connect X*是第一个引入**代理**的比赛：参与者必须提交能够与其他人玩游戏的游戏代理，而不是静态的提交（或与未见数据集评估的笔记本）。评估按步骤进行：'
- en: Upon uploading, a submission plays against itself to ensure it works properly.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传后，提交将自行对弈以确保其正常工作。
- en: If this validation episode is successful, a skill rating is assigned, and the
    submission joins the ranks of all competitors.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这个验证片段成功，就会分配一个技能评分，并且提交将加入所有竞争者的行列。
- en: Each day, several episodes are played for each submission, and subsequently
    rankings are adjusted.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个提交都会播放几个片段，随后进行排名调整。
- en: With that setup in mind, let us proceed toward demonstrating how to build a
    submission for the *Connect X* competition. The code we present is for *X=4*,
    but can be easily adapted for other values or variable *X*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个设置，让我们继续展示如何为*Connect X*比赛构建一个提交。我们提供的代码适用于*X=4*，但可以轻松地适应其他值或变量*X*。
- en: 'First, we install the Kaggle environments package:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们安装Kaggle环境包：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We define an environment in which our agent will be evaluated:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个环境，我们的代理将在其中被评估：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: While a frequent impulse you might have is to try sophisticated methods, it
    is useful to start simple – as we will do here, by using simple heuristics. These
    are combined into a single function in the accompanying code, but for the sake
    of presentation, we describe them one at a time here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能有一个尝试复杂方法的冲动，但开始简单是有用的——正如我们在这里所做的那样，通过使用简单的启发式方法。这些方法在伴随的代码中组合成一个单独的函数，但为了展示，我们一次描述一个。
- en: 'The first rule is checking whether either of the players has a chance to connect
    four checkers vertically and, if so, returning the position at which it is possible.
    We can achieve this by using a simple variable as our input argument, which can
    take on two possible values indicating which player opportunities are being analyzed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条规则是检查是否有任何玩家有机会垂直连接四个棋子，如果有，就返回可能的位置。我们可以通过使用一个简单的变量作为我们的输入参数来实现这一点，它可以取两个可能的值，表示正在分析哪个玩家的机会：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can define an analogous method for horizontal chances:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个类似的方法来处理横向机会：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We repeat the same approach for the diagonal combinations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对对角线组合重复相同的做法：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can combine the logic into a single function checking the opportunities
    (playing the game against an opponent):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将逻辑组合成一个单独的函数，检查机会（与对手玩游戏）：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Those blocks constitute the basics of the logic. While a bit cumbersome to formulate,
    they are a useful exercise in converting an intuition into heuristics that can
    be used in an agent competing in a game.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些块构成了逻辑的基础。虽然制定起来有点繁琐，但它们是将直觉转化为可用于在游戏中竞争的代理的启发式策略的有用练习。
- en: Please see the accompanying code in the repository for a complete definition
    of the agent in this example.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅存储库中的相关代码，以获取本例中代理的完整定义。
- en: 'The performance of our newly defined agent can be evaluated against a pre-defined
    agent, for example, a random one:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们新定义的代理的性能可以与预定义的代理（例如，随机代理）进行比较：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The code above shows you how to set up a solution from scratch for a relatively
    simple problem (there is a reason why *Connect X* is a Playground and not a Featured
    competition). Interestingly, this simple problem can be handled with (almost)
    state-of-the-art methods like AlphaZero: [https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx](https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了如何从头开始设置一个相对简单问题的解决方案（这就是为什么*连接X*是一个游乐场而不是一个特色竞赛的原因）。有趣的是，这个简单的问题可以用（几乎）最先进的方法（如AlphaZero）来处理：[https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx](https://www.kaggle.com/connect4alphazero/alphazero-baseline-connectx)。
- en: With the introductory example behind us, you should be ready to dive into the
    more elaborate (or in any case, not toy example-based) contests.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍性例子之后，你应该准备好深入更复杂（或者至少不是基于玩具示例）的竞赛。
- en: Rock-paper-scissors
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剪刀石头布
- en: 'It is no coincidence that several problems in simulation competitions refer
    to playing games: at varying levels of complexity, games offer an environment
    with clearly defined rules, naturally lending itself to the agent-action-reward
    framework. Aside from Tic-Tac-Toe, connecting checkers is one of the simplest
    examples of a competitive game. Moving up the difficulty ladder (of games), let’s
    have a look at **rock-paper-scissors** and how a Kaggle contest centered around
    this game could be approached.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟竞赛中提到的几个问题涉及玩游戏并非巧合：在各个复杂程度下，游戏提供了一个规则明确的环境，自然适合代理-行动-奖励框架。除了井字棋之外，连接棋盘是竞争性游戏中最简单的例子之一。随着游戏难度等级的提升（即游戏难度），让我们来看看**剪刀石头布**以及围绕这个游戏展开的Kaggle竞赛应该如何进行。
- en: 'The idea of the *Rock, Paper, Scissors* competition ([https://www.kaggle.com/c/rock-paper-scissors/code](https://www.kaggle.com/c/rock-paper-scissors/code))
    was an extension of the basic rock-paper-scissors game (known as *roshambo* in
    some parts of the world): instead of the usual “best of 3” score, we use “best
    of 1,000.”'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*剪刀石头布*竞赛的构想([https://www.kaggle.com/c/rock-paper-scissors/code](https://www.kaggle.com/c/rock-paper-scissors/code))是对基本的剪刀石头布游戏（在一些地区被称为*roshambo*）的扩展：而不是通常的“三局两胜”的得分，我们使用“一千局一胜”。'
- en: 'We will describe two possible approaches to the problem: one rooted in the
    game-theoretic approach, and the other more focused on the algorithmic side.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将描述两种可能的解决问题的方法：一种基于博弈论方法，另一种更侧重于算法方面。
- en: We begin with the **Nash equilibrium**. Wikipedia gives the definition of this
    as the solution to a non-cooperative game involving two or more players, where
    each player is assumed to know the equilibrium strategies of the others, and no
    player can obtain an advantage by changing only their own strategy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从**纳什均衡**开始。维基百科将这个定义为一个涉及两个或更多玩家的非合作博弈的解，假设每个玩家都知道其他玩家的均衡策略，并且没有玩家可以通过仅改变自己的策略来获得优势。
- en: An excellent introduction to rock-paper-scissors in a game-theoretic framework
    can be found at [https://www.youtube.com/watch?v=-1GDMXoMdaY](https://www.youtube.com/watch?v=-1GDMXoMdaY).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在博弈论框架下对剪刀石头布的出色介绍可以在[https://www.youtube.com/watch?v=-1GDMXoMdaY](https://www.youtube.com/watch?v=-1GDMXoMdaY)找到。
- en: 'Denoting our players as red and blue, each cell in the matrix of outcomes shows
    the result of a given combination of moves:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 用红色和蓝色表示我们的玩家，矩阵中的每个单元格都显示了给定移动组合的结果：
- en: '![Obraz zawierający tekst, tablica wyników  Opis wygenerowany automatycznie](img/B17574_12_02.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本的图像，结果表格  自动生成的描述](img/B17574_12_02.png)'
- en: 'Figure 12.2: Payoff matrix for rock-paper-scissors'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2：剪刀石头布的收益矩阵
- en: As an example, if both play Rock (the top-left cell), both gain 0 points; if
    blue plays Rock and red plays Paper (the cell in the second column of the first
    row), red wins – so red gains +1 point and blue has -1 point as a result.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个例子来说明，如果双方都出石头（左上角的单元格），双方都得到0分；如果蓝色方出石头而红色方出布（第一行的第二列的单元格），红色方获胜——因此红色方得到+1分，而蓝色方得到-1分。
- en: If we played each action with an equal probability of 1/3, then the opponent
    must do the same; otherwise, if they play Rock all the time, they will tie against
    Rock, lose against Paper, and win against Scissors – each with a probability of
    1/3 (or one-third of the time). The expected reward, in this case, is 0, in which
    case we can change our strategy to Paper and win all the time. The same reasoning
    can be conducted for the strategy of Paper versus Scissors and Scissors versus
    Rock, for which we will not show you the matrix of outcomes due to redundancy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以1/3的等概率玩每个动作，那么对手也必须这样做；否则，如果他们总是出石头，他们将和石头平局，输给布，赢剪刀——每种情况发生的概率都是1/3（或三分之一的概率）。在这种情况下，期望奖励是0，这时我们可以改变策略为布，并一直获胜。同样的推理也可以用于布对剪刀和剪刀对石头的策略，由于冗余，我们不会展示结果的矩阵。
- en: 'The remaining option in order to be in equilibrium is that both players need
    to play a random strategy – which is the Nash equilibrium. We can build a simple
    agent around this idea:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到均衡，剩余的选项是两个玩家都需要采取随机策略——这就是纳什均衡。我们可以围绕这个想法构建一个简单的智能体：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The magic at the start (writing from a Notebook directly to a file) is necessary
    to satisfy the submission constraints of this particular competition.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时（从笔记本直接写入文件）的魔法是满足这个特定竞赛提交约束的必要条件。
- en: 'How does our Nash agent perform against others? We can find out by evaluating
    the performance:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的纳什智能体与其他智能体相比表现如何？我们可以通过评估性能来了解：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At the time of writing, there is an error that pops up after this import (**Failure
    to load a module named ‘gfootball’**); the official advice from Kaggle is to ignore
    it. In practice, it does not seem to have any impact on executing the code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，导入后出现了一个错误（**无法加载名为‘gfootball’的模块**）；Kaggle的官方建议是忽略它。实际上，它似乎对执行代码没有影响。
- en: 'We start by creating the rock-paper-scissors environment and setting the limit
    to 1,000 episodes per simulation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建石头剪刀布环境，并将模拟的每个模拟的episodes限制为1,000个：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will make use of a Notebook created in this competition that implemented
    numerous agents based on deterministic heuristics ([https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents](https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents))
    and import the code for the agents we compete against from there:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用在这个竞赛中创建的笔记本，该笔记本实现了基于确定性启发式算法的多个智能体（[https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents](https://www.kaggle.com/ilialar/multi-armed-bandit-vs-deterministic-agents)），并从那里导入我们竞争的智能体的代码：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'When we execute the preceding block and run the environment, we can watch an
    animated board for the 1,000 epochs. A snapshot looks like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行前面的代码块并运行环境时，我们可以观察1,000个epoch的动画板。一个快照看起来像这样：
- en: '![](img/B17574_12_03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_12_03.png)'
- en: 'Figure 12.3: A snapshot from a rendered environment evaluating agent performance'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3：评估智能体性能的渲染环境快照
- en: In supervised learning – both classification and regression – it is frequently
    useful to start approaching any problem with a simple benchmark, usually a linear
    model. Even though not state of the art, it can provide a useful expectation and
    a measure of performance. In reinforcement learning, a similar idea holds; an
    approach worth trying in this capacity is the multi-armed bandit, the simplest
    algorithm we can honestly call RL. In the next section, we demonstrate how this
    approach can be used in a simulation competition.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习——无论是分类还是回归——中，通常有用简单基准来开始处理任何问题，通常是线性模型。尽管它不是最先进的，但它可以提供一个有用的期望值和性能度量。在强化学习中，一个类似的想法也成立；在这个能力中值得尝试的方法是多臂老虎机，这是我们能够诚实地称之为RL的最简单算法。在下一节中，我们将展示这种方法如何在模拟竞赛中使用。
- en: Santa competition 2020
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 圣诞竞赛2020
- en: 'Over the last few years, a sort of tradition has emerged on Kaggle: in early
    December, there is a Santa-themed competition. The actual algorithmic side varies
    from year to year, but for our purposes, the 2020 competition is an interesting
    case: [https://www.kaggle.com/c/santa-2020](https://www.kaggle.com/c/santa-2020).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，Kaggle上形成了一种传统：在每年的12月初，会有一个以圣诞为主题的竞赛。实际的算法方面每年都有所不同，但就我们的目的而言，2020年的竞赛是一个有趣的案例：[https://www.kaggle.com/c/santa-2020](https://www.kaggle.com/c/santa-2020)。
- en: 'The setup was a classical **multi-armed bandit** (**MAB**) trying to maximize
    reward by taking repeated action on a vending machine, but with two extras:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 设置是一个经典的**多臂老虎机**（**MAB**）算法，通过在自动售货机上重复操作来最大化奖励，但有两个额外特点：
- en: '**Reward decay**: At each step, the probability of obtaining a reward from
    a machine decreases by 3 percent.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励衰减**：在每一步，从机器获得奖励的概率会降低3%。'
- en: '**Competition**: You are constrained not only by time (a limited number of
    attempts) but also by another player attempting to achieve the same objective.
    We mention this constraint mostly for the sake of completeness, as it is not crucial
    to incorporate explicitly in our demonstrated solution.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞赛**：您不仅受时间的限制（有限次数的尝试），还受另一个试图实现相同目标的玩家的限制。我们主要提到这个限制是为了完整性，因为它不是将我们的解决方案明确包含在内的关键因素。'
- en: For a good explanation of the methods for approaching the general MAB problem,
    the reader is referred to [https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如何解决通用多臂老虎机问题的方法的好解释，读者可以参考[https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html](https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html)。
- en: 'The solution we demonstrate is adapted from [https://www.kaggle.com/ilialar/simple-multi-armed-bandit](https://www.kaggle.com/ilialar/simple-multi-armed-bandit),
    code from *Ilia Larchenko* ([https://www.kaggle.com/ilialar](https://www.kaggle.com/ilialar)).
    Our approach is based on successive updates to the distribution of reward: at
    each step, we generate a random number from a Beta distribution with parameters
    (*a+1*, *b+1*) where:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示的解决方案是从[https://www.kaggle.com/ilialar/simple-multi-armed-bandit](https://www.kaggle.com/ilialar/simple-multi-armed-bandit)改编的，代码来自*Ilia
    Larchenko* ([https://www.kaggle.com/ilialar](https://www.kaggle.com/ilialar))。我们的方法基于对奖励分布的连续更新：在每一步，我们从具有参数(*a+1*,
    *b+1*)的Beta分布中生成一个随机数，其中：
- en: '*a* is the total reward from this arm (number of wins)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是从这个臂获得的总奖励（胜利次数）'
- en: '*b* is the number of historical losses'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是历史损失的数量'
- en: When we need to decide which arm to pull, we select the arm with the highest
    generated number and use it to generate the next step; our posterior distribution
    becomes a prior for the next step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要决定拉哪个臂时，我们选择产生最高数字的臂并使用它来生成下一步；我们的后验分布成为下一步的先验。
- en: 'The graph below shows the shape of a Beta distribution for different pairs
    of (*a*, *b*) values:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了不同(*a*, *b*)值对的Beta分布形状：
- en: '![](img/B17574_12_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![img/B17574_12_04.png](img/B17574_12_04.png)'
- en: 'Figure 12.4: Shape of the beta distribution density for different combinations
    of (a,b) parameters'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：不同(a,b)参数组合的Beta分布密度形状
- en: As you can see, initially, the distribution is flat (Beta(0,0) is uniform),
    but as we gather more information, it concentrates the probability mass around
    the mode, which means there is less uncertainty and we are more confident about
    our judgment. We can incorporate the competition-specific reward decay by decreasing
    the *a* parameter every time an arm is used.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，最初，分布是平坦的（Beta(0,0)是均匀的），但随着我们收集更多信息，概率质量会集中在峰值周围，这意味着不确定性更少，我们对判断更有信心。我们可以通过每次使用一个臂来减少*a*参数来整合特定于比赛的奖励衰减。
- en: 'We begin the creation of our agent by writing a submission file. First, the
    necessary imports and variable initialization:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过编写一个提交文件来开始创建我们的代理。首先，必要的导入和变量初始化：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We define the class specifying an MAB agent. For the sake of reading coherence,
    we reproduce the entire code and include the explanations in comments within it:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个指定MAB代理的类。为了阅读的连贯性，我们重新生成了整个代码，并在其中包含注释说明：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, the core logic of the function is a straightforward implementation
    of the MAB algorithm. An adjustment specific to our contest occurs with the `bandit_state`
    variable, where we apply the decay multiplier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，函数的核心逻辑是对MAB算法的直接实现。在`bandit_state`变量中，我们应用衰减乘数，这是针对我们竞赛的特定调整。
- en: 'Similar to the previous case, we are now ready to evaluate the performance
    of our agent in the contest environment. The code snippet below demonstrates how
    this can be implemented:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个案例类似，我们现在已经准备好评估我们的代理在竞赛环境中的性能。下面的代码片段展示了如何实现这一点：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We see something like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到类似这样的情况：
- en: '![Obraz zawierający tekst, sprzęt elektroniczny, zrzut ekranu, wyświetlanie  Opis
    wygenerowany automatycznie](img/B17574_12_05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、电子设备、屏幕截图、显示的自动生成的描述](img/B17574_12_05.png)'
- en: 'Figure 12.5: Snapshot from a rendered environment evaluating agent performance'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：评估代理性能的渲染环境快照
- en: In this section, we demonstrated how a vintage multi-armed bandit algorithm
    can be utilized in a simulation competition on Kaggle. While useful as a starting
    point, this was not sufficient to qualify for the medal zone, where deep reinforcement
    learning approaches were more popular.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何在Kaggle的模拟竞赛中利用经典的多臂老虎机算法。虽然作为一个起点很有用，但这并不足以获得奖牌区，在那里深度强化学习方法更为流行。
- en: We will follow up with a discussion of approaches based on other methods, in
    a diverse range of competitions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将接着讨论基于其他方法的策略，在多样化的竞赛中。
- en: The name of the game
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏的名称
- en: 'Beyond the relatively elementary games discussed above, simulation competitions
    involve more elaborate setups. In this section, we will briefly discuss those.
    The first example is *Halite*, defined on the competition page ([https://www.kaggle.com/c/halite](https://www.kaggle.com/c/halite))
    in the following manner:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述相对简单的游戏之外，模拟竞赛涉及更复杂的设置。在本节中，我们将简要讨论这些。第一个例子是 *Halite*，在竞赛页面上（[https://www.kaggle.com/c/halite](https://www.kaggle.com/c/halite)）定义为以下方式：
- en: Halite [...] is a resource management game where you build and control a small
    armada of ships. Your algorithms determine their movements to collect halite,
    a luminous energy source. The most halite at the end of the match wins, but it’s
    up to you to figure out how to make effective and efficient moves. You control
    your fleet, build new ships, create shipyards, and mine the regenerating halite
    on the game board.
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Halite [...] 是一款资源管理游戏，你在这里建造并控制一支小型舰队。你的算法决定它们的移动以收集卤素，这是一种发光的能量来源。比赛结束时收集到的卤素最多的一方获胜，但如何制定有效且高效的策略取决于你。你控制你的舰队，建造新的船只，创建造船厂，并在游戏板上开采再生的卤素。
- en: 'This is what the game looks like:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的界面如下：
- en: '![](img/B17574_12_6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_12_6.png)'
- en: 'Figure 12.6: Halite game board'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6：Halite游戏板
- en: 'Kaggle organized two competitions around the game: a Playground edition ([https://www.kaggle.com/c/halite-iv-playground-edition](https://www.kaggle.com/c/halite-iv-playground-edition))
    as well as a regular Featured edition ([https://www.kaggle.com/c/halite](https://www.kaggle.com/c/halite)).
    The classic reinforcement learning approach was less useful in this instance since,
    with an arbitrary number of units (ships/bases) and a dynamic opponent pool, the
    problem of credit assignment was becoming intractable for people with access to
    a “normal” level of computing resources.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle围绕这款游戏组织了两场比赛：一个游乐场版（[https://www.kaggle.com/c/halite-iv-playground-edition](https://www.kaggle.com/c/halite-iv-playground-edition)）以及一个常规的特色版（[https://www.kaggle.com/c/halite](https://www.kaggle.com/c/halite)）。在这种情况下，经典的强化学习方法不太有用，因为，由于有任意数量的单位（船只/基地）和动态的对手池，信用分配问题对于拥有“正常”计算资源的人来说变得难以解决。
- en: 'Explaining the problem of credit assignment in full generality is beyond the
    scope of this book, but an interested reader is encouraged to start with the Wikipedia
    entry ([https://en.wikipedia.org/wiki/Assignment_problem](https://en.wikipedia.org/wiki/Assignment_problem))
    and follow up with this excellent introductory article by Mesnard et al.: [https://proceedings.mlr.press/v139/mesnard21a.html](https://proceedings.mlr.press/v139/mesnard21a.html).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全普遍的意义上解释信用分配问题超出了本书的范围，但感兴趣的读者可以从维基百科条目（[https://en.wikipedia.org/wiki/Assignment_problem](https://en.wikipedia.org/wiki/Assignment_problem)）开始，并阅读Mesnard等人撰写的这篇出色的介绍性文章：[https://proceedings.mlr.press/v139/mesnard21a.html](https://proceedings.mlr.press/v139/mesnard21a.html)。
- en: A description of the winning solution by *Tom van de Wiele* ([https://www.kaggle.com/c/halite/discussion/183543](https://www.kaggle.com/c/halite/discussion/183543))
    provides an excellent overview of the modified approach that proved successful
    in this instance (deep RL with independent credit assignment per unit).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tom van de Wiele*（[https://www.kaggle.com/c/halite/discussion/183543](https://www.kaggle.com/c/halite/discussion/183543)）对获胜解决方案的描述提供了对在此情况下证明成功的修改方法的优秀概述（深度强化学习，每个单位独立分配信用）。'
- en: 'Another competition involving a relatively sophisticated game was *Lux AI*
    ([https://www.kaggle.com/c/lux-ai-2021](https://www.kaggle.com/c/lux-ai-2021)).
    In this competition, participants were tasked with designing agents to tackle
    a multi-variable optimization problem combining resource gathering and allocation,
    competing against other players. In addition, successful agents had to analyze
    the moves of their opponents and react accordingly. An interesting feature of
    this contest was the popularity of a “meta” approach: **imitation learning** ([https://paperswithcode.com/task/imitation-learning](https://paperswithcode.com/task/imitation-learning)).
    This is a fairly novel approach in RL, focused on learning a behavior policy from
    demonstration – without a specific model to describe the generation of state-action
    pairs. A competitive implementation of this idea (at the time of writing) is given
    by Kaggle user Ironbar ([https://www.kaggle.com/c/lux-ai-2021/discussion/293911](https://www.kaggle.com/c/lux-ai-2021/discussion/293911)).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个涉及相对复杂游戏的竞赛是*Lux AI* ([https://www.kaggle.com/c/lux-ai-2021](https://www.kaggle.com/c/lux-ai-2021))。在这个竞赛中，参与者被要求设计智能体来解决一个结合资源收集和分配的多变量优化问题，与其他玩家竞争。此外，成功的智能体还必须分析对手的移动并做出相应的反应。这个竞赛的一个有趣特点是“元”方法的流行：**模仿学习**
    ([https://paperswithcode.com/task/imitation-learning](https://paperswithcode.com/task/imitation-learning))。这是一种在强化学习中相当新颖的方法，专注于从演示中学习行为策略——而不需要特定的模型来描述状态-动作对的生成。这种想法的一个具有竞争力的实现（在撰写本文时）是由Kaggle用户Ironbar提供的
    ([https://www.kaggle.com/c/lux-ai-2021/discussion/293911](https://www.kaggle.com/c/lux-ai-2021/discussion/293911))。
- en: 'Finally, no discussion of simulation competitions in Kaggle would be complete
    without the *Google Research Football with Manchester City F.C.* competition ([https://www.kaggle.com/c/google-football/overview](https://www.kaggle.com/c/google-football/overview)).
    The motivation behind this contest was for researchers to explore AI agents’ ability
    to play in complex settings like football. The competition **Overview** section
    formulates the problem thus:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，没有对Kaggle模拟竞赛的讨论是不完整的，没有提及*Google Research Football with Manchester City
    F.C.*竞赛 ([https://www.kaggle.com/c/google-football/overview](https://www.kaggle.com/c/google-football/overview))。这个竞赛背后的动机是让研究人员探索AI智能体在复杂环境如足球中的能力。竞赛**概述**部分将问题表述如下：
- en: The sport requires a balance of short-term control, learned concepts such as
    passing, and high-level strategy, which can be difficult to teach agents. A current
    environment exists to train and test agents, but other solutions may offer better
    results.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 体育运动需要短期控制、学习概念如传球和高级策略之间的平衡，这很难教会智能体。目前存在一个环境和测试智能体的当前环境，但其他解决方案可能提供更好的结果。
- en: 'Unlike some examples given above, this competition was dominated by reinforcement
    learning approaches:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述一些例子不同，这个竞赛主要由强化学习方法主导：
- en: 'Team Raw Beast (3^(rd)) followed a methodology inspired by *AlphaStar*: [https://www.kaggle.com/c/google-football/discussion/200709](https://www.kaggle.com/c/google-football/discussion/200709)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队原始野兽（第3名）遵循了受*AlphaStar*启发的方法论：[https://www.kaggle.com/c/google-football/discussion/200709](https://www.kaggle.com/c/google-football/discussion/200709)
- en: 'Salty Fish (2^(nd)) utilized a form of self-play: [https://www.kaggle.com/c/google-football/discussion/202977](https://www.kaggle.com/c/google-football/discussion/202977)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盐水鱼（第2版）使用了一种自我对弈的形式：[https://www.kaggle.com/c/google-football/discussion/202977](https://www.kaggle.com/c/google-football/discussion/202977)
- en: 'The winners, WeKick, used a deep learning-based solution with creative feature
    engineering and reward structure adjustment: [https://www.kaggle.com/c/google-football/discussion/202232](https://www.kaggle.com/c/google-football/discussion/202232)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获胜者WeKick使用基于深度学习的解决方案，结合了创造性的特征工程和奖励结构调整：[https://www.kaggle.com/c/google-football/discussion/202232](https://www.kaggle.com/c/google-football/discussion/202232)
- en: Studying the solutions listed above is an excellent starting point to learn
    how RL can be utilized to solve this class of problems.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 研究上述解决方案是学习如何利用强化学习解决这类问题的绝佳起点。
- en: '![](img/Firat_Gonen.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Firat_Gonen.png)'
- en: Firat Gonen
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Firat Gonen
- en: '[https://www.kaggle.com/frtgnn](https://www.kaggle.com/frtgnn)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/frtgnn](https://www.kaggle.com/frtgnn)'
- en: For this chapter’s interview, we spoke to Firat Gonen, a Triple Grandmaster
    in Datasets, Notebooks, and Discussion, and an HP Data Science Global Ambassador.
    He gives us his take on his Kaggle approach, and how his attitude has evolved
    over time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的访谈，我们采访了Firat Gonen，他是数据集、笔记本和讨论的三位大师，同时也是HP数据科学全球大使。他向我们分享了他对Kaggle方法的看法，以及他的态度是如何随着时间的推移而演变的。
- en: What’s your favorite kind of competition and why? In terms of technique and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*My favorite kind evolved over time. I used to prefer very generic tabular
    competitions where a nice laptop and some patience would suffice to master the
    trends. I felt like I used to be able to see the outlying trends between training
    and test sets pretty good. Over time, with being awarded the ambassadorship by
    Z by HP and my workstation equipment, I kind of converted myself towards more
    computer vision competitions, though I still have a lot to learn.*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我最喜欢的比赛类型随着时间的推移而演变。我曾经更喜欢非常通用的表格比赛，只要有一台好笔记本电脑和一些耐心，就能掌握趋势。我觉得我以前能够很好地看到训练集和测试集之间的异常趋势。随着时间的推移，由于Z
    by HP授予我大使称号以及我的工作站设备，我逐渐转向了更多的计算机视觉比赛，尽管我还有很多东西要学。
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何应对Kaggle比赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*I usually prefer to delay the modeling part for as long as I can. I like to
    use that time on EDAs, outliers, reading the forum, etc., trying to be patient.
    After I feel like I’m done with feature engineering, I try to form only benchmark
    models to get a grip on different architecture results. My technique is very similar
    when it comes to professional work as well. I find it useless to try to do the
    best in a huge amount of time; there has to be a balance between time and success.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常喜欢尽可能推迟建模部分。我喜欢利用这段时间进行探索性数据分析（EDA）、处理异常值、阅读论坛等，尽量保持耐心。在我感觉特征工程完成之后，我尝试只构建基准模型，以掌握不同架构的结果。我的技术在与专业工作相关时也非常相似。我认为在大量时间内试图做到最好是没有用的；时间和成功之间必须有一个平衡。
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你参加的一个特别具有挑战性的比赛，以及你用来解决任务的见解。
- en: '*The competition hosted by François Chollet was extremely challenging; the
    very first competition to force us into AGI. I remember I felt pretty powerless
    in that competition, where I learned several new techniques. I think everybody
    did that while remembering data science is not just machine learning. Several
    other techniques like mixed integer programming resurfaced at Kaggle.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 弗朗索瓦·肖莱特举办的比赛极具挑战性；这是第一个迫使我们进入通用人工智能（AGI）的比赛。我记得在那个比赛中，我感到非常无力，我在那里学到了几种新技术。我认为每个人在记住数据科学不仅仅是机器学习的同时，都这样做过。在Kaggle上，其他一些技术，如混合整数规划，也重新浮出水面。
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在你的职业生涯中？如果是的话，是如何帮助的？
- en: '*Of course: I learned a lot of new techniques and stayed up to date thanks
    to Kaggle. I’m in a place in my career where my main responsibility lies mostly
    in management. That’s why Kaggle is very important to me for staying up to date
    in several things.*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当然：多亏了Kaggle，我学到了很多新技术，并保持了知识的更新。在我的职业生涯中，我的主要责任大多在于管理。这就是为什么Kaggle对我保持对多件事情的更新非常重要。
- en: How have you built up your portfolio thanks to Kaggle?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何利用Kaggle建立起你的作品集的？
- en: '*I believe the advantage was in a more indirect way, where people saw both
    practical skills (thanks to Kaggle) and more theoretical skills in my more conventional
    education qualifications.*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为优势在于一种更间接的方式，人们看到了我的更传统教育资格中的实际技能（多亏了Kaggle）和更多的理论技能。
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，没有经验的Kagglers通常忽略了什么？你现在知道的事情，你希望在你最初开始时就知道？
- en: '*I think there are two things newcomers do wrong. The first one is having fear
    in entering a new competition, thinking that they will get bad scores and it will
    be registered. This is nonsense. Everybody has bad scores; it’s all about how
    much you devote to a new competition. The second one is that they want to get
    to the model-building stage ASAP, which is very wrong; they want to see their
    benchmark scores and then they get frustrated. I advise them to take their time
    in feature generation and selection, and also in EDA stages.*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为新来者犯的两个错误。第一个是进入新比赛时的恐惧，认为他们会得到很差的分数，并且会被记录下来。这是胡说八道。每个人都有不好的分数；这完全取决于你投入新比赛的程度。第二个是他们想尽快进入模型构建阶段，这是非常错误的；他们想看到他们的基准分数，然后他们会感到沮丧。我建议他们在特征生成和选择，以及在EDA阶段花时间。
- en: What mistakes have you made in competitions in the past?
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的比赛中，你犯过哪些错误？
- en: '*My mistakes are, unfortunately, very similar to new rookies. I got impatient
    in several competitions where I didn’t pay enough attention to early stages, and
    after some time you feel like you don’t have enough time to go back.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的错误，不幸的是，与新手非常相似。在几场竞赛中，我没有足够关注早期阶段，变得不耐烦，过了一段时间后，你会觉得自己没有足够的时间回头。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据分析或机器学习，有没有你特别推荐使用的工具或库？
- en: '*I would recommend PyCaret for benchmarking to get you speed, and PyTorch for
    a model-building framework.*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*我推荐使用PyCaret进行基准测试以获得速度，以及PyTorch作为模型构建框架。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加竞赛时，他们应该记住或做最重要的事情是什么？
- en: '*Exploratory data analysis and previous similar competition discussions.*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*探索性数据分析以及之前的类似竞赛讨论。*'
- en: Do you use other competition platforms? How do they compare to Kaggle?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否使用其他竞赛平台？它们与Kaggle相比如何？
- en: '*To be honest, I haven’t rolled the dice outside Kaggle, but I have had my
    share of them from a tourist perspective. It takes time to adjust to other platforms.*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*说实话，我还没有在Kaggle之外尝试过，但从游客的角度来看，我已经体验过一些了。适应其他平台需要时间。*'
- en: Summary
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed simulation competitions, a new type of contest
    that is increasing in popularity. Compared to vision or NLP-centered ones, simulation
    contests involve a much broader range of methods (with somewhat higher mathematical
    content), which reflects the difference between supervised learning and reinforcement
    learning.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了模拟竞赛，这是一种越来越受欢迎的新竞赛类型。与以视觉或NLP为中心的竞赛相比，模拟竞赛涉及更广泛的方法（带有一定程度的数学内容），这反映了监督学习和强化学习之间的差异。
- en: This chapter concludes the technical part of the book. In the remainder, we
    will talk about turning your Kaggle Notebooks into a portfolio of projects and
    capitalizing on it to find new professional opportunities.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书的技术部分。在接下来的内容中，我们将讨论如何将你的Kaggle笔记本转化为项目组合，并利用它寻找新的职业机会。
- en: Join our book’s Discord space
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们书的Discord工作空间，每月与作者进行一次“问我任何问题”的会议：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code40480600921811704671.png)'
