<html><head></head><body>
<div id="_idContainer082">
<h1 class="chapter-number" id="_idParaDest-100"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-101"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.2.1">Labeling Image Data Using Rules</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we will explore data labeling techniques tailored specifically for image classification, using Python. </span><span class="koboSpan" id="kobo.3.2">Our primary objective is to clarify the path you need to take to generate precise labels for these images in the dataset, relying on meticulously crafted rules founded upon various image properties. </span><span class="koboSpan" id="kobo.3.3">You will be empowered with the ability to dissect and decode images through manual inspection, harnessing the formidable </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">Python ecosystem.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, you will learn </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">How to create labeling rules based on manual inspection of image visualizations </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">in Python</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">How to create labeling rules based on the size and aspect ratio </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">of images</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">How to apply transfer learning to label image data, using pre-trained models such as </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.12.1">YOLO V3</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.13.1">The overarching goal is to empower you with the ability to generate precise and reliable labels for your data. </span><span class="koboSpan" id="kobo.13.2">We aim to equip you with a versatile set of labeling strategies that can be applied across various machine </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">learning projects.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">We will also introduce transformations such as </span><strong class="bold"><span class="koboSpan" id="kobo.16.1">shearing</span></strong><span class="koboSpan" id="kobo.17.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">flipping</span></strong><span class="koboSpan" id="kobo.19.1"> for image labeling. </span><span class="koboSpan" id="kobo.19.2">We will provide you with the knowledge and techniques required to harness these transformations effectively, giving your labeling process a dynamic edge. </span><span class="koboSpan" id="kobo.19.3">we’ll delve into the intricacies of </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">size</span></strong><span class="koboSpan" id="kobo.21.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">aspect ratio</span></strong><span class="koboSpan" id="kobo.23.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.24.1">bounding box</span></strong><span class="koboSpan" id="kobo.25.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">polygon annotation</span></strong><span class="koboSpan" id="kobo.27.1">,</span><strong class="bold"> </strong><span class="koboSpan" id="kobo.28.1">and </span><strong class="bold"><span class="koboSpan" id="kobo.29.1">polyline annotation</span></strong><span class="koboSpan" id="kobo.30.1">. </span><span class="koboSpan" id="kobo.30.2">You’ll learn how to derive labeling rules based on these quantitative image characteristics, providing a systematic and reliable approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">labeling data.</span></span></p>
<h1 id="_idParaDest-102"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.32.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.33.1">Complete code notebooks for the examples used in this chapter are available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">at </span></span><a href="https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python"><span class="No-Break"><span class="koboSpan" id="kobo.35.1">https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.36.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">The sample image dataset used in this chapter is available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">at </span></span><a href="https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/images"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/images</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<h1 id="_idParaDest-103"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.41.1">Labeling rules based on image visualization</span></h1>
<p><span class="koboSpan" id="kobo.42.1">Image </span><a id="_idIndexMarker307"/><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.43.1">classification is the process of categorizing an image into one or more classes based on its content. </span><span class="koboSpan" id="kobo.43.2">It is a challenging task due to the high variability and complexity of images. </span><span class="koboSpan" id="kobo.43.3">In recent years, machine learning techniques have </span><a id="_idIndexMarker309"/><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.44.1">been applied to image classification with great success. </span><span class="koboSpan" id="kobo.44.2">However, machine learning models require a large amount of labeled data to </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">train effectively.</span></span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.46.1">Image labeling using rules with Snorkel</span></h2>
<p><span class="koboSpan" id="kobo.47.1">Snorkel is an </span><a id="_idIndexMarker311"/><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.48.1">open source data platform that provides a way to generate large amounts of labeled data using weak supervision techniques. </span><span class="koboSpan" id="kobo.48.2">Weak supervision allows you to label data with noisy or incomplete sources of supervision, such as heuristics, rules, </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">or patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.50.1">Snorkel </span><a id="_idIndexMarker313"/><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.51.1">primarily operates within the paradigm of weak supervision rather than traditional semi-supervised learning. </span><span class="koboSpan" id="kobo.51.2">Snorkel is a framework designed for weak supervision, where the labeling process may involve noisy, limited, or imprecise rules rather than a large amount of </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">labeled data.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">In Snorkel, users create </span><strong class="bold"><span class="koboSpan" id="kobo.54.1">labeling functions</span></strong><span class="koboSpan" id="kobo.55.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.56.1">LFs</span></strong><span class="koboSpan" id="kobo.57.1">) that express </span><a id="_idIndexMarker315"/><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.58.1">heuristic or rule-based labeling strategies. </span><span class="koboSpan" id="kobo.58.2">These LFs might not be perfect, and there can be conflicts or noise in the generated labels. </span><span class="koboSpan" id="kobo.58.3">Snorkel’s labeling model then learns to denoise and combine these weak labels to create more accurate and reliable labeling for the </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">training data.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">While semi-supervised learning typically involves having a small amount of labeled data and a large amount of unlabeled data, Snorkel focuses on the weak supervision scenario, allowing users to leverage various sources of noisy or incomplete supervision to train machine </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">In summary, Snorkel is more aligned with the principles of weak supervision, where the emphasis is on handling noisy or imprecise labels generated by heuristic rules, rather than being strictly categorized as a semi-supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">learning framework.</span></span></p>
<p><span class="koboSpan" id="kobo.64.1">In this section, we will explore the concept of weak supervision and how to generate labels </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">using Snorkel.</span></span></p>
<h2 id="_idParaDest-105"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.66.1">Weak supervision</span></h2>
<p><span class="koboSpan" id="kobo.67.1">Weak supervision is a </span><a id="_idIndexMarker317"/><a id="_idIndexMarker318"/><span class="koboSpan" id="kobo.68.1">technique for generating large amounts of labeled data using noisy or incomplete sources of supervision. </span><span class="koboSpan" id="kobo.68.2">The idea is to use a set of LFs that generate noisy labels for each data point. </span><span class="koboSpan" id="kobo.68.3">These labels are then combined to generate a final label for each data point. </span><span class="koboSpan" id="kobo.68.4">The key advantage of weak supervision is that it allows you to generate labeled data quickly and at a </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">low cost.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">Snorkel is a </span><a id="_idIndexMarker319"/><a id="_idIndexMarker320"/><span class="koboSpan" id="kobo.71.1">framework that provides a way to generate labels using weak supervision. </span><span class="koboSpan" id="kobo.71.2">It provides a set of tools to create LFs, combine them, and train a model to learn from the generated labels. </span><span class="koboSpan" id="kobo.71.3">Snorkel uses a technique called data programming to combine the LFs and generate a final label for each </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">data point.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">An LF is a </span><a id="_idIndexMarker321"/><a id="_idIndexMarker322"/><span class="koboSpan" id="kobo.74.1">function that generates a noisy label for a data point. </span><span class="koboSpan" id="kobo.74.2">The label can be any value, including continuous or discrete values. </span><span class="koboSpan" id="kobo.74.3">In the context of image classification, an LF is a function that outputs a label of 1 if the image contains the object of interest, and </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">0 otherwise.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">LFs are created using heuristics, rules, or patterns. </span><span class="koboSpan" id="kobo.76.2">The key idea is to define a set of rules that capture the relevant information for each </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">data point.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Now, let us see how to define the rules and an LF based on the manual visualization of an image’s object color for plant </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">disease labeling.</span></span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.80.1">Rules based on the manual visualization of an image’s object color</span></h2>
<p><span class="koboSpan" id="kobo.81.1">In this section, let us see how we </span><a id="_idIndexMarker323"/><span class="koboSpan" id="kobo.82.1">can use LFs that look for specific visual features that are characteristic of images of a plant’s leaves, which we are interested in classifying as “healthy” or “deceased”. </span><span class="koboSpan" id="kobo.82.2">For instance, we could use an LF that checks whether the image has a certain color distribution, or whether it contains specific shapes that are common in </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">those images.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">Snorkel’s LFs can be used to label images based on various properties, such as the presence of certain objects, colors, textures, and shapes. </span><span class="koboSpan" id="kobo.84.2">Here’s an example of Python code that uses Snorkel LFs to detect images based on their </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">color distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.86.1">Creating labeling rules based on manual inspection of image visualizations is a manual process that often involves the expertise of a human annotator. </span><span class="koboSpan" id="kobo.86.2">This process is commonly used in scenarios where there is no existing labeled dataset, and you need to create labels for machine learning or </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">analysis tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">Here’s a general outline of how you can create labeling rules based on the manual inspection of image visualizations </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">in Python:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.90.1">Collect a representative sample</span></strong><span class="koboSpan" id="kobo.91.1">: Begin by selecting a representative sample of images from your dataset. </span><span class="koboSpan" id="kobo.91.2">This sample should cover the range of variations and categories you want </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">to classify.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Define the labeling criteria</span></strong><span class="koboSpan" id="kobo.94.1">: Clearly define the criteria or rules to label images based on their visual properties. </span><span class="koboSpan" id="kobo.94.2">For example, if you’re classifying images to identify plant diseases from images of leaves, agricultural experts visually inspect leaf images for discoloration, spots, or unusual patterns. </span><span class="koboSpan" id="kobo.94.3">Rules can be defined based on the appearance and location of symptoms. </span><span class="koboSpan" id="kobo.94.4">We will use this example for our </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">demonstration shortly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Create a labeling interface</span></strong><span class="koboSpan" id="kobo.97.1">: You can use existing tools or libraries to create a labeling interface where human annotators can view images and apply labels based on the defined criteria. </span><span class="koboSpan" id="kobo.97.2">Libraries such as Labelbox and Supervisely or custom interfaces, using Python web frameworks such as Flask or Django, can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">this purpose.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.99.1">Annotate the images</span></strong><span class="koboSpan" id="kobo.100.1">: Have human annotators manually inspect each image in your sample and apply labels according to the defined criteria. </span><span class="koboSpan" id="kobo.100.2">This step involves the human annotators visually inspecting the images and making classification decisions, based on their expertise and the </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">provided guidelines.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.102.1">Collect annotations</span></strong><span class="koboSpan" id="kobo.103.1">: Collect the annotations generated by the human annotators. </span><span class="koboSpan" id="kobo.103.2">Each image should have a corresponding label or class assigned based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">visual inspection.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.105.1">Analyze and formalize rules</span></strong><span class="koboSpan" id="kobo.106.1">: After collecting a sufficient number of annotations, analyze the patterns and decisions made by the annotators. </span><span class="koboSpan" id="kobo.106.2">Try to formalize the decision criteria based on the annotations. </span><span class="koboSpan" id="kobo.106.3">For example, you might observe that images with certain visual features were consistently labeled as a </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">specific class.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Convert rules to code</span></strong><span class="koboSpan" id="kobo.109.1">: Translate the formalized decision criteria into code that can automatically classify images based on those rules. </span><span class="koboSpan" id="kobo.109.2">This code can be written in Python and integrated into your machine learning pipeline or </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">analysis workflow.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.111.1">Test and validate rules</span></strong><span class="koboSpan" id="kobo.112.1">: Apply the automated labeling rules to a larger portion of your dataset to ensure that they generalize well. </span><span class="koboSpan" id="kobo.112.2">Validate the rules by comparing the automated labels with ground truth labels if available, or by reviewing a subset of the automatically labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">images manually.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.114.1">Iterate and refine</span></strong><span class="koboSpan" id="kobo.115.1">: Iteratively refine the labeling rules based on feedback, error analysis, and additional manual inspection if necessary. </span><span class="koboSpan" id="kobo.115.2">This process may involve improving the rules, adding more criteria, or </span><a id="_idIndexMarker324"/><a id="_idIndexMarker325"/><span class="No-Break"><span class="koboSpan" id="kobo.116.1">adjusting thresholds.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.117.1">Creating labeling rules based on manual inspection is a labor-intensive process but can be essential to generate labeled data when no other options are available. </span><span class="koboSpan" id="kobo.117.2">The quality of your labeled dataset and the effectiveness of your rules depend on the accuracy and consistency of the human annotators, as well as the clarity of the </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">defined criteria.</span></span></p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.119.1">Real-world applications</span></h2>
<p><span class="koboSpan" id="kobo.120.1">Manual inspection of images for classification, along with the definition of rules or patterns, is common in various real-world applications. </span><span class="koboSpan" id="kobo.120.2">Here are some </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">practical examples:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">Medical </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.123.1">image classification</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.125.1">Example</span></strong><span class="koboSpan" id="kobo.126.1">: Classifying X-ray </span><a id="_idIndexMarker326"/><a id="_idIndexMarker327"/><span class="koboSpan" id="kobo.127.1">or MRI images as “normal” </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">or “abnormal.”</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.130.1">: Radiologists visually inspect images for abnormalities, such as tumors, fractures, or anomalies in anatomy. </span><span class="koboSpan" id="kobo.130.2">Rules can be based on the presence, size, or location of </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">these features.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Plant </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.133.1">disease detection</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.135.1">Example</span></strong><span class="koboSpan" id="kobo.136.1">: Identifying </span><a id="_idIndexMarker328"/><a id="_idIndexMarker329"/><span class="koboSpan" id="kobo.137.1">plant diseases from images </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">of leaves.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.140.1">: Agricultural experts visually inspect leaf images for discoloration, spots, or unusual patterns. </span><span class="koboSpan" id="kobo.140.2">Rules can be defined based on the appearance and location </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">of symptoms.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Food </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.143.1">quality inspection</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Example</span></strong><span class="koboSpan" id="kobo.146.1">: Classifying </span><a id="_idIndexMarker330"/><a id="_idIndexMarker331"/><span class="koboSpan" id="kobo.147.1">food products as “fresh” or “spoiled” </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">from images.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.149.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.150.1">: Food inspectors visually inspect images of fruits, vegetables, or packaged goods for signs of spoilage, mold, or other quality issues. </span><span class="koboSpan" id="kobo.150.2">Rules can be based on color, texture, </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">or shape.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.152.1">Defect detection </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.153.1">in manufacturing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.155.1">Example</span></strong><span class="koboSpan" id="kobo.156.1">: Detecting defects in </span><a id="_idIndexMarker332"/><a id="_idIndexMarker333"/><span class="koboSpan" id="kobo.157.1">manufactured products </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">from images.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.159.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.160.1">: Quality control inspectors visually inspect images of products for defects such as cracks, scratches, or missing components. </span><span class="koboSpan" id="kobo.160.2">Rules can be defined based on the location and characteristics </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">of defects.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.162.1">Traffic </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.163.1">sign recognition</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Example</span></strong><span class="koboSpan" id="kobo.166.1">: Recognizing traffic </span><a id="_idIndexMarker334"/><a id="_idIndexMarker335"/><span class="koboSpan" id="kobo.167.1">signs from images captured by </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">autonomous vehicles.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.169.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.170.1">: Engineers visually inspect images for the presence of signs and their shapes, colors, and symbols. </span><span class="koboSpan" id="kobo.170.2">Rules can be defined based on these </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">visual cues.</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.172.1">Wildlife monitoring</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.174.1">Example</span></strong><span class="koboSpan" id="kobo.175.1">: Identifying and </span><a id="_idIndexMarker336"/><a id="_idIndexMarker337"/><span class="koboSpan" id="kobo.176.1">tracking animals in camera </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">trap images.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.178.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.179.1">: Wildlife experts visually inspect images for the presence of specific animal species, their behavior, or the time of day. </span><span class="koboSpan" id="kobo.179.2">Rules can be based on the appearance and context </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">of animals.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.181.1">Historical </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.182.1">document classification</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.184.1">Example</span></strong><span class="koboSpan" id="kobo.185.1">: Classifying historical </span><a id="_idIndexMarker338"/><a id="_idIndexMarker339"/><span class="koboSpan" id="kobo.186.1">documents based on content </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">or era.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.188.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.189.1">: Archivists visually inspect scanned documents for handwriting style, language, content, or visual elements such as illustrations. </span><span class="koboSpan" id="kobo.189.2">Rules can be defined based on </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">these characteristics.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.191.1">Security </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.192.1">and surveillance</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.194.1">Example</span></strong><span class="koboSpan" id="kobo.195.1">: Identifying security </span><a id="_idIndexMarker340"/><a id="_idIndexMarker341"/><span class="koboSpan" id="kobo.196.1">threats or intruders in surveillance </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">camera footage.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.198.1">Rules/patterns</span></strong><span class="koboSpan" id="kobo.199.1">: Security personnel visually inspect video feeds for unusual behavior, suspicious objects, or unauthorized access. </span><span class="koboSpan" id="kobo.199.2">Rules can be defined based on </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">these observations.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.201.1">In all these examples, experts or human annotators visually examine images, identify relevant patterns or features, and define rules or criteria for classification. </span><span class="koboSpan" id="kobo.201.2">These rules are often based on domain knowledge and experience. </span><span class="koboSpan" id="kobo.201.3">Once established, the rules can be used to create LFs and classify images automatically, assist in decision-making, or prioritize </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">further analysis.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.203.1">A practical example of plant disease detection</span></h2>
<p><span class="koboSpan" id="kobo.204.1">Let us see the example LF for plant disease detection. </span><span class="koboSpan" id="kobo.204.2">In this code, we have created a rule to classify healthy and diseased plants, based on the color distribution of leaves. </span><span class="koboSpan" id="kobo.204.3">One rule is if </span><strong class="source-inline"><span class="koboSpan" id="kobo.205.1">black_pixel_percentage</span></strong><span class="koboSpan" id="kobo.206.1"> in the plant leaves is greater than the threshold value, then we classify that plant as a </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">diseased plant.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">The following are the two different types of </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">plant leaves.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.210.1"><img alt="Figure 5.1 – Healthy and diseased plant leaves" src="image/B18944_05_2_Merged_(2).jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.211.1">Figure 5.1 – Healthy and diseased plant leaves</span></p>
<p><span class="koboSpan" id="kobo.212.1">We </span><a id="_idIndexMarker342"/><span class="koboSpan" id="kobo.213.1">calculate the number of black color pixels in a leaf image and then calculate the percent of </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">black pixels:</span></span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.215.1">Percent of black pixels = count of black pixels in a leaf image/total number of pixels in a </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.216.1">leaf image</span></em></span></p>
<p><span class="koboSpan" id="kobo.217.1">We are going to use the rule that if the black pixel percent in a plant leave image is greater than the threshold value (in this example, 10%), then we classify that plant as a diseased plant and label it as a “</span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">diseased plant.”</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">Similarly, if the black pixel percentage is less than 10%, then we classify that plant as a healthy plant and label it as a “</span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">healthy plant.”</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">The following code snippet shows how to calculate the black pixel percentage in an image using </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">Python libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.223.1">
# Convert the image to grayscale
gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)</span></pre> <p><span class="koboSpan" id="kobo.224.1">This line converts the original color image to grayscale using OpenCV’s </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">cvtColor</span></strong><span class="koboSpan" id="kobo.226.1"> function. </span><span class="koboSpan" id="kobo.226.2">Grayscale images have only one channel (compared to the three channels in a color image), representing the intensity or brightness of each pixel. </span><span class="koboSpan" id="kobo.226.3">Converting to grayscale simplifies </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">subsequent processing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.228.1">
# Apply thresholding to detect regions with discoloration
_, binary_image = cv2.threshold(gray_image, 150, 255,\
    cv2.THRESH_BINARY_INV)</span></pre> <p><span class="koboSpan" id="kobo.229.1">In this line, a </span><a id="_idIndexMarker343"/><span class="koboSpan" id="kobo.230.1">thresholding operation is applied to the grayscale image, </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">gray_image</span></strong><span class="koboSpan" id="kobo.232.1">. </span><span class="koboSpan" id="kobo.232.2">Thresholding is a technique that separates pixels into two categories, based on their intensity values – those above a certain threshold and those below it. </span><span class="koboSpan" id="kobo.232.3">Here’s what each </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">parameter means:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">gray_image</span></strong><span class="koboSpan" id="kobo.235.1">: The grayscale image to </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">be thresholded.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">150</span></strong><span class="koboSpan" id="kobo.238.1">: The threshold value. </span><span class="koboSpan" id="kobo.238.2">Pixels with intensities greater than or equal to 150 will be set to the maximum value (</span><strong class="source-inline"><span class="koboSpan" id="kobo.239.1">255</span></strong><span class="koboSpan" id="kobo.240.1">), while pixels with intensities lower than 150 will be set </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">255</span></strong><span class="koboSpan" id="kobo.245.1">: The maximum value to which pixels above the threshold </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">are set.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">cv2.THRESH_BINARY_INV</span></strong><span class="koboSpan" id="kobo.248.1">: The thresholding type. </span><span class="koboSpan" id="kobo.248.2">In this case, it’s set to “binary inverted,” which means that pixels above the threshold will become 0, and pixels below the threshold will </span><span class="No-Break"><span class="koboSpan" id="kobo.249.1">become 255.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.250.1">The result of this thresholding operation is stored in </span><strong class="source-inline"><span class="koboSpan" id="kobo.251.1">binary_image</span></strong><span class="koboSpan" id="kobo.252.1">, which is a binary image where regions with discoloration </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">are highlighted:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.254.1">
# Calculate the percentage of black pixels (discoloration) in the image
white_pixel_percentage = \
    (cv2.countNonZero(binary_image) / binary_image.size) * 100</span></pre> <p><span class="koboSpan" id="kobo.255.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.256.1">cv2.countNonZero(binary_image)</span></strong><span class="koboSpan" id="kobo.257.1"> function counts the number of non-zero (white) pixels in the binary image. </span><span class="koboSpan" id="kobo.257.2">Since we are interested in black pixels (discoloration), we subtract this count from the total number of pixels in </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">the image.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">binary_image.size</span></strong><span class="koboSpan" id="kobo.260.1">: This is the total number of pixels in the binary image, which is equal to the width multiplied by </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">the height.</span></span></p>
<p><span class="koboSpan" id="kobo.262.1">By dividing the count of non-zero (white) pixels by the total number of pixels and multiplying by 100, we obtain the percentage of white pixels in the image. </span><span class="koboSpan" id="kobo.262.2">This percentage represents the extent of discoloration in </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">the image.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">To calculate the percentage of black pixels (discoloration), you can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.266.1">
black_pixel_percentage = 100 - white_pixel_percentage</span></pre> <p><span class="koboSpan" id="kobo.267.1">Overall, this </span><a id="_idIndexMarker344"/><span class="koboSpan" id="kobo.268.1">code snippet is a simple method to quantitatively measure the extent of discoloration in a grayscale image, by converting it into a binary image and calculating the percentage of black pixels. </span><span class="koboSpan" id="kobo.268.2">It can be useful for tasks such as detecting defects or anomalies in images. </span><span class="koboSpan" id="kobo.268.3">Adjusting the threshold value (in this case, 150) can change the sensitivity of </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">the detection.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">Let us create the labeling function to classify the plant as </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">Healthy</span></strong><span class="koboSpan" id="kobo.272.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">Diseased</span></strong><span class="koboSpan" id="kobo.274.1">, based on the threshold value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">black_pixel_percentage</span></strong><span class="koboSpan" id="kobo.276.1"> in the leaf images, </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">as follows.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.278.1">
# Define a labeling function to classify images as "Healthy"
@labeling_function()
def is_healthy(record):
# Define a threshold for discoloration (adjust as needed)
threshold = 10
# Classify as "Healthy" if the percentage of discoloration is below the threshold
if record['black_pixel_percentage'] &lt; threshold:
    return 1 # Label as "Healthy"
else:
    return 0 # Label as "Diseased"</span></pre> <p><span class="koboSpan" id="kobo.279.1">This LF</span><a id="_idIndexMarker345"/><span class="koboSpan" id="kobo.280.1"> returns labels </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">0</span></strong><span class="koboSpan" id="kobo.282.1"> (a diseased plant) or </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">1</span></strong><span class="koboSpan" id="kobo.284.1"> (a healthy plant) based on the </span><em class="italic"><span class="koboSpan" id="kobo.285.1">black</span></em><span class="koboSpan" id="kobo.286.1"> color pixels percentage in the image. </span><span class="koboSpan" id="kobo.286.2">The complete working code for this plant disease labeling is available in the </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">GitHub repo.</span></span></p>
<p><span class="koboSpan" id="kobo.288.1">In the next section, let us see how we can apply labels using image properties such as size and </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">aspect ratio.</span></span></p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.290.1">Labeling images using rules based on properties</span></h1>
<p><span class="koboSpan" id="kobo.291.1">Let us see an</span><a id="_idIndexMarker346"/><span class="koboSpan" id="kobo.292.1"> example of Python code that demonstrates how to classify images using rules, based on image properties such as size and </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">aspect ratio.</span></span></p>
<p><span class="koboSpan" id="kobo.294.1">Here, we will define rules such as if the black color distribution is greater than 50% in leaves, then that is a diseased plant. </span><span class="koboSpan" id="kobo.294.2">Similarly, in case of detecting a bicycle with a person, if the aspect ratio of an image is greater than some threshold value, then that image has a bicycle with </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">a person.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">In computer vision and image classification, the </span><strong class="bold"><span class="koboSpan" id="kobo.297.1">aspect ratio</span></strong><span class="koboSpan" id="kobo.298.1"> refers to the ratio of the width to the height of an image or object. </span><span class="koboSpan" id="kobo.298.2">It is </span><a id="_idIndexMarker347"/><span class="koboSpan" id="kobo.299.1">a measure of how elongated or stretched an object or image appears along its horizontal and vertical dimensions. </span><span class="koboSpan" id="kobo.299.2">Aspect ratio is often used as a feature or criterion in image analysis and classification. </span><span class="koboSpan" id="kobo.299.3">It’s worth noting that aspect ratio alone is often not sufficient for classification, and</span><a id="_idIndexMarker348"/><span class="koboSpan" id="kobo.300.1"> it is</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.301.1"> typically used in conjunction with other features, such as </span><strong class="bold"><span class="koboSpan" id="kobo.302.1">contour height</span></strong><span class="koboSpan" id="kobo.303.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.304.1">texture</span></strong><span class="koboSpan" id="kobo.305.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.306.1">edge</span></strong><span class="koboSpan" id="kobo.307.1">, to achieve accurate classification results. </span><span class="koboSpan" id="kobo.307.2">Image properties such as </span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.308.1">bounding boxes, polygon annotations, and polyline annotations are commonly used in computer vision tasks for object detection and image segmentation. </span><span class="koboSpan" id="kobo.308.2">These properties help you to label and annotate objects within an image. </span><span class="koboSpan" id="kobo.308.3">Here’s an explanation of each feature along with Python code examples to demonstrate how to work </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">with them:</span></span></p>
<h2 id="_idParaDest-110"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.310.1">Bounding boxes</span></h2>
<p><span class="koboSpan" id="kobo.311.1">A bounding box</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.312.1"> is a rectangular region that encloses an object of interest within an image. </span><span class="koboSpan" id="kobo.312.2">It is defined by four values – (</span><strong class="source-inline"><span class="koboSpan" id="kobo.313.1">x_min</span></strong><span class="koboSpan" id="kobo.314.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">y_min</span></strong><span class="koboSpan" id="kobo.316.1">) for the top-left corner and (</span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">x_max</span></strong><span class="koboSpan" id="kobo.318.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.319.1">y_max</span></strong><span class="koboSpan" id="kobo.320.1">) for the bottom-right corner. </span><span class="koboSpan" id="kobo.320.2">Bounding boxes are often used for object detection and localization. </span><span class="koboSpan" id="kobo.320.3">Here is an example of Python code to create and manipulate </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">bounding boxes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.322.1">
# Define a bounding box as (x_min, y_min, x_max, y_max)
bounding_box = (100, 50, 300, 200)
# Access individual components
x_min, y_min, x_max, y_max = bounding_box
# Calculate width and height of the bounding box
width = x_max - x_min
height = y_max - y_min
# Check if a point (x, y) is inside the bounding box
x, y = 200, 150
is_inside = x_min &lt;= x &lt;= x_max and y_min &lt;= y &lt;= y_max
print(f"Width: {width}, Height: {height}, Is Inside: {is_inside}")</span></pre> <h3><span class="koboSpan" id="kobo.323.1">Polygon annotation</span></h3>
<p><span class="koboSpan" id="kobo.324.1">A polygon annotation</span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.325.1"> is a set of connected vertices that outline the shape of an object in an image. </span><span class="koboSpan" id="kobo.325.2">It is defined by a list of (x, y) coordinates representing the vertices. </span><span class="koboSpan" id="kobo.325.3">Polygon annotations are used for detailed object segmentation. </span><span class="koboSpan" id="kobo.325.4">Here is some example Python code to work with </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">polygon annotations:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.327.1">
# Define a polygon annotation as a list of (x, y) coordinates
polygon = [(100, 50), (200, 50), (250, 150), (150, 200)]
# Calculate the area of the polygon (using shoelace formula)
def polygon_area(vertices):
    n = len(vertices)
    area = 0
    for i in range(n):
        j = (i + 1) % n
        area += (vertices[i][0] * vertices[j][1]) - \
            (vertices[j][0] * vertices[i][1])
    area = abs(area) / 2
    return area
area = polygon_area(polygon)
print(f"Polygon Area: {area}")</span></pre> <h3><span class="koboSpan" id="kobo.328.1">Polyline annotations</span></h3>
<p><span class="koboSpan" id="kobo.329.1">A polyline annotation</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.330.1"> is a series of connected line segments defined by a list of (x, y) coordinates for each vertex. </span><span class="koboSpan" id="kobo.330.2">Polylines are often used to represent shapes with multiple line segments, such as roads or paths. </span><span class="koboSpan" id="kobo.330.3">Here is some Python code to work with </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">polyline annotations:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.332.1">
# Define a polyline annotation as a list of (x, y) coordinates
polyline = [(100, 50), (200, 50), (250, 150), (150, 200)]
# Calculate the total length of the polyline
def polyline_length(vertices):
    length = 0
    for i in range(1, len(vertices)):
        x1, y1 = vertices[i - 1]
        x2, y2 = vertices[i]
        length += ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5
    return length
length = polyline_length(polyline)
print(f"Polyline Length: {length}")</span></pre> <p><span class="koboSpan" id="kobo.333.1">These code</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.334.1"> examples demonstrate how to work with bounding boxes, polygon annotations, and polyline annotations in Python. </span><span class="koboSpan" id="kobo.334.2">You can use these concepts to create rules to label images in computer </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">vision applications.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">Now, let us see the following example of how we can use contour height to classify whether an image contains a person riding a bicycle or just shows a bicycle on </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">its own.</span></span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.338.1">Example 1 – image classification – a bicycle with and without a person</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.339.1">Contour height</span></strong><span class="koboSpan" id="kobo.340.1">, in the</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.341.1"> context of image processing and computer vision, refers </span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.342.1">to the measurement of the vertical extent or size of an object’s outline or contour within an image. </span><span class="koboSpan" id="kobo.342.2">It is typically calculated by finding the minimum and maximum vertical positions (i.e., the topmost and bottommost points) of the object’s boundary </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">or contour.</span></span></p>
<p><span class="koboSpan" id="kobo.344.1">Here’s how contour height is </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">generally determined:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.346.1">Contour detection</span></strong><span class="koboSpan" id="kobo.347.1">: The</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.348.1"> first step is to detect the contour of an object within an image. </span><span class="koboSpan" id="kobo.348.2">Contours are essentially the boundaries that separate an object from </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">its background.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.350.1">A bounding rectangle</span></strong><span class="koboSpan" id="kobo.351.1">: Once the </span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.352.1">contour is detected, a bounding rectangle (often referred to as the “</span><strong class="bold"><span class="koboSpan" id="kobo.353.1">bounding box</span></strong><span class="koboSpan" id="kobo.354.1">”) is drawn around the contour. </span><span class="koboSpan" id="kobo.354.2">This rectangle</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.355.1"> encompasses the </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">entire object.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">Measurement</span></strong><span class="koboSpan" id="kobo.358.1">: To calculate the contour height, the vertical extent of the bounding rectangle is measured. </span><span class="koboSpan" id="kobo.358.2">This is done by finding the difference between the y coordinates (the vertical positions) of the top and bottom sides of the </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">bounding rectangle.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.360.1">In summary, contour </span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.361.1">height provides information about the vertical size of an object within an image. </span><span class="koboSpan" id="kobo.361.2">It can be a useful feature for various computer vision tasks, such as object recognition, tracking, and </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">dimension estimation.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">Let us see how we will use Python functions to detect the following images, based on </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">contour height.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.365.1"><img alt="" role="presentation" src="image/B18944_05_2_Merged_(1).jpg"/></span>
</div>
</div>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.366.1">a: A bicycle with a person                                                     b: A bicycle without a person</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.367.1">Figure 5.2 – A comparison of two images with regards to the contour height</span></p>
<p><span class="koboSpan" id="kobo.368.1">Here, the contour</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.369.1"> height of a person riding a bicycle in an image (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.370.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.371.1">.2a</span></em><span class="koboSpan" id="kobo.372.1">) is greater than the contour height of the image of a bicycle without a person (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.373.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.374.1">.2b</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">Let us use the Python library CV2 Canny edge detector to detect the maximum contour height for the given image </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">as, follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.378.1">
# Define a function to find the contour height of an object using the Canny edge detector
def canny_contour_height(image):</span></pre> <p><span class="koboSpan" id="kobo.379.1">This function takes an image as input and returns the maximum contour height, found using the Canny </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">edge detector:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.381.1">
    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    # Apply the Canny edge detector with low and high threshold values
    edges = cv2.Canny(gray, 100, 200)
    # Find the contours of the edges
    contours, _ = cv2.findContours(edges, \
        cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    # Initialize the maximum height as zero
    max_height = 0
    # Loop through each contour
    for cnt in contours:
        # Find the bounding rectangle of the contour
        x, y, w, h = cv2.boundingRect(cnt)
        # Update the maximum height if the current height is larger
        if h &gt; max_height:
            max_height = h
    # Return the maximum height
    return max_height</span></pre> <p><span class="koboSpan" id="kobo.382.1">Here, Python </span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.383.1">functions are used to find the contour height of the images. </span><span class="koboSpan" id="kobo.383.2">As seen in the images, the results show that the contour height of the person riding a bicycle image is greater than the contour height of the bicycle image. </span><span class="koboSpan" id="kobo.383.3">So, we can classify these two images by using a certain threshold value for the contour height, and if that is greater than that threshold value, then we classify the images as a bicycle with a person; otherwise, if the contour height is less than that threshold value, we classify those images as just showing </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">a bicycle.</span></span></p>
<p><span class="koboSpan" id="kobo.385.1">As shown in the preceding LF, (we learned about labeling functions in </span><a href="B18944_02.xhtml#_idTextAnchor043"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.386.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.387.1">) we can automate such image classification and object detection tasks using Python, and label the images as either a man riding a bicycle or just </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">a bicycle.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">The complete code to find the contour height of the preceding two images is </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">on GitHub.</span></span></p>
<p><span class="koboSpan" id="kobo.391.1">By using a </span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.392.1">diverse set of LFs that capture different aspects of the image content, we can increase the likelihood that at least some of the functions will provide a useful way to distinguish between images that depict a bicycle, a bicycle with a person, or neither. </span><span class="koboSpan" id="kobo.392.2">The probabilistic label generated by the majority label voter model will then reflect the combined evidence provided by all of the LFs, and it can be used to make a more accurate </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">classification decision.</span></span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.394.1">Example 2 – image classification – dog and cat images</span></h2>
<p><span class="koboSpan" id="kobo.395.1">Let us see another example of labeling images to classify dog or cat images, based on rules associated </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">with properties.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">The following are some rules to implement as LFs to detect images of dogs, based on pointy ears and snouts, the shape of the eyes, fur texture, and the shape of the body, as well as additional LFs to detect other features. </span><span class="koboSpan" id="kobo.397.2">The complete code for these functions is available </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">on GitHub.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.399.1">Labeling function 1</span></strong><span class="koboSpan" id="kobo.400.1">: The rule is, if the image has pointy ears and a snout, label it as </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">a dog:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.402.1">
# Define a labeling function to detect dogs based on pointy ears and snouts
def dog_features(image):
    ....
</span><span class="koboSpan" id="kobo.402.2">       # If the image has pointy ears and a snout, label it as a dog
    if has_pointy_ears and has_snout:
        return 1
    else:
        return 0</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.403.1">Labeling function 2</span></strong><span class="koboSpan" id="kobo.404.1">: The</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.405.1"> rule is, if the image has oval-shaped eyes, label it as </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">a cat:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.407.1">
# Define a labeling function to detect cats based on their eyes
def cat_features(image):
   # Label images as positive if they contain cat features #such as oval-shaped eyes
    # If the image has oval-shaped eyes, label it as a cat
    if has_oval_eyes:
        return 1
    else:
        return 0</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.408.1">Labeling function 3</span></strong><span class="koboSpan" id="kobo.409.1">: The rule is, if the image has a texture with high variance, label it as </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">a dog:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.411.1">
# Define a labeling function to detect dogs based on fur texture
def dog_fur_texture(image):
    # If the image has high variance, label it as a dog
    if variance &gt; 100:
        return 1
    else:
        return 0</span></pre> <p><strong class="bold"><span class="koboSpan" id="kobo.412.1">Labeling function 4</span></strong><span class="koboSpan" id="kobo.413.1">: The rule is, if the aspect ratio is close to 1 (indicating a more circular shape), label it as </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">a cat:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.415.1">
# Define a labeling function to detect cats based on their body shape
def cat_body_shape(image):
.....
</span><span class="koboSpan" id="kobo.415.2">    # If the aspect ratio is close to 1 (indicating a more circular shape), label it as a cat
    if abs(aspect_ratio - 1) &lt; 0.1:
        return 1
    else:
        return 0</span></pre> <p><span class="koboSpan" id="kobo.416.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">dog_features</span></strong><span class="koboSpan" id="kobo.418.1"> LF </span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.419.1">looks for the presence of pointy ears and snouts in the image by examining specific regions of the blue channel. </span><span class="koboSpan" id="kobo.419.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.420.1">cat_features</span></strong><span class="koboSpan" id="kobo.421.1"> LF looks for the presence of oval-shaped eyes in the green channel. </span><span class="koboSpan" id="kobo.421.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.422.1">dog_fur_texture</span></strong><span class="koboSpan" id="kobo.423.1"> LF looks for high variance in the grayscale version of the image, which is often associated with dog fur texture. </span><span class="koboSpan" id="kobo.423.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.424.1">cat_body_shape</span></strong><span class="koboSpan" id="kobo.425.1"> LF looks for a circular body shape in the image, which is often associated </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">with cats.</span></span></p>
<p><span class="koboSpan" id="kobo.427.1">These LFs could be combined with Snorkel to create a model and label the images. </span><span class="koboSpan" id="kobo.427.2">In the next section, let us see how we can apply labels using </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">transfer learning.</span></span></p>
<h1 id="_idParaDest-113"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.429.1">Labeling images using transfer learning</span></h1>
<p><span class="koboSpan" id="kobo.430.1">Transfer learning is </span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.431.1">a machine learning technique where a model trained on one task is adapted for a second related task. </span><span class="koboSpan" id="kobo.431.2">Instead of starting the learning process from scratch, transfer learning leverages knowledge gained from solving one problem</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.432.1"> and applies it to a different but related</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.433.1"> problem. </span><span class="koboSpan" id="kobo.433.2">This approach has become increasingly popular in deep learning and has </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.435.1">Faster training</span></strong><span class="koboSpan" id="kobo.436.1">: Transfer learning can significantly reduce the time and computational resources required to train a model. </span><span class="koboSpan" id="kobo.436.2">Instead of training a deep neural network from random initialization, you start with a pre-trained model, which already has learned features </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">and representations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.438.1">Better generalization</span></strong><span class="koboSpan" id="kobo.439.1">: Models pre-trained on large datasets, such as ImageNet for image recognition, have learned general features that are useful for various related tasks. </span><span class="koboSpan" id="kobo.439.2">These features tend to generalize well to new tasks, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">better performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.441.1">Lower data requirements</span></strong><span class="koboSpan" id="kobo.442.1">: Transfer learning can be especially beneficial when you have a limited amount of data for your target task. </span><span class="koboSpan" id="kobo.442.2">Pre-trained models can provide a head start, enabling effective learning with </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">smaller datasets.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.444.1">Domain adaptation</span></strong><span class="koboSpan" id="kobo.445.1">: Transfer </span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.446.1">learning helps adapt models </span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.447.1">from one domain (e.g., natural images) to another (e.g., medical images). </span><span class="koboSpan" id="kobo.447.2">This is valuable when collecting data in the target domain </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">is challenging.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.449.1">Let us see an example of Python code to detect digits in handwritten MNIST images, using </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">Snorkel LFs.</span></span></p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.451.1">Example – digit classification using a pre-trained classifier</span></h2>
<p><span class="koboSpan" id="kobo.452.1">In this</span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.453.1"> example, we will first load the MNIST dataset, using Keras, and then define an LF that uses a digit classification model to classify the digits in each image. </span><span class="koboSpan" id="kobo.453.2">We then load the MNIST images into a Snorkel dataset and apply the LF to generate labels for the specified digit. </span><span class="koboSpan" id="kobo.453.3">Finally, we visualize the labels using </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">Snorkel’s viewer.</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">Note that, in this example, we assume that you have already trained a digit classification model and saved it as a file named </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">digit_classifier.h5</span></strong><span class="koboSpan" id="kobo.457.1">. </span><span class="koboSpan" id="kobo.457.2">You can replace this with any other model of your choice. </span><span class="koboSpan" id="kobo.457.3">Also, make sure to provide the correct path to the model file. </span><span class="koboSpan" id="kobo.457.4">Finally, the labels generated by the LF will be </span><strong class="source-inline"><span class="koboSpan" id="kobo.458.1">1</span></strong><span class="koboSpan" id="kobo.459.1"> if the image has the specified digit, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">-1</span></strong><span class="koboSpan" id="kobo.461.1"> if it doesn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">have it:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.463.1">
#Importing Libraries
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import load_model</span></pre> <p><span class="koboSpan" id="kobo.464.1">In this block, TensorFlow is imported, along with specific modules needed to work with the MNIST dataset and </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">pre-trained models.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">The MNIST dataset is loaded into two sets – </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">x_test</span></strong><span class="koboSpan" id="kobo.468.1"> contains the images, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">y_test</span></strong><span class="koboSpan" id="kobo.470.1"> contains the corresponding labels. </span><span class="koboSpan" id="kobo.470.2">The training set is not used in </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">this snippet:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.472.1">
(_, _), (x_test, y_test) = mnist.load_data()</span></pre> <p><span class="koboSpan" id="kobo.473.1">A pre-trained model is loaded using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.474.1">load_model</span></strong><span class="koboSpan" id="kobo.475.1"> function. </span><span class="koboSpan" id="kobo.475.2">Ensure to replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.476.1">mnist_model.h5</span></strong><span class="koboSpan" id="kobo.477.1"> with the correct path to your pre-trained </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">model file:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.479.1">
model = load_model('mnist_model.h5')</span></pre> <p><span class="koboSpan" id="kobo.480.1">The pixel values of the images are normalized to be in the range [0, 1] by converting the data type to </span><strong class="source-inline"><span class="koboSpan" id="kobo.481.1">float32</span></strong><span class="koboSpan" id="kobo.482.1"> and dividing </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">by 255:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.484.1">
x_test = x_test.astype('float32') / 255</span></pre> <p><span class="koboSpan" id="kobo.485.1">The images are</span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.486.1"> reshaped to match the input shape expected by the model, which is (</span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">batch_size</span></strong><span class="koboSpan" id="kobo.488.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.489.1">height</span></strong><span class="koboSpan" id="kobo.490.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.491.1">width</span></strong><span class="koboSpan" id="kobo.492.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.494.1">channels</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.496.1">
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)</span></pre> <p><span class="koboSpan" id="kobo.497.1">Predictions are made on the test dataset using the pre-trained model, and the predictions for the first image </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">are printed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.499.1">
predictions = model.predict(x_test)
print("predictions",predictions[0])</span></pre> <p><span class="koboSpan" id="kobo.500.1">Class labels for the MNIST digits (0–9) are created as strings </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">and printed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.502.1">
class_labels = [str(i) for i in range(10)]
print("class_labels:", class_labels</span></pre> <p><span class="koboSpan" id="kobo.503.1">The script iterates through the test dataset, printing the index of the maximum prediction value, the predicted digit, and the actual digit label for </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">each image:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.505.1">
for i in range(len(x_test)):
    print("maxpredict", predictions[i].argmax())
    predicted_digit = class_labels[predictions[i].argmax()]
    actual_digit = str(y_test[i])
    print(f"Predicted: {predicted_digit}, Actual: {actual_digit}")</span></pre> <p><span class="koboSpan" id="kobo.506.1">Here is </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">the output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.508.1"><img alt="Figure 5.3 – The output of digital classification" src="image/B18944_05_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.509.1">Figure 5.3 – The output of digital classification</span></p>
<p><span class="koboSpan" id="kobo.510.1">Let us see another</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.511.1"> example of defining rules using a pre-trained classifier for image labeling. </span><span class="koboSpan" id="kobo.511.2">In the following example, we will use a pre-trained model, YOLO V3, to detect a person in the image, and then we will apply an LF to label the large set of </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">image data.</span></span></p>
<h2 id="_idParaDest-115"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.513.1">Example – person image detection using the YOLO V3 pre-trained classifier</span></h2>
<p><span class="koboSpan" id="kobo.514.1">Let’s </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.515.1">get started with </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">the code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.517.1">
# Load an image for object detection using cv2
 image = cv2.imread('path/to/image.jpg')
# Define rules based on image properties
# Returns True if image contains a person, otherwise returns False
# Use a pre-trained person detection model, e.g. </span><span class="koboSpan" id="kobo.517.2">YOLOv3  , to detect people in the image</span></pre> <p><span class="koboSpan" id="kobo.518.1">The predefined YOLO model and weights are open source and can be downloaded </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">at </span></span><a href="https://pjreddie.com/darknet/yolo"><span class="No-Break"><span class="koboSpan" id="kobo.520.1">https://pjreddie.com/darknet/yolo</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.521.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.522.1">
def has_person(image):
# Load the YOLOv3 model with its weights and configuration files
net = cv2.dnn.readNetFromDarknet("path/to/yolov3.cfg", \
    "path/to/yolov3.weights")
# Load the COCO class names (used for labeling detected objects)
classes = []
with open("path/to/coco.names", "r") as f:
        classes = [line.strip() for line in f.readlines()]
# Create a blob from the image and set it as input to the network
blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), \
    swapRB=True, crop=False) net.setInput(blob)
# Run forward pass to perform object detection
detections = net.forward()
# Process and interpret the detection results
for detection in detections:
# Process detection results and draw bounding boxes if needed
# You can use classes to map class IDs to class names
if confidence &gt; confidence_threshold and classes[class_id] == "person":
    if len(boxes) &gt; 0:
        return True
    else:
        return False</span></pre> <p><span class="koboSpan" id="kobo.523.1">In this code, we use </span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.524.1">OpenCV to load the YOLO V3 model, its weights, and its configuration files. </span><span class="koboSpan" id="kobo.524.2">Then, we provide an input image, run a forward pass through the network, and process the </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">detection results.</span></span></p>
<p><span class="koboSpan" id="kobo.526.1">You’ll need to replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">"path/to/yolov3.cfg"</span></strong><span class="koboSpan" id="kobo.528.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">"path/to/coco.names"</span></strong><span class="koboSpan" id="kobo.530.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">"path/to/image.jpg"</span></strong><span class="koboSpan" id="kobo.532.1"> with the actual paths to your YOLOv3 configuration file, the class names file, and the image that you want to perform object </span><span class="No-Break"><span class="koboSpan" id="kobo.533.1">detection on.</span></span></p>
<p><span class="koboSpan" id="kobo.534.1">Remember that YOLO V3 is a complex deep learning model designed for real-time object detection, and using it effectively often requires some knowledge of computer vision and deep </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">learning concepts.</span></span></p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.536.1">Example – bicycle image detection using the YOLO V3 pre-trained classifier</span></h2>
<p><span class="koboSpan" id="kobo.537.1">The </span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.538.1">following is the code for </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">this example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
def has_bicycle(image):
    # Returns True if image contains a bicycle, otherwise returns False
    model = tf.saved_model.load(
        "path/to/faster_rcnn_inception_v2_coco_2018_01_28/saved_model")
    img_resized = cv2.resize(image, (600, 600))
    input_tensor = tf.convert_to_tensor(img_resized)
    input_tensor = input_tensor[tf.newaxis, ...]
    detections = model(input_tensor)
    num_detections = int(detections.pop('num_detections'))
    detections = {key: value[0, :num_detections].numpy() \
        for key, value in detections.items()}</span></pre> <p><span class="koboSpan" id="kobo.541.1">In summary, the code snippet utilizes a pre-trained Faster R-CNN model to perform object detection on an input image. </span><span class="koboSpan" id="kobo.541.2">It resizes the image, converts it to a tensor, and then extracts and processes the detection results. </span><span class="koboSpan" id="kobo.541.3">To specifically detect bicycles, you would need to filter the results based on the class labels provided by the model and check for the </span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.542.1">presence of bicycles in the </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">detected objects.</span></span></p>
<p><span class="koboSpan" id="kobo.544.1">Now, let us explore how we can apply transformations on a given image dataset to generate additional synthetic data. </span><span class="koboSpan" id="kobo.544.2">Additional synthetic data helps in training and achieving more accurate results, as a model will learn about different positions </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">of images.</span></span></p>
<h1 id="_idParaDest-117"><a id="_idTextAnchor121"/><span class="koboSpan" id="kobo.546.1">Labeling images using transformations</span></h1>
<p><span class="koboSpan" id="kobo.547.1">In this section, let us see </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.548.1">the different types of transformations that </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.549.1">can be applied to images to generate synthetic data when there is a limited amount of data. </span><span class="koboSpan" id="kobo.549.2">In machine learning, shearing and flipping are often used as image augmentation techniques to increase the diversity of training data. </span><span class="koboSpan" id="kobo.549.3">It helps improve a model’s ability to recognize objects from different angles </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">or orientations.</span></span></p>
<p><span class="koboSpan" id="kobo.551.1">Shearing can be used in computer vision tasks to correct for perspective distortion in images. </span><span class="koboSpan" id="kobo.551.2">For example, it can be applied to rectify skewe</span><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.552.1">d text in </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">scanned documents.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.554.1">Image shearing</span></strong><span class="koboSpan" id="kobo.555.1"> is a</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.556.1"> transformation that distorts an image by moving its pixels in a specific direction. </span><span class="koboSpan" id="kobo.556.2">It involves shifting the pixels of an image along one of its axes while keeping the other axis unchanged. </span><span class="koboSpan" id="kobo.556.3">There are two primary types </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">of shearing:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.558.1">Horizontal shearing</span></strong><span class="koboSpan" id="kobo.559.1">: In</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.560.1"> this case, pixels are shifted horizontally, usually in a diagonal manner, causing an image to slant left </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">or right</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.562.1">Vertical shearing</span></strong><span class="koboSpan" id="kobo.563.1">: Here, pixels </span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.564.1">are shifted vertically, causing an image to slant up </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">or down</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.566.1">To perform image shearing, you typically specify the amount of shear (the extent of distortion) and the direction (horizontal or vertical). </span><span class="koboSpan" id="kobo.566.2">The amount of shear is usually defined as a shear angle or </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">shear factor.</span></span></p>
<p><span class="koboSpan" id="kobo.568.1">Image shearing is typically accomplished using a shear matrix. </span><span class="koboSpan" id="kobo.568.2">For example, in 2D computer graphics, a horizontal shear matrix might look </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.570.1">
| 1   shear_x |
| 0     1     |</span></pre> <p><span class="koboSpan" id="kobo.571.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">shear_x</span></strong><span class="koboSpan" id="kobo.573.1"> represents the amount of horizontal </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">shearing applied.</span></span></p>
<p><span class="koboSpan" id="kobo.575.1">By applying a random shearing transformation to an image, we can generate multiple versions of the image with slightly different pixel values. </span><span class="koboSpan" id="kobo.575.2">These variations can provide a useful way to identify visual patterns or features that are characteristic of </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">an object.</span></span></p>
<p><span class="koboSpan" id="kobo.577.1">Similarly, </span><strong class="bold"><span class="koboSpan" id="kobo.578.1">image flipping</span></strong><span class="koboSpan" id="kobo.579.1"> is </span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.580.1">another transformation that can be useful to identify flowers. </span><span class="koboSpan" id="kobo.580.2">By flipping an image horizontally or vertically, we can generate new versions of an image that may contain different visual patterns or features. </span><span class="koboSpan" id="kobo.580.3">For example, we could use an LF that checks whether an image is flipped along a certain axis, labeling images that are flipped as positively depicting flowers. </span><span class="koboSpan" id="kobo.580.4">This LF would be able to capture the fact that many flowers have bilateral symmetry, meaning that they look similar when mirrored along a </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">particular axis.</span></span></p>
<p><span class="koboSpan" id="kobo.582.1">Overall, by applying</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.583.1"> image transformations such as shearing or</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.584.1"> flipping, we can generate a larger number of labeled examples that capture different aspects of the image content. </span><span class="koboSpan" id="kobo.584.2">This can help to increase the accuracy of the classification model by providing more varied and robust </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">training data.</span></span></p>
<p><span class="koboSpan" id="kobo.586.1">We will further explore image transformation along with other data augmentation techniques and examples in the </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">next chapter.</span></span></p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.588.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.589.1">In this chapter, we embarked on an enlightening journey into the world of image labeling and classification. </span><span class="koboSpan" id="kobo.589.2">We began by mastering the art of creating labeling rules through manual inspection, tapping into the extensive capabilities of Python. </span><span class="koboSpan" id="kobo.589.3">This newfound skill empowers us to translate visual intuition into valuable data, a crucial asset in the realm of </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">machine learning.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">As we delved deeper, we explored the intricacies of size, aspect ratio, bounding boxes, and polygon and polyline annotations. </span><span class="koboSpan" id="kobo.591.2">We learned how to craft labeling rules based on these quantitative image characteristics, ushering in a systematic and dependable approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">data labeling.</span></span></p>
<p><span class="koboSpan" id="kobo.593.1">Our exploration extended to the transformative realm of image manipulation. </span><span class="koboSpan" id="kobo.593.2">We harnessed the potential of image transformations such as shearing and flipping, enhancing our labeling process with </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">dynamic versatility.</span></span></p>
<p><span class="koboSpan" id="kobo.595.1">Furthermore, we applied our knowledge to real-world scenarios, classifying plant disease images using rule-based LFs. </span><span class="koboSpan" id="kobo.595.2">We honed our skills in predicting objects by leveraging aspect ratio and contour height, a valuable asset in scenarios such as identifying a person riding a bicycle. </span><span class="koboSpan" id="kobo.595.3">Additionally, we delved into the powerful domain of pre-trained models and transfer learning for </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">image classification.</span></span></p>
<p><span class="koboSpan" id="kobo.597.1">But our journey is far from over. </span><span class="koboSpan" id="kobo.597.2">In the upcoming chapter, we will dive even deeper into the realm of image data augmentation. </span><span class="koboSpan" id="kobo.597.3">We’ll explore advanced techniques and learn how to perform image classification using augmented data with </span><strong class="bold"><span class="koboSpan" id="kobo.598.1">support vector machines</span></strong><span class="koboSpan" id="kobo.599.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.600.1">SVMs</span></strong><span class="koboSpan" id="kobo.601.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.602.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.603.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.604.1">CNNs</span></strong><span class="koboSpan" id="kobo.605.1">). </span><span class="koboSpan" id="kobo.605.2">Get ready for the next </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">exciting chapter!</span></span></p>
</div>
</body></html>