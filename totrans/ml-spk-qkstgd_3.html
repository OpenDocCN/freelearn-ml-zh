<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial Intelligence and Machine Learning</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will define what we mean by artificial intelligence, machine learning, and cognitive computing. We will study common classes of algorithms within the field of machine learning and its broader applications, including the following:</p>
<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Reinforced learning</li>
<li>Deep learning</li>
<li>Natural language processing</li>
<li>Cognitive computing</li>
<li>Apache Spark's machine learning library, <kbd>MLlib</kbd>, and how it can be used to implement these algorithms within machine learning pipelines</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial intelligence</h1>
                </header>
            
            <article>
                
<p>Artificial intelligence is a broad term given to the theory and application of machines that exhibit intelligent behavior. Artificial intelligence encompasses many applied fields of study, including machine learning and subsequent deep learning, as illustrated in <em>Figure 3.1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-548 image-border" src="Images/53453b9b-8180-4467-bc05-f1b64c38f347.png" style="width:25.33em;height:25.33em;" width="800" height="800"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.1: Artificial intelligence overview</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine learning</h1>
                </header>
            
            <article>
                
<p>Machine learning is an applied field of study within the broader subject of artificial intelligence that focuses on learning from data by detecting patterns, trends, and relationships in order to make predictions and ultimately deliver actionable insights to help decision making. Machine learning models can be split into three main types: s<em>upervised learning</em>, <em>unsupervised learning</em>, and r<em>einforced learning</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>In supervised learning, the goal is to learn a function that is able to map inputs <em>x</em> to outputs <em>y</em> given a labeled set of input-output pairs <em>D</em>, where <em>D</em> is referred to as the training set and <em>N</em> is the number of input-output pairs in the training set:</p>
<p style="padding-left: 150px" class="mce-root"><img src="Images/162e8bab-2db6-49dc-916d-96a729263d99.png" style="width:27.42em;height:1.75em;" width="3920" height="240"/></p>
<p>In simple applications of supervised learning models, each training input <em>x<sub>i</sub></em> is a numerical vector representing model features such as price, age, and temperature. In complex applications, <em>x<sub>i</sub></em> may represent more complex objects, such as a time series, images, and text.</p>
<p class="mce-root">When the output <em>y<sub>i</sub></em> (also called the response variable) is categorical in nature, then the problem is referred to as a classification problem, where <em>y<sub>i</sub></em> belongs to a finite set consisting of <em>K</em> elements or possible classifications:</p>
<p style="padding-left: 240px"><img src="Images/ed9d104d-c08b-43df-95de-59b612508414.png" style="width:9.25em;height:1.42em;" width="1420" height="220"/></p>
<p>When the output <em>y<sub>i</sub></em> is a real number, then the problem is referred to as a regression problem.</p>
<p>So what does this mean in practice? Well, the training set <em>D</em> is essentially a dataset for which the input features have already been mapped to an output. In other words, we already know what the answer is for the training dataset - it is <em>labelled</em>. For example, if the problem were to predict monthly sales figures for an e-commerce website based on the amount of money spent on online advertising (that is, a regression problem), the training dataset would already map advertising costs (input feature) to known monthly sales figures (output), as illustrated in <em>Figure 3.2</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-549 image-border" src="Images/89aaa636-af32-4b63-9f47-c7aa867064cd.png" style="width:43.92em;height:30.17em;" width="1074" height="738"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.2: Linear regression training dataset</div>
<p>Supervised learning algorithms will then use this labelled training dataset to compute a mathematical function that is the best predictor of the output, given the input features. This function can then be applied to a test dataset in order to quantify its accuracy, and thereafter to a dataset that it has never seen before in order to make predictions!</p>
<p>Regression problems are where we want to predict a numerical outcome. Examples of regression algorithms include <strong>Linear Regression</strong> and <strong>Regression Trees</strong>, and examples of real-world use cases include price, weight, and temperature prediction. Classification problems are where we want to predict a categorical outcome. Examples of classification algorithms include <strong>Logistic Regression</strong>, <strong>Multinomial Logistic Regression</strong>, and <strong>Classification Trees</strong>, and examples of real-world use cases include image classification and email spam classification. We will study these algorithms in more detail in <a href="ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml" target="_blank">Chapter 4</a>, <em>Supervised Learning Using Apache Spark</em>, where we will also develop supervised learning models that can be applied to real-world use cases whilst providing the ability to quantify their accuracy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>In unsupervised learning, the goal is to uncover hidden relationships, trends, and patterns, given only the input data <em>x<sub>i</sub></em> with no output <em>y<sub>i</sub></em>. In this case, we have the following:</p>
<p style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="Images/5cda5569-12fb-4524-953f-e6939182587c.png" style="width:16.67em;height:1.75em;" width="2320" height="240"/></p>
<p>In practice, this means that the emphasis is on uncovering interesting patterns and trends within a dataset in the absence of known and correct answers. Subsequently, unsupervised learning is commonly referred to as <strong>knowledge discovery</strong> given the fact that problems are less well-defined and we are not told what kind of patterns are contained within the data. <strong>Clustering</strong> is a well-known example of an unsupervised learning algorithm where the goal is to segment data points into groups, where all the data points in a specific group share similar features or attributes, as illustrated in <em>Figure 3.3</em>. Real-world use cases of clustering include document classification and clustering customers for marketing purposes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-551 image-border" src="Images/64c7ebf2-ea6e-4e32-a9bc-29b65c097295.png" style="width:40.67em;height:28.75em;" width="1088" height="769"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.3: Clustering unsupervised learning model</div>
<p>We will study unsupervised learning algorithms in more detail in <a href="eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml" target="_blank">Chapter 5</a>, <em>Unsupervised Learning Using Apache Spark</em>, including hands-on development of real-world applications.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Reinforced learning</h1>
                </header>
            
            <article>
                
<p>In reinforced learning, a reward (or punishment) system is employed to impact behavior over time, based on interactions between an agent and its wider environment. An agent will receive state information from the environment and, based on that state, will perform an <em>action</em>. As a result of that action, the environment will transition to a new state that is then provided back to the agent, typically with a reward (or punishment). The goal of the agent is to therefore maximize the cumulative reward it receives. For example, consider the case of a child learning good behavior from bad behavior and being rewarded for good behavior with a treat from its parents. In the case of machines, consider the example of computer-based board game players. By combining deep learning with reinforced learning, computers can learn how to play board games with continually increasing levels of performance such that, over time, they become almost unbeatable!</p>
<p>Reinforced learning is beyond the scope of this book. However, to learn more about deep reinforced learning applied to gaming, please visit <a href="https://deepmind.com/blog/deep-reinforcement-learning/">https://deepmind.com/blog/deep-reinforcement-learning/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep learning</h1>
                </header>
            
            <article>
                
<p>In deep learning, a subfield within the broader field of machine learning, the goal is still to learn a function but by employing an architecture that mimics the neural architecture found in the human brain in order to learn from experience using a hierarchy of concepts or representations. This enables us to develop more complex and powerful functions in order to predict outcomes better.</p>
<p>Many machine learning models employ a two-layer architecture, where some sort of function maps an input to an output. However, in the human brain, multiple layers of processing are found, in other words, a neural network. By mimicking natural neural networks, <strong>artificial neural networks (ANN)</strong> offer the ability to learn complex non-linear representations with no restrictions on the input features and are ideally suited to a wide variety of exciting use cases, including speech, image and pattern recognition, <strong>natural language processing</strong> (<strong>NLP</strong>), fraud detection, forecasting and price prediction.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Natural neuron</h1>
                </header>
            
            <article>
                
<p>Deep learning algorithms mimic the neural architecture found in the human brain. If we were to study a single natural neuron in the human brain, we would find three primary areas of interest, as illustrated in <em>Figure 3.4</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-552 image-border" src="Images/192b5cb4-e0ab-4820-a9b9-ec6c8ebf0d83.png" style="width:35.75em;height:19.17em;" width="1035" height="557"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.4: A natural neuron</div>
<p>Dendrites receive chemical signals and electrical impulses from other neurons that are collected and aggregated in the cell body. The nucleus, found within the cell body, is the control center of the neuron and is responsible for regulating cell functions, producing the proteins required to build new dendrites and for making the neurotransmitter chemicals used as signals. Signals can be classed as either inhibitory or excitatory. If they are inhibitory, this means that they are not transmitted to other neurons. If they are excitatory, this means that they are transmitted to other neurons via the axon. The axon is responsible for communicating signals between neurons, in some cases across distances as long as a couple of meters or a short as a few microns. The neuron therefore, as a single logical unit, is ultimately responsible for communicating information and the average human brain may contain around 100 billion neurons.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial neuron</h1>
                </header>
            
            <article>
                
<p>The core concepts of the natural neuron can be generalized into components of a signal processing system. In this general signal processing system, the signals received by the dendrites can be thought of as the inputs. The nucleus can be thought of as a central processing unit that collects and aggregates the inputs and, depending on the net input magnitude and an activation function, transmits outputs along the axon. This general signal processing system, modeled on a natural neuron, is called an <em>artificial neuron</em> and is illustrated in <em>Figure 3.5</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-553 image-border" src="Images/46651759-ddc3-4669-8e1b-827bc63b1eca.png" style="width:43.42em;height:30.67em;" width="1219" height="862"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.5: An artificial neuron</div>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Weights</h1>
                </header>
            
            <article>
                
<p>In the artificial neuron, weights can amplify or attenuate signals and are used to model the established connections to other neurons as found in the natural world. By changing the weight vectors, we can effect whether that neuron will activate or not based on the aggregation of input values with weights, called the weighted or net input <em>z</em>:</p>
<p style="padding-left: 60px"><img src="Images/749ec865-ad53-41cb-aa03-dbaeec6a27c4.png" style="width:31.67em;height:3.83em;" width="4450" height="540"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Activation function</h1>
                </header>
            
            <article>
                
<p>Once the weighted input plus a bias is calculated, the <strong>activation function</strong>, denoted by the Greek letter <em>phi</em> (Φ), is used to determine the output of the neuron and whether it activates or not. To make this determination, an activation function is typically a non-linear function bounded between two values, thereby adding non-linearity to ANNs. As most real-world data tends to be non-linear in nature when it comes to complex use cases, we require that <span>ANN</span>s have the capability to learn these non-linear concepts or representations. This is enabled by non-linear activation functions. Examples of activation functions include a Heaviside step function, a sigmoid function, and a hyperbolic tangent function.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Heaviside step function</h1>
                </header>
            
            <article>
                
<p>A Heaviside step function is a basic discontinuous function that compares values against a simple threshold and is used for classification where the input data is <em>linearly</em> separable. The neuron is activated if the weighted sum plus a bias exceeds a certain threshold, denoted by the Greek letter <em>theta</em> (<span>θ) in the equation below</span>. If it does not, the neuron is not activated. The following step function is an example of a Heaviside step function that is bounded between <em>1</em> and <em>-1</em>:</p>
<p style="padding-left: 180px"><img src="Images/d7e537cb-d821-4892-92d4-d4c27ba3fa4e.png" style="width:12.83em;height:3.00em;" width="2030" height="480"/></p>
<p>This Heaviside step function is illustrated in <em>Figure 3.6</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-554 image-border" src="Images/3f2a68ff-30a8-4d82-b842-a730b3e6358f.png" style="width:26.25em;height:25.42em;" width="888" height="861"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.6: Heaviside step activation function</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Sigmoid function</h1>
                </header>
            
            <article>
                
<p>A sigmoid function is a non-linear mathematical function that exhibits a sigmoid curve, as illustrated in <em>Figure 3.7</em>, and often refers to the sigmoid or logistic function:</p>
<p style="padding-left: 210px"><img src="Images/620efead-9eb4-4d7f-a693-5323eace3821.png" style="width:10.83em;height:2.50em;" width="1830" height="420"/></p>
<p>In this case, the sigmoid activation function is bounded between 0 and 1 and is smoothly defined for <em>all</em> real input values, making it a better choice of activation function than a basic Heaviside step function. This is because, unlike the Heaviside step function, non-linear activation functions can distinguish data that is <em>not</em> linearly separable, such as image and video data. Note that by using the sigmoid function as the activation function, the artificial neuron will, in fact, correspond to a logistic regression model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-555 image-border" src="Images/fc5496ba-3a94-4ae7-8e5c-a793f447f8a4.png" style="width:31.75em;height:26.25em;" width="938" height="776"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.7: Sigmoid activation function</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hyperbolic tangent function</h1>
                </header>
            
            <article>
                
<p>Finally, a hyperbolic tangent function, as illustrated in <em>Figure 3.8</em>, is the ratio between the hyperbolic sine and cosine functions:</p>
<p style="padding-left: 150px"><img src="Images/a7027299-b4af-437f-aed3-8bf353128cfc.png" style="width:21.50em;height:3.33em;" width="3100" height="480"/></p>
<p class="CDPAlignLeft CDPAlign">In this case, an activation function based on a hyperbolic tangent function is bounded between <em>1</em> and <em>-1</em> and, similar to sigmoid functions, is smoothly defined for all real input values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-556 image-border" src="Images/c9014c8e-7d06-4a12-9390-4d17f9379eb9.png" style="width:34.33em;height:28.58em;" width="1089" height="906"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.8: Hyperbolic tangent function</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial neural network</h1>
                </header>
            
            <article>
                
<p>An <strong>artificial neural network</strong> (<strong>ANN</strong>) is a connected group of artificial neurons, where the artificial neurons are aggregated into linked <strong>neural</strong> <strong>layers</strong> that can be divided into three types:</p>
<ul>
<li>The <strong>input layer</strong> receives input signals from the outside world and passes these input signals to the next layer.</li>
<li><strong>Hidden layers</strong>, if any, perform computations on these signals and pass them to the output layer. Therefore, the outputs of the hidden layer(s) act as inputs to the final output.</li>
</ul>
<ul>
<li>The <strong>output layer</strong> <span>calculates the final output, which then influences the outside world in some manner.</span></li>
</ul>
<p>Artificial neurons are linked across adjacent neural layers by <strong>edges</strong> that have weights associated with them. In general, the addition of more hidden neural layers increases the ability of the <span>ANN</span> to learn more complex concepts or representations. They are termed <em>hidden</em> because of the fact that they do not directly interact with the outside world. Note that all <span>ANN</span>s have an input and an output layer, with zero or more hidden layers. An <span>ANN</span> where signals are propagated in one direction only, in other words, signals are received by the input layer and forwarded to the next layer for processing, are called <strong>feedforward</strong> networks. <span>ANN</span>s where signals may be propagated back to artificial neurons or neural layers that have <em>already</em> processed that signal are called <strong>feedback</strong> networks. <em>Figure 3.9</em> illustrates the logical architecture of a feedforward <span>ANN</span>, where each circle represents an artificial neuron sometimes referred to as a <strong>node</strong> or a <strong>unit</strong>, and the arrows represent <strong>edges</strong> or <strong>connections</strong> between artificial neurons across adjacent neural layers:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-557 image-border" src="Images/1e36af97-c45b-4bd4-9f07-8809de91a4d2.png" style="width:33.67em;height:32.17em;" width="1043" height="995"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.9: Feedforward artificial neural network</div>
<p><span>ANN</span>s can be divided into two classes dependent on their architecture. <strong>Mono-layer</strong> or <strong>single-layer</strong> <span>ANN</span>s are characterized by the aggregation of all its constituent artificial neurons on the same level with no hidden layers. A single-layer perceptron is an example of a mono-layer <span>ANN</span> consisting of just one layer of links between input nodes and output nodes. <strong>Multi-layer </strong><span>ANN</span>s are characterized by the segmentation of artificial neurons across multiple linked layers. A multi-layer perceptron is an example of a multi-layer <span>ANN</span> consisting of one or more hidden layers.</p>
<p><span>ANN</span>s learn by optimizing their weights to deliver a desired outcome, and that by changing weights, <span>ANN</span>s can deliver different results for the same inputs. The goal of optimizing the weights is to minimize a <strong>loss function</strong>—a function that calculates the price paid for inaccurate predictions—by finding the best combination of weights that best predict the outcome. Recall that the weights represent established connections to other neurons; hence, by changing weights, <span>ANN</span>s are, in fact, mimicking natural neural networks by changing the connections between neurons. Various processes for learning optimal weight coefficients are provided in the following sub-sections during our discussions on perceptrons.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Single-layer perceptron</h1>
                </header>
            
            <article>
                
<p><em>Figure 3.10</em> illustrates the architecture of a single-layer perceptron. In this single-layer perceptron, an optimal set of weight coefficients are derived which, when multiplied by the input features, determines whether to activate the neuron or not. Initial weights are set randomly and, if the weighted input results in a predicted output that matches the desired output (for example, in a supervised learning classification context), then no changes to the weights are made. If the predicted output does not match the desired output, then weights are updated to reduce the error.</p>
<p>This makes single-layer perceptrons best suited as classifiers, but only when the classes are <em>linearly separable</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-558 image-border" src="Images/decfc40a-779e-4dbc-8c46-4b38bd9b20e0.png" style="width:38.83em;height:32.92em;" width="1036" height="877"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.10: Single-layer perceptron</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multi-layer perceptron</h1>
                </header>
            
            <article>
                
<p>Multi-layer perceptrons differ from single-layer perceptrons as a result of the introduction of one or more hidden layers, giving them the ability to learn non-linear functions. <em>Figure 3.11</em> illustrates the architecture of a multi-layer perceptron containing one hidden layer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-559 image-border" src="Images/3bcdc382-9e9f-4320-a465-ff853410d8a5.png" style="width:26.58em;height:34.00em;" width="842" height="1073"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.11: Multi-layer perceptron</div>
<p><strong>Backpropagation</strong> is a supervised learning process by which multi-layer perceptrons and other <span>ANN</span>s can learn, that is to say, derive an optimal set of weight coefficients. The first step in backpropagation is, in fact, <strong>forward propagation</strong>, whereby all weights are set randomly initially and the output from the network is calculated (similar to single-layer perceptrons, but this time involving one or more hidden layers). If the predicted output does not match the desired output, the total error at the output nodes is propagated back through the entire network in an effort to readjust all weights in the network so that the error is reduced in the output layer.</p>
<p>Multi-layer neural networks, such as multi-layer perceptrons, are generally much more compute intensive, since the process to optimize weights involves a much greater number of weights and calculations. Therefore, training neural networks, which also typically involves a large number of data points in order to learn a large number of optimal weight coefficients, requires CPU and memory resources that previously were not readily available or cost-effective. However, with the advent of distributed systems, like those described in <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank"/><a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem </em>and the availability of cost-effective, high-performance, and resilient distributed clusters that support the processing of petabytes of data hosted by commodity hardware, research into <span>ANN</span>s and deep learning has exploded, as too has their application to exciting real-world artificial intelligence use cases, including the following:</p>
<ul>
<li>Healthcare and combating disease, including predictive diagnosis, drug discovery, and gene ontology</li>
<li>Speech recognition, including language translation</li>
<li>Image recognition, including visual search</li>
<li>Theoretical physics and astrophysics, including satellite image classification and gravitational wave detection</li>
</ul>
<p>In this sub-section, we have discussed two specific types of <span>ANN, </span>single-layer perceptrons and multi-layer perceptrons, which we will study in more detail in <a href="337b904f-87f1-4741-bd75-d7fd983185f6.xhtml" target="_blank">Chapter 7</a>, <em>Deep Learning Using Apache Spark</em>, including the hands-on development of real-world applications. Other classes of artificial network networks include <strong>convolutional</strong> <strong>neural networks</strong> (also described in <a href="337b904f-87f1-4741-bd75-d7fd983185f6.xhtml" target="_blank">Chapter 7</a>, <em>Deep Learning Using Apache Spark</em>), <strong>recurrent neural networks</strong>, <strong>Kohonen self-organizing neural networks</strong>, and <strong>modular</strong> <strong>neural networks</strong>, which are beyond the scope of this book. To learn more about <span>ANN</span>s and the exciting field of deep learning, please visit <a href="http://deeplearning.net/">http://deeplearning.net/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">NLP</h1>
                </header>
            
            <article>
                
<p>NLP refers to a family of computer science disciplines, including machine learning, linguistics, information engineering, and data management, used to analyze and understand natural languages, including speech and text. NLP can be applied to a wide variety of real-world use cases, including the following:</p>
<ul>
<li><strong>Named entity recognition</strong> (<strong>NER</strong>): Automatically identifying and parsing entities from text, including people, physical addresses, and email addresses</li>
<li><strong>Relationship extraction</strong>: Automatically identifying the types of relationships between parsed entities</li>
</ul>
<ul>
<li><strong>Machine translation and transcription</strong>: Automatically translating from one natural language to another, for example, from English to Chinese</li>
<li><strong>Searching</strong>: Automatically searching across vast collections of structured, semi-structured, and unstructured documents and objects in order to fulfill a natural language query</li>
<li><strong>Speech recognition</strong>: Automatically deriving meaning from human speech</li>
<li><strong>Sentiment analysis</strong>: Automatically identifying human sentiment toward a topic or entity</li>
<li><strong>Question answering</strong>: Automatically answering natural, fully-formed questions</li>
</ul>
<p>A common technique employed in NLP is developing a data engineering pipeline that pre-processes text in order to generate features for input into machine learning models. Common pre-processing techniques include <strong>tokenization</strong> (splitting text into smaller and simpler units called tokens, where tokens are often individual words or terms), <strong>stemming</strong> and <strong>lemmatisation</strong> (reducing tokens to a base form), and removing <strong>stop-words</strong> (such as <em>I</em>, <em>this</em>, and <em>at</em>). The resultant set of terms is converted into features to then feed into machine learning models. A very basic algorithm used to convert the set of terms into features is called <strong>Bag of Words</strong>, which simply counts the number of occurrences of each unique term, thereby converting text into numeric feature vectors.</p>
<p>NLP is important as it provides a means to achieve true seamless interaction between artificially intelligent systems/machines and humans, such as through conversation interfaces. We will study NLP in more detail in <a href="2ad42c13-b5a3-40f6-9940-5316ff6a0451.xhtml" target="_blank">Chapter 6</a>, <em>Natural Language Processing Using Apache Spark</em>, including the hands-on development of real-world applications.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cognitive computing</h1>
                </header>
            
            <article>
                
<p>Similar to NLP, cognitive computing actually refers to a family of computer science disciplines, including machine learning, deep learning, NLP, statistics, business intelligence, data engineering, and information retrieval that, together, are used to develop systems that simulate human thought processes. Real-world implementations of cognitive systems include chatbots and virtual assistants (such as Amazon Alexa, Google Assistant, and Microsoft Cortana) that understand natural human language and provide contextual conversation interfaces, including question-answering, personalized recommendations, and information retrieval systems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine learning pipelines in Apache Spark</h1>
                </header>
            
            <article>
                
<p>To end this chapter, we will take a look at how Apache Spark <span>can be used to implement the algorithms that we have previously discussed by taking a look at how its </span>machine learning library, <kbd>MLlib</kbd>, works under the hood. <kbd>MLlib</kbd> provides a suite of tools designed to make machine learning accessible, scalable, and easy to deploy.</p>
<div class="packt_infobox">Note that as of Spark 2.0, the <kbd>MLlib</kbd> RDD-based API is in maintenance mode. The examples in this book will use the DataFrame-based API, which is now the primary API for <kbd>MLlib</kbd>. For more information, please visit <a href="https://spark.apache.org/docs/latest/ml-guide.html">https://spark.apache.org/docs/latest/ml-guide.html</a>.</div>
<p>At a high level, the typical implementation of machine learning models can be thought of as an ordered pipeline of algorithms, as follows:</p>
<ol>
<li>Feature extraction, transformation, and selection</li>
<li>Train a predictive model based on these feature vectors and labels</li>
<li>Make predictions using the trained predictive model</li>
<li>Evaluate model performance and accuracy</li>
</ol>
<p><kbd>MLlib</kbd> exposes two core abstractions that facilitate this high-level pipeline and allow machine learning models to be developed in Apache Spark:</p>
<ul>
<li><strong>Transformers</strong>: Formally, a transformer converts one DataFrame (see <a href="57c527fc-2555-49e9-bf1c-0567d05da388.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>) into another DataFrame. The new DataFrame will typically contain one or more new columns appended to it. In the context of a machine learning model, an input DataFrame may consist of a column containing the relevant feature vectors. A transformer will then take this input DataFrame and predict a label for each feature vector. The transformer will then output a new DataFrame with a new column containing the predicted labels.</li>
<li><strong>Estimators</strong>: Formally, an estimator abstracts a learning algorithm. In practice, an estimator is a type of learning algorithm, such as a logistic regression algorithm. In this case, the estimator is called <em>LogisticRegression</em> in <kbd>MLlib</kbd>. The estimator will take an input DataFrame and call the <kbd>fit()</kbd> method on it. The output of the <kbd>fit()</kbd> method, and hence the output of the estimator, will be a <em>trained model</em>. In this example, the <em>LogisticRegression</em> estimator will produce a trained <em>LogisticRegressionModel</em> model object. The model object itself is, in fact, a <em>transformer</em>, because the trained model can now take a new DataFrame containing new feature vectors and make predictions on them.</li>
</ul>
<p>Returning to our definition of a pipeline, this can now be extended. A pipeline is, in fact, an ordered sequence of stages where each stage is either a transformer or an estimator.</p>
<p><em>Figure 3.12</em> illustrates a pipeline used to train a model. In <em>Figure 3.12</em>, NLP <em>transformations</em> are applied to tokenize raw training text into a set of words or terms. The tokenizer is referred to as a feature <em>transformer</em>. An algorithm called HashingTF is then applied to take the set of terms and convert it into fixed-length feature vectors (HashingTF ultimately calculates the term frequencies using a hash function). HashingTF is also a <em>transformer</em>. The <em>LogisticRegression</em> estimator is then applied to these feature vectors, via <kbd>LogisticRegression.fit()</kbd>, to generate a trained <em>LogisticRegressionModel</em>, which itself is a type of <em>transformer</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-560 image-border" src="Images/8665b5eb-e972-45ef-a9eb-8d323d6d4997.png" style="width:34.42em;height:14.83em;" width="1098" height="474"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.12: MLlib training pipeline</div>
<p><em>Figure 3.13</em> illustrates a pipeline used to test a model. In this diagram, similar to the training pipeline, a tokenizer feature <em>transformer</em> is used to extract terms from the raw test text, and then the HashingTF <em>transformer</em> is applied to convert the set of terms into fixed-length feature vectors. However, since we already have a trained model generated by the training pipeline in <em>Figure 3.12</em>, the feature vectors are passed as input into this trained model <em>transformer</em> in order to make predictions and output a new DataFrame containing these predictions on the test data:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-561 image-border" src="Images/67484bc6-83e0-4a8d-b914-19c943b1c949.png" style="width:34.25em;height:14.50em;" width="1092" height="462"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.13: MLlib test pipeline</div>
<p>In addition to providing common machine learning algorithms and methods to extract, transform, and select model features and other pipeline abstractions, <kbd>MLlib</kbd> also exposes methods to save trained models and pipelines to an underlying filesystem that can then be loaded later on if and when required. <kbd>MLlib</kbd> also provides utility methods covering operations in statistics, linear algebra, and data engineering. To learn more about <kbd>MLlib</kbd>, please visit <a href="http://spark.apache.org/docs/latest/ml-guide.html">http://spark.apache.org/docs/latest/ml-guide.html</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have defined what is meant by artificial intelligence, machine learning, and cognitive computing. We have explored common machine learning algorithms at a high level, including deep learning and <span>ANN</span>s, as well as taking a look at Apache Spark's machine learning library, <kbd>MLlib</kbd>, and how it <span>can be used to implement these algorithms within machine learning pipelines</span>.</p>
<p>In the next chapter, we will start developing, deploying, and testing supervised machine learning models applied to real-world use cases using <kbd>PySpark</kbd> and <kbd>MLlib</kbd>.</p>


            </article>

            
        </section>
    </div>



  </body></html>