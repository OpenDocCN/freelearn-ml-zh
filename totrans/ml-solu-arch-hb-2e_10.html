<html><head></head><body>
<div class="Basic-Text-Frame" id="_idContainer171">
<h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 class="chapterTitle" id="_idParaDest-260"><span class="koboSpan" id="kobo.2.1">Advanced ML Engineering</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">Congratulations on making it so far! </span><span class="koboSpan" id="kobo.3.2">By now, you should have developed a good understanding of the core fundamental skills that an ML solutions architect needs in order to operate effectively across the ML lifecycle. </span><span class="koboSpan" id="kobo.3.3">In this chapter, we will delve into advanced ML concepts. </span><span class="koboSpan" id="kobo.3.4">Our focus will be on exploring a range of options for distributed model training for large models and datasets. </span><span class="koboSpan" id="kobo.3.5">Understanding the concept and techniques for distributed training is becoming increasingly important as all large-scale model training such as GPT will require distributed training architecture. </span><span class="koboSpan" id="kobo.3.6">Furthermore, we’ll delve into diverse technical approaches aimed at optimizing model inference latency. </span><span class="koboSpan" id="kobo.3.7">As model sizes grow larger, having a good grasp on how to optimize models for low-latency inference is becoming an essential skill in ML engineering. </span><span class="koboSpan" id="kobo.3.8">Lastly, we will close this chapter with a hands-on lab on distributed model training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">Specifically, we will cover the following topics in this chapter:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.5.1">Training large-scale models with distributed training</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.6.1">Achieving low-latency model inference</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.7.1">Hands-on lab – running distributed model training with PyTorch</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-261"><span class="koboSpan" id="kobo.8.1">Technical requirements</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.9.1">You will need access to your AWS environment for the hands-on portion of this chapter. </span><span class="koboSpan" id="kobo.9.2">All the code samples are located at </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10"><span class="url"><span class="koboSpan" id="kobo.10.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10</span></span></a><span class="koboSpan" id="kobo.11.1">.</span></p>
<h1 class="heading-1" id="_idParaDest-262"><span class="koboSpan" id="kobo.12.1">Training large-scale models with distributed training</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.13.1">As </span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.14.1">ML algorithms </span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.15.1">grow more complex and the volumes of available training data expand exponentially, model training times have become a major bottleneck. </span><span class="koboSpan" id="kobo.15.2">Single-device training on massive datasets or gigantic models like large language models is increasingly impractical given memory, time, and latency constraints. </span><span class="koboSpan" id="kobo.15.3">For example, state-of-the-art language models have rapidly scaled from millions of parameters a decade ago to hundreds of billions today. </span><span class="koboSpan" id="kobo.15.4">The following graph illustrates how language models have evolved in recent years:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.16.1"><img alt="Figure 10.1 – The growth of language models  " src="../Images/B20836_10_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.17.1">Figure 10.1: The growth of language models</span></p>
<p class="normal"><span class="koboSpan" id="kobo.18.1">To overcome computational challenges, distributed training techniques have become critical to accelerate model development by parallelizing computation across clusters of GPUs or TPUs in the cloud. </span><span class="koboSpan" id="kobo.18.2">By sharding data and models across devices and nodes, distributed training enables the scaling out of computation to train modern massive models and data volumes in reasonable timeframes. </span><span class="koboSpan" id="kobo.18.3">There are two main types of distributed training: data parallelism and model parallelism. </span><span class="koboSpan" id="kobo.18.4">Before we get into the details of distributed training, let’s quickly review how a neural network trains again:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.19.1"><img alt="Figure 10.2 – Deep neural network training " src="../Images/B20836_10_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.20.1">Figure 10.2: Deep neural network training</span></p>
<p class="normal"><span class="koboSpan" id="kobo.21.1">The </span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.22.1">preceding diagram shows how</span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.23.1"> an </span><strong class="keyWord"><span class="koboSpan" id="kobo.24.1">artificial neural network</span></strong><span class="koboSpan" id="kobo.25.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.26.1">ANN</span></strong><span class="koboSpan" id="kobo.27.1">) trains. </span><span class="koboSpan" id="kobo.27.2">The training data is fed to the ANN in a forward pass. </span><span class="koboSpan" id="kobo.27.3">The loss (the difference between the predicted value and the true value) is calculated at </span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.28.1">the end of the forward pass, and the backward pass calculates the gradients for all the parameters. </span><span class="koboSpan" id="kobo.28.2">These parameters are updated with new values for the next step until the loss is minimized. </span><span class="koboSpan" id="kobo.28.3">In the following sections, we’ll look at distributed model training using data parallelism and model parallelism, two methods for scaling model training for large training datasets and large model sizes.</span></p>
<h2 class="heading-2" id="_idParaDest-263"><span class="koboSpan" id="kobo.29.1">Distributed model training using data parallelism</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.30.1">The</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.31.1"> data-parallel distributed training approach </span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.32.1">partitions a large training dataset into smaller subsets and trains each subset on different devices concurrently. </span><span class="koboSpan" id="kobo.32.2">This parallelization allows multiple training processes to run simultaneously on available compute resources, accelerating the overall training time. </span><span class="koboSpan" id="kobo.32.3">To leverage data-parallel training, the ML frameworks and algorithms used need to have support for distributed training. </span><span class="koboSpan" id="kobo.32.4">Frameworks like TensorFlow and PyTorch both provide modules and libraries for data parallelism training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.33.1">As we discussed earlier, one key task in</span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.34.1"> training </span><strong class="keyWord"><span class="koboSpan" id="kobo.35.1">deep learning</span></strong><span class="koboSpan" id="kobo.36.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.37.1">DL</span></strong><span class="koboSpan" id="kobo.38.1">) models is to calculate the gradients concerning the loss function for every batch of the data, and then update the model parameters with gradient information to minimize the loss gradually. </span><span class="koboSpan" id="kobo.38.2">Instead of running the gradient calculations and parameter updates on a single device, the basic concept behind data-parallel distributed training is to run multiple training processes using the same algorithm in parallel, with each process using a different subset of the training dataset. </span><span class="koboSpan" id="kobo.38.3">The </span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.39.1">following diagram shows the main concept behind data parallelism in training:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.40.1"><img alt="Figure 10.3 – Data parallelism concept " src="../Images/B20836_10_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.41.1">Figure 10.3: Data parallelism concept</span></p>
<p class="normal"><span class="koboSpan" id="kobo.42.1">As you can see, there are three nodes in a cluster participating in a distributed data-parallel training job, with each node having two devices. </span><span class="koboSpan" id="kobo.42.2">The partial gradients that are calculated by each device are represented by w0 ~ w5 for each of the devices on the nodes, while W is the value for a global parameter for the model. </span><span class="koboSpan" id="kobo.42.3">Specifically, data-parallel distributed training has the following main steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.43.1">Each device (CPU or GPU) on every node loads a copy of the same algorithm and a subset of the training data.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.44.1">Each device runs a training loop to calculate the gradients (w0~w5) to optimize its loss function and exchange the gradients with other devices in the cluster at each training step.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.45.1">The gradients from all the devices are aggregated and the common model parameters (W) are calculated using these aggregated gradients.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.46.1">Each device pulls down the newly calculated common model parameters (W) and continues with the next step of model training.</span></li>
<li class="numberedList"><em class="italic"><span class="koboSpan" id="kobo.47.1">Steps 2</span></em><span class="koboSpan" id="kobo.48.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.49.1">4</span></em><span class="koboSpan" id="kobo.50.1"> are repeated until the model training is completed.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.51.1">In a </span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.52.1">distributed training setting, efficiently exchanging gradients and parameters across processes is one of the most important aspects of ML system engineering design. </span><span class="koboSpan" id="kobo.52.2">Several distributed training topologies have been developed over the years to optimize communications across different training processes. </span><span class="koboSpan" id="kobo.52.3">In this chapter, we will discuss two of the most widely adopted topologies for data-parallel distributed training: </span><strong class="keyWord"><span class="koboSpan" id="kobo.53.1">parameter server</span></strong><span class="koboSpan" id="kobo.54.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.55.1">AllReduce</span></strong><span class="koboSpan" id="kobo.56.1">.</span></p>
<h3 class="heading-3" id="_idParaDest-264"><span class="koboSpan" id="kobo.57.1">Parameter server overview</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.58.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.59.1">parameter server</span></strong><span class="koboSpan" id="kobo.60.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.61.1">PS</span></strong><span class="koboSpan" id="kobo.62.1">) is a </span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.63.1">topology built on the concept of server nodes and worker nodes. </span><span class="koboSpan" id="kobo.63.2">The </span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.64.1">worker nodes are responsible for running the training loops and calculating the gradients, while the server nodes are responsible for aggregating the gradients and calculating the globally shared parameters. </span><span class="koboSpan" id="kobo.64.2">The following diagram shows the architecture of a PS:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.65.1"><img alt="Figure 10.4 – Parameter server architecture " src="../Images/B20836_10_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.66.1">Figure 10.4: Parameter server architecture</span></p>
<p class="normal"><span class="koboSpan" id="kobo.67.1">Here, the</span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.68.1"> server node is called the PS, and is usually implemented as a key-value or vector store for storing gradients and parameters. </span><span class="koboSpan" id="kobo.68.2">As the number of model parameters to manage can become very large, there can also be multiple server nodes for managing the global parameters and gradient aggregations. </span><span class="koboSpan" id="kobo.68.3">In a multi-parameter server configuration, there is also a server manager that manages and coordinates all the server nodes to ensure consistency.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.69.1">In this architecture, the worker nodes only communicate with the PS nodes to exchange gradients and parameters, and not with each other. </span><span class="koboSpan" id="kobo.69.2">In a multi-server node environment, each server node also communicates with every other server node to replicate the parameters for reliability and scalability. </span><span class="koboSpan" id="kobo.69.3">The gradients and parameters are exchanged so that updates can be implemented synchronously and asynchronously. </span><span class="koboSpan" id="kobo.69.4">The synchronous gradient update strategy blocks the devices from processing the next mini-batch of data until the gradients from all the devices have been synchronized. </span><span class="koboSpan" id="kobo.69.5">This means that each update has to wait for the slowest device to complete. </span><span class="koboSpan" id="kobo.69.6">This can slow down training and make the training process less robust in terms of device failure. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.70.1">On the positive side, synchronous updates do not have to worry about stale gradients, which can lead to higher model accuracy. </span><span class="koboSpan" id="kobo.70.2">Asynchronous updates do not need to wait for all the devices to be synchronized before processing the next mini-batch of data, though this might lead to reduced accuracy.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.71.1">The main limitation of this </span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.72.1">approach is that the PS can become a communication bottleneck, particularly for large models with billions or trillions of parameters. </span><span class="koboSpan" id="kobo.72.2">As the model size grows, the amount of data that needs to be transmitted between the workers and the PS increases significantly, leading to potential communication overhead and bandwidth constraints.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.73.1">Additionally, as the number of worker nodes increases, the PS needs to handle an increasing number of gradient updates and parameter distributions, which can become a scalability challenge. </span><span class="koboSpan" id="kobo.73.2">This centralized architecture can limit the overall throughput and efficiency of the distributed training process, especially when the number of workers becomes very large. </span><span class="koboSpan" id="kobo.73.3">Furthermore, slower or underperforming worker nodes can slow down the entire training process, as the PS must wait for all gradients before updating the parameters.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.74.1">Implementing the PS in frameworks</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.75.1">PS distributed</span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.76.1"> training is natively supported by several DL frameworks, including TensorFlow. </span><span class="koboSpan" id="kobo.76.2">Specifically, TensorFlow supports PS-based distributed training natively with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.77.1">ParameterServerStrategy</span></code><span class="koboSpan" id="kobo.78.1"> API. </span><span class="koboSpan" id="kobo.78.2">The following code sample shows how to instantiate the </span><code class="inlineCode"><span class="koboSpan" id="kobo.79.1">ParameterServerStrategy</span></code><span class="koboSpan" id="kobo.80.1"> API for TensorFlow:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.81.1">strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.82.1">In this code sample, the </span><code class="inlineCode"><span class="koboSpan" id="kobo.83.1">cluster_resolver</span></code><span class="koboSpan" id="kobo.84.1"> parameter helps discover and resolve the IP addresses of workers.</span></p>
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.85.1">ParameterServerStrategy</span></code><span class="koboSpan" id="kobo.86.1"> can be used directly with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.87.1">model.fit()</span></code><span class="koboSpan" id="kobo.88.1"> function of Keras or a custom training loop by wrapping the model with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.89.1">strategy.scope()</span></code><span class="koboSpan" id="kobo.90.1"> syntax. </span><span class="koboSpan" id="kobo.90.2">See the following sample syntax on how to use </span><code class="inlineCode"><span class="koboSpan" id="kobo.91.1">scope()</span></code><span class="koboSpan" id="kobo.92.1"> to wrap a model for distributed training:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.93.1">with</span></span><span class="koboSpan" id="kobo.94.1"> strategy.scope()
    model = &lt;model architecture definition&gt;
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.95.1">In </span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.96.1">addition to a PS implementation, which is natively supported within DL libraries, there are also general-purpose PS training frameworks, such as BytePS from ByteDance and Herring from Amazon, which work with different DL frameworks. </span><span class="koboSpan" id="kobo.96.2">SageMaker uses Herring under the hood for data-parallel distributed training through its SageMaker distributed training library. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.97.1">One of the shortcomings of the PS strategy is the inefficient use of network bandwidth. </span><span class="koboSpan" id="kobo.97.2">The Herring library addresses this shortcoming by combining AWS </span><strong class="keyWord"><span class="koboSpan" id="kobo.98.1">Elastic Fabric Adapter</span></strong><span class="koboSpan" id="kobo.99.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.100.1">EFA</span></strong><span class="koboSpan" id="kobo.101.1">) and the </span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.102.1">parameter sharding technique, which makes use of network bandwidth to achieve faster distributed training. </span><span class="koboSpan" id="kobo.102.2">EFA takes advantage of cloud resources and their characteristics, such as multi-path backbones, to improve network communication efficiency. </span><span class="koboSpan" id="kobo.102.3">You can find out more about Herring at </span><a href="https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud"><span class="url"><span class="koboSpan" id="kobo.103.1">https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud</span></span></a><span class="koboSpan" id="kobo.104.1">.</span></p>
<h3 class="heading-3" id="_idParaDest-265"><span class="koboSpan" id="kobo.105.1">AllReduce overview</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.106.1">While </span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.107.1">the PS architecture is easy to </span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.108.1">understand and set up, it does come with several challenges. </span><span class="koboSpan" id="kobo.108.2">For example, the PS architecture requires additional nodes for the PSs, and it is also hard to determine the right ratio between server nodes and worker nodes to ensure the server nodes do not become bottlenecks.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.109.1">The AllReduce topology tries to improve some of the limitations of PSs by eliminating the server nodes and distributing all the gradient aggregation and global parameter updates to all</span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.110.1"> workers, hence it’s called </span><strong class="keyWord"><span class="koboSpan" id="kobo.111.1">AllReduce</span></strong><span class="koboSpan" id="kobo.112.1">. </span><span class="koboSpan" id="kobo.112.2">The following diagram shows the topology of AllReduce:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.113.1"><img alt="Figure 10.5 – AllReduce architecture " src="../Images/B20836_10_05.png"/></span></figure>
<figure class="mediaobject"><span class="koboSpan" id="kobo.114.1">Figure 10.5: AllReduce architecture</span></figure>
<p class="normal"><span class="koboSpan" id="kobo.115.1">In an </span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.116.1">AllReduce topology, each node </span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.117.1">sends gradients of parameters to all the other nodes at each training step. </span><span class="koboSpan" id="kobo.117.2">Then, each node aggregates the gradients and performs a reduce function (such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.118.1">average</span></code><span class="koboSpan" id="kobo.119.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.120.1">sum</span></code><span class="koboSpan" id="kobo.121.1">, or </span><code class="inlineCode"><span class="koboSpan" id="kobo.122.1">max</span></code><span class="koboSpan" id="kobo.123.1">) locally before calculating the new parameters using the next training step. </span><span class="koboSpan" id="kobo.123.2">Since every node needs to communicate with every other node, this results in a large number of networks of communication between the nodes, and duplicate compute and storage are required as every node has a copy of all the gradients.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.124.1">A more efficient </span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.125.1">AllReduce architecture is Ring AllReduce. </span><span class="koboSpan" id="kobo.125.2">In this architecture, each node only sends some gradients to its next neighboring node, and each node is responsible for aggregating the gradients for the global parameters that it is assigned to calculate. </span><span class="koboSpan" id="kobo.125.3">This architecture greatly reduces the amount of network communication in a cluster and compute overhead, so it is more efficient for model training. </span><span class="koboSpan" id="kobo.125.4">The following diagram shows the Ring AllReduce architecture:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.126.1"><img alt="Figure 10.6 – Ring AllReduce " src="../Images/B20836_10_06.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.127.1">Figure 10.6: Ring AllReduce</span></p>
<p class="normal"><span class="koboSpan" id="kobo.128.1">Compared to </span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.129.1">the PS approach, the </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.130.1">Ring AllReduce approach has better scalability as the number of workers increases. </span><span class="koboSpan" id="kobo.130.2">Since there is no centralized PS, the communication load is distributed among the workers, reducing the potential bottleneck. </span><span class="koboSpan" id="kobo.130.3">The Ring AllReduce approach also has a lower communication overhead, especially for large models with billions or trillions of parameters. </span><span class="koboSpan" id="kobo.130.4">Instead of sending individual gradients to a central server, the gradients are summed and passed along a ring topology, reducing the overall amount of data that needs to be transmitted.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.131.1">Overall, the Ring AllReduce approach offers better scalability and efficiency for large-scale distributed training. </span><span class="koboSpan" id="kobo.131.2">It distributes the communication load among workers, reducing potential bottlenecks and synchronization overhead. </span><span class="koboSpan" id="kobo.131.3">However, the PS approach may still be suitable for smaller-scale distributed training scenarios or when fault tolerance is less of a concern.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.132.1">Implementing AllReduce and Ring AllReduce in frameworks</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.133.1">The AllReduce </span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.134.1">and Ring AllReduce </span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.135.1">architectures are natively supported within multiple DL frameworks, including TensorFlow and PyTorch.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.136.1">TensorFlow supports AllReduce distributed training across multiple GPUs on one machine with its </span><code class="inlineCode"><span class="koboSpan" id="kobo.137.1">tf.distribute.MirroredStrategy</span></code><span class="koboSpan" id="kobo.138.1"> API. </span><span class="koboSpan" id="kobo.138.2">With this strategy, each GPU has a copy of the model, and all the model parameters are mirrored across different devices. </span><span class="koboSpan" id="kobo.138.3">An efficient AllReduce mechanism is used to keep these parameters in sync. </span><span class="koboSpan" id="kobo.138.4">The following code sample shows how to instantiate the </span><code class="inlineCode"><span class="koboSpan" id="kobo.139.1">MirroredStrategy</span></code><span class="koboSpan" id="kobo.140.1"> API:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.141.1">strategy = tf.distribute.MirroredStrategy()
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.142.1">For </span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.143.1">multi-machine distributed training, TensorFlow uses the </span><code class="inlineCode"><span class="koboSpan" id="kobo.144.1">tf.distribute.MultiWorkerMirroredStrategy</span></code><span class="koboSpan" id="kobo.145.1"> API. </span><span class="koboSpan" id="kobo.145.2">Similar to </span><code class="inlineCode"><span class="koboSpan" id="kobo.146.1">MirroredStrategy</span></code><span class="koboSpan" id="kobo.147.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.148.1">MultiWorkerMirroredStrategy</span></code><span class="koboSpan" id="kobo.149.1"> creates copies of all the parameters across all the devices on all the machines and synchronizes them with the AllReduce mechanism. </span><span class="koboSpan" id="kobo.149.2">The following code sample shows how to instantiate the </span><code class="inlineCode"><span class="koboSpan" id="kobo.150.1">MultiWorkerMirroredStrategy</span></code><span class="koboSpan" id="kobo.151.1"> API:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.152.1">strategy = tf.distribute.MultiWorkerMirroredStrategy()
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.153.1">Similar to </span><code class="inlineCode"><span class="koboSpan" id="kobo.154.1">ParameterServerStrategy</span></code><span class="koboSpan" id="kobo.155.1">, </span><code class="inlineCode"><span class="koboSpan" id="kobo.156.1">MirroredStrategy</span></code><span class="koboSpan" id="kobo.157.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.158.1">MultiWorkerMirroredStrategy</span></code><span class="koboSpan" id="kobo.159.1"> can work with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.160.1">keras model.fit()</span></code><span class="koboSpan" id="kobo.161.1"> function or a custom training loop. </span><span class="koboSpan" id="kobo.161.2">To associate a model with a training strategy, you can use the same </span><code class="inlineCode"><span class="koboSpan" id="kobo.162.1">strategy.scope()</span></code><span class="koboSpan" id="kobo.163.1"> syntax.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.164.1">PyTorch also</span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.165.1"> provides native support for AllReduce-based distributed training via its </span><code class="inlineCode"><span class="koboSpan" id="kobo.166.1">torch.nn.DataParallel</span></code><span class="koboSpan" id="kobo.167.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.168.1">torch.nn.parallel.DistributedDataParallel</span></code><span class="koboSpan" id="kobo.169.1"> APIs. </span><span class="koboSpan" id="kobo.169.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.170.1">torch.nn.DataParallel</span></code><span class="koboSpan" id="kobo.171.1"> API supports single-process multi-threading across GPUs on the same machine, while </span><code class="inlineCode"><span class="koboSpan" id="kobo.172.1">torch.nn.parallel.DistributedDataParallel</span></code><span class="koboSpan" id="kobo.173.1"> supports multi-processing across GPUs and machines. </span><span class="koboSpan" id="kobo.173.2">The following code sample shows how to initiate a distributed training cluster and wrap a model for distributed training using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.174.1">DistributedDataParallel</span></code><span class="koboSpan" id="kobo.175.1"> API:</span></p>
<pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.176.1">torch.distributed.init_process_group(...)
model = torch.nn.parallel.DistributedDataParallel(model, ...)
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.177.1">Another popular implementation of the general-purpose Ring AllReduce architecture is </span><strong class="keyWord"><span class="koboSpan" id="kobo.178.1">Horovod</span></strong><span class="koboSpan" id="kobo.179.1">, which</span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.180.1"> was created by the engineers at Uber. </span><span class="koboSpan" id="kobo.180.2">Horovod works with multiple DL frameworks, including TensorFlow and PyTorch. </span><span class="koboSpan" id="kobo.180.3">You can find out more about </span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.181.1">Horovod at </span><a href="https://github.com/horovod/horovod"><span class="url"><span class="koboSpan" id="kobo.182.1">https://github.com/horovod/horovod</span></span></a><span class="koboSpan" id="kobo.183.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-266"><span class="koboSpan" id="kobo.184.1">Distributed model training using model parallelism</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.185.1">Compared </span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.186.1">to data parallelism, model parallelism is still relatively low in its adoption since most of the distributed training that happens today involves data parallelism that deals with large datasets. </span><span class="koboSpan" id="kobo.186.2">However, the applications of state-of-the-art big DL algorithms such as BERT, GPT, and T5 are driving the increasing adoption of model parallelism. </span><span class="koboSpan" id="kobo.186.3">The qualities of these models </span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.187.1">are known to increase with the model’s size, and these large NLP models require a large amount of memory to store the model’s states (which include the model’s parameters, optimizer states, and gradients) and memory for other overheads.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.188.1">As such, these models can no longer fit into the memory of a single GPU. </span><span class="koboSpan" id="kobo.188.2">While data parallelism helps solve the large dataset challenge, it cannot help with training large models due to its large memory size requirements. </span><span class="koboSpan" id="kobo.188.3">Model parallelism allows you to split a single large model across multiple devices so that the total memory across multiple devices is enough to hold a copy of the model. </span><span class="koboSpan" id="kobo.188.4">Model parallelism also allows for a larger batch size for model training as a result of the larger collective memory across multiple devices. </span><span class="koboSpan" id="kobo.188.5">There are two main approaches to splitting the model for parallel distributed training: splitting by layers and splitting by tensors. </span><span class="koboSpan" id="kobo.188.6">Next, let’s explore these two approaches in more detail.</span></p>
<h3 class="heading-3" id="_idParaDest-267"><span class="koboSpan" id="kobo.189.1">Naïve model parallelism overview</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.190.1">As an</span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.191.1"> ANN consists</span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.192.1"> of many layers, one way to split the model is to distribute the layers across multiple devices. </span><span class="koboSpan" id="kobo.192.2">For example, if you have an </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.193.1">8-layer </span><strong class="keyWord"><span class="koboSpan" id="kobo.194.1">multi-layer perceptron</span></strong><span class="koboSpan" id="kobo.195.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.196.1">MLP</span></strong><span class="koboSpan" id="kobo.197.1">)</span><strong class="keyWord"> </strong><span class="koboSpan" id="kobo.198.1">network and two GPUs (GPU0 and GPU1), you can simply place the first four layers in GPU0 and the last four layers in GPU1. </span><span class="koboSpan" id="kobo.198.2">During training, the first four layers of the model are trained as you would normally train a model in a single device. </span><span class="koboSpan" id="kobo.198.3">When the first four layers are complete, the output from the fourth layer will be copied from GPU0 to GPU1, incurring a communication overhead. </span><span class="koboSpan" id="kobo.198.4">After getting the output from GPU0, GPU1 continues training layers five to eight. </span><span class="koboSpan" id="kobo.198.5">The following diagram illustrates splitting a model by layers across multiple devices:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.199.1"><img alt="Figure 10.7 – Naïve model parallelism " src="../Images/B20836_10_07.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.200.1">Figure 10.7: Naïve model parallelism</span></p>
<p class="normal"><span class="koboSpan" id="kobo.201.1">Implementing </span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.202.1">model parallelism by splitting requires knowledge of the training task. </span><span class="koboSpan" id="kobo.202.2">It is not a trivial task to design an efficient model </span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.203.1">parallelism strategy. </span><span class="koboSpan" id="kobo.203.2">Here are a few heuristics that could be helpful for the split-layer design:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.204.1">Place neighboring layers on the same devices to minimize communication overhead.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.205.1">Balance the workload between devices.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.206.1">Different layers have different compute and memory utilization properties.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.207.1">Training an ANN model is inherently a sequential process, which means that the network layers are processed sequentially, while the backward process will only start when the forward process is completed. </span><span class="koboSpan" id="kobo.207.2">When you’re splitting layers across multiple devices, only the device currently processing the layers on it will be busy; the other devices will be idle, wasting compute resources, which results in a waste of hardware resources. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.208.1">The following diagram illustrates the processing of sequences for the forward and backward passes for one batch of data:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.209.1"><img alt="Figure 10.8 – Naïve model parallelism  " src="../Images/B20836_10_08.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.210.1">Figure 10.8: Naïve model parallelism</span></p>
<p class="normal"><span class="koboSpan" id="kobo.211.1">In the preceding diagram, </span><strong class="keyWord"><span class="koboSpan" id="kobo.212.1">F0</span></strong><span class="koboSpan" id="kobo.213.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.214.1">F1</span></strong><span class="koboSpan" id="kobo.215.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.216.1">F2</span></strong><span class="koboSpan" id="kobo.217.1"> are the forward passes on the different neural network layers on each device. </span><strong class="keyWord"><span class="koboSpan" id="kobo.218.1">B2</span></strong><span class="koboSpan" id="kobo.219.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.220.1">B1</span></strong><span class="koboSpan" id="kobo.221.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.222.1">B0</span></strong><span class="koboSpan" id="kobo.223.1"> are the backward passes for the layers on each device. </span><span class="koboSpan" id="kobo.223.2">As you can see, when one of the devices is busy with either a forward pass or a backward pass, the other devices are idle. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.224.1">Naïve model parallelism </span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.225.1">has the benefit of implementation </span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.226.1">simplicity, and it is suitable for models with a large number of layers. </span><span class="koboSpan" id="kobo.226.2">However, it has scalability challenges due to the sequential nature of layer execution. </span><span class="koboSpan" id="kobo.226.3">In addition, it may run into potential load imbalance issues if layers have varying computational requirements. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.227.1">Next, let’s look at an approach (pipeline model parallelism) that can help increase resource utilization.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.228.1">Pipeline model parallelism overview</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.229.1">To resolve</span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.230.1"> the resource-idling issue, pipeline model parallelism</span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.231.1"> can be implemented. </span><span class="koboSpan" id="kobo.231.2">This improves on naïve model parallelism so that different devices can work in parallel on the different stages of the training pipeline on a smaller chunk of data batch, commonly known as a micro-batch. </span><span class="koboSpan" id="kobo.231.3">The following diagram shows how pipeline model parallelism works:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.232.1"><img alt="Figure 10.9 – Pipeline model parallelism " src="../Images/B20836_10_09.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.233.1">Figure 10.9: Pipeline model parallelism</span></p>
<p class="normal"><span class="koboSpan" id="kobo.234.1">With pipeline </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.235.1">model parallelism, instead of processing one batch of data through each full forward and backward pass, that one batch of data is broken down </span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.236.1">into smaller mini-batches. </span><span class="koboSpan" id="kobo.236.2">In the preceding diagram, after </span><strong class="keyWord"><span class="koboSpan" id="kobo.237.1">Device 0</span></strong><span class="koboSpan" id="kobo.238.1"> completes the forward pass for the first mini-batch, </span><strong class="keyWord"><span class="koboSpan" id="kobo.239.1">Device 1</span></strong><span class="koboSpan" id="kobo.240.1"> can start its forward pass on the output of the </span><strong class="keyWord"><span class="koboSpan" id="kobo.241.1">Device 1</span></strong><span class="koboSpan" id="kobo.242.1"> forward pass. </span><span class="koboSpan" id="kobo.242.2">Instead of waiting for </span><strong class="keyWord"><span class="koboSpan" id="kobo.243.1">Device 1</span></strong><span class="koboSpan" id="kobo.244.1"> and </span><strong class="keyWord"><span class="koboSpan" id="kobo.245.1">Device 2</span></strong><span class="koboSpan" id="kobo.246.1"> to complete their forward passes and backward passes, </span><strong class="keyWord"><span class="koboSpan" id="kobo.247.1">Device 0</span></strong><span class="koboSpan" id="kobo.248.1"> starts to process the next mini-batch of data. </span><span class="koboSpan" id="kobo.248.2">This scheduled pipeline allows for higher utilization of the hardware resources, resulting in faster model training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.249.1">There are other variations of pipeline parallelism. </span><span class="koboSpan" id="kobo.249.2">One example is interleaved parallelism, where a backward execution is prioritized whenever possible. </span><span class="koboSpan" id="kobo.249.3">This improves the utilization of devices for end-to-end model training. </span><span class="koboSpan" id="kobo.249.4">The following diagram shows how an interleaved pipeline works:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.250.1"><img alt="Figure 10.10 – Interleaved pipeline " src="../Images/B20836_10_10.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.251.1">Figure 10.10: Interleaved pipeline</span></p>
<p class="normal"><span class="koboSpan" id="kobo.252.1">Pipeline model parallelism has been implemented in various frameworks and products such as the SageMaker distributed training library and DeepSpeed distributed training framework, which we will cover in greater detail in a later section.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.253.1">Next, let’s look at an overview of tensor parallelism, also known as tensor slicing.</span></p>
<h3 class="heading-3" id="_idParaDest-268"><span class="koboSpan" id="kobo.254.1">Tensor parallelism/tensor slicing overview</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.255.1">As we</span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.256.1"> mentioned earlier, tensor parallelism is another approach to split </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.257.1">a large model to make it fit into memory. </span><span class="koboSpan" id="kobo.257.2">Before we dive into this, let’s quickly review what a tensor is and how it is processed by an ANN.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.258.1">A </span><strong class="keyWord"><span class="koboSpan" id="kobo.259.1">tensor</span></strong><span class="koboSpan" id="kobo.260.1"> is a </span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.261.1">multi-dimensional matrix of a single data type such as a 32-bit</span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.262.1"> floating-point or 8-bit integer. </span><span class="koboSpan" id="kobo.262.2">In the forward pass of neural network training, a dot product is used on the input tensor and weight matrix tensors (the connections between the input tensors and the neurons in the hidden layer). </span><span class="koboSpan" id="kobo.262.3">You can find out more about</span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.263.1"> dot products at </span><a href="https://en.wikipedia.org/wiki/Dot_product"><span class="url"><span class="koboSpan" id="kobo.264.1">https://en.wikipedia.org/wiki/Dot_product</span></span></a><span class="koboSpan" id="kobo.265.1">. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.266.1">The following diagram illustrates a dot product between the input vector and the weight matrix:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.267.1"><img alt="Figure 10.11 – Matrix calculation  " src="../Images/B20836_10_11.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.268.1">Figure 10.11: Matrix calculation</span></p>
<p class="normal"><span class="koboSpan" id="kobo.269.1">In this matrix calculation, you get an output vector of </span><strong class="keyWord"><span class="koboSpan" id="kobo.270.1">[5,11,17]</span></strong><span class="koboSpan" id="kobo.271.1">. </span><span class="koboSpan" id="kobo.271.2">If there is a single device for dot product calculation, three separate calculations will be performed sequentially to get the output vector.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.272.1">But what if we break up the single weights matrix into three vectors and use a dot product separately? </span><span class="koboSpan" id="kobo.272.2">This can be seen in the following diagram:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.273.1"><img alt="Figure 10.12 – Splitting the matrix calculation " src="../Images/B20836_10_12.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.274.1">Figure 10.12: Splitting the matrix calculation</span></p>
<p class="normal"><span class="koboSpan" id="kobo.275.1">As you can see, you </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.276.1">would get three separate values that are the same as the individual values in the output vector in the preceding diagram. </span><span class="koboSpan" id="kobo.276.2">If there are three separate devices for performing dot product calculations, we can perform these three dot product calculations in parallel and combine the values into a single vector at the end if needed. </span><span class="koboSpan" id="kobo.276.3">This is the basic concept of how tensor parallelism works. </span><span class="koboSpan" id="kobo.276.4">With tensor parallelism, each device works</span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.277.1"> independently without the need for any communication until the end, which is when the results need to be synchronized. </span><span class="koboSpan" id="kobo.277.2">This strategy allows for faster tensor processing as multiple devices can work in parallel to reduce the training time and increase the utilization of computing devices.</span></p>
<h3 class="heading-3" id="_idParaDest-269"><span class="koboSpan" id="kobo.278.1">Implementing model-parallel training</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.279.1">To </span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.280.1">implement model parallelism, you can manually design the parallelism strategy by deciding how to split the layers and tensors, as well as their placements, across different devices and nodes. </span><span class="koboSpan" id="kobo.280.2">However, it is not trivial to do this efficiently, especially for large clusters. </span><span class="koboSpan" id="kobo.280.3">To make the model parallelism implementation easier, several model parallelism library packages have been developed. </span><span class="koboSpan" id="kobo.280.4">In this section, we’ll take a closer look at some of these libraries. </span><span class="koboSpan" id="kobo.280.5">Note that the frameworks we will discuss can support both data parallelism and model parallelism and that both techniques are often used together to train large models with large training datasets.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.281.1">Megatron-LM</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.282.1">Megatron-LM is </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.283.1">an open-source distributed training</span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.284.1"> framework developed by Nvidia. </span><span class="koboSpan" id="kobo.284.2">It supports data parallelism, tensor parallelism, and pipeline model parallelism, as well as a combination of all three for extreme-scale model training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.285.1">Megatron-LM</span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.286.1"> implements micro-batch-based pipeline model parallelism to improve device utilization. </span><span class="koboSpan" id="kobo.286.2">It also implements periodic pipeline flushes to ensure that the optimizer steps are synchronized across devices. </span><span class="koboSpan" id="kobo.286.3">Two different pipeline schedules are supported by Megatron-LM, as follows:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.287.1">The default schedule works by completing the forward pass for all micro-batches first, before starting the backward pass for all the batches.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.288.1">The interleaved stage schedule works by running multiple different subsets of layers on a single device, instead of running just a single continuous set of layers. </span><span class="koboSpan" id="kobo.288.2">This can further improve the utilization of devices and reduce idle time.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.289.1">Megatron-LM</span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.290.1"> implements a specific tensor parallelism strategy for transformer-based models. </span><span class="koboSpan" id="kobo.290.2">A transformer consists mainly of self-attention blocks, followed by a two-layer MLP. </span><span class="koboSpan" id="kobo.290.3">For the MLP portion, Megatron-LM splits the weight matrix by columns. </span><span class="koboSpan" id="kobo.290.4">The matrices for the self-attention heads are also partitioned by columns. </span><span class="koboSpan" id="kobo.290.5">The following diagram shows how the different parts of the transformers can be split:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.291.1"><img alt="Figure 10.13 – Tensor parallelism for transformers " src="../Images/B20836_10_13.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.292.1">Figure 10.13: Tensor parallelism for transformers</span></p>
<p class="normal"><span class="koboSpan" id="kobo.293.1">Using data parallelism, pipeline model parallelism, and tensor parallelism together, Megatron-LM can be used to train extremely large transformer-based models (with a trillion parameters) scaled across thousands of GPUs.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.294.1">Training </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.295.1">using Megatron-LM involves the following key steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.296.1">Initializing the Megatron library using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.297.1">initialize_megatron()</span></code><span class="koboSpan" id="kobo.298.1"> function.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.299.1">Setting up the Megatron model optimizer using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.300.1">setup_model_and_optimizer()</span></code><span class="koboSpan" id="kobo.301.1"> function by wrapping the original model.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.302.1">Training the model using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.303.1">train()</span></code><span class="koboSpan" id="kobo.304.1"> function, which takes the Megatron model and optimizer as input.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.305.1">Megatron-LM </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.306.1">has been used for many large-model training projects, such as BERT, GPT, and the Biomedical domain language model. </span><span class="koboSpan" id="kobo.306.2">Its scalable architecture can be used to train models with trillions of parameters.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.307.1">DeepSpeed</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.308.1">DeepSpeed </span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.309.1">is an </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.310.1">open-source distributed training framework developed by Microsoft. </span><span class="koboSpan" id="kobo.310.2">Similar to Megatron-LM, DeepSpeed also supports tensor-slicing (another name for splitting tensors) parallelism, pipeline parallelism, and data parallelism. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.311.1">DeepSpeed implements micro-batch-based pipeline model parallelism, where a batch is broken into micro-batches to be processed by different devices in parallel. </span><span class="koboSpan" id="kobo.311.2">Specifically, DeepSpeed implements interleaved pipeline parallelism to optimize resource efficiency and utilization. </span><span class="koboSpan" id="kobo.311.3">Similar to Megatron-LM, DeepSpeed can use data parallelism, pipeline model parallelism, and tensor parallelism together to train extremely large deep neural networks. </span><span class="koboSpan" id="kobo.311.4">This is also known as DeepSpeed 3D parallelism.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.312.1">One core capability of the DeepSpeed framework is its </span><strong class="keyWord"><span class="koboSpan" id="kobo.313.1">Zero Redundancy Optimizer</span></strong><span class="koboSpan" id="kobo.314.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.315.1">ZeRO</span></strong><span class="koboSpan" id="kobo.316.1">). </span><span class="koboSpan" id="kobo.316.2">ZeRO is </span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.317.1">capable of managing memory efficiently by partitioning parameters, optimizer states, and gradients across devices instead of keeping a copy on all devices. </span><span class="koboSpan" id="kobo.317.2">The partitions are brought together at runtime when needed. </span><span class="koboSpan" id="kobo.317.3">This allows ZeRO to reduce the memory footprint by eight times compared to regular data parallelism techniques. </span><span class="koboSpan" id="kobo.317.4">ZeRO is also capable of using CPU and GPU memory together to train large models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.318.1">The attention-based mechanism is widely adopted in DL models, such as the transformer model, to address text and image inputs. </span><span class="koboSpan" id="kobo.318.2">However, its ability to address long input sequences is limited due to its large memory and compute requirements. </span><span class="koboSpan" id="kobo.318.3">DeepSpeed helps alleviate this issue with its implementation of a sparse attention kernel – a technology that reduces the compute and memory requirements of attention computation via block-sparse computation.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.319.1">One major bottleneck in large-scale distributed training is the communication overhead due to gradient sharing and updates. </span><span class="koboSpan" id="kobo.319.2">Communication compression, such as 1-bit compression, has been adopted as an effective mechanism to reduce the communication overhead. </span><span class="koboSpan" id="kobo.319.3">DeepSpeed </span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.320.1">has an implementation of a 1-bit Adam optimizer, which can reduce the communication overhead by up to five times to improve the training speed. </span><span class="koboSpan" id="kobo.320.2">1-bit compression works by representing each number using 1 bit, combined with error compensation, which remembers the error during gradient compression and adds the error back to the next step to compensate for the error.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.321.1">To use DeepSpeed, you </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.322.1">need to modify your training script. </span><span class="koboSpan" id="kobo.322.2">The following steps explain the main changes you need to make to a training script to run distributed training:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.323.1">Use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.324.1">deepspeed.initialize()</span></code><span class="koboSpan" id="kobo.325.1"> function to wrap the model and return a DeepSpeed model engine. </span><span class="koboSpan" id="kobo.325.2">This model engine will be used to run a forward pass and a backward pass.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.326.1">Use the returned DeepSpeed model engine to run the forward pass, backward pass, and step function to update the model parameters.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.327.1">DeepSpeed primarily supports the PyTorch framework and requires minor code changes to adopt model training using PyTorch. </span><span class="koboSpan" id="kobo.327.2">DeepSpeed has been used for training models with hundreds of billions of parameters and has delivered some of the fastest model training times. </span><span class="koboSpan" id="kobo.327.3">You can find out more about </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.328.1">DeepSpeed at </span><a href="https://www.deepspeed.ai"><span class="url"><span class="koboSpan" id="kobo.329.1">https://www.deepspeed.ai</span></span></a><span class="koboSpan" id="kobo.330.1">.</span></p>
<h4 class="heading-4"><span class="koboSpan" id="kobo.331.1">SageMaker distributed training library</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.332.1">Amazon’s </span><strong class="keyWord"><span class="koboSpan" id="kobo.333.1">SageMaker distributed training</span></strong><span class="koboSpan" id="kobo.334.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.335.1">SMD</span></strong><span class="koboSpan" id="kobo.336.1">) library is part of the Amazon SageMaker</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.337.1"> service</span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.338.1"> offering. </span><span class="koboSpan" id="kobo.338.2">SMD supports data parallelism (by using Herring under the hood) and interleaved pipeline model parallelism. </span><span class="koboSpan" id="kobo.338.3">Unlike DeepSpeed and Megatron-LM, where you need to manually decide on your model </span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.339.1">partitions, </span><strong class="keyWord"><span class="koboSpan" id="kobo.340.1">SageMaker Model Parallel</span></strong><span class="koboSpan" id="kobo.341.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.342.1">SMP</span></strong><span class="koboSpan" id="kobo.343.1">) has a feature for automated model splitting support.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.344.1">This automated model-splitting feature of SMP balances memory and communication constraints between devices to optimize performance. </span><span class="koboSpan" id="kobo.344.2">Automated model splitting takes place during the first training step, where a version of a model is constructed in CPU memory. </span><span class="koboSpan" id="kobo.344.3">The graph is analyzed, a partition decision is made, and different model partitions are loaded into different GPUs. </span><span class="koboSpan" id="kobo.344.4">The partition software performs framework-specific analysis for TensorFlow and PyTorch to determine the partition decision. </span><span class="koboSpan" id="kobo.344.5">It considers graph structures such as variable/parameter sharing, parameter sizes, and constraints to balance the number of variables and the number of operations for each device to come up with split decisions.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.345.1">To use the </span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.346.1">SMD library, you need to make some changes to your existing training scripts and create SageMaker training jobs. </span><span class="koboSpan" id="kobo.346.2">There are different instructions for TensorFlow and PyTorch. </span><span class="koboSpan" id="kobo.346.3">The following are examples </span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.347.1">for the PyTorch framework:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.348.1">Modify the PyTorch training script:</span><ol class="romanList" style="list-style-type: lower-roman;">
<li class="romanList" value="1"><span class="koboSpan" id="kobo.349.1">Call </span><code class="inlineCode"><span class="koboSpan" id="kobo.350.1">smp.init()</span></code><span class="koboSpan" id="kobo.351.1"> to initialize the library.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.352.1">Wrap the model with </span><code class="inlineCode"><span class="koboSpan" id="kobo.353.1">smp.DistributedModel()</span></code><span class="koboSpan" id="kobo.354.1">.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.355.1">Wrap the optimizer with </span><code class="inlineCode"><span class="koboSpan" id="kobo.356.1">smp.DistributedOptimizer()</span></code><span class="koboSpan" id="kobo.357.1">.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.358.1">Restrict each process to its own device through </span><code class="inlineCode"><span class="koboSpan" id="kobo.359.1">torch.cuda.set_device(smp.local_rank())</span></code><span class="koboSpan" id="kobo.360.1">.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.361.1">Use the wrapped model to perform a forward pass and a backward pass.</span></li>
<li class="romanList"><span class="koboSpan" id="kobo.362.1">Use the distributed optimizer to update the parameters.</span></li>
</ol>
</li>
<li class="numberedList"><span class="koboSpan" id="kobo.363.1">Create a SageMaker training job using SageMaker PyTorch Estimator and enable SMP distributed training.</span></li>
</ol>
<h4 class="heading-4"><span class="koboSpan" id="kobo.364.1">FairScale</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.365.1">FairScale is a </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.366.1">distributed training framework developed </span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.367.1">by Facebook AI Research (FAIR). </span><span class="koboSpan" id="kobo.367.2">It is built on top of the popular PyTorch deep learning library and provides a set of utilities and APIs for efficient distributed training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.368.1">FairScale supports various distributed training paradigms, including data parallelism, model parallelism, and a combination of both. </span><span class="koboSpan" id="kobo.368.2">FairScale provides efficient implementations of these techniques, along with optimizations and techniques to reduce communication overhead and improve scalability.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.369.1">One of the key features of FairScale is its support for various model parallel strategies, such as tensor parallelism, pipeline parallelism, and hybrid parallelism. </span><span class="koboSpan" id="kobo.369.2">These strategies allow users to distribute large models across multiple accelerators in different ways, enabling efficient utilization of available hardware resources and better scaling for massive models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.370.1">In addition to distributed training capabilities, FairScale also offers tools for optimizing memory usage, such as activation checkpointing, gradient checkpointing, and mixed precision training. </span><span class="koboSpan" id="kobo.370.2">These techniques help reduce the memory footprint of large models, allowing users to train models that would otherwise exceed the available memory on a single device.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.371.1">FairScale is designed to be user-friendly and easy to integrate into existing PyTorch codebases. </span><span class="koboSpan" id="kobo.371.2">It </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.372.1">provides a high-level API that abstracts away many of the complexities of distributed training, allowing users to focus on model development and experimentation rather than low-level implementation details. </span><span class="koboSpan" id="kobo.372.2">To use FairScale, you</span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.373.1"> simply install the package using </span><code class="inlineCode"><span class="koboSpan" id="kobo.374.1">pip install fairscale</span></code><span class="koboSpan" id="kobo.375.1"> and import the library into your training script using </span><code class="inlineCode"><span class="koboSpan" id="kobo.376.1">import fairscale</span></code><span class="koboSpan" id="kobo.377.1">. </span><span class="koboSpan" id="kobo.377.2">You can then use its various supported features for distributed data and model-parallel training.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.378.1">While distributed model training allows us to train extremely large models, running inferences on these large models can result in high latency due to the size of the models and other technological constraints. </span><span class="koboSpan" id="kobo.378.2">Next, let’s explore the various techniques we can use to achieve low-latency inference.</span></p>
<h1 class="heading-1" id="_idParaDest-270"><span class="koboSpan" id="kobo.379.1">Achieving low-latency model inference</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.380.1">As ML models </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.381.1">continue to grow and get deployed to different hardware devices, latency can become an issue for certain inference use cases that require low-latency and high-throughput inferences, such as real-time fraud detection. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.382.1">To reduce the overall model inference latency for a real-time application, there are different optimization considerations and techniques we can use, including model optimization, graph optimization, hardware acceleration, and inference engine optimization.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.383.1">In this section, we will focus on model optimization, graph optimization, and hardware optimization. </span><span class="koboSpan" id="kobo.383.2">Before we get into these various topics, let’s first understand how model inference works, specifically for DL models, since that’s what most of the inference optimization processes focus on.</span></p>
<h2 class="heading-2" id="_idParaDest-271"><span class="koboSpan" id="kobo.384.1">How model inference works and opportunities for optimization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.385.1">As we discussed</span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.386.1"> earlier in this book, DL models are constructed as computational graphs with nodes and edges, where the nodes represent the different operations and the edges represent the data flow. </span><span class="koboSpan" id="kobo.386.2">Examples </span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.387.1">of such operations include addition, matrix multiplication, activation (for example, Sigmoid and ReLU), and pooling. </span><span class="koboSpan" id="kobo.387.2">These operations perform computations on tensors as inputs and produce tensors as outputs. </span><span class="koboSpan" id="kobo.387.3">For example, the </span><em class="italic"><span class="koboSpan" id="kobo.388.1">c=matmul(a,b)</span></em><span class="koboSpan" id="kobo.389.1"> operation takes </span><em class="italic"><span class="koboSpan" id="kobo.390.1">a</span></em><span class="koboSpan" id="kobo.391.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.392.1">b</span></em><span class="koboSpan" id="kobo.393.1"> as input tensors and produces </span><em class="italic"><span class="koboSpan" id="kobo.394.1">c</span></em><span class="koboSpan" id="kobo.395.1"> as the output tensor. </span><span class="koboSpan" id="kobo.395.2">Deep learning frameworks, such as TensorFlow and PyTorch, have built-in operators to support different operations. </span><span class="koboSpan" id="kobo.395.3">The implementation of an operator is also called a kernel.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.396.1">During inference time</span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.397.1"> for a trained model, the DL framework’s runtime will walk through the computational graph and invoke the appropriate kernels (such as add or Sigmoid) for each of the nodes in the graph. </span><span class="koboSpan" id="kobo.397.2">The kernel will </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.398.1">take various inputs, such as the inference data samples, learned model parameters, and intermediate outputs, from the preceding operators and perform specific computations according to the data flow defined by the computational graph to produce the final predictions. </span><span class="koboSpan" id="kobo.398.2">The size of a trained model is mainly determined by the number of nodes in a graph, as well as the number of model parameters and their numerical precisions (for example, floating-point 32, floating-point 16, or integer 8).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.399.1">Different hardware providers such as Nvidia and Intel also provide hardware-specific implementations of kernels for common computational graph operations. </span><span class="koboSpan" id="kobo.399.2">cuDNN is the library from Nvidia for optimized kernel implementations for their GPU devices, while MKL-DNN is the library from Intel for optimized kernel implementations for Intel chips. </span><span class="koboSpan" id="kobo.399.3">These hardware-specific implementations take advantage of the unique capabilities of the underlying hardware architecture. </span><span class="koboSpan" id="kobo.399.4">They can perform better than the kernels that are implemented by the DL framework implementation since the framework implementations are hardware agnostic.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.400.1">Now we understand how inference works, let’s explore some common optimization techniques </span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.401.1">we can use to improve model latency.</span></p>
<h2 class="heading-2" id="_idParaDest-272"><span class="koboSpan" id="kobo.402.1">Hardware acceleration</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.403.1">Different </span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.404.1">hardware produces varying inference latency performance for different ML models. </span><span class="koboSpan" id="kobo.404.2">The list of </span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.405.1">common hardware</span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.406.1"> for model inference includes the CPU, GPU, </span><strong class="keyWord"><span class="koboSpan" id="kobo.407.1">application-specific integrated circuit</span></strong><span class="koboSpan" id="kobo.408.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.409.1">ASIC</span></strong><span class="koboSpan" id="kobo.410.1">), </span><strong class="keyWord"><span class="koboSpan" id="kobo.411.1">field-programmable gate array</span></strong><span class="koboSpan" id="kobo.412.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.413.1">FPGA</span></strong><span class="koboSpan" id="kobo.414.1">), and</span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.415.1"> edge hardware (such as Nvidia Jetson Nano). </span><span class="koboSpan" id="kobo.415.2">In this section, we will review the core architecture characteristics for some of these pieces of hardware and how their designs help with model inference acceleration. </span><span class="koboSpan" id="kobo.415.3">It is worth noting that while this section focuses on inference, some of the hardware is also suitable for training acceleration.</span></p>
<h3 class="heading-3" id="_idParaDest-273"><span class="koboSpan" id="kobo.416.1">Central processing units (CPUs)</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.417.1">A CPU is a </span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.418.1">general-purpose chip for running computer programs. </span><span class="koboSpan" id="kobo.418.2">It consists of four main building blocks:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.419.1">The control unit is the brain of the CPU that directs the operations of the CPU; that is, it instructs other components such as memory.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.420.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.421.1">arithmetic logic unit</span></strong><span class="koboSpan" id="kobo.422.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.423.1">ALU</span></strong><span class="koboSpan" id="kobo.424.1">) is the </span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.425.1">basic unit that performs arithmetic and logical operations, such as addition and subtraction, on the input data.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.426.1">The address generation unit is used for calculating an address to access memory.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.427.1">Memory management, which is used for all memory components such as the main memory and the local cache. </span><span class="koboSpan" id="kobo.427.2">A CPU can also be made up of multiple cores, with each core having a control unit and ALUs.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.428.1">The degree of parallel executions in a CPU mainly depends on how many cores it has. </span><span class="koboSpan" id="kobo.428.2">Each core normally runs a single thread at a time, except for hyper-threading (a proprietary simultaneous multi-threading implementation from Intel). </span><span class="koboSpan" id="kobo.428.3">The more cores it has, the higher the degree of parallel executions. </span><span class="koboSpan" id="kobo.428.4">A CPU is designed to handle a large set of instructions and manage the operations of many other components; it usually has high performance and a complex core, but there aren’t many of them. </span><span class="koboSpan" id="kobo.428.5">For example, the Intel Xeon processor can have up to 56 cores.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.429.1">CPUs are usually not suited for neural network-based model inference if low latency is the main requirement. </span><span class="koboSpan" id="kobo.429.2">Neural network inference mainly involves operations that can be parallelized at a large scale (for example, matrix multiplication). </span><span class="koboSpan" id="kobo.429.3">Since the total number of cores for a CPU is usually small, it cannot be parallelized at scale to meet the needs of neural network inference. </span><span class="koboSpan" id="kobo.429.4">On the positive side, CPUs are more cost-effective and usually have good memory capacities for hosting larger models.</span></p>
<h3 class="heading-3" id="_idParaDest-274"><span class="koboSpan" id="kobo.430.1">Graphics processing units (GPUs)</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.431.1">The </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.432.1">design of a GPU is the opposite of the design of a CPU. </span><span class="koboSpan" id="kobo.432.2">Instead of having a few powerful cores, it has thousands of less powerful cores that are designed to perform a small set of instructions highly efficiently. </span><span class="koboSpan" id="kobo.432.3">The basic design of a GPU core is like that of a CPU. </span><span class="koboSpan" id="kobo.432.4">It also contains a control unit, ALU, and a local memory cache. </span><span class="koboSpan" id="kobo.432.5">However, the GPU control unit handles a much simpler instruction set, and the local memory is much smaller.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.433.1">When the </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.434.1">GPU processes instructions, it schedules blocks of threads, and within each block of threads, all the threads perform the same operations but on different pieces of data – a parallelization scheme </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.435.1">called </span><strong class="keyWord"><span class="koboSpan" id="kobo.436.1">Single Instruction Multiple Data</span></strong><span class="koboSpan" id="kobo.437.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.438.1">SIMD</span></strong><span class="koboSpan" id="kobo.439.1">). </span><span class="koboSpan" id="kobo.439.2">This architecture fits nicely with how a DL model works, where many neurons perform the same operation (mainly matrix multiplication) on different pieces of data.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.440.1">The </span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.441.1">Nvidia GPU architecture contains two main components:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.442.1">The global memory component</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.443.1">The </span><strong class="keyWord"><span class="koboSpan" id="kobo.444.1">streaming multiprocessor</span></strong><span class="koboSpan" id="kobo.445.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.446.1">SM</span></strong><span class="koboSpan" id="kobo.447.1">) component</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.448.1">An SM is </span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.449.1">analogous</span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.450.1"> to a CPU and each SM has many </span><strong class="keyWord"><span class="koboSpan" id="kobo.451.1">Computer Unified Device Architecture</span></strong><span class="koboSpan" id="kobo.452.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.453.1">CUDA</span></strong><span class="koboSpan" id="kobo.454.1">) cores, special functional units that perform different arithmetic operations. </span><span class="koboSpan" id="kobo.454.2">It also has a small, shared memory and cache, and many registers. </span><span class="koboSpan" id="kobo.454.3">A CUDA core is responsible for functions such as floating-point/integer operations, logic calculation, and branching. </span><span class="koboSpan" id="kobo.454.4">The thread block mentioned previously is executed by the SM. </span><span class="koboSpan" id="kobo.454.5">The global memory is located on the same GPU board. </span><span class="koboSpan" id="kobo.454.6">When you’re training an ML model, both the model and the data need to be loaded into the global memory.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.455.1">In a multi-GPU configuration, low-latency and high-throughput communication channels are available, such as the Nvidia NVLink. </span><span class="koboSpan" id="kobo.455.2">GPUs are well suited for low-latency and high-throughput neural network model inferences due to their massive number of CUDA cores for large-scale parallelism.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.456.1">At the time of writing, the latest Nvidia GPU generation is the Blackwell B200 GPU. </span><span class="koboSpan" id="kobo.456.2">It is built with 208 billion transistors, and its NVLink switch system allows multi-GPU communication across multiple servers at 1.8TB/s. </span><span class="koboSpan" id="kobo.456.3">For large-scale distributed training, a cluster of 576 B200 GPUs can work together. </span><span class="koboSpan" id="kobo.456.4">Another noteworthy feature of the B200 is its 2</span><sup class="superscript"><span class="koboSpan" id="kobo.457.1">nd</span></sup><span class="koboSpan" id="kobo.458.1"> generation Transformer Engine designed to accelerate the training of transformers.</span></p>
<h3 class="heading-3" id="_idParaDest-275"><span class="koboSpan" id="kobo.459.1">Application-specific integrated circuit</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.460.1">An </span><strong class="keyWord"><span class="koboSpan" id="kobo.461.1">application-specific integrated circuit</span></strong><span class="koboSpan" id="kobo.462.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.463.1">ASIC</span></strong><span class="koboSpan" id="kobo.464.1">) is a </span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.465.1">primary alternative to a GPU. </span><span class="koboSpan" id="kobo.465.2">ASIC chips </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.466.1">are purpose-designed for particular DL architectures for computation and data flow, so are faster and require less power than GPUs. </span><span class="koboSpan" id="kobo.466.2">For</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.467.1"> example, Google’s </span><strong class="keyWord"><span class="koboSpan" id="kobo.468.1">Tensor Processing Unit</span></strong><span class="koboSpan" id="kobo.469.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.470.1">TPU</span></strong><span class="koboSpan" id="kobo.471.1">) has </span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.472.1">dedicated </span><strong class="keyWord"><span class="koboSpan" id="kobo.473.1">Matrix Units</span></strong><span class="koboSpan" id="kobo.474.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.475.1">MXUs</span></strong><span class="koboSpan" id="kobo.476.1">) designed for efficient matrix computations, and AWS offers the Inferentia chip, an ASIC designed for model inference. </span><span class="koboSpan" id="kobo.476.2">To speed up model inference, the Amazon Inferentia chip and Google’s TPU chip both use the systolic array mechanism to</span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.477.1"> speed up arithmetic calculations for deep neural networks. </span><span class="koboSpan" id="kobo.477.2">While general-purpose chips such as CPUs and GPUs use local registers between </span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.478.1">different ALU computations to transfer data and results, a systolic array allows you to chain multiple ALUs to reduce register access to speed up processing. </span><span class="koboSpan" id="kobo.478.2">The following diagram shows how data flows within a systolic array architecture versus a regular architecture that’s used in CPUs and GPUs:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.479.1"><img alt="Figure 10.14 – Systolic array processing versus CPU/GPU processing " src="../Images/B20836_10_14.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.480.1">Figure 10.14: Systolic array processing versus CPU/GPU processing</span></p>
<p class="normal"><span class="koboSpan" id="kobo.481.1">The </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.482.1">Amazon Inferentia chip can be used directly with Amazon SageMaker for inference with improved latency. </span><span class="koboSpan" id="kobo.482.2">You can do this by selecting one of the supported Inferentia chips for model deployment.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.483.1">While not directly applicable to inference, it is worth mentioning that AWS the also provides AWS Trainium accelerator, a </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.484.1">purpose-built chip for training large deep learning models. </span><span class="koboSpan" id="kobo.484.2">Each Trainium accelerator consists of 2 NeuronCore cores. </span><span class="koboSpan" id="kobo.484.3">Each </span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.485.1">core is</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.486.1"> an independent compute engine with 4 main</span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.487.1"> engines: the </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.488.1">TensorEngine, VectorEngine, ScalarEngine, and GPSIMD-Engine. </span><span class="koboSpan" id="kobo.488.2">It also has on-chip SRAM memory.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.489.1">Each engine of the</span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.490.1"> NeuronCore is optimized</span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.491.1"> for unique computations. </span><span class="koboSpan" id="kobo.491.2">The ScalarEngine is optimized for scalar computation and can be highly parallelized with support for various data types such as FP32, FP16, BF16, INT8, INT16, and Int32. </span><span class="koboSpan" id="kobo.491.3">It can perform 1,600 floating-point operations per cycle. </span><span class="koboSpan" id="kobo.491.4">The VectorEngine is optimized for vector computation. </span><span class="koboSpan" id="kobo.491.5">The VectorEngine is also highly parallelized and can perform 2,500 floating-point operations per cycle. </span><span class="koboSpan" id="kobo.491.6">The TensorEngine is based on a power-optimized systolic array, which </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.492.1">is highly optimized for tensor computations. </span><span class="koboSpan" id="kobo.492.2">Each TensorEngine can deliver over 100 TFLOPS of FP16/BF16 tensor computations. </span><span class="koboSpan" id="kobo.492.3">GPSIMD-Engine consists of 8 fully programmable 512-bit wide general-purpose processors, which can execute straight-line C-code, and have direct access to the other NeuronCore-v2 engines, as well as the SRAM memory.</span></p>
<h2 class="heading-2" id="_idParaDest-276"><span class="koboSpan" id="kobo.493.1">Model optimization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.494.1">When you’re </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.495.1">processing </span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.496.1">computational graphs for DL model inference, the size of the neural network (such as its number of layers, neurons, and so on), the number of model parameters, and the numerical precision of the model parameters directly impact the performance of model inference. </span><span class="koboSpan" id="kobo.496.2">The model optimization approach focuses on reducing the size of the neural network, the number of model parameters, and the numerical precisions to reduce inference latency. </span><span class="koboSpan" id="kobo.496.3">In general, there are two main approaches to model optimization: quantization and pruning.</span></p>
<h3 class="heading-3" id="_idParaDest-277"><span class="koboSpan" id="kobo.497.1">Quantization</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.498.1">Traditionally, deep</span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.499.1"> neural networks </span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.500.1">are </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.501.1">trained with </span><strong class="keyWord"><span class="koboSpan" id="kobo.502.1">floating-point 32 bit</span></strong><span class="koboSpan" id="kobo.503.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.504.1">FP32</span></strong><span class="koboSpan" id="kobo.505.1">). </span><span class="koboSpan" id="kobo.505.2">However, for many neural networks, FP32 is not needed for the required precision.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.506.1">Quantization for DL is a network compression approach that uses lower precision numbers, such</span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.507.1"> as </span><strong class="keyWord"><span class="koboSpan" id="kobo.508.1">floating-point 16 bit</span></strong><span class="koboSpan" id="kobo.509.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.510.1">FP16</span></strong><span class="koboSpan" id="kobo.511.1">) or </span><strong class="keyWord"><span class="koboSpan" id="kobo.512.1">integer 8 bit</span></strong><span class="koboSpan" id="kobo.513.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.514.1">INT8</span></strong><span class="koboSpan" id="kobo.515.1">) instead of FP32, to represent static model parameters and perform numerical computation</span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.516.1"> with dynamic data inputs/activation, all while having minimal or no impact on model performance. </span><span class="koboSpan" id="kobo.516.2">For </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.517.1">example, an INT8 representation takes up four times less space than the FP32 representation, which significantly reduces the memory requirements and computational costs for neural networks, which means it can improve the overall latency for model inference.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.518.1">There are </span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.519.1">different types of quantization algorithms, including uniform and non-uniform quantization algorithms. </span><span class="koboSpan" id="kobo.519.2">Both approaches map real values in a continuous domain to discrete</span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.520.1"> lower-precision </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.521.1">values in the quantized domain. </span><span class="koboSpan" id="kobo.521.2">In the uniform case, the quantized values in the quantized domains are evenly spaced, whereas the non-uniform case has varying quantized values. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.522.1">The following diagram shows the difference between a uniform and a non-uniform quantization:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.523.1"><img alt="Figure 10.15 – Uniform and non-uniform quantization " src="../Images/B20836_10_15.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.524.1">Figure 10.15: Uniform and non-uniform quantization</span></p>
<p class="normal"><span class="koboSpan" id="kobo.525.1">Quantization can be performed both post-training and during training (quantization-aware training). </span><span class="koboSpan" id="kobo.525.2">Post-training quantization takes a trained model, quantizes the weights, and regenerates a quantized model. </span><span class="koboSpan" id="kobo.525.3">Quantization-aware training involves fine-tuning a full precision model. </span><span class="koboSpan" id="kobo.525.4">During training, the higher-precision real numbers are reduced to lower-precision numbers.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.526.1">Other techniques</span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.527.1"> include mixed-precision training, a method that utilizes lower precision, such as 16-bit floating-point or 8-bit integers, for computations during the training of ML models, while retaining higher precision (32-bit) for weight updates. </span><span class="koboSpan" id="kobo.527.2">This approach aims to achieve notable speedups and memory savings, particularly on hardware optimized for low-precision operations. </span><span class="koboSpan" id="kobo.527.3">Another</span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.528.1"> technique is quantization with knowledge distillation, which involves transferring knowledge from a larger, high-precision teacher model to a smaller, quantized student model. </span><span class="koboSpan" id="kobo.528.2">The student model is trained to replicate the outputs of the teacher model, enabling it to acquire a more accurate quantized representation. </span><span class="koboSpan" id="kobo.528.3">While this technique can achieve higher accuracy compared to direct quantization, it necessitates an additional training process.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.529.1">Quantization support is natively available in DL frameworks such as PyTorch and TensorFlow. </span><span class="koboSpan" id="kobo.529.2">For example, PyTorch supports both forms of quantization via its </span><code class="inlineCode"><span class="koboSpan" id="kobo.530.1">torch.quantization</span></code><span class="koboSpan" id="kobo.531.1"> package. </span><span class="koboSpan" id="kobo.531.2">TensorFlow supports quantization through the </span><code class="inlineCode"><span class="koboSpan" id="kobo.532.1">tf.lite</span></code><span class="koboSpan" id="kobo.533.1"> package.</span></p>
<h3 class="heading-3" id="_idParaDest-278"><span class="koboSpan" id="kobo.534.1">Pruning (also known as sparsity)</span></h3>
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.535.1">Pruning</span></strong><span class="koboSpan" id="kobo.536.1"> is</span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.537.1"> another </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.538.1">network compression technique that eliminates some of the model weights and neurons that don’t impact model performance to reduce the size of the model to make inference faster. </span><span class="koboSpan" id="kobo.538.2">For example, weights that are close to zero or redundant can usually be removed.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.539.1">Pruning techniques can be classified into static and dynamic pruning. </span><span class="koboSpan" id="kobo.539.2">Static pruning takes place </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.540.1">offline before the model is deployed, while </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.541.1">dynamic pruning is performed during runtime. </span><span class="koboSpan" id="kobo.541.2">Here, we will discuss some of the key concepts and approaches for static pruning.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.542.1">Static pruning</span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.543.1"> mainly </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.544.1">consists of three steps:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.545.1">Parameter selection for pruning targeting.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.546.1">Pruning the neurons.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.547.1">Fine-tuning or retraining if needed. </span><span class="koboSpan" id="kobo.547.2">Retraining may improve the model performance of the pruned neural network.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.548.1">There are several approaches for</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.549.1"> selecting the parameters for static pruning, including the magnitude-based approach, the penalty-based approach, and dropout removal:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.550.1">Magnitude-based approach</span></strong><span class="koboSpan" id="kobo.551.1">: It is widely accepted that large model weights are</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.552.1"> more important than small model weights. </span><span class="koboSpan" id="kobo.552.2">So, one intuitive way to select weights for pruning is to look at zero-value weights or those weights within a defined absolute threshold. </span><span class="koboSpan" id="kobo.552.3">The magnitude of the neural network activation layer can also be used to determine whether the associated neurons can be removed.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.553.1">Penalty-based approach</span></strong><span class="koboSpan" id="kobo.554.1">: In</span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.555.1"> the penalty-based approach, the goal is to modify the loss function or add additional constraints so that some weights are forced to become zeros or near-zeros. </span><span class="koboSpan" id="kobo.555.2">The weights that are zeros or close to zeros can then be pruned. </span><span class="koboSpan" id="kobo.555.3">An example of the penalty-based approach is using LASSO to shrink the weights of features.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.556.1">Dropout removal</span></strong><span class="koboSpan" id="kobo.557.1">: Dropout </span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.558.1">layers are used in deep neural network training as regularizers to avoid overfitting data. </span><span class="koboSpan" id="kobo.558.2">While dropout layers are useful in training, they are not useful for inference and can be removed to reduce the number of parameters without impacting the model’s performance.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.559.1">Regularization-based pruning</span></strong><span class="koboSpan" id="kobo.560.1">: This</span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.561.1"> technique involves adding regularization terms to the loss function during training, which encourages the model to learn sparse representations. </span><span class="koboSpan" id="kobo.561.2">Examples include L1 regularization (LASSO), which promotes sparsity by driving some weights to exactly zero, and Group Lasso, which prunes entire filters or channels.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.562.1">Reinforcement-based pruning</span></strong><span class="koboSpan" id="kobo.563.1">: This </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.564.1">method adopts RL to compress models automatically.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.565.1">DL frameworks, such as TensorFlow and PyTorch, provide APIs for pruning models. </span><span class="koboSpan" id="kobo.565.2">For example, you can use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.566.1">tensorflow_model_optimization</span></code><span class="koboSpan" id="kobo.567.1"> package and its </span><code class="inlineCode"><span class="koboSpan" id="kobo.568.1">prune_low_magnitude</span></code><span class="koboSpan" id="kobo.569.1"> API for magnitude-based pruning. </span><span class="koboSpan" id="kobo.569.2">PyTorch provides model-pruning support via its </span><code class="inlineCode"><span class="koboSpan" id="kobo.570.1">torch.nn.utils.prune</span></code><span class="koboSpan" id="kobo.571.1"> API.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.572.1">The </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.573.1">primary trade-off with both quantization and pruning is the potential loss of model accuracy or performance in exchange for efficiency gains. </span><span class="koboSpan" id="kobo.573.2">More aggressive quantization or pruning can lead to higher compression rates but may also result in a larger drop in accuracy. </span><span class="koboSpan" id="kobo.573.3">Finding the right balance between compression and accuracy is crucial. </span><span class="koboSpan" id="kobo.573.4">Quantized or pruned models may also have limited portability across different hardware platforms or deep learning frameworks, as the optimized formats and sparse structures may not be universally supported.</span></p>
<h2 class="heading-2" id="_idParaDest-279"><span class="koboSpan" id="kobo.574.1">Graph and operator optimization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.575.1">In addition to hardware acceleration and model optimization, there are additional optimization techniques that focus on the execution optimization of the computational graph, as well as hardware-specific operator and tensor optimization.</span></p>
<h3 class="heading-3" id="_idParaDest-280"><span class="koboSpan" id="kobo.576.1">Graph optimization</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.577.1">Graph optimization</span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.578.1"> focuses</span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.579.1"> on reducing the number of operations that are performed in computational graphs to speed up inference. </span><span class="koboSpan" id="kobo.579.2">Multiple techniques are used for graph optimization, including operator fusion, dead code elimination, and constant folding.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.580.1">Operator fusion combines multiple operations in a subgraph into a single operation to improve latency. </span><span class="koboSpan" id="kobo.580.2">In a typical execution of a subgraph with multiple operations, system memory is accessed for read/write to transfer data between operations, which is an expensive task. </span><span class="koboSpan" id="kobo.580.3">Operator fusion reduces the number of memory accesses, as well as optimizing the overall computations since the computations are now happening in a single kernel without the intermediate results being saved to memory. </span><span class="koboSpan" id="kobo.580.4">This approach also reduces the memory footprint due to a smaller number of operations being performed. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.581.1">The following diagram shows the concept of operator fusion:</span></p>
<figure class="mediaobject"><span class="koboSpan" id="kobo.582.1"><img alt="Figure 10.16 – Graph operator fusion " src="../Images/B20836_10_16.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.583.1">Figure 10.16: Graph operator fusion</span></p>
<p class="normal"><span class="koboSpan" id="kobo.584.1">In the </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.585.1">preceding diagram, the </span><strong class="keyWord"><span class="koboSpan" id="kobo.586.1">matrix multiplication</span></strong><span class="koboSpan" id="kobo.587.1">, </span><strong class="keyWord"><span class="koboSpan" id="kobo.588.1">add</span></strong><span class="koboSpan" id="kobo.589.1">, and </span><strong class="keyWord"><span class="koboSpan" id="kobo.590.1">ReLU</span></strong><span class="koboSpan" id="kobo.591.1"> operators are being fused into a single operator for execution in a single kernel to reduce memory access and the time needed to start multiple kernels.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.592.1">Constant folding</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.593.1"> is the process of evaluating constants at compile time instead of runtime to speed up processing during runtime. </span><span class="koboSpan" id="kobo.593.2">For example, for the following expression, </span><em class="italic"><span class="koboSpan" id="kobo.594.1">A</span></em><span class="koboSpan" id="kobo.595.1"> can be assigned to a value of 300 at compile time instead of being dynamically calculated at runtime, which requires a more computational cycle: </span><em class="italic"><span class="koboSpan" id="kobo.596.1">A = 100 + 200</span></em><span class="koboSpan" id="kobo.597.1">. </span><span class="koboSpan" id="kobo.597.2">Dead code elimination removes the code that does not affect the program’s results. </span><span class="koboSpan" id="kobo.597.3">This ensures that the program doesn’t waste computation on useless operations.</span></p>
<h3 class="heading-3" id="_idParaDest-281"><span class="koboSpan" id="kobo.598.1">Operator optimization</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.599.1">Operator optimization (also known as tensor optimization) focuses on hardware-specific optimization </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.600.1">for a specific model. </span><span class="koboSpan" id="kobo.600.2">Different </span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.601.1">hardware devices have different memory layouts and computational units and, as such, hardware-specific optimization is often required to take full advantage of the hardware architecture. </span><span class="koboSpan" id="kobo.601.2">Multiple techniques have been developed for operator optimization on different hardware devices, including the following:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.602.1">Nested parallelism, which takes advantage of the GPU memory hierarchy and enables data reuse across threads through shared memory regions.</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.603.1">Memory latency hiding, which overlaps the memory operation with computation to maximize memory and compute resources.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.604.1">While graph optimization, operator optimization, and model optimization address different areas of optimization, they are often combined to provide end-to-end optimization.</span></p>
<h2 class="heading-2" id="_idParaDest-282"><span class="koboSpan" id="kobo.605.1">Model compilers</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.606.1">Manually </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.607.1">optimizing end-to-end </span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.608.1">model performance is non-trivial. </span><span class="koboSpan" id="kobo.608.2">Adding the dimensions of multiple ML frameworks and a wide range of target hardware devices for optimization makes this a very challenging problem. </span><span class="koboSpan" id="kobo.608.3">To simplify the optimization process for different ML frameworks and different devices, several open-source and commercial products have been developed. </span><span class="koboSpan" id="kobo.608.4">We will briefly talk about a few such packages in this section.</span></p>
<h3 class="heading-3" id="_idParaDest-283"><span class="koboSpan" id="kobo.609.1">TensorFlow XLA</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.610.1">TensorFlow </span><strong class="keyWord"><span class="koboSpan" id="kobo.611.1">Accelerated Linear Algebra</span></strong><span class="koboSpan" id="kobo.612.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.613.1">XLA</span></strong><span class="koboSpan" id="kobo.614.1">) is a </span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.615.1">DL compiler for </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.616.1">TensorFlow. </span><span class="koboSpan" id="kobo.616.2">It compiles a TensorFlow graph into a sequence of execution kernels specifically optimized for the model. </span><span class="koboSpan" id="kobo.616.3">XLA transforms the original TensorFlow graph</span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.617.1"> into an </span><strong class="keyWord"><span class="koboSpan" id="kobo.618.1">intermediate representation</span></strong><span class="koboSpan" id="kobo.619.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.620.1">IR</span></strong><span class="koboSpan" id="kobo.621.1">) before performing several optimizations on the IR, such as operator fusion for faster computation. </span><span class="koboSpan" id="kobo.621.2">The output from the optimization step is then used for generating hardware-specific code that optimizes the performance of the different target hardware devices, such as CPUs and GPUs. </span><span class="koboSpan" id="kobo.621.3">XLA is used at Google in production for many accelerators.</span></p>
<h3 class="heading-3" id="_idParaDest-284"><span class="koboSpan" id="kobo.622.1">PyTorch Glow</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.623.1">PyTorch Glow</span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.624.1"> is a </span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.625.1">DL compiler for multiple DL frameworks. </span><span class="koboSpan" id="kobo.625.2">Similar to XLA, it also uses an IR to represent the original computational graph to perform optimizations. </span><span class="koboSpan" id="kobo.625.3">Unlike XLA, PyTorch Glow uses two layers of IRs. </span><span class="koboSpan" id="kobo.625.4">The first layer is used for performing domain-specific optimizations such as quantization, while the second IR layer is used for memory-related optimization such as memory latency hiding. </span><span class="koboSpan" id="kobo.625.5">After the second layer of IR optimization, the target device-dependent code is generated for running the models on different devices.</span></p>
<h3 class="heading-3" id="_idParaDest-285"><span class="koboSpan" id="kobo.626.1">Apache TVM</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.627.1">Apache </span><strong class="keyWord"><span class="koboSpan" id="kobo.628.1">Tensor Virtual Machine</span></strong><span class="koboSpan" id="kobo.629.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.630.1">TVM</span></strong><span class="koboSpan" id="kobo.631.1">) is an </span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.632.1">open-source compiler framework for model optimization. </span><span class="koboSpan" id="kobo.632.2">It </span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.633.1">optimizes and compiles models built with different frameworks, such as PyTorch and TensorFlow, for different target CPUs, GPUs, and specialized hardware devices for accelerated performance. </span><span class="koboSpan" id="kobo.633.2">TVM supports optimization at different levels, including graph optimization and operator optimization targeting specific hardware. </span><span class="koboSpan" id="kobo.633.3">It also comes with a runtime for efficiently executing the compiled models.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.634.1">One key feature of TVM is AutoTVM, which uses ML to search for the optimal sequences of code execution for different hardware devices. </span><span class="koboSpan" id="kobo.634.2">This ML-based search algorithm can significantly outperform baseline benchmarks by using vendor-provided optimization libraries such as cuDNN. </span><span class="koboSpan" id="kobo.634.3">This ML-based approach also enables efficient compilation scaling for a large number of hardware devices.</span></p>
<h3 class="heading-3" id="_idParaDest-286"><span class="koboSpan" id="kobo.635.1">Amazon SageMaker Neo</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.636.1">Amazon SageMaker Neo </span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.637.1">is the model-compiling feature</span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.638.1"> in SageMaker. </span><span class="koboSpan" id="kobo.638.2">It mainly uses Apache TVM as its underlying compiler library. </span><span class="koboSpan" id="kobo.638.3">With SageMaker Neo, you take a model that’s been trained on different ML/DL frameworks such as TensorFlow and PyTorch, choose the target processors such as Intel, Apple, ARM, or Nvidia, and then SageMaker Neo compiles an optimized model for the target hardware. </span><span class="koboSpan" id="kobo.638.4">Neo also provides a runtime library for each target platform to load and execute the compiled model. </span><span class="koboSpan" id="kobo.638.5">SageMaker Neo is a managed offering, so you don’t need to manage the underlying infrastructure and processes for model compilation and deployment.</span></p>
<h2 class="heading-2" id="_idParaDest-287"><span class="koboSpan" id="kobo.639.1">Inference engine optimization</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.640.1">One </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.641.1">common model deployment pattern is to use open-source inference engines or commercial </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.642.1">hosting platforms for model serving. </span><span class="koboSpan" id="kobo.642.2">So, inference engine optimization is another approach that helps reduce model latency and inference throughput. </span><span class="koboSpan" id="kobo.642.3">In this section, we will talk about a few considerations. </span><span class="koboSpan" id="kobo.642.4">Note that there are no universal rules for inference engine optimization as it is sometimes engine- and model-specific. </span><span class="koboSpan" id="kobo.642.5">It is important to test and validate different configurations for the final deployment.</span></p>
<h3 class="heading-3" id="_idParaDest-288"><span class="koboSpan" id="kobo.643.1">Inference batching</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.644.1">If you have a</span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.645.1"> large number of inference requests and there is no strict latency requirement on a single prediction request, then inference batching is a technique that can help reduce the total inference time for the requests. </span><span class="koboSpan" id="kobo.645.2">With inference batching, instead of running predictions one at a time for each request, multiple requests are batched together and sent to the inference engine. </span><span class="koboSpan" id="kobo.645.3">This technique reduces the total number of requests round-trips, thus reducing the total inference time. </span><span class="koboSpan" id="kobo.645.4">Inference engines such as TensorFlow Serving and TorchServe provide built-in support for batch inference. </span><span class="koboSpan" id="kobo.645.5">You can find the configuration details for TorchServe and TensorFlow Serving batch inference at </span><a href="https://pytorch.org/serve/batch_inference_with_ts.html"><span class="url"><span class="koboSpan" id="kobo.646.1">https://pytorch.org/serve/batch_inference_with_ts.html</span></span></a><span class="koboSpan" id="kobo.647.1"> and </span><a href="https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration"><span class="url"><span class="koboSpan" id="kobo.648.1">https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration</span></span></a><span class="koboSpan" id="kobo.649.1">, respectively.</span></p>
<h3 class="heading-3" id="_idParaDest-289"><span class="koboSpan" id="kobo.650.1">Enabling parallel serving sessions</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.651.1">If your </span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.652.1">model hosting server has multiple compute cores, you can configure the number of parallel serving sessions to maximize the utilization of the available cores. </span><span class="koboSpan" id="kobo.652.2">For example, you can configure the </span><code class="inlineCode"><span class="koboSpan" id="kobo.653.1">TENSORFLOW_INTRA_OP_PARALLELISM</span></code><span class="koboSpan" id="kobo.654.1"> setting in TensorFlow Serving based on the number of cores that can run multiple serving sessions in parallel to optimize throughput. </span><span class="koboSpan" id="kobo.654.2">TorchServe has settings for the number of workers per model and the number of threads for parallelization optimization.</span></p>
<h3 class="heading-3" id="_idParaDest-290"><span class="koboSpan" id="kobo.655.1">Picking a communication protocol</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.656.1">Inference engines</span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.657.1"> such as TensorFlow and TorchServe provide support for the gRPC protocol, which is a faster serialization format than the REST protocol. </span><span class="koboSpan" id="kobo.657.2">The gPRC protocol provides better overall performance but does have performance benchmarks as different models could behave differently. </span><span class="koboSpan" id="kobo.657.3">The REST protocol may be your preferred option depending on your specific requirements.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.658.1">With that, you have learned about the technical approaches to large-scale training and low-latency model inference.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.659.1">Now that we have discussed some of the considerations for inference optimization, we will next explore some of the more recent advancements in inference technology for large language models.</span></p>
<h2 class="heading-2" id="_idParaDest-291"><span class="koboSpan" id="kobo.660.1">Inference in large language models</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.661.1">As </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.662.1">language </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.663.1">models continue to grow in size, the task of deploying and running them becomes increasingly challenging. </span><span class="koboSpan" id="kobo.663.2">Even with model optimization efforts, these expansive models often exceed the memory capacity of a single GPU. </span><span class="koboSpan" id="kobo.663.3">To address this hurdle and enable the deployment of these large language models, numerous ML inference frameworks have emerged. </span><span class="koboSpan" id="kobo.663.4">Let’s delve into a few of these frameworks to gain insight into how they facilitate the inference process for these language models.</span></p>
<h3 class="heading-3" id="_idParaDest-292"><span class="koboSpan" id="kobo.664.1">Text Generation Inference (TGI)</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.665.1">TGI is an </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.666.1">optimized </span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.667.1">serving solution by HuggingFace for deploying open-source large language models like Falcon and FLAN-T5. </span><span class="koboSpan" id="kobo.667.2">TGI has the following key</span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.668.1"> capabilities for large language model inference:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.669.1">Tensor parallelism</span></strong><span class="koboSpan" id="kobo.670.1">: This feature allows a large language model to be deployed across multiple GPUs so it can fit into the combined GPU memory as well as faster inference across multiple GPUs. </span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.671.1">Quantization</span></strong><span class="koboSpan" id="kobo.672.1">: TGI can perform model quantization with the bitsandbytes and GPT-Q quantization library packages to reduce the model size.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.673.1">Continuous batching</span></strong><span class="koboSpan" id="kobo.674.1">: This feature increases the throughput of the inference by running </span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.675.1">multiple input sequences by using the same loaded model parameters.</span></li>
</ul>
<h3 class="heading-3" id="_idParaDest-293"><span class="koboSpan" id="kobo.676.1">DeepSpeed-Inference</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.677.1">In addition to</span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.678.1"> being a framework for large-scale distributed training, DeepSpeed can also help optimize the inference of large language models. </span><span class="koboSpan" id="kobo.678.2">It has</span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.679.1"> the following key features for inference:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.680.1">Model parallelism</span></strong><span class="koboSpan" id="kobo.681.1">: This feature fits a large model that would otherwise not fit into GPU memory by splitting a model across multiple GPU devices.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.682.1">Inference-optimized kernel</span></strong><span class="koboSpan" id="kobo.683.1">: DeepSpeed can fuse element-wise operations, matrix multiplications, transpositions, and reductions all into a single kernel, significantly reducing the number of kernel invocations as well as main memory access to reduce main memory access latency. </span><span class="koboSpan" id="kobo.683.2">DeepSpeed-Inference kernels are also fine-tuned to maximize the memory bandwidth utilization for loading the parameters.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.684.1">Quantization</span></strong><span class="koboSpan" id="kobo.685.1">: A novel approach to quantizing models, called Mixture of Quantization, involves both shrinking the model and reducing the inference cost during production.</span></li>
</ul>
<h3 class="heading-3" id="_idParaDest-294"><span class="koboSpan" id="kobo.686.1">FastTransformer</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.687.1">FasterTransformer </span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.688.1">is a high-performance</span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.689.1"> library developed by Nvidia, designed to accelerate the inference of transformer-based neural networks, particularly for large models that span multiple GPUs and nodes in a distributed setup. </span><span class="koboSpan" id="kobo.689.2">This open-source framework is focused on optimizing transformer blocks, encompassing both encoder and decoder components. </span><span class="koboSpan" id="kobo.689.3">It has the </span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.690.1">following key features supporting inference of transformer-based models:</span></p>
<ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.691.1">Model parallelism</span></strong><span class="koboSpan" id="kobo.692.1">: With FastTransformer, a transformer model can be split across multiple GPUs for faster inference.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.693.1">Optimization support:</span></strong><span class="koboSpan" id="kobo.694.1"> FasterTransformer optimizes transformer-based models with techniques</span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.695.1"> like layer fusion, multi-head attention acceleration using data caching, </span><strong class="keyWord"><span class="koboSpan" id="kobo.696.1">General Matrix Multiply</span></strong><span class="koboSpan" id="kobo.697.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.698.1">GEMM</span></strong><span class="koboSpan" id="kobo.699.1">) kernel autotuning for efficient matrix multiplication, and support for lower-precision data types like FP16, BF16, and INT8. </span><span class="koboSpan" id="kobo.699.2">Operations on these data types can be accelerated by Tensor Cores on the recent NVIDIA GPUs.</span></li>
</ul>
<h1 class="heading-1" id="_idParaDest-295"><span class="koboSpan" id="kobo.700.1">Hands-on lab – running distributed model training with PyTorch</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.701.1">As an ML solutions architect, you need to explore and design different model-training paradigms to meet different model-training requirements. </span><span class="koboSpan" id="kobo.701.2">In this hands-on lab, you will use the SageMaker Training service to run data-parallel distributed training. </span><span class="koboSpan" id="kobo.701.3">We will use PyTorch’s </span><code class="inlineCode"><span class="koboSpan" id="kobo.702.1">torch.nn.parallel.DistributedDataParallel</span></code><span class="koboSpan" id="kobo.703.1"> API as the distributed training framework and run the training job on a small cluster. </span><span class="koboSpan" id="kobo.703.2">We will reuse the dataset and training scripts from the hands-on lab in </span><em class="chapterRef"><span class="koboSpan" id="kobo.704.1">Chapter 8</span></em><span class="koboSpan" id="kobo.705.1">, </span><em class="italic"><span class="koboSpan" id="kobo.706.1">Building a Data Science Environment Using AWS ML Services</span></em><span class="koboSpan" id="kobo.707.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-296"><span class="koboSpan" id="kobo.708.1">Problem statement</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.709.1">In </span><em class="italic"><span class="koboSpan" id="kobo.710.1">Chapter 8</span></em><span class="koboSpan" id="kobo.711.1">, we trained a financial sentiment model using the data science environment you created using SageMaker. </span><span class="koboSpan" id="kobo.711.2">The model was trained using a single GPU in the Studio Notebook and SageMaker training service. </span><span class="koboSpan" id="kobo.711.3">Anticipating the future needs of model training with large datasets, we need to design an ML training process using multiple GPUs to scale out training horizontally.</span></p>
<h2 class="heading-2" id="_idParaDest-297"><span class="koboSpan" id="kobo.712.1">Dataset description</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.713.1">We will use the financial phrase dataset for this lab: </span><a href="https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news"><span class="url"><span class="koboSpan" id="kobo.714.1">https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news</span></span></a><span class="koboSpan" id="kobo.715.1">.</span></p>
<h2 class="heading-2" id="_idParaDest-298"><span class="koboSpan" id="kobo.716.1">Modifying the training script</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.717.1">First, we </span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.718.1">need to add distributed training support to the training script. </span><span class="koboSpan" id="kobo.718.2">To start, create a </span><code class="inlineCode"><span class="koboSpan" id="kobo.719.1">code</span></code><span class="koboSpan" id="kobo.720.1"> directory in your Studio notebook environment, create a copy of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.721.1">train.py</span></code><span class="koboSpan" id="kobo.722.1"> file and save it to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.723.1">code</span></code><span class="koboSpan" id="kobo.724.1"> directory, and then rename the file </span><code class="inlineCode"><span class="koboSpan" id="kobo.725.1">train-dis.py</span></code><span class="koboSpan" id="kobo.726.1">, and open the </span><code class="inlineCode"><span class="koboSpan" id="kobo.727.1">train-dis.py</span></code><span class="koboSpan" id="kobo.728.1"> file. </span><span class="koboSpan" id="kobo.728.2">You will need to make changes to the following three main functions. </span><span class="koboSpan" id="kobo.728.3">The following steps are meant to highlight the key changes needed. </span><span class="koboSpan" id="kobo.728.4">To run the lab, you can download the modified </span><code class="inlineCode"><span class="koboSpan" id="kobo.729.1">train-dis.py</span></code><span class="koboSpan" id="kobo.730.1"> file from </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10"><span class="url"><span class="koboSpan" id="kobo.731.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10</span></span></a><span class="koboSpan" id="kobo.732.1">. </span><span class="koboSpan" id="kobo.732.2">The following are </span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.733.1">the key changes to be made in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.734.1">train-dis.py</span></code><span class="koboSpan" id="kobo.735.1"> file:</span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><strong class="keyWord"><span class="koboSpan" id="kobo.736.1">Modifying the </span></strong><code class="inlineCode"><span class="koboSpan" id="kobo.737.1">train()</span></code><strong class="keyWord"><span class="koboSpan" id="kobo.738.1"> function</span></strong><span class="koboSpan" id="kobo.739.1">: You need to make some changes to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.740.1">train()</span></code><span class="koboSpan" id="kobo.741.1"> function to enable distributed training. </span><span class="koboSpan" id="kobo.741.2">The following are the key changes that are required:</span><ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.742.1">Process group initialization</span></strong><span class="koboSpan" id="kobo.743.1">: To enable distributed training, we need to initialize and register each training process on each device to be included in the training group. </span><span class="koboSpan" id="kobo.743.2">This can be achieved by calling the </span><code class="inlineCode"><span class="koboSpan" id="kobo.744.1">torch.distributed.init_process_group()</span></code><span class="koboSpan" id="kobo.745.1"> function. </span><span class="koboSpan" id="kobo.745.2">This function will block until all the processes have been registered. </span><span class="koboSpan" id="kobo.745.3">There are a few concepts that we need to be familiar with during this initialization step:</span><ul>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.746.1">World size</span></strong><span class="koboSpan" id="kobo.747.1">: This is the total number of processes in a distributed training group. </span><span class="koboSpan" id="kobo.747.2">Since we will run one process on each device (CPU or GPU), the world size is also the same as the total number of devices in a training cluster. </span><span class="koboSpan" id="kobo.747.3">For example, if you have two servers and each server has two GPUs, then the world size is four for this training group. </span><span class="koboSpan" id="kobo.747.4">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.748.1">torch.distributed.init_process_group()</span></code><span class="koboSpan" id="kobo.749.1"> function uses this information to understand how many processes to include in the distributed training job.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.750.1">Rank</span></strong><span class="koboSpan" id="kobo.751.1">: This is the unique index that’s assigned to each process in the training group. </span><span class="koboSpan" id="kobo.751.2">For example, the ranks for all the processes in a training group with a world size of four would be [0,1,2,3]. </span><span class="koboSpan" id="kobo.751.3">This unique index helps uniquely identify each process within a training group for communication.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.752.1">Local rank</span></strong><span class="koboSpan" id="kobo.753.1">: This uniquely identifies a device in a server node. </span><span class="koboSpan" id="kobo.753.2">For example, if there are two devices in a server node, the local rank for two devices would be [0,1]. </span><span class="koboSpan" id="kobo.753.3">Local rank allows you to select a specific device to load the model and data for model training.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.754.1">Backend</span></strong><span class="koboSpan" id="kobo.755.1">: This is the low-level communication library for exchanging and aggregating data among the different processes. </span><span class="koboSpan" id="kobo.755.2">PyTorch distributed training supports several communication backends, including NCCL, MPI, and Gloo. </span><span class="koboSpan" id="kobo.755.3">You choose a different backend based on the device and networking configuration. </span><span class="koboSpan" id="kobo.755.4">It uses these backends to send, receive, broadcast, or reduce data during distributed training. </span><span class="koboSpan" id="kobo.755.5">We are not going to get into the technical details of these backends in this book. </span><span class="koboSpan" id="kobo.755.6">If you are interested in how these backends work, you can easily find internet sources that cover these topics.</span></li>
</ul>
</li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.756.1">Wrap the training algorithm with PyTorch distributed library</span></strong><span class="koboSpan" id="kobo.757.1">: To use the</span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.758.1"> PyTorch distributed library support for training, you need to wrap the algorithm with the PyTorch distributed training library. </span><span class="koboSpan" id="kobo.758.2">You can achieve this with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.759.1">torch.nn.parallel.DistributedDataParallel()</span></code><span class="koboSpan" id="kobo.760.1"> API. </span><span class="koboSpan" id="kobo.760.2">This allows the algorithm to participate in distributed training to exchange gradients and update global parameters.</span></li>
<li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.761.1">Saving model using a single device</span></strong><span class="koboSpan" id="kobo.762.1">: In a multi-device server node, you only want one device to save the final model to avoid I/O conflicts. </span><span class="koboSpan" id="kobo.762.2">You can achieve this by selecting a device with a specific local rank ID.</span></li>
</ul>
</li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.763.1">Modifying the </span></strong><code class="inlineCode"><span class="koboSpan" id="kobo.764.1">get_data_loader()</span></code><strong class="keyWord"><span class="koboSpan" id="kobo.765.1"> function</span></strong><span class="koboSpan" id="kobo.766.1">: To ensure a different subset of training data is loaded onto different devices on the server nodes, we need to configure the PyTorch DataLoader API to load data based on the rank of the training process. </span><span class="koboSpan" id="kobo.766.2">This can be done using the </span><code class="inlineCode"><span class="koboSpan" id="kobo.767.1">torch.utils.data.distributed.DistributedSampler</span></code><span class="koboSpan" id="kobo.768.1"> API.</span></li>
<li class="numberedList"><strong class="keyWord"><span class="koboSpan" id="kobo.769.1">Adding multi-processing launch support for multi-device server nodes</span></strong><span class="koboSpan" id="kobo.770.1">: For server nodes with multiple devices, we need to spawn several parallel processes based on the number of devices available. </span><span class="koboSpan" id="kobo.770.2">To enable this, we can use </span><code class="inlineCode"><span class="koboSpan" id="kobo.771.1">torch.multiprocessing</span></code><span class="koboSpan" id="kobo.772.1"> to kick off multiple running processes on each node.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.773.1">With the training script modified, we will update the laucher notebook next.</span></p>
<h2 class="heading-2" id="_idParaDest-299"><span class="koboSpan" id="kobo.774.1">Modifying and running the launcher notebook</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.775.1">We are </span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.776.1">now ready to </span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.777.1">modify the launcher notebook to kick off the model training job: </span></p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.778.1">To start, copy the </span><code class="inlineCode"><span class="koboSpan" id="kobo.779.1">bert-financial-sentiment-Launcher.ipynb</span></code><span class="koboSpan" id="kobo.780.1"> file from </span><code class="inlineCode"><span class="koboSpan" id="kobo.781.1">chapter 8</span></code><span class="koboSpan" id="kobo.782.1"> and save it as </span><code class="inlineCode"><span class="koboSpan" id="kobo.783.1">bert-financial-sentiment-dis-Launcher.ipynb</span></code><span class="koboSpan" id="kobo.784.1">. </span><span class="koboSpan" id="kobo.784.2">Open the new notebook and replace the second cell’s content with the following code:
        </span><pre class="programlisting code"><code class="hljs-code"><span class="hljs-key ord"><span class="koboSpan" id="kobo.785.1">from</span></span><span class="koboSpan" id="kobo.786.1"> sagemaker.pytorch </span><span class="hljs-key ord"><span class="koboSpan" id="kobo.787.1">import</span></span><span class="koboSpan" id="kobo.788.1"> PyTorch 
 
output_path = </span><span class="hljs-string"><span class="koboSpan" id="kobo.789.1">f"s3://</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.790.1">{bucket}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.791.1">/</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.792.1">{prefix}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.793.1">"</span></span><span class="koboSpan" id="kobo.794.1"> 
 
estimator = PyTorch(
    entry_point=</span><span class="hljs-string"><span class="koboSpan" id="kobo.795.1">"train-dis.py"</span></span><span class="koboSpan" id="kobo.796.1">,
    source_dir=</span><span class="hljs-string"><span class="koboSpan" id="kobo.797.1">"code"</span></span><span class="koboSpan" id="kobo.798.1">,
    role=role,
    framework_version=</span><span class="hljs-string"><span class="koboSpan" id="kobo.799.1">"1.6"</span></span><span class="koboSpan" id="kobo.800.1">,
    py_version=</span><span class="hljs-string"><span class="koboSpan" id="kobo.801.1">"py3"</span></span><span class="koboSpan" id="kobo.802.1">,
    instance_count=</span><span class="hljs-number"><span class="koboSpan" id="kobo.803.1">2</span></span><span class="koboSpan" id="kobo.804.1">,  
    instance_type= </span><span class="hljs-string"><span class="koboSpan" id="kobo.805.1">"ml.g4dn.12xlarge"</span></span><span class="koboSpan" id="kobo.806.1">, 
    output_path=output_path,
    hyperparameters={
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.807.1">"epochs"</span></span><span class="koboSpan" id="kobo.808.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.809.1">10</span></span><span class="koboSpan" id="kobo.810.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.811.1">"lr"</span></span><span class="koboSpan" id="kobo.812.1"> : </span><span class="hljs-number"><span class="koboSpan" id="kobo.813.1">5e-5</span></span><span class="koboSpan" id="kobo.814.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.815.1">"num_labels"</span></span><span class="koboSpan" id="kobo.816.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.817.1">3</span></span><span class="koboSpan" id="kobo.818.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.819.1">"train_file"</span></span><span class="koboSpan" id="kobo.820.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.821.1">"train.csv"</span></span><span class="koboSpan" id="kobo.822.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.823.1">"test_file"</span></span><span class="koboSpan" id="kobo.824.1"> : </span><span class="hljs-string"><span class="koboSpan" id="kobo.825.1">"test.csv"</span></span><span class="koboSpan" id="kobo.826.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.827.1">"MAX_LEN"</span></span><span class="koboSpan" id="kobo.828.1"> : </span><span class="hljs-number"><span class="koboSpan" id="kobo.829.1">315</span></span><span class="koboSpan" id="kobo.830.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.831.1">"batch_size"</span></span><span class="koboSpan" id="kobo.832.1"> : </span><span class="hljs-number"><span class="koboSpan" id="kobo.833.1">64</span></span><span class="koboSpan" id="kobo.834.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.835.1">"test_batch_size"</span></span><span class="koboSpan" id="kobo.836.1"> : </span><span class="hljs-number"><span class="koboSpan" id="kobo.837.1">10</span></span><span class="koboSpan" id="kobo.838.1">,
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.839.1">"backend"</span></span><span class="koboSpan" id="kobo.840.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.841.1">"nccl"</span></span><span class="koboSpan" id="kobo.842.1">
    },
    
)
estimator.fit({</span><span class="hljs-string"><span class="koboSpan" id="kobo.843.1">"training"</span></span><span class="koboSpan" id="kobo.844.1">: inputs_train, </span><span class="hljs-string"><span class="koboSpan" id="kobo.845.1">"testing"</span></span><span class="koboSpan" id="kobo.846.1">: inputs_test})
</span></code></pre>
<p class="normal"><span class="koboSpan" id="kobo.847.1">The main code changes are as follows:</span></p>
<ul>
<li class="bulletList"><span class="koboSpan" id="kobo.848.1">Point the entry point to the new training script (</span><code class="inlineCode"><span class="koboSpan" id="kobo.849.1">entry_point="train-dis.py"</span></code><span class="koboSpan" id="kobo.850.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.851.1">Increase the number of compute instance from 1 to 2 for multi-node training (</span><code class="inlineCode"><span class="koboSpan" id="kobo.852.1">instance_count=2</span></code><span class="koboSpan" id="kobo.853.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.854.1">Change the instance type for multi-GPU support (</span><code class="inlineCode"><span class="koboSpan" id="kobo.855.1">instance_type= "ml.g4dn.12xlarge"</span></code><span class="koboSpan" id="kobo.856.1">)</span></li>
<li class="bulletList"><span class="koboSpan" id="kobo.857.1">Increase the batch size because there are now more GPUs to handle a larger batch size (</span><code class="inlineCode"><span class="koboSpan" id="kobo.858.1">"batch_size" : 64</span></code><span class="koboSpan" id="kobo.859.1">)</span></li>
</ul> </li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="2"><span class="koboSpan" id="kobo.860.1">Download </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt"><span class="url"><span class="koboSpan" id="kobo.861.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt</span></span></a><span class="koboSpan" id="kobo.862.1"> and upload it to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.863.1">code</span></code><span class="koboSpan" id="kobo.864.1"> directory in your Studio Notebook. </span><span class="koboSpan" id="kobo.864.2">This </span><code class="inlineCode"><span class="koboSpan" id="kobo.865.1">requirements.txt</span></code><span class="koboSpan" id="kobo.866.1"> contains the library packages to be installed in the trainer container. </span><span class="koboSpan" id="kobo.866.2">In this case, we want to install the transformer package.</span></li>
</ol>
<p class="normal"><span class="koboSpan" id="kobo.867.1">If you </span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.868.1">don’t want to manually revise the launcher notebook, you can download the revised launcher notebook at </span><a href="https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb"><span class="url"><span class="koboSpan" id="kobo.869.1">https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb</span></span></a><span class="koboSpan" id="kobo.870.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.871.1">Now, just </span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.872.1">execute each cell in the new notebook to kick off the distributed training. </span><span class="koboSpan" id="kobo.872.2">You can track the training status directly inside the notebook, and the detail status in CloudWatch Logs. </span><span class="koboSpan" id="kobo.872.3">You should see a total of eight processes running in parallel. </span><span class="koboSpan" id="kobo.872.4">Take note of the total training time and accuracy and see how they compare with the results you got from </span><em class="chapterRef"><span class="koboSpan" id="kobo.873.1">Chapter 8</span></em><span class="koboSpan" id="kobo.874.1">, </span><em class="italic"><span class="koboSpan" id="kobo.875.1">Building a Data Science Environment Using AWS ML Services</span></em><span class="koboSpan" id="kobo.876.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.877.1">Congratulations! </span><span class="koboSpan" id="kobo.877.2">You have successfully trained a BERT model using the PyTorch distributed training library.</span></p>
<h1 class="heading-1" id="_idParaDest-300"><span class="koboSpan" id="kobo.878.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.879.1">In this chapter, we delved into the advanced topics of ML engineering. </span><span class="koboSpan" id="kobo.879.2">We covered distributed training for handling extensive datasets and large-scale models, along with strategies for achieving low-latency inference. </span><span class="koboSpan" id="kobo.879.3">Hopefully, you now have a solid understanding of data parallelism and model parallelism, as well as the diverse technology choices available, such as the PyTorch distributed library and SageMaker distributed training library, for implementing distributed training using these approaches. </span><span class="koboSpan" id="kobo.879.4">Additionally, you should be well equipped to discuss various techniques for optimizing models to minimize inference latency, including the utilization of model compiler tools designed for automated model optimization. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.880.1">So far, we have focused on training ML models from scratch and designing ML platforms for the training and deployment of ML models to support the development of intelligent applications. </span><span class="koboSpan" id="kobo.880.2">However, we don’t always need to build models from scratch. </span><span class="koboSpan" id="kobo.880.3">In the next chapter, we will explore ready-to-use AI services and see how AI services can be used to build intelligent applications quickly.</span></p>
<h1 class="heading-1"><span class="koboSpan" id="kobo.881.1">Leave a review!</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.882.1">Enjoying this book? </span><span class="koboSpan" id="kobo.882.2">Help readers like you by leaving an Amazon review. </span><span class="koboSpan" id="kobo.882.3">Scan the QR code below to get a free eBook of your choice.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.883.1"><img alt="" role="presentation" src="../Images/Review_Copy.png"/></span></p>
<p class="normal"><em class="italic"><span class="koboSpan" id="kobo.884.1">*Limited Offer</span></em></p>
</div>
</body></html>