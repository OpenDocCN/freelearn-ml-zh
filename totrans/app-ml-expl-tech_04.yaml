- en: '*Chapter 3*: Data-Centric Approaches'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*：数据中心化方法'
- en: In the *Defining explanation methods and approaches* section of [*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014),
    *Foundational Concepts of Explainability Techniques*, when we looked at the various
    dimensions of explainability, we discussed how data is one of the important dimensions.
    In fact, all **machine learning** (**ML**) algorithms depend on the underlying
    data being used.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B18216_01_ePub.xhtml#_idTextAnchor014)“可解释性技术基础概念”的*定义解释方法和途径*部分，当我们查看可解释性的各个维度时，我们讨论了数据是其中一个重要维度。实际上，所有**机器学习**（**ML**）算法都依赖于所使用的基础数据。
- en: In the previous chapter, we discussed various *model explainability methods*.
    Most of the methods discussed in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, are model-centric. The concepts and ideas discussed
    were focused on making black-box models interpretable. But recently, the ML and
    AI communities have realized the core importance of data for any analysis or modeling
    purposes. So, more and more AI researchers are exploring new ideas and concepts
    around **data-centric AI**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了各种**模型可解释性方法**。在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中讨论的大多数方法都是模型中心的。讨论的概念和思想集中在使黑盒模型可解释。但最近，机器学习和人工智能社区已经意识到数据对于任何分析或建模目的的核心重要性。因此，越来越多的AI研究人员正在探索围绕**数据中心化AI**的新想法和概念。
- en: Since data plays a vital role in the model-building and prediction process,
    it is even more important for us to explain the functioning of any ML and AI algorithm
    with respect to the underlying data. From what I have observed from my experience
    in this field, the failure of any ML systems in production happens neither due
    to the poor choice of ML algorithm nor due to an inefficient model training or
    tuning process, but rather it occurs mostly due to inconsistencies in the underlying
    dataset. So, this chapter focuses on the concepts of **data-centric explainable
    AI** (**XAI**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据在模型构建和预测过程中发挥着至关重要的作用，因此，我们更有必要从基础数据的角度解释任何机器学习和人工智能算法的工作原理。从我在这领域的经验观察来看，任何机器学习系统在生产中的失败并不是由于选择了糟糕的机器学习算法，也不是由于模型训练或调整过程效率低下，而是大多数情况下是由于基础数据集的不一致性。因此，本章专注于**数据中心化可解释人工智能**（**XAI**）的概念。
- en: 'The goal of this chapter is to introduce you to the concepts of data-centric
    XAI. After reading this chapter, you will get to know about the various methods
    that can be performed to check the quality of the data, which might influence
    the model outcome. For production-level ML systems, the inference data can have
    issues related to its consistency and quality. So, monitoring these drifts is
    extremely important. Additionally, there can be external noise or perturbations
    affecting the data that can impact the model. So, these are some data-centric
    approaches that we will be discussing that are used for explaining ML models.
    In this chapter, the following main topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是向您介绍数据中心化XAI的概念。阅读本章后，您将了解可以执行的各种方法，以检查可能影响模型结果的数据质量。对于生产级别的机器学习系统，推理数据可能存在与其一致性和质量相关的问题。因此，监控这些漂移至关重要。此外，可能存在影响数据的外部噪声或扰动，这可能会影响模型。因此，我们将讨论一些用于解释机器学习模型的数据中心化方法。在本章中，以下主要主题将被涵盖：
- en: Introduction to data-centric XAI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心化XAI简介
- en: Thorough data analysis and profiling processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 详尽的数据分析和配置文件过程
- en: Monitoring and anticipating drifts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控和预测漂移
- en: Checking adversarial robustness
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查对抗鲁棒性
- en: Measuring data forecastability
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量数据可预测性
- en: Now, let's dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨！
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Similar to [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, for this chapter, certain tutorial examples to implement some of the
    techniques to perform data-centric XAI in Python on certain interesting datasets
    have been provided. We will be using Python Jupyter notebooks to run the code
    and visualize the output throughout this book. The code and dataset resources
    for this chapter can be downloaded or cloned from the following GitHub repository:
    [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    Other important Python frameworks required to run the code will be mentioned in
    the notebooks with other relevant details to understand the code implementation
    of these concepts. Other important Python frameworks required to run the code
    will be mentioned in the notebooks along with other relevant details to understand
    the code implementation of these concepts. Please note that this chapter mainly
    focuses on providing a conceptual understanding of the topics covered. The Jupyter
    notebooks will help you gain the supplementary knowledge that is required to implement
    these concepts in practice. I recommend that you first read this chapter and then
    work on executing the Jupyter notebooks.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033)中的*模型可解释性方法*类似，本章提供了一些教程示例，以实现某些在特定有趣数据集上执行以数据为中心的XAI技术的技术。我们将使用Python
    Jupyter笔记本来运行代码并在整本书中可视化输出。本章的代码和数据集资源可以从以下GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)。其他运行代码所需的Python框架将在笔记本中提及，以及其他相关细节，以了解这些概念代码的实现。其他运行代码所需的Python框架将在笔记本中提及，以及了解这些概念代码实现的相关细节。请注意，本章主要侧重于提供对所涵盖主题的概念性理解。Jupyter笔记本将帮助您获得在实践实现这些概念所需的补充知识。我建议您首先阅读本章，然后尝试执行Jupyter笔记本。
- en: Introduction to data-centric XAI
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据为中心的XAI简介
- en: '*Andrew Ng*, one of the influential thought leaders in the field of AI and
    ML, has often highlighted the importance of using a systematic approach to build
    AI systems with high-quality data. He is one of the pioneers of the idea of **data-centric
    AI**, which focuses on developing systematic processes to develop models using
    clean, consistent, and reliable data, instead of focusing on the code and the
    algorithm. If the data is consistent, unambiguous, balanced, and available in
    sufficient quantity, this leads to faster model building, improved accuracy, and
    faster deployment for any production-level system.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*安德鲁·吴*是AI和ML领域有影响力的思想领袖之一，他经常强调使用系统方法构建具有高质量数据的AI系统的重要性。他是**以数据为中心的AI**理念的先驱之一，该理念侧重于开发系统流程，使用干净、一致和可靠的数据来开发模型，而不是专注于代码和算法。如果数据是一致的、明确的、平衡的，并且有足够的数量，这将导致模型构建更快、准确性更高，以及任何生产级系统的部署更快。'
- en: Unfortunately, all AI and ML systems that exist in production today are not
    in alignment with the principles of data-centric AI. Consequently, there can be
    severe issues with the underlying data that seldom get detected but eventually
    lead to the failure of ML systems. That is why **data-centric XAI** is important
    to inspect and evaluate the quality of the data being used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，今天所有在生产中存在的AI和ML系统都不符合以数据为中心的AI原则。因此，可能存在严重的数据问题，这些问题很少被发现，但最终会导致ML系统失败。这就是为什么**以数据为中心的XAI**对于检查和评估所使用数据的质量很重要。
- en: When we talk about explaining the functioning of any black-box model with respect
    to the data, we should inspect the *volume of the data*, the *consistency of the
    data* (particularly for supervised ML problems), and the *purity and integrity
    of the data*. Now, let's discuss these important aspects of data-centric XAI and
    understand why they are important.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论根据数据解释任何黑盒模型的运作时，我们应该检查数据的*量*、数据的*一致性*（尤其是对于监督式ML问题），以及数据的*纯净性和完整性*。现在，让我们讨论数据为中心的XAI的重要方面，并了解为什么它们很重要。
- en: Analyzing data volume
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据量
- en: One of the classical problems of ML algorithms is the lack of generalization
    due to **overfitting**. Overfitting can be reduced by adding more data or by getting
    datasets of the appropriate volume to solve the underlying problem. So, the very
    first question we should ask about the data to explain the black-box model is
    "*Is the model trained on sufficient data?*" But for any industrial application,
    since data is very expensive, adding more data is not always feasible. So, the
    question should be "*How do we find out if the model was trained on sufficient
    data?*"
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法的经典问题之一是由于**过拟合**导致的泛化不足。可以通过添加更多数据或获取适当体积的数据集来解决潜在问题，从而减少过拟合。因此，我们应该首先问关于数据的问题来解释黑盒模型：“*模型是否在足够的数据上训练过？*”但对于任何工业应用，由于数据非常昂贵，添加更多数据并不总是可行的。因此，问题应该是：“*我们如何找出模型是否在足够的数据上训练过？*”
- en: One way to inspect whether the model was trained on sufficient data is by training
    models with 70%, 80%, and 90% of the training dataset. If the model accuracy shows
    an increasing trend, with an increase in the volume of the data, that means the
    volume of the data can influence the model's performance. If the accuracy of the
    trained model, which has been trained on the entire training dataset, is low,
    then an increasing trend of model accuracy with increasing data volume indicates
    that the model is not trained on sufficient data. Therefore, more data is needed
    to make the model more accurate and generalized.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 检查模型是否在足够的数据上训练过的一种方法是通过使用70%、80%和90%的训练数据集来训练模型。如果模型准确率显示出随着数据量的增加而增加的趋势，这意味着数据量可以影响模型的表现。如果基于整个训练数据集训练的模型的准确率较低，那么随着数据量的增加而增加的模型准确率趋势表明模型没有在足够的数据上训练。因此，需要更多数据来使模型更准确和更具泛化能力。
- en: For production systems, if data is continuously flowing and there is no constraint
    on the availability of data, continuous training and monitoring of the models
    should be done on changing volumes of the data to understand and analyze its impact
    on the overall model performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，如果数据持续流动且没有对数据可用性的限制，应该对数据的变化量进行持续的训练和监控，以了解和分析其对整体模型性能的影响。
- en: Analyzing data consistency
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据一致性
- en: Data consistency is another important factor to inspect when explaining ML models
    with respect to the data. One of the fundamental steps of analyzing data consistency
    is by understanding the distribution of the data. If the *data is not evenly distributed*,
    if there is a *class imbalance*, or if the *data is skewed toward a particular
    direction*, it is very likely that the model will be *biased* or less efficient
    for a particular segment of the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据一致性是解释机器学习模型时需要检查的另一个重要因素。分析数据一致性的基本步骤之一是理解数据的分布。如果数据*分布不均匀*，如果存在*类别不平衡*，或者如果*数据偏向特定方向*，那么模型很可能对数据的一个特定部分存在*偏差*或效率较低。
- en: For production systems, it has been often observed that the inference data used
    in the production application might have some variance with the data used during
    training and validation. This phenomenon is known as **data drift**, and it refers
    to an unexpected change in the data structure or the statistical properties of
    the dataset, thus making the data corrupt and hampering the functioning of the
    ML system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生产系统，通常观察到在生产应用中使用的推理数据可能与训练和验证期间使用的数据进行一些差异。这种现象被称为**数据漂移**，它指的是数据结构或数据集的统计属性出现意外变化，从而使数据变得不完整并妨碍机器学习系统的运行。
- en: Data drift is very common for most real-time predictive models. This is simply
    because, in most situations, the data distribution changes over a period of time.
    This can happen due to a variety of reasons, for instance, if the system through
    which the data is being collected (for example, sensors) starts malfunctioning
    or needs to be recalibrated, then data drift can occur. Other external factors
    such as the surrounding temperature and surrounding noise can also introduce data
    drift. There can be a natural change in the relationship between the features
    that might cause data to drift. Consequently, if the training data is significantly
    different from the inference data, the model will make an error when predicting
    the outcome.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数实时预测模型来说，数据漂移非常常见。这仅仅是因为，在大多数情况下，数据分布会在一段时间内发生变化。这可能是由多种原因造成的，例如，如果收集数据所通过的系统（例如，传感器）开始出现故障或需要重新校准，那么就会发生数据漂移。其他外部因素，如周围温度和周围噪声，也可能引入数据漂移。特征之间的关系可能会发生自然变化，这可能导致数据漂移。因此，如果训练数据与推理数据显著不同，模型在预测结果时将产生错误。
- en: Now, sometimes, there can be a drift in the whole dataset or there can be a
    drift in one or more features. If there is a drift in a single feature, this is
    referred to as **feature drift**. There are multiple ways to detect feature drift
    such as the **Population Stability Index** (**PSI**) ([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)),
    **Kullback–Leibler Divergence** (**KL Divergence**)([https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)),
    and **Wasserstein metric** (**Earth Movers Distance**)([http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf)).
    The application of some of these techniques has been demonstrated at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    These statistical methods are popular ways to measure the distance between two
    data distributions. So, if the distance is significantly large, this is an indication
    of the presence of drift.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有时整个数据集可能会出现漂移，或者一个或多个特征可能会出现漂移。如果单个特征出现漂移，这被称为**特征漂移**。有多种方法可以检测特征漂移，例如**人口稳定性指数**（**PSI**）([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)）、**Kullback–Leibler
    散度**（**KL 散度**）([https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)）和**Wasserstein
    距离**（**地球搬家距离**）([http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf](http://infolab.stanford.edu/pub/cstr/reports/cs/tr/99/1620/CS-TR-99-1620.ch4.pdf))。其中一些技术的应用已在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)中展示。这些统计方法是衡量两个数据分布之间距离的流行方式。因此，如果距离显著较大，这表明存在漂移。
- en: Apart from feature drift, **Label Drift** or **Concept Drift** can occur if
    the statistical properties of the target variable change over a period of time
    due to unknown reasons. However, overall data consistency is an important parameter
    for *root cause analysis inspection* when interpreting black-box models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征漂移，如果由于未知原因，目标变量的统计特性在一段时间内发生变化，则可能会发生**标签漂移**或**概念漂移**。然而，在解释黑盒模型时，整体数据一致性是一个重要的参数，用于**根本原因分析检查**。
- en: Analyzing data purity
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析数据纯净度
- en: The datasets used for practical industrial problems are never clean, even though
    most organizations spend a significant amount of time and investment on data engineering
    and data curation to drive a culture of *data-driven decision-making*. Yet, almost
    all practical datasets are messy and require a systematic approach to curation
    and preparation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用于实际工业问题的数据集永远不会是干净的，尽管大多数组织在数据工程和数据整理上投入了大量的时间和资金，以推动**数据驱动决策**的文化。然而，几乎所有实际数据集都是杂乱的，需要系统性的整理和准备方法。
- en: When we train a model, usually, we put our efforts into data preprocessing and
    preparation steps such as *finding duplicates or unique values*, *removing noise
    or unwanted values* from the data, *detecting outliers*, *handling missing values*,
    *handling features with mixed data types*, and even *transforming raw features
    or feature engineering* to get better ML models. On a high level, these methods
    are meant for removing impurities from the training data. But what if *a black-box
    ML model is trained on a dataset with less purity and, hence, performs poorly*?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个模型时，通常，我们会将精力投入到数据预处理和准备步骤中，例如*查找重复或唯一值*，*从数据中移除噪声或不想要的值*，*检测异常值*，*处理缺失值*，*处理具有混合数据类型的特征*，甚至*转换原始特征或进行特征工程*以获得更好的机器学习模型。从高层次来看，这些方法旨在从训练数据中去除杂质。但是，如果在一个**黑盒机器学习模型**上训练的数据纯净度较低，因此表现不佳，会怎样呢？
- en: 'That is why analyzing data purity is an important step in data-centric XAI.
    Aside from the data preprocessing and preparation methods mentioned earlier, there
    are other data integrity issues that exist as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，分析数据纯净度是数据为中心的XAI的一个重要步骤。除了前面提到的数据预处理和准备方法之外，还存在其他数据完整性问题，如下所示：
- en: '**Label ambiguity**: For supervised ML problems, **label ambiguity** can be
    a very critical problem. If two or more instances of a dataset, which are very
    similar, have multiple labels, then this can lead to label ambiguity. Ambiguous
    labeling of the target variable can increase the difficulty of even domain experts
    classifying the samples correctly. Label ambiguity can be a very common problem,
    as, usually, labeled datasets are prepared by human subject-matter experts who
    are prone to *human error*.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签模糊性**：对于监督式机器学习问题，**标签模糊性**可能是一个非常关键的问题。如果数据集中两个或多个非常相似的实例具有多个标签，那么这可能导致标签模糊性。目标变量的模糊标签可能会增加即使是领域专家正确分类样本的难度。标签模糊性可能是一个非常常见的问题，因为通常，标记数据集是由容易犯**人为错误**的人类领域专家准备的。'
- en: '**Dominating features frequency change** (**DFCC**): Inspecting DFFC in the
    training and inference dataset is another parameter that can cause data integrity
    issues. In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, when we discussed feature importance, we understood that not all features
    within the dataset are equally important, and some of the features have more influential
    power on the model''s decision-making process. These are the dominating features
    in the dataset, and if the variance in the values of the dominating features in
    the training and the inference dataset is high, it is very likely that the model
    will make errors when predicting the outcome.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主导特征频率变化**（**DFCC**）：检查训练和推理数据集中的DFCC是另一个可能导致数据完整性问题的参数。在[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中，当我们讨论特征重要性时，我们了解到数据集中并非所有特征都同等重要，有些特征对模型的决策过程有更大的影响力。这些是数据集中的主导特征，如果训练和推理数据集中主导特征值的方差很高，那么模型在预测结果时出现错误的概率非常高。'
- en: Other data purity issues, such as the introduction of a new label or new feature
    category, or out of bound values (or anomalies) for a particular feature in the
    inference set, can cause the failure of ML systems in production.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据纯净度问题，例如引入新的标签或新的特征类别，或者在推理集中某个特定特征的越界值（或异常值），可能导致生产中的机器学习系统失败。
- en: 'The following table shows certain important data purity checks that can be
    performed using the **Deepchecks Python framework**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了可以使用**Deepchecks Python框架**执行的某些重要数据纯净度检查：
- en: '![Figure 3.1 – Data purity checks using the Deepchecks framework'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.1 – 使用Deepchecks框架进行数据纯净度检查'
- en: '](img/B18216_03_1.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_03_1.jpg)'
- en: Figure 3.1 – Data purity checks using the Deepchecks framework
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 使用Deepchecks框架进行数据纯净度检查
- en: Data-centric XAI also includes other parameters that can be analyzed such as
    *adversarial robustness* ([https://adversarial-ml-tutorial.org/introduction/](https://adversarial-ml-tutorial.org/introduction/)),
    the *trust score comparison* ([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)),
    *covariate shift* ([https://arxiv.org/abs/2111.08234](https://arxiv.org/abs/2111.08234)),
    *data leakage between the training and validation datasets* ([https://machinelearningmastery.com/data-leakage-machine-learning/](https://machinelearningmastery.com/data-leakage-machine-learning/)),
    and *model performance sensitivity analysis* based on data alterations. All of
    these concepts apply to both tabular data and unstructured data such as images
    and text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以数据为中心的XAI还包括其他可以分析的参数，例如*对抗鲁棒性*([https://adversarial-ml-tutorial.org/introduction/](https://adversarial-ml-tutorial.org/introduction/))、*信任分数比较*([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783))、*协变量偏移*([https://arxiv.org/abs/2111.08234](https://arxiv.org/abs/2111.08234))、*训练集和验证集之间的数据泄露*([https://machinelearningmastery.com/data-leakage-machine-learning/](https://machinelearningmastery.com/data-leakage-machine-learning/))以及基于数据变更的*模型性能敏感性分析*。所有这些概念都适用于表格数据和非结构化数据，如图像和文本。
- en: To explore practical ways of data purity analysis, you can refer to the Jupyter
    notebook at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb).
    We will discuss these topics later in the chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索数据纯度分析的实际方法，你可以参考[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter03/Data_Centric_XAI_part_1.ipynb)上的Jupyter笔记本。我们将在本章后面讨论这些主题。
- en: Thorough data analysis and profiling process
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 详尽的数据分析和概要化流程
- en: 'In the previous section, you were introduced to the concept of data-centric
    XAI in which we discussed three important aspects of data-centric XAI: analyzing
    data volume, data consistency, and data purity. You might already be aware of
    some of the methods of data analysis and data profiling that we are going to learn
    in this section. But we are going to assume that we already have a trained ML
    model and, now, we are working toward explaining the model''s decision-making
    process by adopting data-centric approaches.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你被介绍到了以数据为中心的XAI的概念，我们讨论了数据为中心XAI的三个重要方面：分析数据量、数据一致性和数据纯度。你可能已经了解了一些我们将要学习的数据分析和数据概要化方法。但我们将假设我们已经有了一个训练好的机器学习模型，现在我们正在通过采用以数据为中心的方法来解释模型的决策过程。
- en: The need for data analysis and profiling processes
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需要进行数据分析和概要化流程
- en: In [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, when we discussed knowledge extraction using **exploratory data analysis**
    (**EDA**), we discovered that this was a pre-hoc analysis process, in which we
    try to understand the data to form relevant hypotheses. As data scientists, these
    initial hypotheses are important as they allow us to take the necessary steps
    to build a better model. But let's suppose that we have a baseline trained ML
    model and the model is not performing as expected because it is not meeting the
    benchmark accuracy scores that were set.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第二章*](B18216_02_ePub.xhtml#_idTextAnchor033) *模型可解释性方法*中，当我们讨论使用**探索性数据分析**（**EDA**）进行知识提取时，我们发现这是一个预先分析过程，其中我们试图理解数据以形成相关假设。作为数据科学家，这些初步假设很重要，因为它们允许我们采取必要的步骤来构建更好的模型。但让我们假设我们有一个基线训练好的机器学习模型，而这个模型的表现并不如预期，因为它没有达到设定的基准精度分数。
- en: Following the principles of model-centric approaches, most data scientists might
    want to spend more time in hyperparameter tuning, training for a greater number
    of epochs, feature engineering, or choosing a more complex algorithm. After a
    certain point, these methods will become limited and provide a very small boost
    to the model's accuracy. That is when data-centric approaches prove to be very
    efficient.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循以模型为中心的方法原则，大多数数据科学家可能会想要花更多时间在超参数调整、训练更多轮次、特征工程或选择更复杂的算法上。然而，在某个点上，这些方法将变得有限，并且对模型精度的提升非常微小。这时，以数据为中心的方法证明是非常有效的。
- en: Data analysis as a precautionary step
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析作为预防措施
- en: By the principles of data-centric explainability approaches, at first, we try
    to perform a thorough analysis of the underlying dataset. We try to randomly reshuffle
    the data to create different training and validation sets and observe any overfitting
    or underfitting effects. If the model is overfitting or underfitting, clearly
    more data is required to generalize the model. If the available data is not sufficient
    in volume, there are ways to generate synthetic or artificial data. One such popular
    technique used for image classification is **data augmentation** ([https://research.aimultiple.com/data-augmentation/](https://research.aimultiple.com/data-augmentation/)).
    The **synthetic minority oversampling technique** (**SMOTE**) ([https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/))
    is also a powerful method that you can use to increase the size of the dataset.
    Some of these data-centric approaches are usually practiced during conventional
    ML workflows. However, we need to realize the importance of these steps for the
    explainability of black-box models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据以数据为中心的可解释性方法的原则，首先，我们尝试对基础数据集进行彻底分析。我们尝试随机重新排列数据以创建不同的训练集和验证集，并观察任何过拟合或欠拟合效应。如果模型过拟合或欠拟合，显然需要更多的数据来泛化模型。如果可用的数据量不足，有方法可以生成合成或人工数据。用于图像分类的一种流行技术是**数据增强**（[https://research.aimultiple.com/data-augmentation/](https://research.aimultiple.com/data-augmentation/)）。**合成少数过采样技术**（**SMOTE**）（[https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/））也是一种强大的方法，您可以使用它来增加数据集的大小。这些以数据为中心的方法通常在传统的机器学习工作流程中实践。然而，我们需要认识到这些步骤对于黑盒模型可解释性的重要性。
- en: Once we have done enough tests to understand whether the volume of the data
    is sufficient, we can try to inspect the consistency and purity of the data at
    a segmented level. If we are working on a classification problem, we can try to
    understand whether the model performance is consistent for all the classes. If
    not, we can isolate the particular class or classes for which the model performance
    is poor. Then, we check for data drifts, feature drifts, concept drifts, label
    ambiguity, data leakage (for example, when unseen test data trickles into the
    training data), and other data integrity checks for that particular class (or
    classes). Usually, if there is any abnormality with the data for a particular
    class (or classes), these checks are sufficient to isolate the problem. A thorough
    data analysis acts as a precautionary step to detect any loopholes in the modeling
    process.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们进行了足够的测试以了解数据量是否充足，我们就可以尝试在分段级别上检查数据的一致性和纯度。如果我们正在处理一个分类问题，我们可以尝试了解模型性能是否对所有类别都是一致的。如果不一致，我们可以隔离模型性能较差的特定类别或类别。然后，我们检查数据漂移、特征漂移、概念漂移、标签模糊、数据泄露（例如，当未见过的测试数据逐渐进入训练数据时），以及针对该特定类别（或类别）的其他数据完整性检查。通常，如果特定类别（或类别）的数据有任何异常，这些检查就足以隔离问题。彻底的数据分析作为预防措施，用于检测建模过程中的任何漏洞。
- en: Building robust data profiles
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建稳健的数据概要
- en: Another approach is to build statistical profiles of the data and then compare
    the profiles between the training data and the inference data. A statistical profile
    of a dataset is a collection of certain statistical measures of its feature values
    segmented by the target variable class (or, in the case of a regression problem,
    the bin of values). The selection of the statistical measures might change from
    use case to use case, but usually, I select statistical measures such as the mean,
    median, average variance, average standard deviation, coefficient of variation
    (standard deviation/mean), and z-scores ((value – mean)/standard deviation) for
    creating data profiles. In the case of time series data, measures such as the
    moving average and the moving median can also be very important.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是建立数据的统计概要，然后比较训练数据和推理数据之间的概要。数据集的统计概要是一组统计度量，这些度量按目标变量类别（或，在回归问题的情况下，按值区间）分段。统计度量的选择可能因用例而异，但通常，我选择如均值、中位数、平均方差、平均标准差、变异系数（标准差/均值）和z分数（（值
    - 均值）/标准差）等统计度量来创建数据概要。在时间序列数据的情况下，移动平均和移动中位数等度量也非常重要。
- en: 'Next, let''s try to understand how this approach is useful. Suppose there is
    an arbitrary dataset that has three classes (namely class 0, 1, and 2) and only
    two features: feature 1 and feature 2\. When we try to prepare the statistical
    profile, we would try to calculate certain statistical measures (such as the mean,
    median, and average variance in this example) for each feature and each class.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，让我们尝试理解这种方法的有用之处。假设有一个任意数据集，它有三个类别（即类别0、1和2）并且只有两个特征：特征1和特征2。当我们尝试准备统计概览时，我们会尝试计算每个特征和每个类别的某些统计量（例如，在这个例子中的均值、中位数和平均方差）。 '
- en: 'So, for class 0, a set of profile values consisting of the mean of feature
    1, the median of feature 1, the average variance of feature 1, the mean of feature
    2, the median of feature 2, and the average variance of feature 2 will be generated.
    Similarly, for class 1 and class 2, a set of profile values will be created for
    each class. The following table represents the statistical profile of the arbitrary
    dataset that we have considered for this example:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于类别0，将生成一个包含特征1的均值、特征1的中位数、特征1的平均方差、特征2的均值、特征2的中位数和特征2的平均方差的概览值集合。同样，对于类别1和类别2，将为每个类别创建一个概览值集合。以下表格代表了我们在本例中考虑的任意数据集的统计概览：
- en: '![Figure 3.2 – A table showing a statistical profile segmented by each class
    for an arbitrary dataset'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2 – 一个表格，展示了针对任意数据集按每个类别分割的统计概览'
- en: '](img/B18216_03_2.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_03_2.jpg)'
- en: Figure 3.2 – A table showing a statistical profile segmented by each class for
    an arbitrary dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 一个表格，展示了针对任意数据集按每个类别分割的统计概览
- en: These statistical measures of the feature values can be used to compare the
    different classes. If a trained model predicts a particular class, we can compare
    the feature values with the statistical profile values for that particular class
    to get a fair idea about the influential features contributing to the decision-making
    process of the model. But more importantly, we can create separate statistical
    profiles for the validation set, test set, and inference data used in the production
    systems and compare them with the statistical profile of the training set. If
    the absolute percentage change between the values is significantly higher (say,
    > 20%), then this indicates the presence of data drift.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征值的统计量可以用来比较不同的类别。如果一个训练模型预测了特定的类别，我们可以将该类别的特征值与统计概览值进行比较，以获得关于模型决策过程中影响特征的一个公平的概念。但更重要的是，我们可以为验证集、测试集和生产系统中使用的推理数据创建单独的统计概览，并将它们与训练集的统计概览进行比较。如果值之间的绝对百分比变化显著较高（比如说，>
    20%），那么这表明存在数据漂移。
- en: In our example, let's suppose that if the absolute percentage change in the
    average variance score for feature 1 for class 1 is about 25% between the training
    data and the inference data, then we have a feature drift for feature 1, and this
    might lead to poor model performance with the inference data for the production
    systems. Statistical profiles can also be created for unstructured data such as
    images and text, although the choice of statistical measures might be slightly
    complicated.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，假设如果类别1的特征1的平均方差分数在训练数据和推理数据之间绝对百分比变化约为25%，那么我们就有了特征1的特征漂移，这可能会导致生产系统中的推理数据模型性能不佳。统计概览也可以为非结构化数据，如图像和文本，创建，尽管统计量的选择可能稍微复杂一些。
- en: In general, this approach is very easy to implement and it helps us to validate
    whether the data used for training a model and the data used during testing or
    inference are consistent or not, which is an important step for data-centric model
    explainability. In the next section, we will discuss about the importance of monitoring
    and anticipating drifts for explaining ML systems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种方法很容易实现，并且它帮助我们验证用于训练模型的数据和用于测试或推理的数据是否一致，这对于以数据为中心的模型可解释性是一个重要的步骤。在下一节中，我们将讨论监控和预测漂移对于解释机器学习系统的重要性。
- en: Monitoring and anticipating drifts
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和预测漂移
- en: In the previous section, we understood how a thorough data analysis and data
    profiling approach can help us to identify data issues related to volume, consistency,
    and purity. Usually, during the initial data exploration process, most data scientists
    try to inspect issues in the dataset in terms of volume and purity and perform
    necessary preprocessing and feature engineering steps to handle these issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'But the detection of data consistency for real-time systems and production
    systems is a challenging problem for almost all ML systems. Additionally, issues
    relating to data consistency are often overlooked and are quite unpredictable
    as they can happen at any point in time in production systems. Some of the cases
    where data consistency issues can occur are listed as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: They can occur due to natural reasons such as changes in external environmental
    conditions or due to the natural wear and tear of sensors or systems capturing
    the inference data.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can happen due to human-induced reasons such as any physical damage caused
    to the system collecting the data, any bug in the software program running the
    algorithm due to which the input data is being transformed incorrectly, or any
    noise introduced to the system while upgrading an older version of the system.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, all of these situations can introduce data drifts and concept drifts, which
    eventually lead to the poor performance of ML models. And since drifts are very
    common in reality, issues related to drifts should be pre-anticipated and should
    be considered during the design process of any ML system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Detecting drifts
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After trained models are deployed for any production ML system, performance
    monitoring and feedback based on model performance is a necessary process. As
    we monitor the model performance, checking for any data or concept drifts is also
    critical in this step. At this point, you might be wondering two things:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '*What is the best way to identify the presence of a drift?*'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What happens when we detect the presence of a drift?*'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As discussed in the *Analyzing data consistency* section, there are two types
    of data drifts – *feature drifts* and *concept drifts*. Feature drifts happen
    when the statistical properties of the features or the independent variables change
    due to an unforeseen reason. In comparison, concept drift occurs when the target
    class variable, which the model is trying to predict, changes its initial relationship
    with the input features in a dynamic setting. In both cases, there is a statistical
    change in the underlying data. So, my recommendation for detecting drifts is to
    use the data profiling method discussed in the previous section.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: A real-time monitoring dashboard is always helpful for any real-time application
    to monitor any drift. In the dashboard, try to have necessary visualizations for
    each class and each feature, comparing the statistical profile values with the
    actual live values flowing into the trained model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Particularly for concept drifts, comparing the correlations of the features
    with the target outcome is extremely helpful. Since drifts can arise after a certain
    period of time or even during a specific point in time due to external reasons,
    it is always advisable to monitor the statistical properties of the inference
    data in a time window period (for instance, for 50 consecutive data points or
    100 consecutive data points) rather than a continuous cumulative basis. For the
    purposes of feedback, necessary alerts and triggers can be set when abnormal data
    points are detected in the inference data, which might indicate the presence of
    data drift.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Selection of statistical measures
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, the selection of statistical measures can be difficult. So, we usually
    go for some popular distribution metrics to detect the presence of data drift
    using a quantitative approach. One such metric is called **trust score distribution**
    ([https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the trust score distribution plot obtained using
    the *Deepchecks Python framework*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – An example of the trust score distribution between the training
    dataset and the inference dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_03_3.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – An example of the trust score distribution between the training
    dataset and the inference dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Trust score is a distribution metric used to measure the agreement between the
    ML classifier on the training set and an updated **k-Nearest Neighbor** (**kNN**)
    classifier on the inference dataset. The preceding diagram shows a trust score
    distribution plot between the training dataset and the inference dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, the distributions should be almost the same for both the train and
    test datasets. However, if the trust score distribution for the inference set
    is skewed toward the extreme left, this indicates that the trained model has less
    confidence in the inference data, thereby alluding to the presence of drift. If
    the distribution of the trust score on the inference data is skewed toward the
    extreme right, there might be some problem with the model and there is a high
    probability of data leakage, as ideally, the trained model cannot be more confident
    in the test data in comparison to the training data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: To detect feature drifts on categorical features, the popular choice of metric
    is the **population stability index** (**PSI**) ([https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)).
    This is a statistical method used to measure the shift in a variable over a period
    of time. If the overall drift score is more than 0.2 or 20%, then the drift is
    considered to be significant, establishing the presence of feature drift.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To detect feature drifts in numeric features, the **Wasserstein metric** ([https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867](https://kowshikchilamkurthy.medium.com/wasserstein-distance-contraction-mapping-and-modern-rl-theory-93ef740ae867))
    is the popular choice. This is a distance function for measuring the distance
    between two probability distributions. Similar to PSI, if the drift score using
    the Wasserstein metric is higher than 20%, this is considered to be significant
    and the numerical feature is considered to have feature drift.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates feature drift estimation using the Wasserstein
    (Earth Mover''s) distance and **Predictive Power Score** (**PPS**) with the Deepchecks
    framework:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Feature drift estimation using Wasserstein (Earth Mover''s)
    distance and PPS of features'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_03_4.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Feature drift estimation using Wasserstein (Earth Mover's) distance
    and PPS of features
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Similar concept drifts can also be detected using these metrics. For regression
    problems, the Wasserstein metric is effective, while for classification problems
    PSI is more effective. You can see the application of these methods on a practical
    dataset at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03).
    Additionally, there are other statistical methods that are extremely useful for
    detecting data drifts such as **Kullback-Leibler Divergence** (**KL Divergence**),
    the **Bhattacharyya distance**, **Jensen-Shannon Divergence** (**JS Divergence**),
    and more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, our focus is not on learning these metrics, but I strongly
    recommend you to take a look at the *Reference* section to find out more about
    these metrics and their application for finding data drifts. These methods are
    also applicable to images. Instead of structured feature values, the distributions
    of the pixel intensity value of the image datasets are used to detect drifts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are aware of certain effective ways in which to detect drifts, *what
    do we do when we have identified the presence of drifts?* The first step is to
    alert our stakeholders if the ML system is already in production. Incorrect predictions
    due to data drift can impact many end users, which might, ultimately, lead to
    the loss of trust of the end users. The next step is to check whether the drift
    is *temporary*, *seasonal*, or *permanent* in nature. Analysis of the nature of
    the drift can be challenging, but if the changes that are causing the drift can
    be identified and reverted, then that is the best solution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: If the drift is temporary, the first step is to identify the temporary change
    that caused the drift and then revert the changes. For seasonal drifts, seasonal
    changes to the data should be accounted for during the training process or as
    an additional preprocessing step to normalize any seasonal effects on the data.
    This is so that the model is aware of the seasonal pattern in the data. However,
    if the drift is permanent, then the only option is to retrain the model on the
    new data and deploy the newly trained model for the production system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: In the context of XAI, the detection of drifts can justify the failure of any
    ML model or algorithm and helps to improve the model by identifying the root cause
    of the failure. In the next section, we will discuss another data-centric quality
    inspection step that can be performed to ensure the robustness of ML models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Checking adversarial robustness
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we discussed the importance of anticipating and monitoring
    drifts for any production-level ML system. Usually, this type of monitoring is
    done after the model has been deployed in production. But even before the model
    is deployed in production, it is extremely critical to check for the **adversarial
    robustness** of the model.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Most ML models are prone to adversarial attacks or an injection of noise to
    the input data, causing the model to fail by making incorrect predictions. The
    degree of adversarial attacks increases with the model's complexity, as complex
    models are very sensitive to noisy data samples. So, checking for adversarial
    robustness is about evaluating how sensitive the trained model is toward adversarial
    attacks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: In this section, first, we will try to understand the impact of adversarial
    attacks on the model and why this is important in the context of XAI. Then, we
    will discuss certain techniques that we can use to increase the adversarial robustness
    of ML models. Finally, we will discuss the methods that are used to evaluate the
    adversarial robustness of models, which can be performed as an exercise before
    deploying ML models into production, and how this forms a vital part of explainable
    ML systems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Impact of adversarial attacks
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the past few years, adversarial attacks have been a cause of great concern
    for the AI community. These attacks can inject noise to modify the input data
    in such a way that a human observer can easily identify the correct outcome but
    an ML model can be easily fooled and start predicting completely incorrect outcomes.
    The extent of the attack depends on the attacker's access to the model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Usually, in production systems, the trained model (especially the model parameters)
    is fixed and cannot be modified. But the inference data flowing into the model
    can be polluted with abrupt noise signals, thus making the model misclassify.
    Human experts are extremely efficient in filtering out the injected noise, but
    ML models fail to isolate the noise from the actual data if the model has not
    been exposed to such noisy samples during the training phase. Sometimes, these
    attacks can be *targeted* attacks, too.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a face recognition system allows access to only a specific person,
    adversarial attacks can modify the image of any person to a specific person by
    introducing some noise. In this case, an adversarial algorithm would have to be
    trained using the target sample to construct the noise signal. There are other
    forms of adversarial attacks as well, which can modify the model during the training
    phase itself. However, since we are discussing this in the context of XAI, we
    will concentrate on the impact of adversarial effects on trained ML models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of adversarial attacks that can impact trained ML
    models:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast Gradient Sign Method** (**FGSM**): FGSM is one such method that uses
    gradients of deep learning models to learn adversarial samples. For image classifiers,
    this can be a common problem, as FGSM creates perturbations on the pixel values
    of an image by adding or subtracting pixel intensity values depending on the *direction
    of the gradient descent* of the model. This can fool the model to misclassify
    and severely affect the performance of the model, but it does not create any problem
    for a human observer. Even if the modification appears to be negligible, the method
    adds an evenly distributed noise that is enough to cause the misclassifications.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Carlini & Wagner** (**C&W**) **attack**: Another common adversarial attack
    is the C&W attack. This method uses the three norm-based distance metrics (![](img/B18216_03_001.png),
    ![](img/B18216_03_002.png), and ![](img/B18216_03_003.png)) to find adversarial
    examples, such that the distance between the adversarial example and the original
    sample is minimal. Detecting C&W attacks is more difficult than FGSM attacks.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targeted adversarial patch attacks**: Sometimes, injecting noise (that is,
    the addition of noisy random pixels) into the entire image is not necessary. The
    addition of a noisy image segment to only a small portion of the image can be
    equally harmful to the model. Targeted adversarial patch attacks can generate
    a small adversarial patch that is then superimposed with the original sample,
    thus occluding the key features of the data and making the model classify incorrectly.
    There are other forms of adversarial attacks too, and many more new methods can
    be discovered in the future. However, the impact will still be the same.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how different adversarial attacks can introduce
    noise in an image, thereby making it difficult for the model to give correct predictions.
    Despite the addition of noise, we, as human beings, can still predict the correct
    outcome, but the trained model is completely fooled by adversarial attacks:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Adversarial attacks on the inference data leading to incorrect
    model predictions'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_03_5.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Adversarial attacks on the inference data leading to incorrect
    model predictions
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial attacks can force an ML model to produce incorrect outcomes that
    can severely affect end users. In the next section, let's try to explore ways
    to increase the adversarial robustness of models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Methods to increase adversarial robustness
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In production systems, adversarial attacks can mostly inject noise into the
    inference data. So, to reduce the impact of adversarial attacks, we would either
    need to teach the model to filter out the noise or expose the presence of noisy
    samples during the training process or train the models to detect adversarial
    samples:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The easiest option is to filter out the noise as a defense mechanism to increase
    the adversarial robustness of ML models. Any adversarial noise results in an abrupt
    change in the input samples. In order to filter out any abrupt change from any
    signal, we usually try to apply a smoothing filter such as **Spatial smoothing**.
    Spatial smoothing is equivalent to the **blurring operation** in images and is
    used to reduce the impact of the adversarial attack. From experience, I have observed
    that an *adaptive median spatial smoothing* ([https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm](https://homepages.inf.ed.ac.uk/rbf/HIPR2/median.htm)),
    which works at a local level through a windowing approach is more effective than
    smoothing at a global level. The statistical measure of the median is always more
    effective in filtering out noise or outliers from the data.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another approach to increase adversarial robustness is by introducing adversarial
    examples during the training process. By using the technique of **data augmentation**,
    we can generate adversarial samples from the original data and include the augmented
    data during the training process. If training the model from scratch using augmented
    adversarial samples is not feasible, then the trained ML model can actually be
    fine-tuned on the adversarial samples using **transfer learning**. Here, the trained
    model weights can be taken to be fine-tuned on the newer samples.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process of training a model with adversarial samples is often referred to
    as **adversarial training**. We can even train a separate model using adversarial
    training, just to detect adversarial samples from original samples, and use it
    along with the main model to trigger alerts if adversarial samples are generated.
    The idea of exposing the model to possible adversarial samples is similar to the
    idea of *stress testing* in cyber security ([https://ieeexplore.ieee.org/document/6459909](https://ieeexplore.ieee.org/document/6459909)).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 3.6* illustrates how spatial smoothing can be used as a defense mechanism
    to minimize the impact of adversarial attacks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Using spatial smoothing as a defense mechanism to minimize the
    impact of adversarial attacks'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_03_6.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Using spatial smoothing as a defense mechanism to minimize the
    impact of adversarial attacks
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Using the methods that we have discussed so far, we can increase the adversarial
    robustness of trained ML models to a great extent. In the next section, we will
    try to explore ways to evaluate the adversarial robustness of ML models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating adversarial robustness
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have learned certain approaches in which to defend against adversarial
    attacks, the immediate question that might come to mind is *how can we measure
    the adversarial robustness of models?*
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, I have never come across any dedicated metric to quantitatively
    measure the adversarial robustness of ML models, but it is an important research
    topic for the AI community. The most common approaches by which data scientists
    evaluate the adversarial robustness of ML models are **stress testing** and **segmented
    stress testing**:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In **stress testing**, adversarial examples are generated by FGSM or C&W methods.
    Following this, the model's accuracy is measured on the adversarial examples and
    compared to the model accuracy obtained with the original data. The strength of
    the adversarial attack can also be increased or decreased to observe the variation
    of the model performance with the attack strength. Sometimes, a particular class
    or feature can become more vulnerable to adversarial attacks than the entire dataset.
    In those scenarios, segmented stress testing is beneficial.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In **segmented stress testing**, instead of measuring the adversarial robustness
    of the entire model on the entire dataset, segments of the dataset (either for
    specific classes or for specific features) are considered to compare the model
    robustness with the adversarial attack strengths. Adversarial examples can be
    generated with random noise or with Gaussian noise. For certain datasets, quantitative
    metrics such as the **Peak Signal-to-Noise ratio** (**PSNR**) ([https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html](https://www.ni.com/nl-be/innovations/white-papers/11/peak-signal-to-noise-ratio-as-an-image-quality-metric.html))
    and **Erreur Relative Globale Adimensionnelle de Synthese** (**ERGAS**) ([https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216](https://www.researchgate.net/figure/Erreur-Relative-Globale-Adimensionnelle-de-Synthese-ERGAS-values-of-fused-images_tbl1_248978216))
    are used to measure the data or signal quality. Otherwise, the adversarial robustness
    of ML models can be quantitatively inspected by the model's prediction of adversarial
    samples.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More than the method of evaluating adversarial robustness, inspecting adversarial
    robustness and monitoring the detection of adversarial attacks is an essential
    part of explainable ML systems. Next, let's discuss the importance of measuring
    data forecastability as a method to provide data-centric model explainability.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Measuring data forecastability
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about the importance of analyzing data by inspecting
    its consistency and purity, looking for monitoring drifts, and checking for any
    adversarial attacks to explain the working of ML models. But some datasets are
    extremely complex and, hence, training accurate models even with complex algorithms
    is not feasible. If the trained model is not accurate, it is prone to make incorrect
    predictions. Now the question is *how do we gain the trust of our end users if
    we know that the trained model is not extremely accurate in making the correct
    predictions?*
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: I would say that the best way to gain trust is by being transparent and clearly
    communicating what is feasible. So, measuring **data forecastability** and communicating
    the model's efficiency to end users helps to set the right expectation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Data forecastability is an estimation of the model's performance using the underlying
    data. For example, let's suppose we have a model to predict the stock price of
    a particular company. The stock price data that is being modeled by the ML algorithm
    can predict the stock price with a maximum of 60% accuracy. Beyond that point,
    it is not practically possible to generate a more accurate outcome using the given
    dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: But let's say that if other external factors are considered to supplement the
    current data, the model's accuracy can be boosted. This proves that it is not
    the ML algorithm that is limiting the performance of the system, but rather the
    dataset that is used for modeling does not have sufficient information to get
    a better model performance. Hence, it is a limitation of the dataset that can
    be estimated by measure of data forecastability.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a number of model evaluation visualizations that
    can be used to analyze data forecastability using the Deepchecks framework:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Data forecastability using the model evaluation report and the
    Deepchecks framework'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_03_7.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Data forecastability using the model evaluation report and the
    Deepchecks framework
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss how to estimate data forecastability.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Estimating data forecastability
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data forecastability is estimated using the model evaluation metrics. Data
    forecastability can also be measured by performing model error analysis. The choice
    of the metrics depends on the type of dataset and the type of problem being solved.
    For example, take a look at the following list:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: For time series data, data forecastability is obtained by metrics such as the
    **mean absolute percentage error** (**MAPE**), the **symmetric mean absolute percentage
    error** (**SMAPE**), the **coefficient of variation** (**CoV**), and more.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For classification problems, I usually go for **ROC-AUC Scores**, the **confusion
    matrix**, **precision**, **recall**, and **F1 scores** along with **accuracy**.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For regression problems, we can look at the **mean square error** (**MSE**),
    the **R2 score**, the **root mean square error** (**RMSE**), the **sum of squared
    errors** (**SSE**), and more.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might have already used most of these metrics to evaluate trained ML models.
    Data forecastability is not just about evaluating trained models according to
    your choice of metric, but is the measure of predictability of the model using
    the given dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose you are applying three different ML algorithms such as decision
    trees, support vector machine (SVM), and random forests for a classification problem,
    and your choice of metric is recall. This is because your goal is to minimize
    the impact of false positives. After rigorous training and validation on the unseen
    data, you are able to obtain recall scores of 70% with decision tree, 85% with
    SVM, and 90% with random forest. What do you think your data forecastability will
    be? Is it 70%, 90%, or 81.67% (the average of the three scores)?
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: I would say that the correct answer is between 70% and 90%. It is always better
    to consider forecastability as a ballpark estimate, as providing a range of values
    rather than a single value gives an idea of the best-case and worst-case scenarios.
    Communicating about the data forecastability increases the confidence of the end
    stakeholders in ML systems. If the end users are consciously aware that the algorithm
    is only 70% accurate, they will not blindly trust the model even if the system
    predicts incorrectly. The end users would be more considerate if the model outcome
    does not match the actual outcome when they are aware of the model's limitations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Most ML systems in production have started using prediction probability or model
    confidence as a measure of data forecastability, which is communicated to the
    end users. For example, nowadays, most weather forecasting applications show that
    there is a certain percentage of chance (or probability) for rainfall or snowfall.
    Therefore, data forecastability increases the explainability of AI algorithms
    by setting up the right expectation for the accuracy of the predicted outcome.
    It is not just the measure of the model performance, but rather a measure of the
    predictability of a model which is trained on a specific dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of the chapter. Let's summarize what we have discussed
    in the following section.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's try to summarize what you have learned in this chapter. In this chapter,
    we focused on data-centric approaches for XAI. We learned the importance of explaining
    black-box models with respect to the underlying data, as data is the central part
    of any ML model. The concept of data-centric XAI might be new to many of you,
    but it is an important area of research for the entire AI community. Data-centric
    XAI can provide explainability to the black-box model in terms of data volume,
    data consistency, and data purity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric explainability methods are still active research topics, and there
    is no single Python framework that exists that covers all of the various aspects
    of data-centric XAI. Please explore the supplementary Jupyter notebook tutorials
    provided at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter03)
    to gain more practical knowledge on this topic.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We learned about the idea of thorough data inspection and data profiling to
    estimate the consistency of training data and inference data. Monitoring data
    drifts for production ML systems is also an essential part of the data-centric
    XAI process. Apart from data drifts, estimating the adversarial robustness of
    ML models and the detection of adversarial attacks form an important part of the
    process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about the importance of data forecastability to set the
    right expectation to end stakeholders about what the model can achieve and how
    this is a necessary practice that can increase the trust of our end users.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: You have been introduced to many statistical concepts in this chapter. Covering
    everything about each statistical method is beyond the scope of this chapter.
    However, I strongly recommend that you go through the reference links shared to
    understand these topics in greater depth.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of part 1 of this book, in which you have been exposed
    to the conceptual understanding of certain key topics of XAI. From the next chapter
    onward, we will start exploring popular Python frameworks for applying the concepts
    of XAI to practical real-world problems. In the next chapter, we will cover an
    important XAI framework called LIME and examine how it can be used in practice.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To gain additional information about the topics in this chapter, please refer
    to the following resources:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '*Andrew Ng Launches A Campaign For Data-Centric AI*: [https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5333db3a74f5)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Landing.AI: Data-Centric AI*: [https://landing.ai/data-centric-ai/](https://landing.ai/data-centric-ai/)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jiang et al. "To Trust Or Not To Trust A Classifier" (2018)*: [https://arxiv.org/abs/1805.11783](https://arxiv.org/abs/1805.11783)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deepchecks*: [https://docs.deepchecks.com/en/stable/](https://docs.deepchecks.com/en/stable/)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Statistical distance*: [https://en.wikipedia.org/wiki/Statistical_distance](https://en.wikipedia.org/wiki/Statistical_distance)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lin et al. "Examining Distributional Shifts by Using Population Stability
    Index (PSI) for Model Validation and Diagnosis"*: [https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf](https://www.lexjansen.com/wuss/2017/47_Final_Paper_PDF.pdf)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Wasserstein metric*: [https://en.wikipedia.org/wiki/Wasserstein_metric](https://en.wikipedia.org/wiki/Wasserstein_metric)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bhattacharyya Distance based Concept Drift Detection Method For evolving data
    stream*: [https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream](https://www.researchgate.net/publication/352044688_Bhattacharyya_Distance_based_Concept_Drift_Detection_Method_For_evolving_data_stream)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
