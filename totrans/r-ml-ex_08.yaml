- en: Chapter 7. Social Media Analysis – Analyzing Twitter Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Connected is the word that describes life in the 21^(st) century. Though various
    factors contribute to the term connected, there's one aspect which has played
    a pivotal role. It's called the Web. The Web, which has made distance an irrelevant
    metric and blurred socio-economic boundaries, is a world in itself and we all
    are a part of it. The Web or Internet in particular has been a central entity
    in this data-driven revolution. As we have seen in our previous chapters, for
    most modern day problems, it is the Web/Internet (henceforth used interchangeably)
    that acts as a source of data. Be it e-commerce platforms or financial domain,
    the Internet provides us with huge amounts of data every second. There's another
    ocean of data within this virtual world which touches our lives at a very personal
    level. Social networks, or social media, is a behemoth of information and the
    topic for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we covered the financial domain, where we analyzed
    and predicted credit risk for customers of a certain bank. We now shift gears
    and move into the realm of social media and see how machine learning and R empower
    us to uncover insights from this ocean of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data mining specifics for social networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance and use of different data visualizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of how to connect and collect Twitter data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing Twitter data to uncover amazing insights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seeing how social networks pose new challenges to the data mining process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social networks (Twitter)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all use social networks day in and day out. There are numerous social networks
    catering to all sorts of ideologies and philosophies, but Facebook and Twitter
    (barring a couple more) have become synonymous with the term social network itself.
    These two social networks enjoy popularity not only because of their uniqueness
    and the quality of service but because of the way they enable us to interact in
    a very intuitive way. As we saw with recommendation engines used in e-commerce
    websites (see [Chapter 4](part0032_split_000.html#UGI01-973e731d75c2419489ee73e3a0cf4be8
    "Chapter 4. Building a Product Recommendation System"), *Building a Product Recommendation
    System*), social networks have existed long before Facebook, Twitter, or even
    the Internet.
  prefs: []
  type: TYPE_NORMAL
- en: Social networks have interested scientists and mathematicians alike. It is an
    interdisciplinary topic which spans but is not limited to sociology, psychology,
    biology, economics, communication studies, and information science. Various theories
    have been developed to analyze social networks and their impact on human lives
    in the form of factors influencing economics, demographics, health, language,
    literacy, crime, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Studies done as early as the late 1800s form the basis of what we today refer
    to as social networks. A social network, as the word itself says, is a sort of
    connection/network between nodes or entities represented by humans and elements
    affecting social life. More formally, it is a network depicting relationships
    and interactions. Hence, it is not surprising to see various graph theories and
    algorithms being employed to understand social networks. Where the 19^(th) and
    20^(th) centuries were limited to theoretical models and painstaking social experiments,
    the 21^(st) century's technology has opened the doors for these theories to be
    tested, fine tuned, and modeled to help understand the dynamics of social interactions.
    Though testing these theories by some social networks (called social experiments)
    have been caught in controversies, such topics are beyond the scope of this book.
    We shall limit ourselves to the algorithmic/data science space and leave the controversies
    for the experts to discuss.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Milgram Experiment, or the small world experiment, was conducted in the
    late 1960s to examine the average path length for people in United States. As
    part of this experiment, random people were selected as starting points of a mail
    chain. These random people were tasked to send the mail to the next person so
    that the mail gets one step closer to its destination (somewhere in Boston) and
    so on. An average of six hops to the destination is the documented result of this
    famous experiment. Urban folklore suggests the phrase *6 degrees of separation*
    originated from this experiment, even though Dr. Milgram never used the term himself!
    He conducted many more experiments; search and be amazed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.simplypsychology.org/milgram.html](http://www.simplypsychology.org/milgram.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into the specifics, let us try and understand the reason behind
    choosing Twitter as our point of analysis for this and the upcoming chapter. Let
    us begin with understanding what Twitter is and why is it so popular with both
    end users and data scientists alike.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter, as we all know, is a social network/micro-blogging service that enables
    its users to send and receive tweets of a maximum of 140 characters. But what
    makes Twitter so popular is the way it caters to the basic human instincts. We,
    humans, are curious creatures with an incessant need to be heard. It is important
    for us to have someone or some place to voice our opinions. We love to share our
    experiences, feats, failures, and ideas. At some level or other, we also want
    to know what our peers are up to, what's keeping celebrities busy, or simply what's
    on the news. Twitter addresses just that.
  prefs: []
  type: TYPE_NORMAL
- en: With multiple social networks existing long before Twitter came into existence,
    it wasn't some other service which Twitter replaced. In our view, it was the way
    Twitter organized the information and its users that clicked. Its unique *Follow*
    model of relationship caters to our hunger for curiosity, while its short, free,
    and high-speed communication platform enables the users to speak out and be heard
    globally. By allowing users to follow a person or an entity of interest, it enables
    us to keep up with their latest happenings without the other user following us
    back. The *Follow* model tips Twitter's relationships towards more of an interest
    graph rather than the friendship model usually found in social networks such as
    Facebook.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter is known and used across the globe for the super-fast spread of information
    (and rumors). It has been innovatively used in certain circumstances unimaginable
    before, such as finding people in times of natural calamities such as earthquakes
    or typhoons. It has been used to spread information so far and deep that it takes
    viral proportions. The asymmetric relationships and high speed information exchange
    aid in making Twitter such a dynamic entity. If we closely analyze and study the
    data and dynamics of this social network we can uncover many insights. Hence,
    it is the topic for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Interesting links**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.technologyreview.com/s/419368/how-twitter-helps-in-a-disaster/](https://www.technologyreview.com/s/419368/how-twitter-helps-in-a-disaster/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.citylab.com/tech/2015/04/how-twitter-maps-can-be-useful-during-disasters/391436/](http://www.citylab.com/tech/2015/04/how-twitter-maps-can-be-useful-during-disasters/391436/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.researchgate.net/publication/282150020_A_Systematic_Literature_Review_of_Twitter_Research_from_a_Socio-Political_Revolution_Perspective?channel=doi&linkId=56050b3f08ae5e8e3f3125cb&showFulltext=true](https://www.researchgate.net/publication/282150020_A_Systematic_Literature_Review_of_Twitter_Research_from_a_Socio-Political_Revolution_Perspective?channel=doi&linkId=56050b3f08ae5e8e3f3125cb&showFulltext=true)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.696123](http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.696123)'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.psmag.com/nature-and-technology/how-to-use-social-media-usefully](http://www.psmag.com/nature-and-technology/how-to-use-social-media-usefully)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply some data science to tweets using #RMachineLearningByExample!'
  prefs: []
  type: TYPE_NORMAL
- en: Data mining @social networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have traveled quite a distance so far through the chapters of this book,
    understanding various concepts and learning some amazing algorithms. We have even
    worked on projects that have applications in our daily lives. In short, we have
    done data mining without using the term explicitly. Let us now take this opportunity
    to formally define data mining.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mining, in the classical sense of the word, refers to the extraction of useful
    minerals from the Earth (such as coal mining). Put in the context of the information
    age, mining refers to the extraction of useful information from large pools of
    data. Thus, if we look carefully, **Knowledge** **Mining** or **Knowledge Discovery
    from** **Data** (**KDD**) seems to be a better representation than the term data
    mining. As is the case with many keywords, short and sweet catches the attention.
    Thus, you may find in many places the terms Knowledge Discovery from Data and
    data mining being used interchangeably, which is rightly so. The process of data
    mining, analogous to the mining of minerals, involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing to remove noise and unwanted data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data transformation to transform the data into relevant form for analysis
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data/pattern evaluation to uncover interesting insights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data presentation to visualize knowledge in a useful form
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data mining isn't about using a search engine to get information, say regarding
    snakes. Rather it is about uncovering hidden insights like snakes are the only
    creatures found on every continent except Antarctica!
  prefs: []
  type: TYPE_NORMAL
- en: If we take a minute to understand the preceding steps, we can see that we used
    exactly the same process across our projects. Please keep in mind that we have
    simply formalized and presented the process we have been following across chapters
    and not missed or modified any step done in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Mining social network data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have formally defined data mining and seen the steps involved in
    transforming data to knowledge, let us focus on data from social networks. While
    data mining methodology is independent of the source of data, there are certain
    things to be kept in mind which could lead to better processing and improved results.
  prefs: []
  type: TYPE_NORMAL
- en: Like the mining of any other type of data, domain knowledge is definitely a
    plus for mining social network data. Even though social network analysis is an
    interdisciplinary subject (as discussed in the previous section), it primarily
    involves the analysis of data pertaining to users or entities and their interactions.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we have seen all sorts of data from e-commerce platforms
    to banks to data related to the characteristics of flowers. The data we have seen
    has had different attributes and characteristics. But if we look carefully, the
    data was a result of some sort of measurement or event capture.
  prefs: []
  type: TYPE_NORMAL
- en: Coming onto the social network's domain, the playground is a little, if not
    completely different. Unlike what we have seen so far, data from social media
    platforms is extremely dynamic. When we say dynamic, we refer to the actual content
    on a data point and not its structure. The data point itself may (or may not)
    be structured, but the content itself is not.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us be specific and talk about data contained in a tweet. A sample tweet
    looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mining social network data](img/00214.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://twitter.com/POTUS/status/680464195993911296](https://twitter.com/POTUS/status/680464195993911296)'
  prefs: []
  type: TYPE_NORMAL
- en: A tweet, as we all know, is a 140 character message. Since the message is generated
    by a user (usually), the actual message may be of a different length, language,
    and or it may contain images, links, videos, and more. Thus, a tweet is a structured
    data point which contains the handle of the user (`@POTUS`), the name of the user
    (`President Obama`), the message (`From the Obama family...`), along with information
    related to when was it tweeted (`26 Dec 2015`), the number of likes, and the number
    of retweets. A tweet may also contain hashtags, hyperlinks, images, and videos
    embedded within the message. As we will see in the coming sections, a tweet contains
    tons of metadata (data about the data) apart from the attributes discussed preceding.
    Similarly, data from other social networks also contains a lot more information
    than what usually meets the eye.
  prefs: []
  type: TYPE_NORMAL
- en: This much information from a single tweet coupled with millions of users tweeting
    frantically every second across the globe presents a huge amount of data with
    interesting patterns waiting to be discovered.
  prefs: []
  type: TYPE_NORMAL
- en: In its true sense, Twitter's data (and of social networks in general) represents
    the 3 Vs (Volume, Variety, and Velocity) of big data very well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '143,199 tweets per second is a record achieved during the airing of the film
    Castle in the Sky in Japan on August 3, 2013\. The average tweets per second is
    usually around 5700; the record multiplied it 25 times! Read more about it on
    the Twitter blog: [https://blog.twitter.com/2013/new-tweets-per-second-record-and-how](https://blog.twitter.com/2013/new-tweets-per-second-record-and-how)'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the mining of data from a social network involves understanding the structure
    of the data point, the underlying philosophy or use of the social network (Twitter
    is used for quick exchange of information, while LinkedIn is used for professional
    networking), the velocity and volume of the data being generated, along with the
    thinking cap of a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: Towards the end of the chapter, we will also touch upon the challenges presented
    by social networks to the usual mining methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Data and visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When the amount of data is growing exponentially every passing minute, the outcome
    of data mining activity must empower decision-makers to quickly identify action
    points. The outcome should be free of noise/excess information, yet be crisp and
    complete enough to be useable.
  prefs: []
  type: TYPE_NORMAL
- en: This unique challenge of presenting information in its most convenient and useable
    form for easy consumption by its intended audience (which may be nontechnical)
    is an important aspect of the data mining process. So far in this book, we have
    analyzed data and made use of line graphs, bar graphs, histograms, and scatter
    plots to uncover and present insights. Before we make use of these and a few more
    visualizations/graphs in this chapter as well, let us try and understand their
    importance and use them wisely.
  prefs: []
  type: TYPE_NORMAL
- en: While working on a data mining assignment, we usually get so engrossed in the
    data, its complexities, algorithms, and whatnot, that we tend to overlook the
    part where we have to make the outcome consumable rather than a difficult to read
    sheet of numbers and jargon. Apart from making sure that the final report/document
    contains the correct and verified figures, we also need to make sure that the
    figures are presented in such a manner that it is easy for the end user to make
    use of it. To enable easy consumption of this information/knowledge, we take the
    help of different visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Since this isn't a book on visualizations, we've taken the liberty of skipping
    the usual line graphs, bar graphs, pie charts, histograms, and other details.
    Let us understand some unconventional yet widely known/used visualizations before
    we use them in the coming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Word clouds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Social networks generate data in different forms and formats. The data on such
    platforms may be created, shared, modified, quoted, or used in various different
    ways. To represent complex relationships, one of the most widely used visualizations
    for social network data are **tag** **clouds** or **word clouds**. For example,
    objects such as text, images, videos, and blogs on these platforms are frequently
    tagged. Thus, a tag cloud/word cloud represents statistics of user-generated tags.
    These tags may represent the relative frequency of the use of words or their presence
    in multiple objects. The words/tags are differentiated using different font sizes
    and colors to represent the statistic of choice (mostly frequency).
  prefs: []
  type: TYPE_NORMAL
- en: '![Word clouds](img/00215.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A word cloud depicting frequently used words in a subset of tweets
  prefs: []
  type: TYPE_NORMAL
- en: Treemaps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To represent data of high dimensionality, it is usually not possible to visualize
    all dimensions at the same time. Treemaps are one such type of visualization that
    partition all dimensions into subsets and present them in a hierarchical manner.
    Specifically, treemaps partition dimensions into a set of nested rectangles. One
    of the mostly widely cited examples of a treemap is the newsmap, which visualizes
    news aggregated by Google news and displays it in different categories shown by
    different colors; color gradients denote the appearance of the article (on a time
    scale), while the size of the rectangle denotes the popularity of the news item.
  prefs: []
  type: TYPE_NORMAL
- en: '![Treemaps](img/00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Treemap showing news aggregated by Google News
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [http://newsmap.jp/](http://newsmap.jp/)'
  prefs: []
  type: TYPE_NORMAL
- en: Pixel-oriented maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Visualizations not only make outcomes easier to understand, they are very utilitarian
    as well. Most of the time, the outcome of an analysis process is multidimensional.
    To represent this data graphically on a two dimensional screen/piece of paper
    is a challenge. This is where pixel-oriented visualizations come into the picture.
    For an *n-dimensional* data set, pixel-oriented visualizations map each *n-dimensional*
    data point to a single pixel in *n* different sub-windows. Thus, each data point
    is split across *n* windows, one for each dimension. These help us map a large
    amount of data in single visualization. Pixel-oriented visualization look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pixel-oriented maps](img/00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample pixel-oriented maps
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [http://bib.dbvis.de/uploadedFiles/163.pdf](http://bib.dbvis.de/uploadedFiles/163.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Other visualizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from the already mentioned visualizations, there are many other interesting
    visualizations, which come in handy for different use cases. For example, visualizations
    such as box plots come in handy for understanding data distribution and outlier
    detection. Similarly, there are visualizations such as Chernoff faces, scatter
    plots, network graphs, and so on which have their own merits and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Please do note that visualization is in itself a field of study and this section
    is merely trying to touch the tip of the iceberg. We urge readers to go through
    books/online content as shared in the *References* section of the chapter to read
    more on this.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Twitter APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Twitter is as much a delight for tweeple (people using Twitter to tweet) as
    it is for data scientists. The APIs and the documentation are well updated and
    easy to use. Let us get started with the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Twitter has one of easiest yet most powerful set of APIs available of any social
    network out there. These APIs have been used by Twitter itself and data scientists
    to understand the dynamics of the Twitter world. Twitter APIs make use of four
    different objects, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tweets**: A tweet is the central entity that defines Twitter itself. As discussed
    in the previous section, a tweet contains far more information (metadata) than
    just the content/message of the tweet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Users**: Anybody or anything that can tweet, follow, or perform any of Twitter''s
    actions is a user. Twitter is unique in its definition of user, which need not
    necessarily be a human. `@MarsCuriosity` is one such nonhuman popular Twitter
    handle with over 2 million followers!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entities**: These are structured pieces of information extracted from the
    tweet object itself. These may include information on URLs, hashtags, user mentions,
    and so on. These objects enable quicker processing without parsing the tweet text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Places**: A tweet may also have location attached to it. This information
    may be used for various purposes, such as displaying *Trending Topics Near You*
    or targeted marketing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preceding objects from the Twitter APIs have been explained at length on
    the website [https://dev.twitter.com/](https://dev.twitter.com/). We urge readers
    to go through it to understand the objects and APIs even better.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter has libraries available in all major programming languages/platforms.
    We will be making use of TwitteR, that is, Twitter's library for R.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Twitter Best Practices**'
  prefs: []
  type: TYPE_NORMAL
- en: Twitter has a set of *best practices* and a list of dos and don'ts specified
    clearly on its developer site, [https://dev.twitter.com/](https://dev.twitter.com/),
    which talks about security/authentication, privacy, and more. Since Twitter supports
    a huge customer base with high availability, it tracks the usage of its APIs as
    well to keep its systems healthy. There is a defined rate limit on the number
    of times their APIs are queried. Kindly go through the best practices and be a
    `#gooddeveloper`!
  prefs: []
  type: TYPE_NORMAL
- en: Registering the application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have enough background about Twitter and its API objects, let us
    get our hands dirty. The first step when starting to use the APIs is to inform
    Twitter about your application. Twitter uses the standard **Open Authentication**
    (**OAuth**) protocol for authorizing a third party app. OAuth uses an application's
    consumer key, consumer secret, access token, and access token secret to allow
    it to use APIs and data of the connected service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following quick steps will set us up for the game:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to Twitter's Application Management Console at [https://apps.twitter.com/](https://apps.twitter.com/)
    and log in with your credentials or create an account if you don't have one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create New App** and fill in the details for the app's name, website,
    and so on. For our purposes, we will name our app `TwitterAnalysis_rmre`. For
    callback URL use `http://127.0.0.1:1410` to point back to your local system. You
    may choose any other port number as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Create your Twitter Application** to complete the process. Your Application
    Management Console would look like the following screenshot:![Registering the
    application](img/00218.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Twitter application page
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations, your app is created and registered with Twitter. But before
    we can use it, there's one more piece to it. We need to create access tokens,
    and to do that we perform the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link **Keys and Access Tokens** on the Twitter app's details page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down and click on **Create My Access Token** to generate an access token
    for your profile.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Keys and Access Tokens** page looks like the following screenshot after
    completing the preceding steps:![Registering the application](img/00219.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Application keys and access tokens
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will be using the same application for this as well as in the coming chapter.
    Make a note of the consumer key, consumer secret, access token and access secret;
    we will need these in our application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The keys and secrets generated for OAuth are sensitive pieces of information.
    They enable access for your app to Twitter's data. Please keep them as safe as
    you would keep your passwords (even safer than that). `#SafetyFirst`.
  prefs: []
  type: TYPE_NORMAL
- en: Connect/authenticate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have everything ready at Twitter's end, let us set things up at
    R's end as well. Before we start playing with the data from Twitter, the first
    step would be to connect and authenticate ourselves through the app we just created
    using R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make use of R''s TwitteR library by Jeff Gentry. This library or client
    allows us to use Twitter''s web APIs through R. We will use the method `setup_twitter_oauth()`
    to connect to Twitter using our app''s credentials (keys and access tokens). Kindly
    replace `XXXX` in the following code with your access keys/tokens generated in
    the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Connect/authenticate](img/00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This will open up your browser and ask you to log in using your Twitter credentials
    and authorize this app, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Connect/authenticate](img/00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Authorize app to fetch data
  prefs: []
  type: TYPE_NORMAL
- en: Once authorized, the browser will be redirected to the callback URL we mentioned
    when we created the app on Twitter. You may use a more informative URL for the
    user as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![Connect/authenticate](img/00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Congratulations, you are now connected to the ocean of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting sample tweets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we are connected to Twitter using R, it''s time to extract some latest
    tweets and analyze what we get. To extract tweets, we will use the handle for
    Twitter''s account 001 (Twitter''s founder and first user), Jack Dorsey, `@jack`.
    The following snippet of code extracts the latest 300 tweets from him:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains text combined with unprintable characters and URLs due
    to Twitter''s content-rich data. We will look at the metadata of a tweet in a
    bit, but before that, the extracted information looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting sample tweets](img/00223.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample tweets
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the attributes and functions available to analyze and manipulate each
    tweet, use the `getClass` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output will be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting sample tweets](img/00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Twitter data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have tested our tools, libraries, and connections to Twitter APIs,
    the time has come to begin our search for the hidden treasures in Twitter land.
    Let's wear our data miner's cap and start digging!
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will be working on Twitter data gathered from searching
    keywords (or hashtags in Twitter vocabulary) and user timelines. Using this data,
    we will be uncovering some interesting insights while using different functions
    and utilities from TwitteR and other R packages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that our process will implicitly follow the steps outlined for data
    mining. In the spirit of brevity, we might take the liberty to not mention each
    of the steps explicitly. We are mining for some *gold-plated* insights; rest assured
    nothing is skipped!
  prefs: []
  type: TYPE_NORMAL
- en: Every year, we begin with a new zeal to achieve great feats and improve upon
    our shortcomings. Most of us make promises to ourselves in the form of New Year's
    resolutions. Let us explore what tweeple are doing with their resolutions in 2016!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Twitter data changes very rapidly and your results/plots may vary
    from the ones depicted in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the same app and its credentials to connect and tap into Twitter
    for data. The following code works in exactly the same way that we extracted sample
    tweets in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Apart from connecting to Twitter, we have also loaded required packages, such
    as `ggplot`, `stringr`, `tm`, and `wordcloud`. We will see where and how these
    packages are useful as we proceed.
  prefs: []
  type: TYPE_NORMAL
- en: Once connected to our data source, we can proceed towards collecting the required
    data. Since we are planning to learn about tweeple and their New Year's resolutions,
    we will extract data for the hashtag `#ResolutionsFor2016`. We can also use any
    hashtag, such as `#NewYearResolutions`, `#2016Resolutions`, or a combination of
    hashtags to get relevant tweets. The following piece of code not only extracts
    tweets, but also converts the list of tweet/status objects into an R data frame.
    We also convert each of the tweets to UTF-8 to handle text from different languages.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazing fact**: Twitter is available in 48 different languages and counting!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we saw in the previous section, a tweet contains far more information than
    mere text. One of the various attributes is the status source. The status source
    denotes the device from where the tweet was made. It may be a mobile phone, tablet,
    and so on. Before we apply major transformations and clean up tweet objects, we
    apply a quick transformation to transform status source to meaningful form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code transforms `statusSource` from values such as `<a href=\"http://twitter.com/download/android\"
    rel=\"nofollow\">Twitter for Android</a>` to simply Android and assigns it to
    a new attribute named `tweetSource`.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the data, the next set of steps in the data mining process is to
    clean up the data. We use the text mining package `tm` to perform transformation
    and cleanup. The `Corpus` function in particular helps us handle tweet/status
    objects as a collection of documents. We then use the `tm_map` utility from the
    same package to apply/map transformations such as converting all text to lower
    case, removing punctuation, numbers, and stop words. Stop words is a list of the
    most commonly used words, such as a, an, the, and so on, which can safely be removed
    while analyzing text without loss of meaning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The final transformation before we proceed to the next step of analyzing our
    data for hidden patterns/insights is a term-document matrix. As the name itself
    says, a term-document matrix is a matrix representation in which terms act as
    rows while columns are represented by documents. Each entry in this matrix represents
    the number of occurrences of a term in a given document. More formally, a term-document
    matrix is a matrix representation that describes the frequency of terms in a collection
    of documents. This representation is extremely useful in natural language processing
    applications. It is an optimized data structure that enables quick searches, topical
    modeling, and more. The data structure can be explained using the following simple
    example where we have two text documents, **TD1** and **TD2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Twitter data mining](img/00225.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Sample term-document matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The tm package provides us another easy-to-use utility called term-document
    matrix (`TermDocumentMatrix` is also available), which we use to convert our `Corpus`
    object into the required form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Frequent words and associations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term-document matrix thus prepared contains words from each of the tweets
    (post the cleanup and transformations) as rows, while columns represent the tweet
    themselves.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick check, let us see which of the words are most frequently used in
    our dataset. Let the threshold be set to `30` occurrences or more. We use the
    apply utility to iterate each term in our term-document matrix and sum its occurrences.
    The function helps us filter out the terms that have appeared 30 times or more.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00226.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Terms with 30 or more occurrences across tweets
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding screenshot shows, words such as healthy, inspire, and positivity
    feature in the list of words with 30 or more occurrences. We all have a lot in
    common when it comes to yearly goals, no?
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding manipulation was a quick check to see if we really have tweets
    that help us find out something interesting about New Year''s resolutions. Let
    us now take a formal approach and identify frequent terms in our data set. We
    will also try and present the information in a creative yet easy-to-understand
    representation. To get the most frequent terms in our data set, we use the function
    `findFreqTerms` from the `tm` package again. This function provides us an abstraction
    over quick hacks, such as the one we previously used. `findFreqTerms` also lets
    us set a minimum and maximum threshold for term frequencies. For our case, we
    will only mention the lower bound and see the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results look something like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We get about 107 terms with a minimum occurrence of 10\. If you look carefully,
    the terms we saw with frequencies of at least 30 also appear in this list, and
    rightly so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we are certain that there are terms/words with occurrences of more
    than 10, let us create a data frame and plot the terms versus their frequencies
    as we decided previously. We use the `rowSums` function to calculate the total
    occurrence of each term/word. We then pick a subset of terms which have more than
    10 occurrences and plot them using `ggplot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding piece of code generates the following frequency graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00228.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Upon analyzing the preceding graph, we can quickly get some interesting points:'
  prefs: []
  type: TYPE_NORMAL
- en: The words **mom**, **elected**, **president**, and **trillionaire** feature
    in the top 10\. Strange set, yet interesting. More on this in a bit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Health features high in the list, but doesn't make it to the top 10\. So, it
    seems like health is on the cards but not very high. This is the same for **fitness**
    and **diet**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the words in this list are positive in essence. Words such as **happy**,
    **hope**, **positivity**, **change**, and so on all point to the upbeat mood while
    taking up New Year's resolutions!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though the preceding graph gives us quite a lot of information regarding the
    words and their frequencies in a nice layout, it still doesn't show us the full
    picture. Remember that we crafted a subset of items from our data set before generating
    this graph? We did that on purpose, otherwise the graph would have become too
    long and words with lesser frequencies would clutter the whole thing. Another
    point which this graph misses out is the relative difference in the frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our aim is to see the relative difference between the frequencies, we need
    a different visualization altogether. Here comes word cloud to the rescue. Using
    the `wordcloud` library, we can easily generate word clouds from a dataframe using
    a one liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The wordcloud using the complete data frame looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00229.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding word cloud renders words in decreasing order of frequency. The
    size of each word emphasizes its frequency. You can play around with the `wordcloud`
    function to generate some interesting visualizations or even art!
  prefs: []
  type: TYPE_NORMAL
- en: 'A lot of words appear in the preceding graphs, but isn''t it rather interesting
    to see the word trillionaire pop up in the top 10? What could be the reason for
    it? Was it a spam post by a bot, or a tweet by some celebrity that went viral,
    or something completely different altogether? Let''s check out the top tweet in
    this list and see if it contains the word trillionaire:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is what you get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It turns out that our hunch was right. It was a New Year resolution tweet by
    a celebrity that went viral. A quick search on Twitter reveals the tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://twitter.com/mishacollins?lang=en](https://twitter.com/mishacollins?lang=en)'
  prefs: []
  type: TYPE_NORMAL
- en: A bit further searching reveals Misha Collins is a famous actor from the television
    series Supernatural. We can also see that the above resolution was retweeted a
    staggering 5k times! It's interesting to note that the number of likes is 14k,
    outnumbering the retweets. Can we infer that tweeple prefer likes/hearts to retweets?
    It can also be seen that words such as mom, learn, trillionaire, elected, and
    President all occur as most frequent words without a doubt. Indirectly, we can
    also infer that Supernatural has a huge fan following on Twitter and that Castiel
    (Misha's role in the TV series) is a popular character from the show. A bit of
    a surprise is his resolution to learn to crochet, no?
  prefs: []
  type: TYPE_NORMAL
- en: Moving on from supernatural stuff, let us go back to the fitness debate. Fitness
    is important to most of us. Activities such as exercising or hitting the gym see
    a surge during the initial months/weeks of the year. Let's see how health-conscious
    our friends on Twitter are!
  prefs: []
  type: TYPE_NORMAL
- en: 'Since a lot of words such as health, diet, fitness, gym, and so on point towards
    a healthy lifestyle, let us try and find words associated with the word *fitness*
    itself. `findAssocs` is a handy function which helps us find words from a term-document
    matrix that have at least a specified level of correlation to a given word. We
    will use the output from this function to prepare a term-association (correlation)
    graph using `ggplot`. The process is similar to how we prepared the preceding
    frequency graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The words most closely correlated to the word fitness are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00232.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The same data is more readable in graphical form, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequent words and associations](img/00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As evident from the preceding graph, terms such as **lossweight**, **workout**,
    **getfit**, and so on. prove our point that tweeple are as concerned about health
    as we are. It is interesting to note the occurrence of the term *yogavideos* in
    this list. It looks like yoga is catching up with other techniques of staying
    fit in 2016\. There's **meditation** on the list too.
  prefs: []
  type: TYPE_NORMAL
- en: Popular devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have dealt with the visible components of a tweet, such as the text,
    retweet counts, and so on, and we were able to extract many interesting insights.
    Let us take out our precision tools and dig deeper into our data.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned a couple times in the above sections, a tweet has far more information
    than what meets the eye. One such piece of information is about the source of
    the tweet. Twitter was born of the SMS era, and many of its characteristics, such
    as the 140 character word limit, are reminiscent of that era. It would be interesting
    to see how tweeple use Twitter, that is, what devices are used to access and post
    on Twitter frequently. Though the world has moved a long way from the SMS era,
    mobile phones are ubiquitous. To get this information, we will make use of the
    attribute `tweetSource` from our dataframe `trendingTweets.df`. We created this
    additional attribute from the `statusSource` attribute already existing in the
    `tweet` object (see the beginning of this section for a quick recap).
  prefs: []
  type: TYPE_NORMAL
- en: We shall use a subset of the data frame `trendingTweets.df` based upon retweet
    counts for the sake of clarity. We will use `ggplot` again to visualize our results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is your result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Popular devices](img/00234.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Without a doubt, the iPhone is the most preferred device, followed by Android
    and the Web. It is interesting to see that people use the Web/website to retweet
    more than the iPad! Windows Phone clearly has some serious issues to tackle here.
    Can we also infer that the iPhone is the preferred device amongst tweeples? Or
    does the iPhone provide a better experience than any other device for Twitter?
    Or we could even go deeper and say that Twitter on iPhone has an easier-to-access
    "retweets" button than any other device. Inferences such as these and many more,
    require a bit more digging than this, but all of this has a lot of knowledge/potential
    that could be used by managements, UX teams, and so on to improve and change things
    around.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have seen clustering and classification in previous chapters (see [Chapter
    2](part0022_split_000.html#KVCC1-973e731d75c2419489ee73e3a0cf4be8 "Chapter 2. Let's
    Help Machines Learn"), *Let's Help Machines Learn*) and uncovered some interesting
    facts about the data at hand. For our current use case, even though our tweets
    are all related to 2016 resolutions, we can never be sure of the kinds of resolutions
    tweeple make. This makes it a very apt use case for hierarchical clustering. Unlike
    k-means or other clustering algorithms that require a preset number of clusters
    before computation, hierarchical clustering algorithms work independently of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take this opportunity to understand hierarchical clustering before we
    apply it to our data. Hierarchical clustering, like any other clustering algorithm,
    helps us group similar items together. The exact details for this algorithm in
    general can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initialize**: This is the first step, where each element is assigned to a
    cluster of its own. For a dataset containing *n* elements, the algorithm creates
    *n* different clusters with one element in each of them. A distance/similarity
    measure is decided at this step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Merge**: During this step, depending upon the distance/similarity measure
    chosen, the closest pair of clusters are identified and merged into a single cluster.
    This step results in one fewer clusters than the total clusters so far.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute**/**recompute**: We compute/recompute distances/similarities between
    the new cluster formed in the Merge step and the existing clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **merge** and **compute** steps are repeated until we are left with a single
    cluster containing all *n* items. As the name suggests, this algorithm generates
    a hierarchical structure with the leaves denoting individual elements as clusters
    combined based upon similarity/distance as we go toward the root of the tree.
    The output tree is generally referred to as a **dendrogram**.
  prefs: []
  type: TYPE_NORMAL
- en: The merge step is where variations of this algorithm exist. There are several
    ways in which the closest clusters could be identified. From simple methods, such
    as single-link, which consider the shortest distance between any two elements
    of the two clusters in consideration as the distance measure, to complex ones
    such as Ward's method which uses variance to find the most compact clusters, there
    are several methods that could be employed depending upon the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the Twitter world, let us use hierarchical clustering to see
    which terms/tweets are the closest. For our current use case, we will use the
    single method for our merge criteria. You may try out different algorithms and
    observe the differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform hierarchical clustering, we first treat our dataset to remove sparse
    terms for the sake of clarity. For this, the `removeSparseTerms` function helps
    us remove rows of data that have sparsity below a specified limit. We then use
    the `hclust` utility to form clusters. The output of this utility is directly
    plottable. Let us write some code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output *dendrogram* is amazingly simple to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hierarchical clustering](img/00235.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: If you observe the second cluster from right, it contains terms **trillionaire**,
    **elected**, **mom**, **call**, and so on. Mapping back to the top retweeted tweet
    from Mischa Collins, all these terms are mentioned in that single tweet and our
    algorithm has rightly clustered them together. Smart, isn't it? As a small exercise,
    observe other clusters and see how the terms occur in the tweets that contain
    them. One important observation to make here is that the *dendrogram* correctly
    maps all frequent terms under a single root, which reaffirms that all these terms
    point to our central theme of 2016 resolutions!
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, our analysis has been about tweets related to New Year's resolutions
    from users across the world. We have analyzed tweets related to a topic of our
    choice. Ignoring spam and other noisy tweets, more or less, our data conformed
    to a single topic. The topic itself constituted a group of words (such as health,
    trillionaire, fitness, diet, mom, and so on) which broadly describe different
    resolutions. To broaden our scope of analysis and to discover even more insights,
    let us touch upon the concept of topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling is a process of discovering patterns in a corpus of unlabeled
    text that represents the gist of the corpus. A topic itself may be described as
    a group of words that occur together to describe a large body of text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another definition tweeted during one of the conferences on topic modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://twitter.com/footnotesrising/status/264823621799780353](https://twitter.com/footnotesrising/status/264823621799780353)'
  prefs: []
  type: TYPE_NORMAL
- en: The aim of topic modeling is to automatically identify the underlying theme
    of a corpus and thus be useful in applications that require information retrieval
    based on a theme but in absence of known keywords (the exact opposite of our current
    usage of search engines). For example, wouldn't it be amazing to learn about relations
    between two countries from a newspaper's archive by using the theme *relations
    between country one and country two* rather than searching for a keyword and then
    following link after link. Please note that following links to discover information
    is equally powerful, but it leaves a lot to be desired.
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways to perform topic modeling is through **Latent Dirichlet Allocation**
    (**LDA**); it is one of the most powerful and widely used models.
  prefs: []
  type: TYPE_NORMAL
- en: LDA was presented by David M Blie in his paper *Introduction to Probabilistic
    Topic Models* in 2003\. LDA, as his paper says, can be defined as a generative
    model that allows sets of observations to be explained by unobserved groups that
    explain why some parts of the data is similar. LDA works upon the assumption that
    documents exhibit multiple topics.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is a probabilistic model and the mathematics of it are fairly involved and
    beyond the scope of this book. In a nonmathematical way, LDA can be explained
    as a model/process that helps identify the topics that have resulted in the generation
    of a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For further reading, refer to Blei's paper.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf](https://www.cs.princeton.edu/~blei/papers/Blei2011.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A blog which explains everything in simple words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/](http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)'
  prefs: []
  type: TYPE_NORMAL
- en: For our purpose/use case, we can assume LDA as a model/process which helps us
    to identify the underlying (hidden/latent) topics from a corpus of unlabeled text.
    Luckily, R abstracts most of the mathematical details in the form of a library
    called `topicmodels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the purpose of topic modeling, we shall use a new set of tweets. The **International
    Space** **Station** (**ISS**) has multiple Twitter handles, and one of them is
    `@ISS_Research`, which particularly caters to research related tweets from the
    ISS. Let us explore what `@ISS_Research` is up to these days by analyzing the
    tweets from its timeline. We will analyze these tweets to identify the underlying
    topics of research at the ISS. For this purpose, we will use the same process
    to extract tweets and perform transformations/cleanup as we have done before.
    The following snippet of code does this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have tweets in the required format, the `LDA` utility from the `topicmodels`
    package helps us uncover the hidden topics/patterns. The LDA utility requires
    the number of topics as input along with the document-term matrix. We will try
    eight topics for now. The following code uses `LDA` to extract six terms for each
    of the eight topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of topics generated using LDA is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/00237.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A visual representation would be easier to understand. We can make use of `qplot`
    to quickly plot the topics across time on an area chart, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated chart looks like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic modeling](img/00238.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Let us now analyze the outputs. The list of terms per topic generated by LDA
    seems to give us a nice insight into the kind of work/research going on at the
    ISS. Terms such as mars, microgravity, flower, Cygnus, and so on tell us about
    the main areas of research or at least the topics about which scientists/astronauts
    on the ISS are talking. Terms such as stationcdrkelly and astrotimpeake look more
    like Twitter handles.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A quick exercise would be to use the current `@ISS_Research` timeline data and
    mine for the handles, such as `stationcdrkelly`, to discover more information.
    Who knows, it may turn out be a nice list of astronauts to follow!
  prefs: []
  type: TYPE_NORMAL
- en: The `qplot` output adds the time dimension to our plain list of topics. Analyzing
    topics across the time dimension helps us understand when a particular research
    topic was discussed or when something amazing was announced. Topic two in the
    list, or the fourth one from the top in the graph legend comprises the word flower.
    Since scientists were successful in blooming some orange flowers in space recently,
    the above graph helps us get an idea that the news first broke on Twitter on/around
    15^(th) January. A quick look on Twitter/news websites confirms that the news
    broke by tweet on 18^(th) January 2016…close enough!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Colorful area charts**'
  prefs: []
  type: TYPE_NORMAL
- en: Try removing the option `scale_fill_grey()` from `qplot` to get some beautiful
    charts that are far easier to read than plain gray scale.
  prefs: []
  type: TYPE_NORMAL
- en: So, finally we learnt about topic modeling using LDA on data from the ISS and
    found what amazing things scientists and astronauts are doing up there in outer
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with social network data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we close the chapter, let us look at the different challenges posed
    by social networks to the process of data mining. The following points present
    a few arguments, questions, and challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: No doubt the data generated by social networks classifies as big data in every
    aspect. It has all the volume, velocity, and variety in it to overwhelm any system.
    Yet, interestingly, the challenge with such a huge source of data is the availability
    of enough granular data. If we zoom into our data sets and try to use data on
    a per user basis, we find that there isn't enough data to do some of the most
    common tasks, such as making recommendations!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social networks such as Twitter handle millions of users creating and sharing
    tons of data every second. To keep their systems up and running at all times,
    they put limits upon the amount of data that can be tapped using their APIs (security
    is also a major reason behind these limits, though). These limits put data science
    efforts in a quandary as it is difficult to obtain sufficient samples of data
    that represent the population correctly/completely. Insufficient samples may result
    in incorrect patterns or missing out on patterns altogether.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing and evaluation of results is also a challenge with social network
    analysis. While preprocessing data, we remove noisy content. With data coming
    in all shapes and sizes, determining noisy content is far more of a challenge
    than simply removing stopwords. Evaluation of results is another challenge, as
    there is no ground truth available in most cases, and due to the limitations presented
    here and otherwise, it is difficult to ascertain the validity of results with
    confidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The arguments/challenges presented above call for innovative and creative ways
    to be devised by data scientists, and that is what makes their job interesting
    and highly rewarding.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some of the well-known books on visualization are as follows :'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.amazon.in/Information-Dashboard-Design-At-Glance/dp/1938377001](http://www.amazon.in/Information-Dashboard-Design-At-Glance/dp/1938377001)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X](http://www.amazon.com/Visual-Display-Quantitative-Information/dp/096139210X)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.amazon.com/Information-Visualization-Second-Interactive-Technologies/dp/1558608192](http://www.amazon.com/Information-Visualization-Second-Interactive-Technologies/dp/1558608192)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some well known blogs on this are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tableau** **specific**: [http://www.jewelloree.com/](http://www.jewelloree.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://flowingdata.com/](http://flowingdata.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.informationisbeautiful.net/](http://www.informationisbeautiful.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://infosthetics.com/](http://infosthetics.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.visualisingdata.com/](http://www.visualisingdata.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://eagereyes.org/](https://eagereyes.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://thedailyviz.com/](http://thedailyviz.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**D3**: [https://d3js.org/](https://d3js.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social network analysis is one the trending topics in the world of data science.
    As we have seen throughout the chapter, these platforms not only provide us with
    ways to connect but they also present a unique opportunity to study human dynamics
    at a global scale. Through this chapter, we have learned some interesting techniques.
    We started off by understanding data mining in the social network context followed
    by the importance of visualizations. We focused on Twitter and understood different
    objects and APIs to manipulate them. We used various packages from R, such as
    `TwitteR` and `TM`, to connect, collect, and manipulate data for our analysis.
    We used data from Twitter to learn about frequency throughout. Finally, we presented
    some of the challenges posed by social networks words and associations, popular
    devices used by tweeple, hierarchical clustering and even touched upon topic modeling.
    We used `ggplot2` and `wordcloud` to visualize our results to the data mining
    process in general. While concluding this chapter, we are sure that by now you
    can appreciate the amazing dynamics behind these platforms and R's ability to
    analyze it all. We aren't done with `@Twitter` yet, hold on to your `#sentiments`!
  prefs: []
  type: TYPE_NORMAL
