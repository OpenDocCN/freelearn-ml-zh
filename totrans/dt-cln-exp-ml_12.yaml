- en: '*Chapter 9*: K-Nearest Neighbors, Decision Tree, Random Forest, and Gradient
    Boosted Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：K近邻、决策树、随机森林和梯度提升回归'
- en: As is true for support vector machines, K-nearest neighbors and decision tree
    models are best known as classification models. However, they can also be used
    for regression and present some advantages over classical linear regression. K-nearest
    neighbors and decision trees can handle nonlinearity well and no assumptions regarding
    the Gaussian distribution of features need to be made. Moreover, by adjusting
    our value of *k* for **K-nearest neighbors** (**KNN**) or maximal depth for decision
    trees, we can avoid fitting the training data too precisely.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于支持向量机而言，K近邻和决策树模型最著名的用途是分类模型。然而，它们也可以用于回归，并且相对于经典的线性回归，它们具有一些优势。K近邻和决策树可以很好地处理非线性，并且不需要对特征的高斯分布做出任何假设。此外，通过调整**K近邻**（**KNN**）的*k*值或决策树的最大深度，我们可以避免对训练数据拟合得太精确。
- en: This brings us back to a theme from the previous two chapters – how to increase
    model complexity, including accounting for nonlinearity, without overfitting.
    We have seen how allowing some bias can reduce variance and give us more reliable
    estimates of model performance. We will continue to explore that balance in this
    chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们回到了前两章的主题——如何在不过度拟合的情况下增加模型复杂度，包括考虑非线性。我们已经看到，允许一些偏差可以减少方差，并给我们提供更可靠的模型性能估计。我们将继续在本章中探索这种平衡。
- en: 'Specifically, we will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下主要内容：
- en: Key concepts for K-nearest neighbors regression
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K近邻回归的关键概念
- en: K-nearest neighbors regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K近邻回归
- en: Key concepts for decision tree and random forest regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树和随机森林回归的关键概念
- en: Decision tree and random forest regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树和随机森林回归
- en: Using gradient boosted regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度提升回归
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will work with the scikit-learn and `matplotlib` libraries.
    We will also work with XGBoost. You can use `pip` to install these packages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用scikit-learn和`matplotlib`库。我们还将使用XGBoost。你可以使用`pip`来安装这些包。
- en: Key concepts for K-nearest neighbors regression
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K近邻回归的关键概念
- en: Part of the appeal of the KNN algorithm is that it is quite straightforward
    and easy to interpret. For each observation where we need to predict the target,
    KNN finds the *k* training observations whose features are most similar to those
    of that observation. When the target is categorical, KNN selects the most frequent
    value of the target for the *k* training observations. (We often select an odd
    value for *k* for classification problems to avoid ties.)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法的部分吸引力在于它相当直接且易于解释。对于每个需要预测目标的观测，KNN会找到*k*个训练观测，其特征与该观测最相似。当目标是分类时，KNN会选择*k*个训练观测中最频繁的目标值。（我们通常在分类问题中选择一个奇数作为*k*，以避免平局。）
- en: When the target is numeric, KNN gives us the average value of the target for
    the *k* training observations. By *training* observation, I mean those observations
    that have known target values. No real training is done with KNN, as it is what
    is called a lazy learner. I will discuss that in more detail later in this section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是数值时，KNN会给出*k*个训练观测的目标平均值。通过*训练*观测，我指的是那些具有已知目标值的观测。KNN实际上并不进行真正的训练，因为它被称为懒惰学习器。我将在本节稍后详细讨论这一点。
- en: '*Figure 9.1* illustrates using K-nearest neighbors for classification with
    values of 1 and 3 for *k*. When *k* is 1, our new observation will be assigned
    the red label. When *k* is 3, it will be assigned blue:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.1*展示了使用K近邻进行分类，其中*k*的值为1和3：当*k*为1时，我们的新观测将被分配红色标签。当*k*为3时，它将被分配蓝色标签：'
- en: '![Figure 9.1 – K-nearest neighbors with a k of 1 and 3 ](img/B17978_09_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – K近邻，k值为1和3](img/B17978_09_001.jpg)'
- en: Figure 9.1 – K-nearest neighbors with a k of 1 and 3
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – K近邻，k值为1和3
- en: 'But what do we mean by similar, or nearest, observations? There are several
    ways to measure similarity, but a common measure is the Euclidean distance. The
    Euclidean distance is the sum of the squared difference between two points. This
    may remind you of the Pythagorean theorem. The Euclidean distance from point a
    to point b is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们所说的相似或最近的观测意味着什么？有几种方法可以衡量相似度，但一个常见的衡量标准是欧几里得距离。欧几里得距离是两点之间平方差的和。这可能会让你想起勾股定理。从点a到点b的欧几里得距离如下：
- en: '![](img/B17978_09_0011.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_09_0011.jpg)'
- en: 'A reasonable alternative to Euclidean distance is Manhattan distance. The Manhattan
    distance from point a to point b is as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离是欧几里得距离的一个合理替代方案。从点 a 到点 b 的曼哈顿距离如下：
- en: '![](img/B17978_09_002.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B17978_09_002](img/B17978_09_002.jpg)'
- en: 'Manhattan distance is sometimes called taxicab distance. This is because it
    reflects the distance between two points along a path on a grid. *Figure 9.2*
    illustrates the Manhattan distance and compares it to the Euclidean distance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离有时也被称为出租车距离。这是因为它反映了两个点在网格路径上的距离。*图 9.2* 展示了曼哈顿距离，并将其与欧几里得距离进行了比较：
- en: '![Figure 9.2 – Euclidean and Manhattan measures of distance ](img/B17978_09_0021.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 欧几里得和曼哈顿距离度量](img/B17978_09_0021.jpg)'
- en: Figure 9.2 – Euclidean and Manhattan measures of distance
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 欧几里得和曼哈顿距离度量
- en: Using Manhattan distance can yield better results when features are very different
    in terms of type or scale. However, we can treat the choice of distance measure
    as an empirical question; that is, we can try both (or other distance measures)
    and see which gives us the best-performing model. We will demonstrate this with
    a grid search in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征在类型或尺度上非常不同时，使用曼哈顿距离可以获得更好的结果。然而，我们可以将距离度量的选择视为一个经验问题；也就是说，我们可以尝试两者（或其他距离度量），并看看哪个给我们提供了性能最好的模型。我们将在下一节通过网格搜索来演示这一点。
- en: As you likely suspect, KNN models are sensitive to the choice of *k*. Lower
    values of *k* will result in a model that attempts to identify subtle distinctions
    between observations. Of course, there is a substantial risk of overfitting at
    very low values of *k*. But at higher values of *k*, our model may not be flexible
    enough. We are once again confronted with the variance-bias trade-off. Lower *k*
    values result in less bias and more variance, while high values result in the
    opposite.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所怀疑的，KNN 模型的敏感性取决于 *k* 的选择。*k* 的值较低时，模型会试图识别观测值之间的细微差别。当然，在 *k* 值非常低的情况下，过拟合的风险很大。但在
    *k* 值较高时，我们的模型可能不够灵活。我们再次面临方差-偏差权衡。较低的 *k* 值导致偏差较小而方差较大，而较高的 *k* 值则相反。
- en: There is no definitive answer to the choice of *k*. A good rule of thumb is
    to start with the square root of the number of observations. However, just as
    we would do for the distance measure, we should test a model’s performance at
    different values of *k*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *k* 的选择没有明确的答案。一个很好的经验法则是从观测值的平方根开始。然而，正如我们对距离度量的处理一样，我们应该测试模型在不同 *k* 值下的性能。
- en: K-nearest neighbors is a lazy learner algorithm, as I have already mentioned.
    No calculations are performed at training time. The learning happens mainly during
    testing. This has some disadvantages. KNN may not be a good choice when there
    are many instances or dimensions in the data, and the speed of predictions matters.
    It also tends not to perform well when we have sparse data – that is, datasets
    with many 0 values.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，K近邻是一种懒惰学习算法。在训练时间不进行任何计算。学习主要发生在测试期间。这有一些缺点。当数据中有许多实例或维度时，KNN 可能不是一个好的选择，而且预测速度很重要。它也往往在稀疏数据（即具有许多
    0 值的数据集）的情况下表现不佳。
- en: K-nearest neighbors is a non-parametric algorithm. No assumptions are made about
    the attributes of the underlying data, such as linearity or normally distributed
    features. It can often give us decent results when a linear model would not. We
    will build a KNN regression model in the next section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: K近邻是非参数算法。不对底层数据的属性做出假设，例如线性或正态分布的特征。当线性模型不起作用时，它通常能给出相当不错的结果。我们将在下一节构建 KNN
    回归模型。
- en: K-nearest neighbors regression
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K近邻回归
- en: As mentioned previously, K-nearest neighbors can be a good alternative to linear
    regression when the assumptions of ordinary least squares do not hold, and the
    number of observations and dimensions is small. It is also very easy to specify,
    so even if we do not use it for our final model, it can be valuable for diagnostic
    purposes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当普通最小二乘法的假设不成立，且观测值和维度数量较少时，K近邻算法可以成为线性回归的一个很好的替代方案。它也非常容易指定，因此即使我们最终模型中不使用它，它也可以用于诊断目的。
- en: In this section, we will use KNN to build a model of the ratio of female to
    male incomes at the level of country. We will base this on labor force participation
    rates, educational attainment, teenage birth frequency, and female participation
    in politics at the highest level. This is a good dataset to experiment with because
    the small sample size and feature space mean that it is not likely to tax your
    system’s resources. The small number of features also makes it easier to interpret.
    The drawback is that it might be hard to find significant results. That being
    said, let’s see what we find.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用KNN构建一个国家层面的女性与男性收入比模型。我们将基于劳动力参与率、教育成就、青少年出生频率和最高级别的女性政治参与率。这是一个很好的数据集进行实验，因为小样本量和特征空间意味着它不太可能消耗你的系统资源。特征数量较少也使得它更容易解释。缺点是可能难以找到显著的结果。话虽如此，让我们看看我们发现了什么。
- en: Note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be working with the income gap dataset throughout this chapter. The
    dataset has been made available for public use by the United Nations Development
    Program at [https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development).
    There is one record per country with aggregate employment, income, and education
    data by gender for 2015.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中一直使用收入差距数据集。该数据集由联合国开发计划署在[https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development)上提供供公众使用。每个国家都有一个记录，包含按性别划分的2015年的综合就业、收入和教育数据。
- en: 'Let’s start building our model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的模型：
- en: 'First, we must import some of the same `sklearn` libraries we used in the previous
    two chapters. We must also import `KNeighborsRegressor` and an old friend from
    [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058), *Feature Selection* – that
    is, `SelectFromModel`. We will use `SelectFromModel` to add feature selection
    to the pipeline we will construct:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们必须导入我们在前两章中使用的一些相同的`sklearn`库。我们还必须导入`KNeighborsRegressor`和来自[*第5章*](B17978_05_ePub.xhtml#_idTextAnchor058)的老朋友——*特征选择*——即`SelectFromModel`。我们将使用`SelectFromModel`来将特征选择添加到我们将构建的管道中：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We also need the `OutlierTrans` class that we created in [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091),
    *Linear* *Regression Models*. We will use it to identify outliers based on the
    interquartile range, as we first discussed in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要在[*第7章*](B17978_07_ePub.xhtml#_idTextAnchor091)的*线性回归模型*中创建的`OutlierTrans`类。我们将使用它根据四分位数范围来识别异常值，正如我们在[*第3章*](B17978_03_ePub.xhtml#_idTextAnchor034)的*识别和修复缺失值*中首先讨论的那样：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we must load the income data. We also need to construct a series for the
    ratio of female to male incomes, the years of education ratio, the labor force
    participation ratio, and the human development index ratio. Lower values for any
    of these measures suggest a possible advantage for males, assuming a positive
    relationship between these features and the ratio of female to male incomes. For
    example, we would expect the income ratio to improve – that is, to get closer
    to 1.0 – as the labor force participation ratio gets closer to 1.0– that is, when
    the labor force participation of women equals that of men.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须加载收入数据。我们还需要构建一个女性与男性收入比、教育年限比、劳动力参与比和人类发展指数比的序列。这些指标中的任何一项的值较低都表明男性可能具有优势，假设这些特征与女性与男性收入比之间存在正相关关系。例如，我们预计收入比会提高——也就是说，接近1.0——当劳动力参与比接近1.0时——也就是说，当女性的劳动力参与率等于男性的时候。
- en: 'We must drop rows where our target, `incomeratio`, is missing:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须删除目标`incomeratio`缺失的行：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s look at a few rows of data:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些数据行：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s also look at some descriptive statistics:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看一些描述性统计：
- en: '[PRE4]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We have 177 observations with our target variable, `incomeratio`. A couple of
    features, `humandevratio` and `genderinequality`, have more than 15 missing values.
    We will need to impute some reasonable values there. We will also need to do some
    scaling as some features have very different ranges than others, from `incomeratio`
    and `incomepercapita` on one end to `educratio` and `humandevratio` on the other.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有177个观测值和目标变量`incomeratio`。一些特征，如`humandevratio`和`genderinequality`，有超过15个缺失值。我们将在那里需要填充一些合理的值。我们还需要进行一些缩放，因为一些特征的范围与其他特征非常不同，从一端的`incomeratio`和`incomepercapita`到另一端的`educratio`和`humandevratio`。
- en: Note
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The dataset has separate human development indices for women and men. The index
    is a measure of health, access to knowledge, and standard of living. The `humandevratio`
    feature, which we calculated earlier, divides the score for women by the score
    for men. The `genderinequality` feature is an index of health and labor market
    policies in countries that have a disproportionate impact on women. `femaleperparliament`
    is the percentage of the highest national legislative body that is female.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有妇女和男人的单独人类发展指数。该指数是健康、知识获取和生活水平的衡量标准。我们之前计算的`humandevratio`特征将妇女的得分除以男人的得分。`genderinequality`特征是那些对妇女有不成比例影响的国家的健康和劳动力市场政策的指数。`femaleperparliament`是最高国家立法机构中女性所占的百分比。
- en: 'We should also look at a heatmap of the correlations of features and the features
    with the target. It is a good idea to keep the higher correlations (either negative
    or positive) in mind when we are doing our modeling. The higher positive correlations
    are represented with the warmer colors. `laborforcepartratio`, `humandevratio`,
    and `maternalmortality` are all positively correlated with our target, the latter
    somewhat surprisingly. `humandevratio` and `laborforcepartratio` are also correlated,
    so our model may have some trouble disentangling the influence of each. Some feature
    selection should help us figure out which feature is more important. (We will
    need to use a wrapper or embedded feature selection method to tease that out well.
    We discuss those methods in detail in [*Chapter 5*](B17978_05_ePub.xhtml#_idTextAnchor058),
    *Feature Selection*.) Look at the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看特征与目标之间的相关性热图。在我们建模时，记住更高的相关性（无论是负的还是正的）是一个好主意。更高的正相关用较暖的颜色表示。`laborforcepartratio`、`humandevratio`和`maternalmortality`都与我们的目标正相关，后者有些令人惊讶。`humandevratio`和`laborforcepartratio`也是相关的，因此我们的模型可能难以区分每个特征的影响。一些特征选择将帮助我们弄清楚哪个特征更重要。（我们需要使用包装器或嵌入式特征选择方法来很好地揭示这一点。我们将在[*第5章*](B17978_05_ePub.xhtml#_idTextAnchor058)，*特征选择*）中详细讨论这些方法。）看看以下代码：
- en: '[PRE5]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This produces the following plot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 9.3 – Correlation matrix ](img/B17978_09_003.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 相关系数矩阵](img/B17978_09_003.jpg)'
- en: Figure 9.3 – Correlation matrix
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 相关系数矩阵
- en: 'Next, we must set up the training and testing DataFrames:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须设置训练和测试数据框：
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are now ready to set up the KNN regression model. We will also build a pipeline
    to handle outliers, do an imputation based on the median value of each feature,
    scale features, and do some feature selection with scikit-learn’s `SelectFromModel`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好设置KNN回归模型。我们还将构建一个管道来处理异常值，基于每个特征的中间值进行插补，缩放特征，并使用scikit-learn的`SelectFromModel`进行一些特征选择：
- en: 'We will use linear regression for our feature selection, but we can choose
    any algorithm that will return feature importance values. We will set the feature
    importance threshold to 80% of the mean feature importance. The mean is the default.
    Our choice here is fairly arbitrary, but I like the idea of keeping features that
    are just below the average feature importance level, in addition to those with
    higher importance of course:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用线性回归进行特征选择，但我们可以选择任何会返回特征重要性值的算法。我们将特征重要性阈值设置为平均特征重要性的80%。平均值是默认值。我们的选择在这里相当随意，但我喜欢保留那些低于平均特征重要性水平的特征的想法，当然还有那些重要性更高的特征：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are now ready to do a grid search to find the best parameters. First, we
    will create a dictionary, `knnreg_params`, to indicate that we want the KNN model
    to select values of *k* from 3 to 19, skipping even numbers. We also want the
    grid search to find the best distance measure – Euclidean, Manhattan, or Minkowski:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好进行网格搜索以找到最佳参数。首先，我们将创建一个字典，`knnreg_params`，以表明我们希望KNN模型从3到19选择*k*的值，跳过偶数。我们还希望网格搜索找到最佳的距离度量
    – 欧几里得、曼哈顿或闵可夫斯基：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will pass those parameters to the `RandomizedSearchCV` object and then fit
    the model. We can use the `best_params_` attribute of `RandomizedSearchCV` to
    see the selected hyperparameters for our feature selection and KNN regression.
    These results suggest that the best hyperparameter values are 11 for *k* for KNN
    and Manhattan for the distance metric:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把这些参数传递给`RandomizedSearchCV`对象，然后拟合模型。我们可以使用`RandomizedSearchCV`的`best_params_`属性来查看特征选择和KNN回归选择的超参数。这些结果表明，对于KNN的*k*最佳超参数值是11，对于距离度量是曼哈顿：
- en: 'The best model has a negative mean squared error of -0.05\. This is fairly
    decent, given the small sample size. It is less than 10% of the median value of
    `incomeratio`, which is 0.6:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型具有-0.05的负均方误差。考虑到样本量小，这相当不错。它小于`incomeratio`的中位数值的10%，而`incomeratio`的中位数值为0.6：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s take a look at the features that were selected during the feature selection
    step of the pipeline. Only two features were selected – `laborforcepartratio`
    and `humandevratio`. Note that this step is not necessary to run our model. It
    just helps us interpret it:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看在管道的特征选择步骤中选定的特征。只选择了两个特征 – `laborforcepartratio` 和 `humandevratio`。请注意，这一步不是运行我们的模型的必要步骤。它只是帮助我们解释它：
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is a tad easier if you are using *scikit-learn* 1.0 or later. You can
    use the `get_feature_names_out` method in that case:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你使用的是*scikit-learn* 1.0或更高版本，这会容易一些。在这种情况下，你可以使用`get_feature_names_out`方法：
- en: '[PRE11]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We should also take a peek at some of the other top results. There is a model
    that uses `euclidean` distance that performs nearly as well as the best model:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该看看一些其他的前几名结果。有一个使用`euclidean`距离的模型，其表现几乎与最佳模型一样好：
- en: '[PRE12]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s look at the residuals for this model. We can use the predict method of
    the `RandomizedSearchCV` object to generate predictions on the testing data. The
    residuals are nicely balanced around 0\. There is a little bit of negative skew
    but that’s not bad either. There is low kurtosis, but we are good with there not
    being much in the way of tails in this case. It likely reflects not very much
    in the way of outlier residuals:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看这个模型的残差。我们可以使用`RandomizedSearchCV`对象的`predict`方法在测试数据上生成预测。残差在0附近很好地平衡。有一点负偏斜，但这也不算坏。峰度低，但我们对此没有太多尾巴的情况感到满意。这很可能反映了异常值残差不多：
- en: '[PRE13]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s plot the residuals:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制残差图：
- en: '[PRE14]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This produces the following plot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 9.4 – Residuals for the income ratio model with KNN regression ](img/B17978_09_004.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 使用KNN回归的收入的比率模型的残差](img/B17978_09_004.jpg)'
- en: Figure 9.4 – Residuals for the income ratio model with KNN regression
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 使用KNN回归的收入的比率模型的残差
- en: The residuals also look pretty decent when we plot them. There are a couple
    of countries, however, where we are more than 0.1 off in our prediction. We over-predict
    in both of those cases. (The dashed red line is the average residual amount.)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们绘制残差时，它们看起来也很不错。然而，有几个国家，我们的预测误差超过了0.1。在这两种情况下，我们都高估了。 （虚线红色线是平均残差量。）
- en: 'Let’s also look at a scatterplot. Here, we can see that the two large over-predictions
    are at different ends of the predicted range. In general, the residuals are fairly
    constant across the predicted income ratio range. We just may want to do something
    with the two outliers:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看散点图。在这里，我们可以看到两个大的高估值位于预测范围的两端。总的来说，残差在预测的收入比率范围内相对恒定。我们可能只是想对两个异常值做些处理：
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following plot:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 9.5 – Scatterplot of predictions and residuals for the income ratio
    model with KNN regression ](img/B17978_09_005.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 使用KNN回归的收入的比率模型的预测值和残差的散点图](img/B17978_09_005.jpg)'
- en: Figure 9.5 – Scatterplot of predictions and residuals for the income ratio model
    with KNN regression
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 使用KNN回归的收入的比率模型的预测值和残差的散点图
- en: 'We should take a closer look at the countries where there were high residuals.
    Our model does not do a good job of predicting income ratios for either Afghanistan
    or the Netherlands, over-predicting a fair bit in both cases. Recall that our
    feature selection step gave us a model with just two predictors: `laborforcepartratio`
    and `humandevratio`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该更仔细地看看那些残差较高的国家。我们的模型在预测阿富汗或荷兰的收入比率方面做得不好，在两种情况下都高估了很多。回想一下，我们的特征选择步骤给我们提供了一个只有两个预测因子（`laborforcepartratio`
    和 `humandevratio`）的模型。
- en: For Afghanistan, the labor force participation ratio (the participation of females
    relative to that of males) is very near the minimum of 0.19 and the human development
    ratio is at the minimum. This still does not get us close to predicting the very
    low income ratio (the income of women relative to that of men), which is also
    at the minimum.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于阿富汗，劳动力参与率（女性相对于男性的参与率）非常接近最低的0.19，而人类发展比率处于最低水平。这仍然不能使我们接近预测非常低的收入比率（女性相对于男性的收入），这个比率也是最低的。
- en: 'For the Netherlands, the labor force participation ratio of 0.83 is a fair
    bit above the median of 0.75, but the human development ratio is right at the
    median. This is why our model predicts an income ratio a little above the median
    of 0.6\. The actual income ratio for the Netherlands is, then, surprisingly low:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于荷兰来说，劳动力参与率0.83比中位数0.75高出不少，但人类发展比率正好是中位数。这就是为什么我们的模型预测的收入比率略高于0.6的中位数。荷兰的实际收入比率出人意料地低：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we can see some of the advantages of KNN regression. We can get okay predictions
    on difficult-to-model data without spending a lot of time specifying a model.
    Other than some imputation and scaling, we did not do any transformations or create
    interaction effects. We did not need to worry about nonlinearity either. KNN regression
    can handle that fine.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到KNN回归的一些优点。我们可以在不花费大量时间指定模型的情况下，对难以建模的数据获得不错的预测。除了某些插补和缩放之外，我们没有进行任何转换或创建交互效应。我们也不必担心非线性。KNN回归可以很好地处理这一点。
- en: But this approach would probably not scale very well. A lazy learner was fine
    in this example. For more industrial-level work, however, we often need to turn
    to an algorithm with many of the advantages of KNN, but without some of the disadvantages.
    We will explore decision trees and random forest regression in the remainder of
    this chapter.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法可能扩展得不是很好。在这个例子中，一个懒惰的学习者是可以的。然而，对于更工业级的工作，我们通常需要转向一个具有许多KNN优点但没有一些缺点的算法。我们将在本章的剩余部分探讨决策树和随机森林回归。
- en: Key concepts for decision tree and random forest regression
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和随机森林回归的关键概念
- en: Decision trees are an exceptionally useful machine learning tool. They have
    some of the same advantages as KNN – they are non-parametric, easy to interpret,
    and can work with a wide range of data – but without some of the limitations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一个非常有用的机器学习工具。它们具有与KNN相似的一些优点——它们是非参数的，易于解释，并且可以处理广泛的数据——但没有一些限制。
- en: 'Decision trees group the observations in a dataset based on the values of their
    features. This is done with a series of binary decisions, starting from an initial
    split at the root node, and ending with a leaf for each grouping. All observations
    with the same values, or the same range of values, along the branches from the
    root node to that leaf, get the same predicted value for the target. When the
    target is numeric, that is the average value for the target for the training observations
    at that leaf. *Figure 9.6* illustrates this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树根据数据集中观测值的特征值对观测值进行分组。这是通过一系列二元决策来完成的，从根节点的一个初始分割开始，以每个分组的叶子节点结束。所有沿着从根节点到该叶子的分支具有相同值或相同值范围的观测值，都会得到相同的预测值。当目标是数值时，这就是该叶子节点训练观测值的平均目标值。*图9.6*展示了这一点：
- en: '![Figure 9.6 – Decision tree model of nightly hours of sleep ](img/B17978_09_006.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 每晚睡眠小时数的决策树模型](img/B17978_09_006.jpg)'
- en: Figure 9.6 – Decision tree model of nightly hours of sleep
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 每晚睡眠小时数的决策树模型
- en: This is a model of nightly hours of sleep for individuals based on weekly hours
    worked, number of children, number of other adults in the home, and whether the
    person is enrolled in school. (These results are based on hypothetical data.)
    The root node is based on weekly hours worked and splits the data into observations
    with hours worked greater than 30 and 30 or less. The numbers in parentheses are
    the percentage of the training data that reaches that node. 60% of the observations
    have hours worked greater than 30\. On the left-hand side of the tree, our model
    further splits the data by the number of children and then by the number of other
    adults in the home. On the other side of the tree, which represents observations
    with hours worked less than or equal to 30, the only additional split is by enrollment
    in school.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于每周工作时间、孩子数量、家庭中其他成年人数量以及个人是否在学校注册的每周睡眠小时数的个人模型。（这些结果基于假设数据。）根节点基于每周工作时间，将数据分为工作时间超过30小时和30小时或以下的观察结果。括号中的数字是该节点达到的训练数据百分比。60%的观察结果工作时间超过30小时。在树的左侧，我们的模型进一步通过孩子数量和家中其他成年人的数量来分割数据。在树的另一侧，代表工作时间少于或等于30小时的观察结果，唯一的额外分割是通过学校注册情况。
- en: I realize now that all readers will not see this in color. We can navigate up
    the tree from each leaf to describe how the tree has segmented the data. 15% of
    observations have 0 other adults in the home, more than 1 child, and weekly hours
    worked greater than 30\. These observations have an average nightly hours slept
    value of 4.5 hours. This will be the predicted value for new observations with
    the same characteristics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在意识到，并非所有读者都能看到这个颜色。我们可以从每个叶子节点向上导航树，描述树是如何分割数据的。15%的观测值在家中没有其他成年人，有超过1个孩子，每周工作时间超过30小时。这些观测值的平均每晚睡眠时间为4.5小时。这将是对具有相同特征的新观测值的预测值。
- en: 'You might be wondering how the decision tree algorithm selected the threshold
    amounts for the numeric features. Why greater than 30 for weekly hours worked
    or greater than 1 for the number of children, for example? The algorithm selects
    the split at each level, starting with the root, which minimizes the sum of squared
    errors. More precisely, splits are chosen that minimize:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道决策树算法是如何选择数值特征的阈值。例如，为什么每周工作时间大于30小时或孩子数量大于1小时？算法从根节点开始，在每个级别上选择分割，以最小化平方误差之和。更精确地说，选择的分割可以最小化：
- en: '![](img/B17978_09_0031.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_09_0031.jpg)'
- en: You may have noticed the similarity with optimization for linear regression.
    But there are several advantages of decision tree regression over linear regression.
    Decision trees can be used to model both linear and nonlinear relationships without
    us having to modify features. We also can avoid feature scaling with decision
    trees, as the algorithm can deal with very different ranges in our features.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了与线性回归优化的相似之处。但是，决策树回归相对于线性回归有几个优点。决策树可以用来模拟线性关系和非线性关系，而无需我们修改特征。我们还可以使用决策树避免特征缩放，因为算法可以处理我们特征中的非常不同的范围。
- en: The main disadvantage of decision trees is their high variance. Depending on
    the characteristics of our data, we can get a very different model each time we
    fit a decision tree. We can use ensemble methods, such as bagging or random forest,
    to address this issue.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的主要缺点是它们的方差很高。根据我们数据的特点，每次拟合决策树时，我们可能会得到一个非常不同的模型。我们可以使用集成方法，如袋装法或随机森林，来解决这个问题。
- en: Using random forest regression
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机森林回归
- en: Random forests, perhaps not surprisingly, are collections of decision trees.
    But this would not distinguish a random forest from bootstrap aggregating, commonly
    referred to as bagging. Bagging is often used to reduce the variance of machine
    learning algorithms that have high variances, such as decision trees. With bagging,
    we generate random samples from our dataset. Then, we run our model, such as a
    decision tree regression, on each of those samples, averaging the predictions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林，可能不出所料，是决策树的集合。但这并不能区分随机森林和自助聚合，通常称为袋装法。袋装法通常用于减少具有高方差的机器学习算法的方差，如决策树。在袋装法中，我们从数据集中生成随机样本。然后，我们在每个样本上运行我们的模型，例如决策树回归，并对预测进行平均。
- en: However, the samples generated with bagging can be correlated, and the resulting
    decision trees may have many similarities. This is more likely to be the case
    when there are just a few features that explain much of the variation. Random
    forests address this issue by limiting the number of features that can be selected
    for each tree. A good rule of thumb is to divide the number of features available
    by 3 to determine the number of features to use for each split for each decision
    tree. For example, if there are 21 features, we would use seven for each split.
    We will build both decision tree and random forest regression models in the next
    section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用袋装法生成的样本可能相关，并且产生的决策树可能有很多相似之处。这种情况在只有少数特征可以解释大部分变化时更为可能。随机森林通过限制每个树可以选定的特征数量来解决此问题。一个很好的经验法则是将可用特征的数量除以3，以确定每个决策树每个分割要使用的特征数量。例如，如果有21个特征，我们会在每个分割中使用7个。我们将在下一节中构建决策树和随机森林回归模型。
- en: Decision tree and random forest regression
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树和随机森林回归
- en: 'We will use a decision tree and a random forest in this section to build a
    regression model with the same income gap data we worked with earlier in this
    chapter. We will also use tuning to identify the hyperparameters that give us
    the best-performing model, just as we did with KNN regression. Let’s get started:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用决策树和随机森林构建一个回归模型，使用与本章前面相同的工作收入差距数据。 我们还将使用调整来识别给我们带来最佳性能模型的超参数，就像我们在KNN回归中所做的那样。
    让我们开始吧：
- en: 'We must load many of the same libraries as we did with KNN regression, plus
    `DecisionTreeRegressor` and `RandomForestRegressor` from scikit-learn:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须加载与KNN回归相同的许多库，以及来自scikit-learn的`DecisionTreeRegressor`和`RandomForestRegressor`：
- en: '[PRE17]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We must also import our class for handling outliers:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还必须导入我们用于处理异常值的类：
- en: '[PRE18]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We must load the same income gap data that we worked with previously and create
    testing and training DataFrames:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须加载之前使用过的相同收入差距数据，并创建测试和训练数据框：
- en: '[PRE19]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Let’s start with a relatively simple decision tree– one without too many levels.
    A simple tree can easily be shown on one page.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从相对简单的决策树开始 – 一个没有太多层级的树。 一个简单的树可以很容易地在一页上展示。
- en: A decision tree example with interpretation
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有解释的决策树示例
- en: 'Before we build our decision tree regressor, let’s just look at a quick example
    with maximum depth set to a low value. Decision trees are more difficult to explain
    and plot as the depth increases. Let’s get started:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建决策树回归器之前，让我们先看看一个最大深度设置为低值的快速示例。 随着深度的增加，决策树解释和绘图变得更加困难。 让我们开始吧：
- en: 'We start by instantiating a decision tree regressor, limiting the depth to
    three, and requiring that each leaf has at least five observations. We create
    a pipeline that only preprocesses the data and passes the resulting NumPy array,
    `X_train_imp`, to the `fit` method of the decision tree regressor:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先实例化一个决策树回归器，限制深度为三，并要求每个叶节点至少有五个观测值。 我们创建一个仅预处理数据的管道，并将结果NumPy数组`X_train_imp`传递给决策树回归器的`fit`方法：
- en: '[PRE20]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This generates the following plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 9.7 – Decision tree example with a maximum depth of 3 ](img/B17978_09_007.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 最大深度为3的决策树示例](img/B17978_09_007.jpg)'
- en: Figure 9.7 – Decision tree example with a maximum depth of 3
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 最大深度为3的决策树示例
- en: 'We will not go over all nodes on this tree. We can get the general idea of
    how to interpret a decision tree regression plot by describing the path down to
    a couple of leaf nodes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会遍历这棵树上的所有节点。 通过描述通往几个叶节点的路径，我们可以了解如何解释决策树回归图的一般方法：
- en: '**Interpreting the leaf node with labor force participation ratio <= 0.307:**'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释劳动力参与率 <= 0.307的叶节点：**'
- en: The root node split is based on labor force participation ratios less than or
    equal to 0.601\. (Recall that the labor force participation ratio is the ratio
    of female participation rates to male participation rates.) 34 countries fall
    into that category. (True values for the split test are to the left. False values
    are to the right.) There is another split after that that is also based on the
    labor force participation ratio, this time with the split at 0.378. There are
    13 countries with values less than or equal to that. Finally, we reach the leaf
    node furthest to the left for countries with a labor force participation ratio
    less than or equal to 0.307\. Six countries have labor force participation ratios
    that low. Those six countries have an average income ratio of 0.197\. Our decision
    tree regressor would then predict 0.197 for the income ratio for testing instances
    with labor force participation ratios less than or equal to 0.307.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点分割是基于劳动力参与率小于或等于0.601的。 (回想一下，劳动力参与率是女性参与率与男性参与率的比率。) 有34个国家属于这一类别。 (分割测试的真实值在左侧。虚假值在右侧。)
    之后还有另一个基于劳动力参与率的分割，这次分割点为0.378。 有13个国家的值小于或等于这个值。 最后，我们到达了劳动力参与率小于或等于0.307的最左侧的叶节点。
    有六个国家的劳动力参与率如此之低。 这六个国家的平均收入比率为0.197。 然后，我们的决策树回归器将预测劳动力参与率小于或等于0.307的测试实例的收入比率为0.197。
- en: '**Interpreting the leaf node with labor force participation ratio between 0.601
    and 0.811, and humandevratio <= 0.968:**'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释劳动力参与率在0.601到0.811之间，且humandevratio <= 0.968的叶节点：**'
- en: There are 107 countries with labor force participation ratios greater than 0.601\.
    This is shown on the right-hand side of the tree. There is another binary split
    when the labor force participation ratio is less than or equal to 0.811, which
    is split further based on the human development ratio being less than or equal
    to 0.968\. This takes us to a leaf node that has 31 countries, those with human
    development ratio less than or equal to 0.968, and a labor force participation
    ratio less than or equal to 0.811, but greater than 0.601\. The decision tree
    regressor would predict the average value for income ratio for those 31 countries,
    0.556, for all testing instances with values for human development ratio and labor
    force participation ratio in those ranges.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 有107个国家的劳动力参与率大于0.601。这显示在树的右侧。当劳动力参与率小于或等于0.811时，还有一个二进制分割，进一步基于人类发展比率小于或等于0.968进行分割。这带我们到一个有31个国家的叶子节点，这些国家的人类发展比率小于或等于0.968，劳动力参与率小于或等于0.811，但大于0.601。决策树回归器将预测这31个国家的收入比率平均值，为0.556，对于所有测试实例，其人类发展比率和劳动力参与率的值都在这些范围内。
- en: 'Interestingly, we have not done any feature selection yet, but this first effort
    to build a decision tree model already suggests that income ratio can be predicted
    with just two features: `laborforcepartratio` and `humandevratio`.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们还没有进行任何特征选择，但这次构建决策树模型的初步尝试已经表明，仅用两个特征就可以预测收入比率：`laborforcepartratio`和`humandevratio`。
- en: Although the simplicity of this model makes it very easy to interpret, we have
    not done the work we need to do to find the best hyperparameters yet. Let’s do
    that next.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个模型的简单性使得它非常容易解释，但我们还没有完成寻找最佳超参数所需的工作。让我们接下来做这件事。
- en: Building and interpreting our actual model
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和解释我们的实际模型
- en: 'Follow these steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行：
- en: 'First, we instantiate a new decision tree regressor and create a pipeline that
    uses it. We also create a dictionary for some of the hyperparameters– that is,
    for the maximum tree depth and the minimum number of samples (observations) for
    each leaf. Notice that we do not need to scale either our features or the target,
    as that is not necessary with a decision tree:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们实例化一个新的决策树回归器并创建一个使用它的管道。我们还为一些超参数创建了一个字典——也就是说，对于最大树深度和每个叶子的最小样本数（观察值）。请注意，我们不需要对我们的特征或目标进行缩放，因为在决策树中这不是必要的：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we must set up a randomized search based on the dictionary from the previous
    step. The best parameters for our decision tree are minimum samples of 5 and a
    maximum depth of 9:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须根据上一步的字典设置一个随机搜索。我们的决策树的最佳参数是5个最小样本和9的最大深度：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we discussed in the previous section, decision trees have many of the advantages
    of KNN for regression. They are easy to interpret and do not make many assumptions
    about the underlying data. However, decision trees can still work reasonably well
    with large datasets. A less important, but still helpful, advantage of decision
    trees is that they do not require feature scaling.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中讨论的，决策树在回归方面具有许多KNN的优点。它们容易解释，并且对底层数据没有太多假设。然而，决策树仍然可以与大型数据集合理地工作。决策树的一个不那么重要但仍然有用的优点是，它们不需要特征缩放。
- en: But decision trees do have high variance. It is often worth sacrificing the
    interpretability of decision trees for a related method, such as random forest,
    which can substantially reduce that variance. We discussed the random forest algorithm
    conceptually in the previous section. We’ll try it out with the income gap data
    in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 但决策树确实有高方差。通常值得牺牲决策树的可解释性以换取相关方法，例如随机森林，这可以显著减少方差。我们在上一节中概念性地讨论了随机森林算法。我们将在下一节中使用收入差距数据尝试它。
- en: Random forest regression
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: 'Recall that random forests can be thought of as decision trees with bagging;
    they improve bagging by reducing the correlation between samples. This sounds
    complicated but it is as easy to implement as decision trees are. Let’s take a
    look:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，随机森林可以被视为具有袋装法的决策树；它们通过减少样本之间的相关性来改进袋装法。这听起来很复杂，但它的实现与决策树一样简单。让我们看看：
- en: 'We will start by instantiating a random forest regressor and creating a dictionary
    for the hyperparameters. We will also create a pipeline for the pre-processing
    and the regressor:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先实例化一个随机森林回归器并为超参数创建一个字典。我们还将为预处理和回归器创建一个管道：
- en: '[PRE23]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will pass the pipeline and the hyperparameter dictionary to the `RandomizedSearchCV`
    object to run the grid search. There is a minor improvement in terms of the score:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把管道和超参数字典传递给`RandomizedSearchCV`对象以运行网格搜索。在得分方面有轻微的改进：
- en: '[PRE24]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s take a look at the residuals:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看残差：
- en: '[PRE25]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following plot:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 9.8 – Histogram of residuals for the random forest model on income
    ratio ](img/B17978_09_008.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 随机森林模型在收入比率上的残差直方图](img/B17978_09_008.jpg)'
- en: Figure 9.8 – Histogram of residuals for the random forest model on income ratio
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 随机森林模型在收入比率上的残差直方图
- en: 'Let’s also take a look at a scatterplot of residuals by predictions:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看残差与预测的散点图：
- en: '[PRE26]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This produces the following plot:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 9.9 – Scatterplot of predictions by residuals for the random forest
    model on income ratio ](img/B17978_09_009.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图9.9 – 随机森林模型在收入比率上的预测与残差散点图](img/B17978_09_009.jpg)'
- en: Figure 9.9 – Scatterplot of predictions by residuals for the random forest model
    on income ratio
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 – 随机森林模型在收入比率上的预测与残差散点图
- en: 'Let’s take a look a closer look at the one significant outlier where we are
    seriously over-predicting:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们仔细看看一个显著的异常值，我们在这里严重高估了：
- en: '[PRE27]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We still have trouble with the Netherlands, but the fairly even distribution
    of residuals suggests that this is anomalous. It is actually good news, in terms
    of our ability to predict an income ratio for new instances, showing that our
    model is not working too hard to fit this unusual case.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在荷兰仍然有困难，但残差的相对均匀分布表明这是异常的。实际上，这是一个好消息，从我们预测新实例收入比率的能力来看，表明我们的模型并没有过于努力地拟合这个不寻常的案例。
- en: Using gradient boosted regression
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度提升回归
- en: We can sometimes improve upon random forest models by using gradient boosting
    instead. Similar to random forests, gradient boosting is an ensemble method that
    combines learners, typically trees. But unlike random forests, each tree is built
    to learn from the errors of previous trees. This can significantly improve our
    ability to model complexity.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时可以通过使用梯度提升来改进随机森林模型。与随机森林类似，梯度提升是一种集成方法，它结合了学习者，通常是树。但与随机森林不同，每棵树都是根据前一棵树的错误来学习的。这可以显著提高我们建模复杂性的能力。
- en: Although gradient boosting is not particularly prone to overfitting, we have
    to be even more careful with our hyperparameter tuning than we have to be with
    random forest models. We can slow the learning rate, also known as shrinkage.
    We can also adjust the number of estimators (trees). The choice of learning rate
    influences the number of estimators needed. Typically, if we slow the learning
    rate, our model will require more estimators.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然梯度提升不太容易过拟合，但我们必须比随机森林模型更加小心地进行超参数调整。我们可以减慢学习率，也称为收缩。我们还可以调整估计器的数量（树的数量）。学习率的选择影响所需估计器的数量。通常，如果我们减慢学习率，我们的模型将需要更多的估计器。
- en: 'There are several tools for implementing gradient boosting. We will work with
    two of them: gradient boosted regression from scikit-learn and XGBoost.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种工具可以用于实现梯度提升。我们将使用其中的两个：来自scikit-learn的梯度提升回归和XGBoost。
- en: We will work with data on housing prices in this section. We will try to predict
    housing prices in Kings County in Washington State in the United States, based
    on the characteristics of the home and of nearby homes.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理房价数据。我们将尝试根据房屋及其附近房屋的特征，预测华盛顿州金斯县的房价。
- en: Note
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset on housing prices in Kings County can be downloaded by the public
    at [https://www.kaggle.com/datasets/harlfoxem/housesalesprediction](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction).
    It has several bedrooms, bathrooms, and floors, the square feet of the home and
    the lot, the condition of the home, the square feet of the 15 nearest homes, and
    more as features.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集关于金斯县的房价可以由公众在[https://www.kaggle.com/datasets/harlfoxem/housesalesprediction](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction)下载。它包含多个卧室、浴室和楼层，房屋和地块的平方英尺，房屋状况，15个最近房屋的平方英尺，以及更多作为特征。
- en: 'Let’s start working on the model:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建模型：
- en: 'We will start by importing the modules we will need. The two new ones are `GradientBoostingRegressor`
    and `XGBRegressor` from XGBoost:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入所需的模块。其中两个新模块来自XGBoost，分别是`GradientBoostingRegressor`和`XGBRegressor`：
- en: '[PRE28]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let’s load the housing data and look at a few instances:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载房价数据并查看一些实例：
- en: '[PRE29]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We should also look at some descriptive statistics. We do not have any missing
    values. Our target variable, `price`, has some extreme values, not surprisingly.
    This will probably present a problem for modeling. We also need to handle some
    extreme values for our features:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些描述性统计。我们没有缺失值。我们的目标变量`price`有一些极端值，不出所料。这可能会在建模中引起问题。我们还需要处理一些特征的极端值：
- en: '[PRE30]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s create a histogram of housing prices:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个房价直方图：
- en: '[PRE31]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This generates the following plot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 9.10 – Histogram of housing prices ](img/B17978_09_010.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 房价直方图](img/B17978_09_010.jpg)'
- en: Figure 9.10 – Histogram of housing prices
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 房价直方图
- en: We may have better luck if we use a log transformation of our target for our
    modeling, as we tried in [*Chapter 4*](B17978_04_ePub.xhtml#_idTextAnchor043)*,
    Encoding, Transforming, and Scaling Features* with the COVID total cases data.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们使用目标变量的对数变换来进行建模，可能会更有运气，正如我们在[*第4章*](B17978_04_ePub.xhtml#_idTextAnchor043)*，编码、转换和缩放特征*中尝试的那样，使用COVID总病例数据。
- en: '[PRE32]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This produces the following plot:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 9.11 – Histogram of the housing price log ](img/B17978_09_011.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图9.11 – 房价对数直方图](img/B17978_09_011.jpg)'
- en: Figure 9.11 – Histogram of the housing price log
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 房价对数直方图
- en: 'This looks better. Let’s take a look at the skew and kurtosis for both the
    price and price log. The log looks like a big improvement:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这看起来更好。让我们看看价格和价格对数的偏度和峰度。对数看起来有很大的改进：
- en: '[PRE33]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We should also look at some correlations. The square feet of the living area,
    the square feet of the area above the ground level, the square feet of the living
    area of the nearest 15 homes, and the number of bathrooms are the features that
    are most correlated with price. The square feet of the living area and the square
    feet of the living area above ground level are very highly correlated. We will
    likely need to decide between one or the other in our model:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些相关性。居住面积平方英尺、地面以上面积平方英尺、最近15个家庭的居住面积平方英尺以及浴室数量是与价格最相关的特征。居住面积平方英尺和地面以上居住面积平方英尺高度相关。我们可能需要在模型中在这两者之间做出选择：
- en: '[PRE34]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This produces the following plot:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 9.12 – Correlation matrix of the housing features ](img/B17978_09_012.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – 房地产特征的相关矩阵](img/B17978_09_012.jpg)'
- en: Figure 9.12 – Correlation matrix of the housing features
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 房地产特征的相关矩阵
- en: 'Next, we create training and testing DataFrames:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建训练和测试数据框：
- en: '[PRE35]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We also need to set up our column transformations. For all of the numeric features,
    which is every feature except for `waterfront`, we will check for extreme values
    and then scale the data:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要设置我们的列转换。对于所有数值特征，即除了`waterfront`之外的所有特征，我们将检查极端值，然后缩放数据：
- en: '[PRE36]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, we are ready to set up a pipeline for our pre-processing and our model.
    We will instantiate a `GradientBoostingRegressor` object and set up feature selection.
    We will also create a dictionary of hyperparameters to use in the randomized grid
    search we will do in the next step:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好设置预处理和模型的管道。我们将实例化一个`GradientBoostingRegressor`对象并设置特征选择。我们还将创建一个超参数字典，用于我们在下一步中进行的随机网格搜索：
- en: '[PRE37]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we are ready to run a randomized grid search. We get a pretty decent mean
    squared error score, given that the average for `price_log` is about 13:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好运行随机网格搜索。考虑到`price_log`的平均值约为13，我们得到了相当不错的均方误差分数：
- en: '[PRE38]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Unfortunately, the mean fit time was quite long:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不幸的是，平均拟合时间相当长：
- en: '[PRE39]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let’s try XGBoost instead:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用XGBoost：
- en: '[PRE40]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We do not get a better score, but the mean fit time and score time have improved
    dramatically:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们没有获得更好的分数，但平均拟合时间和分数时间有了显著提高：
- en: '[PRE41]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: XGBoost has become a very popular gradient boosting tool for many reasons, some
    of which you have seen in this example. It can produce very good results, very
    quickly, with little model specification. We do need to carefully tune our hyperparameters
    to get the preferred variance-bias trade-off, but this is also true with other
    gradient boosting tools, as we have seen.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost由于许多原因已经成为一个非常受欢迎的梯度提升工具，其中一些你已经在本例中看到了。它可以快速产生非常好的结果，而且模型指定很少。我们确实需要仔细调整我们的超参数以获得首选的方差-偏差权衡，但这也适用于其他梯度提升工具，正如我们所看到的。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored some of the most popular non-parametric regression
    algorithms: K-nearest neighbors, decision trees, and random forests. Models built
    with these algorithms can perform well, with a few limitations. We discussed some
    of the advantages and limitations of each of these techniques, including dimension
    and observation limits, as well as concerns about the time required for training,
    for KNN models. We discussed the key challenge with decision trees, which is high
    variance, but also how that can be addressed by a random forest model. We explored
    gradient boosted regression trees as well and discussed some of their advantages.
    We continued to improve our skills regarding hyperparameter tuning since each
    algorithm required a somewhat different strategy.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了最受欢迎的一些非参数回归算法：K最近邻算法、决策树和随机森林。使用这些算法构建的模型可以表现良好，但也存在一些限制。我们讨论了这些技术的优缺点，包括维度和观测限制，以及对于KNN模型所需训练时间的担忧。我们讨论了决策树的关键挑战，即高方差，以及随机森林模型如何解决这一问题。我们还探讨了梯度提升回归树，并讨论了它们的一些优点。我们继续提高我们在超参数调整方面的技能，因为每个算法都需要一种不同的策略。
- en: We discuss supervised learning algorithms where the target is categorical over
    the next few chapters, starting with perhaps the most familiar classification
    algorithm, logistic regression.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将讨论监督学习算法，其中目标变量是分类的。首先，我们将从可能最熟悉的分类算法——逻辑回归开始讨论。
