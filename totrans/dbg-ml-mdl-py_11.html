<html><head></head><body>
<div id="_idContainer107">
<h1 class="chapter-number" id="_idParaDest-186"><a id="_idTextAnchor300"/><span class="koboSpan" id="kobo.1.1">11</span></h1>
<h1 id="_idParaDest-187"><a id="_idTextAnchor301"/><span class="koboSpan" id="kobo.2.1">Avoiding and Detecting Data and Concept Drifts</span></h1>
<p><span class="koboSpan" id="kobo.3.1">We talked about the effect of data and concept drifts in machine learning modeling in </span><a href="B16369_09.xhtml#_idTextAnchor261"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.5.1">, </span><em class="italic"><span class="koboSpan" id="kobo.6.1">Testing and Debugging for Production</span></em><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">In this chapter, we want to go deeper into these concepts and practice detecting drifts </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">Here, you will learn about the importance of concepts we introduced earlier, such as model versioning and model monitoring, to avoid drifts and practice with some of the Python libraries for </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">drift detection.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In this chapter, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Avoiding drifts in </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">your models</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.15.1">Detecting drifts</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.16.1">By the end of this chapter, you will be able to detect drifts in your machine learning models in Python and have reliable models </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">in production.</span></span></p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor302"/><span class="koboSpan" id="kobo.18.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.19.1">The following requirements apply to this chapter as they help you better understand the concepts, allow you to use them in your projects, and to practice with the </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">provided code:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.21.1">Python library requirements are </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">as follows:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">sklearn</span></strong><span class="koboSpan" id="kobo.24.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">1.2.2</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">numpy</span></strong><span class="koboSpan" id="kobo.27.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">1.22.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">pandas</span></strong><span class="koboSpan" id="kobo.30.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">1.4.4</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">alibi_detect</span></strong><span class="koboSpan" id="kobo.33.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">0.11.1</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">lightgbm</span></strong><span class="koboSpan" id="kobo.36.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">3.3.5</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">evidently</span></strong><span class="koboSpan" id="kobo.39.1"> &gt;= </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">0.2.8</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.41.1">Understanding of the following </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">is required:</span></span><ul><li><span class="koboSpan" id="kobo.43.1">Data and </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">concept drift</span></span></li><li><span class="koboSpan" id="kobo.45.1">Data and </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">model versioning</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.47.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">at </span></span><a href="https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter11"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter11</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.50.1">.</span></span></p>
<h1 id="_idParaDest-189"><a id="_idTextAnchor303"/><span class="koboSpan" id="kobo.51.1">Avoiding drifts in your models</span></h1>
<p><span class="koboSpan" id="kobo.52.1">Data and concept drifts</span><a id="_idIndexMarker613"/><span class="koboSpan" id="kobo.53.1"> challenge the reliability of machine learning models in production. </span><span class="koboSpan" id="kobo.53.2">Drifts in our machine learning projects can have different characteristics. </span><span class="koboSpan" id="kobo.53.3">Some of these characteristics that could help you to detect drifts in your projects and plan to resolve them are </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Magnitude</span></strong><span class="koboSpan" id="kobo.56.1">: We might face magnitudes</span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.57.1"> of difference across the data distribution that result in drift in our machine learning models. </span><span class="koboSpan" id="kobo.57.2">Small changes in the data distribution may be difficult to detect, while large changes may be </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">more noticeable.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.59.1">Frequency</span></strong><span class="koboSpan" id="kobo.60.1">: Drifts might occur in </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">different frequencies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.62.1">Gradual versus sudden</span></strong><span class="koboSpan" id="kobo.63.1">: Data drift can occur gradually where changes in the data distribution happen slowly over time, or it can occur suddenly where changes happen quickly </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">and unexpectedly.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Predictability</span></strong><span class="koboSpan" id="kobo.66.1">: Some types of drift may be predictable, such as changes that occur seasonally or due to external events. </span><span class="koboSpan" id="kobo.66.2">Other types of drift may be unpredictable, such as sudden changes in consumer behavior or </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">market trends.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Intentionality</span></strong><span class="koboSpan" id="kobo.69.1">: Drift can be intentional, such as changes made to the data generation process, or unintentional, such as changes that occur naturally </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">over time.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.71.1">We need to use techniques and practices that help us avoid the occurrence and pile-up of drifts in our machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">modeling projects.</span></span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor304"/><span class="koboSpan" id="kobo.73.1">Avoiding data drift</span></h2>
<p><span class="koboSpan" id="kobo.74.1">Having access to different versions</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.75.1"> of the data in different stages of the machine learning life cycle of our models can help us to better detect drift by comparing the data in training and production, assessing data processing pre-training, or identifying data selection criteria that could have caused drift. </span><span class="koboSpan" id="kobo.75.2">Model monitoring also helps us to identify drifts early on and </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">avoid pile-up.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">Let’s practice drift monitoring by simply checking the mean of the distribution of features between versions of data used for model training, and the new data in production. </span><span class="koboSpan" id="kobo.77.2">We will first define a class to monitor for data drift. </span><span class="koboSpan" id="kobo.77.3">Here, we consider drift in a feature if the difference between the mean of the distributions between the two versions of the data is bigger </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">than 0.1:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.79.1">
class DataDriftMonitor:    def __init__(self, baseline_data: np.array,
        threshold_mean: float = 0.1):
            self.baseline = self.calculate_statistics(
                baseline_data)
            self.threshold_mean = threshold_mean
    def calculate_statistics(self, data: np.array):
        return np.mean(data, axis=0)
    def assess_drift(self, current_data: np.array):
        current_stats = self.calculate_statistics(
            current_data)
        drift_detected = False
        for feature in range(0, len(current_stats)):
            baseline_stat = self.baseline[feature]
            current_stat = current_stats[feature]
            if np.abs(current_stat - baseline_stat) &gt; self.threshold_mean:
                drift_detected = True
                print('Feature id with drift:
                    {}'.format(feature))
                print('Mean of original distribution:
                    {}'.format(baseline_stat))
                print('Mean of new distribution:
                    {}'.format(current_stat))
                break
        return drift_detected</span></pre>
<p><span class="koboSpan" id="kobo.80.1">Then, we use it to identify</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.81.1"> drift between two </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">synthetic datasets:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.83.1">
np.random.seed(23)# Generating a synthetic dataset, as the original data, with 100 datapoints and 5 features
# from a normal distribution centered around 0 with std of 1
baseline_data = np.random.normal(loc=0, scale=1,
    size=(100, 5))
# Create a DataDriftMonitor instance
monitor = DataDriftMonitor(baseline_data,
    threshold_mean=0.1)
# Generating a synthetic dataset, as the original data, with 100 datapoints and 5 features from a normal distribution #centered around 0.2 with std of 1
current_data = np.random.normal(loc=0.15, scale=1,
    size=(100, 5))
# Assess data drift
drift_detected = monitor.assess_drift(current_data)
if drift_detected:
    print("Data drift detected.")
else:
    print("No data drift detected.")</span></pre>
<p><span class="koboSpan" id="kobo.84.1">This generates</span><a id="_idIndexMarker617"/> <span class="No-Break"><span class="koboSpan" id="kobo.85.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.86.1">
Feature id with drift: 1Mean of original distribution: -0.09990597519469419
Mean of new distribution: 0.09662442557421645
Data drift detected.</span></pre>
<h2 id="_idParaDest-191"><a id="_idTextAnchor305"/><span class="koboSpan" id="kobo.87.1">Addressing concept drift</span></h2>
<p><span class="koboSpan" id="kobo.88.1">We can similarly</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.89.1"> define classes and functions with criteria to detect concept drift, as we practiced for data drift detection. </span><span class="koboSpan" id="kobo.89.2">But we can also check, either programmatically or as part of quality assurance when bringing our machine learning models into production, for external factors that might cause concept drift such as environmental factors, changes in institutional or governmental policies, et cetera. </span><span class="koboSpan" id="kobo.89.3">In addition to monitoring the data, we can benefit from feature engineering to select features that are more robust to concept drift or ensemble models to be adapted dynamically in case of </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">concept </span><a id="_idTextAnchor306"/><span class="koboSpan" id="kobo.91.1">drift.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">Although avoiding drift in our models is ideal, we need to be ready to detect and eliminate it in practice. </span><span class="koboSpan" id="kobo.92.2">Next, you will learn techniques to detect drift in your model. </span><span class="koboSpan" id="kobo.92.3">From a practical perspective, avoiding and detecting drifts in your model are very similar. </span><span class="koboSpan" id="kobo.92.4">But there are better</span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.93.1"> techniques than simply checking the mean of feature distributions (as we used for avoiding data drift in this section) that we will practice in the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">next se</span><a id="_idTextAnchor307"/><span class="koboSpan" id="kobo.95.1">ction.</span></span></p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor308"/><span class="koboSpan" id="kobo.96.1">Detecting drifts</span></h1>
<p><span class="koboSpan" id="kobo.97.1">Avoiding drifts</span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.98.1"> altogether in all our models is not possible, but we can aim to detect them early on and eliminate them. </span><span class="koboSpan" id="kobo.98.2">Here, we are going to practice drift detection with </span><strong class="source-inline"><span class="koboSpan" id="kobo.99.1">alibi_detect</span></strong><span class="koboSpan" id="kobo.100.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.101.1">evidently</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.102.1">in </span><a id="_idTextAnchor309"/><span class="koboSpan" id="kobo.103.1">Python.</span></span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor310"/><span class="koboSpan" id="kobo.104.1">Practicing with alibi_detect for drift detection</span></h2>
<p><span class="koboSpan" id="kobo.105.1">One of the widely-used Python libraries</span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.106.1"> for drift detection</span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.107.1"> that we want to practice with is </span><strong class="source-inline"><span class="koboSpan" id="kobo.108.1">alibi_detect</span></strong><span class="koboSpan" id="kobo.109.1">. </span><span class="koboSpan" id="kobo.109.2">We will first import the necessary Python functions and classes and generate a synthetic dataset with 10 features and 10,000 samples using </span><strong class="source-inline"><span class="koboSpan" id="kobo.110.1">make_classification</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.111.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.112.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.114.1">
import numpy as npimport pandas as pd
import lightgbm as lgb
from alibi_detect.cd import KSDrift
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import balanced_accuracy_score as bacc
# Generate synthetic data
X, y = make_classification(n_samples=10000, n_features=10,
    n_classes=2, random_state=42)</span></pre>
<p><span class="koboSpan" id="kobo.115.1">Then, we split the data into train and </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">test sets:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.117.1">
# Split into train and test setsX_train, X_test, y_train, y_test = train_test_split(X, y,
    test_size=0.2, random_state=42)</span></pre>
<p><span class="koboSpan" id="kobo.118.1">Then, we train a </span><strong class="source-inline"><span class="koboSpan" id="kobo.119.1">LightGBM</span></strong><span class="koboSpan" id="kobo.120.1"> classifier</span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.121.1"> on the </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">training</span></span><span class="No-Break"><a id="_idIndexMarker624"/></span><span class="No-Break"><span class="koboSpan" id="kobo.123.1"> data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.124.1">
train_data = lgb.Dataset(X_train, label=y_train)params = {
    "objective": "binary",
    "metric": "binary_logloss",
    "boosting_type": "gbdt"
}
clf = lgb.train(train_set = train_data, params = params,
    num_boost_round=100)</span></pre>
<p><span class="koboSpan" id="kobo.125.1">We now evaluate the performance of the model on the test set and define a test label DataFrame to use for </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">drift detection:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.127.1">
# Predict on the test sety_pred = clf.predict(X_test)
y_pred = [1 if iter &gt; 0.5 else 0 for iter in y_pred]
# Calculate the balanced accuracy of the predictions
balanced_accuracy = bacc(y_test, y_pred)
print('Balanced accuracy on the synthetic test set:
    {}'.format(balanced_accuracy))
# Create a DataFrame from the test data and predictions
df = pd.DataFrame(X_test,
    columns=[f"feature_{i}" for i in range(10)])
df["actual"] = y_test
df["predicted"] = y_pred</span></pre>
<p><span class="koboSpan" id="kobo.128.1">Now, we use the defined DataFrame of predictions and actual labels of the test data points to detect drift. </span><span class="koboSpan" id="kobo.128.2">We initialize the </span><strong class="source-inline"><span class="koboSpan" id="kobo.129.1">KSDrift</span></strong><span class="koboSpan" id="kobo.130.1"> detector from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.131.1">alibi_detect</span></strong><span class="koboSpan" id="kobo.132.1"> package and fit it onto the training data. </span><span class="koboSpan" id="kobo.132.2">We use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">predict</span></strong><span class="koboSpan" id="kobo.134.1"> method of the detector to calculate the drift scores and p-values on the test data. </span><span class="koboSpan" id="kobo.134.2">The drift scores indicate the level of drift for each feature, while the p-values</span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.135.1"> indicate the statistical significance of the drift. </span><span class="koboSpan" id="kobo.135.2">If any of the drift</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.136.1"> scores or p-values are above a certain threshold, we may consider the model to be experiencing drift and take appropriate action, such as retraining the model with </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">updated data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.138.1">
# Initialize the KSDrift detectordrift_detector = KSDrift(X_train)
# Calculate the drift scores and p-values
drift_scores = drift_detector.predict(X_test)
p_values = drift_detector.predict(X_test,
    return_p_val=True)
# Print the drift scores and p-values
print("Drift scores:")
print(drift_scores)
print("P-values:")
print(p_values)</span></pre>
<p><span class="koboSpan" id="kobo.139.1">Here are the resulting drift scores and p-values. </span><span class="koboSpan" id="kobo.139.2">As all the p-values are greater than 0.1, and considering the threshold</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.140.1"> is 0.005, we can say that no drift is detected</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.141.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">this case:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.143.1">
Drift scores:{'data': {'is_drift': 0, 'distance': array([0.02825 , 0.024625, 0.0225  , 0.01275 , 0.014   , 0.017125,0.01775 , 0.015125, 0.021375, 0.014625], dtype=float32), 'p_val': array([0.15258548, 0.28180763, 0.38703775, 0.95421314, 0.907967 ,0.72927415, 0.68762517, 0.8520056 , 0.45154762, 0.87837887],dtype=float32), 'threshold': 0.005}, 'meta': {'name': 'KSDrift', 'online': False, 'data_type': None, 'version': '0.11.1', 'detector_type': 'drift'}}
P-values:
{'data': {'is_drift': 0, 'distance': array([0.02825 , 0.024625, 0.0225  , 0.01275 , 0.014   , 0.017125,0.01775 , 0.015125, 0.021375, 0.014625], dtype=float32), 'p_val': array([0.15258548, 0.28180763, 0.38703775, 0.95421314, 0.907967 ,0.72927415, 0.68762517, 0.8520056 , 0.45154762, 0.87837887],dtype=float32), 'threshold': 0.005}, 'meta': {'name': 'KSDrift', 'online': False, 'data_type': None, 'version': '0.11.1', 'detector_t</span><a id="_idTextAnchor311"/><span class="koboSpan" id="kobo.144.1">ype': 'drift'}}</span></pre>
<h2 id="_idParaDest-194"><a id="_idTextAnchor312"/><span class="koboSpan" id="kobo.145.1">Practicing with evidently for drift detection</span></h2>
<p><span class="koboSpan" id="kobo.146.1">Another widely-used Python library</span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.147.1"> for drift detection that we will practice with here is </span><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">evidently</span></strong><span class="koboSpan" id="kobo.149.1">. </span><span class="koboSpan" id="kobo.149.2">After importing the necessary libraries, we load the diabetes dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">from </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.151.1">scikit-learn</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.153.1">
import pandas as pdimport numpy as np
from sklearn import datasets
from evidently.report import Report
from evidently.metrics import DataDriftTable
from evidently.metrics import DatasetDriftMetric
diabetes_data = datasets.fetch_openml(name='diabetes',
    version=1, as_frame='auto')
diabetes = diabetes_data.frame
diabetes = diabetes.drop(['class', 'pres'], axis = 1)</span></pre>
<p><span class="koboSpan" id="kobo.154.1">The following table shows the features</span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.155.1"> we want to work on from the diabetes dataset</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.156.1"> for drift detection and </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">their meanings:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-9">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.158.1">Feature</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.159.1">Description</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">preg</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.161.1">Number of </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">times pregnant</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">plas</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.164.1">Plasma glucose concentration after 2 hours in an oral glucose </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">tolerance test</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">skin</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.167.1">Triceps skinfold </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">thickness (mm)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">insu</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.170.1">2-hour serum insulin (</span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">mu U/ml)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">mass</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.173.1">Body mass index (weight in kg/(height </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">in m)^2)</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">pedi</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.176.1">Diabetes </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">pedigree function</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">Age</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.179.1">Age (years)</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.180.1">Table 11.1 – Feature names and their description in diabetes dataset used for drift detection (Efron et al., 2004)</span></p>
<p><span class="koboSpan" id="kobo.181.1">We separate two sets of datapoints called reference and current sets, then generate a drift report using </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">Report()</span></strong><span class="koboSpan" id="kobo.183.1"> from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">evidently.report.Reference</span></strong><span class="koboSpan" id="kobo.185.1"> set to include all individuals aged less</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.186.1"> than or equal to 40 years, and the current set</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.187.1"> to include others in the dataset aged more than </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">40 years:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.189.1">
diabetes_reference = diabetes[diabetes.age &lt;= 40]diabetes_current = diabetes[diabetes.age &gt; 40]
data_drift_dataset_report = Report(metrics=[
    DatasetDriftMetric(),
    DataDriftTable(),
])
data_drift_dataset_report.run(
    reference_data=diabetes_reference,
    current_data=diabetes_current)
Data_drift_dataset_report</span></pre>
<p><span class="koboSpan" id="kobo.190.1">The following illustration is of the report we generated for the diabetes dataset, considering the selected features and separated reference and </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">current sets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<span class="koboSpan" id="kobo.192.1"><img alt="Figure 11.1 – Drift report for ﻿the separated reference and current data from the diabetes dataset" src="image/B16369_11_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.193.1">Figure 11.1 – Drift report for the separated reference and current data from the diabetes dataset</span></p>
<p><span class="koboSpan" id="kobo.194.1">We can see that </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">age</span></strong><span class="koboSpan" id="kobo.196.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">preg</span></strong><span class="koboSpan" id="kobo.198.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">plas</span></strong><span class="koboSpan" id="kobo.200.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.201.1">insu</span></strong><span class="koboSpan" id="kobo.202.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">skin</span></strong><span class="koboSpan" id="kobo.204.1"> are the features</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.205.1"> with significant differences in their distributions</span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.206.1"> between the reference and current sets, which are specified as features with detected drift in the report shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.207.1">Figure 11</span></em></span><em class="italic"><span class="koboSpan" id="kobo.208.1">.1</span></em><span class="koboSpan" id="kobo.209.1">. </span><span class="koboSpan" id="kobo.209.2">In spite of the significance of the difference between the distributions, having complementary statistics such as difference of mean could be helpful to develop a more reliable drift detection strategy. </span><span class="koboSpan" id="kobo.209.3">We can also get the distribution of the features from the report, such as the distributions of </span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">age</span></strong><span class="koboSpan" id="kobo.211.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">preg</span></strong><span class="koboSpan" id="kobo.213.1"> in the reference and current sets in </span><em class="italic"><span class="koboSpan" id="kobo.214.1">Figures 11.2 </span></em><span class="koboSpan" id="kobo.215.1">and</span><em class="italic"> </em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.216.1">11.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">, respectively:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<span class="koboSpan" id="kobo.218.1"><img alt="Figure 11.2 – Distribution of ﻿the age feature in both current and reference data" src="image/B16369_11_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.219.1">Figure 11.2 – Distribution of the age feature in both current and reference data</span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<span class="koboSpan" id="kobo.220.1"><img alt="Figure 11.3 – Distribution of ﻿the preg feature in both current and reference data" src="image/B16369_11_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.221.1">Figure 11.3 – Distribution of the preg feature in both current and reference data</span></p>
<p><span class="koboSpan" id="kobo.222.1">When we detect drifts</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.223.1"> in our models, we might need to retrain them by</span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.224.1"> ingesting new data or by filtering part of the data that might be the source of the drift. </span><span class="koboSpan" id="kobo.224.2">We might also need to change model training if concept drift </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">is detecte</span><a id="_idTextAnchor313"/><span class="koboSpan" id="kobo.226.1">d.</span></span></p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor314"/><span class="koboSpan" id="kobo.227.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.228.1">In this chapter, you learned about the importance of avoiding drift in your machine learning models, and how you can benefit from the concepts you learned in previous chapters such as model versioning and monitoring to do so. </span><span class="koboSpan" id="kobo.228.2">You also practiced with two libraries for drift detection in Python: </span><strong class="source-inline"><span class="koboSpan" id="kobo.229.1">alibi_detect</span></strong><span class="koboSpan" id="kobo.230.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">evidently</span></strong><span class="koboSpan" id="kobo.232.1">. </span><span class="koboSpan" id="kobo.232.2">Using these or similar libraries will help you to eliminate drift in your models and have reliable models </span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">in production.</span></span></p>
<p><span class="koboSpan" id="kobo.234.1">In the next chapter, you will learn about different types of deep neural network models and how to use PyTorch to develop reliable deep </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">learning model</span><a id="_idTextAnchor315"/><span class="koboSpan" id="kobo.236.1">s.</span></span></p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor316"/><span class="koboSpan" id="kobo.237.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.238.1">Could you explain the difference between magnitude and frequency as two characteristics of drift in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">learning modeling?</span></span></li>
<li><span class="koboSpan" id="kobo.240.1">What is an example of a statistical test we can use for data </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">drift detectio</span><a id="_idTextAnchor317"/><span class="koboSpan" id="kobo.242.1">n?</span></span></li>
</ol>
<h1 id="_idParaDest-197"><span class="koboSpan" id="kobo.243.1">Referenc</span><a id="_idTextAnchor318"/><span class="koboSpan" id="kobo.244.1">es</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.245.1">Ackerman, Samuel, et al. </span><span class="koboSpan" id="kobo.245.2">“</span><em class="italic"><span class="koboSpan" id="kobo.246.1">Detection of data drift and outliers affecting machine learning model performance over time</span></em><span class="koboSpan" id="kobo.247.1">.” </span><span class="koboSpan" id="kobo.247.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">arXiv:2012.09258 (2020).</span></span></li>
<li><span class="koboSpan" id="kobo.249.1">Ackerman, Samuel, et al. </span><span class="koboSpan" id="kobo.249.2">“</span><em class="italic"><span class="koboSpan" id="kobo.250.1">Automatically detecting data drift in machine learning classifiers</span></em><span class="koboSpan" id="kobo.251.1">.” </span><span class="koboSpan" id="kobo.251.2">arXiv preprint </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">arXiv:2111.05672 (2021).</span></span></li>
<li><span class="koboSpan" id="kobo.253.1">Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani (2004) “</span><em class="italic"><span class="koboSpan" id="kobo.254.1">Least Angle Regression</span></em><span class="koboSpan" id="kobo.255.1">,” Annals of Statistics (with </span><span class="No-Break"><span class="koboSpan" id="kobo.256.1">discussion), 407-499</span></span></li>
<li><span class="koboSpan" id="kobo.257.1">Gama, João, et al. </span><span class="koboSpan" id="kobo.257.2">“</span><em class="italic"><span class="koboSpan" id="kobo.258.1">A survey on concept drift adaptation</span></em><span class="koboSpan" id="kobo.259.1">.” </span><span class="koboSpan" id="kobo.259.2">ACM computing surveys (CSUR) 46.4 (</span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">2014): 1-37.</span></span></li>
<li><span class="koboSpan" id="kobo.261.1">Lu, Jie, et al. </span><span class="koboSpan" id="kobo.261.2">“</span><em class="italic"><span class="koboSpan" id="kobo.262.1">Learning under concept drift: A review</span></em><span class="koboSpan" id="kobo.263.1">.” </span><span class="koboSpan" id="kobo.263.2">IEEE transactions on knowledge and data engineering 31.12 (</span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">2018): 2346-2363.</span></span></li>
<li><span class="koboSpan" id="kobo.265.1">Mallick, Ankur, et al. </span><span class="koboSpan" id="kobo.265.2">“</span><em class="italic"><span class="koboSpan" id="kobo.266.1">Matchmaker: Data drift mitigation in machine learning for large-scale systems</span></em><span class="koboSpan" id="kobo.267.1">.” </span><span class="koboSpan" id="kobo.267.2">Proceedings of Machine Learning and Systems 4 (</span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">2022): 77-94.</span></span></li>
<li><span class="koboSpan" id="kobo.269.1">Zenisek, Jan, Florian Holzinger, and Michael Affenzeller. </span><span class="koboSpan" id="kobo.269.2">“</span><em class="italic"><span class="koboSpan" id="kobo.270.1">Machine learning based concept drift detection for predictive maintenance</span></em><span class="koboSpan" id="kobo.271.1">.” </span><span class="koboSpan" id="kobo.271.2">Computers &amp; Industrial Engineering 137 (</span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">2019): 106031.</span></span></li>
</ul>
</div>


<div class="Content" id="_idContainer108">
<h1 id="_idParaDest-198" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor319"/><span class="koboSpan" id="kobo.1.1">Part 4:Deep Learning Modeling</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part of the book, we will lay the foundation with an introduction to the underlying theories of deep learning, and then transition to hands-on exploration of fully connected neural networks. </span><span class="koboSpan" id="kobo.2.2">We will then learn about more advanced techniques including convolutional neural networks, transformers, and graph neural networks. </span><span class="koboSpan" id="kobo.2.3">Concluding this part, we will spotlight the cutting-edge advancements in machine learning, with a keen focus on generative modeling and an introduction to reinforcement and self-supervised learning. </span><span class="koboSpan" id="kobo.2.4">Throughout these chapters, practical examples are provided using Python and PyTorch, ensuring that we gain both theoretical knowledge as well as </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">hands-on experience.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B16369_12.xhtml#_idTextAnchor320"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 12</span></em></a><span class="koboSpan" id="kobo.7.1">, </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Going Beyond ML Debugging with Deep Learning</span></em></li>
<li><a href="B16369_13.xhtml#_idTextAnchor342"><em class="italic"><span class="koboSpan" id="kobo.9.1">Chapter 13</span></em></a><span class="koboSpan" id="kobo.10.1">, </span><em class="italic"><span class="koboSpan" id="kobo.11.1">Advanced Deep Learning Techniques</span></em></li>
<li><a href="B16369_14.xhtml#_idTextAnchor379"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 14</span></em></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Introduction to Recent Advancements in Machine Learning</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer109">
</div>
</div>
<div>
<div id="_idContainer110">
</div>
</div>
<div>
<div id="_idContainer111">
</div>
</div>
<div>
<div id="_idContainer112">
</div>
</div>
<div>
<div id="_idContainer113">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer114">
</div>
</div>
<div>
<div id="_idContainer115">
</div>
</div>
<div>
<div id="_idContainer116">
</div>
</div>
</body></html>