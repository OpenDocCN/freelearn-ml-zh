<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Winning the Casino Slot Machines with Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">If you have been following <strong class="calibre3">machine learning</strong> (<strong class="calibre3">ML</strong>) news, I am sure you will have encountered this kind of headline: <em class="calibre15">computers performing better than world champions in various games</em>. If you haven't, the following are sample news snippets from my quick Google search that are worth spending time reading to understand the situation:</p>
<ul class="calibre9">
<li class="calibre10"><span>Check this out: </span><a href="https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/" class="calibre8">https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/</a><span>:</span></li>
</ul>
<p class="CDPAlignCenter1"><img class="aligncenter120" src="assets/c22df343-7a83-4023-914e-93d75586eafa.png"/></p>
<ul class="calibre9">
<li class="calibre10"><span>See this: </span><a href="https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/" class="calibre8">https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/</a>:</li>
</ul>
<p class="CDPAlignCenter1"><img class="aligncenter121" src="assets/b0739d12-24c3-46f6-9062-02488a93f2be.png"/></p>
<p class="mce-root"><strong class="calibre3">Reinforcement learning</strong> (<strong class="calibre3">RL</strong>) is a subarea of <strong class="calibre3">artificial intelligence</strong> (<strong class="calibre3">AI</strong>) that powers computer systems who are able to demonstrate better performance in games such as Atari Breakout and Go than human players.</p>
<p class="mce-root">In this chapter, we will look at the following topics:</p>
<ul class="calibre9">
<li class="calibre10">The concept of RL</li>
<li class="calibre10">The multi-arm bandit problem</li>
<li class="calibre10">Methods for solving the multi-arm bandit problem</li>
<li class="calibre10">Real-world applications of RL</li>
<li class="calibre10">Implementing a project using RL techniques to maximize our chances of winning at a multi-arm bandit machine</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding RL</h1>
                </header>
            
            <article>
                
<p class="mce-root">RL is a very important area but is sometimes overlooked by practitioners for solving complex, real-world problems. It is unfortunate that even most ML textbooks focus only on supervised and unsupervised learning while totally ignorning RL. </p>
<p class="mce-root">RL as an area has picked up momentum in recent years; however, its origins date back to 1980. It was invented by Rich Sutton and Andrew Barto, Rich's PhD thesis advisor. It was thought of as archaic, even back in the 1980s. Rich, however, believed in RL and its promise, maintaining that it would eventually be recognized.</p>
<p class="mce-root">A quick Google search with the term RL shows that RL methods are often used in games, such as checkers and chess. Gaming problems are problems that require taking actions over time to find a long-term optimal solution to a dynamic problem. They are dynamic in the sense that the conditions are constantly changing, sometimes in response to other agents, which can be adversarial.</p>
<p class="mce-root">Although the success of RL is proven in the area of games, it is also an emerging area that is increasingly applied in other fields, such as finance, economics, and other inter-disciplinary areas. There are a number of methods in the RL area that have grown independently within the AI and operations research communities. Therefore, it is key area for a ML practitioners to learn about.</p>
<p class="mce-root">In simple terms, RL is an area that mainly focuses on creating models that learn from mistakes. Imagine that a person is put in a new environment. At first, they will make mistakes, but they will learn from them, so that when the same situation should arise in future, they will not make the same mistake again. RL uses the same technique to train the model as follows:</p>
<p class="mce-root">Environment ----------&gt; Try and fail -----------&gt; Learn from failures ----------&gt; Reach goal</p>
<p class="mce-root">Historically, you couldn't use ML to get an algorithm learn how to become better than a human at performing a certain task. All that could be done was model the machine's behavior after a human's actions and, maybe, the computer would run through them faster. RL, however, makes it possible to create models that become better at performing certain tasks than humans.</p>
<p class="mce-root">Isaac Abhadu, CEO and co-founder at SYBBIO, had this wonderful explanation on Quora detailing the working of RL compared to supervised learning. He stated that an RL framework, in a nutshell, is very similar to that of supervised learning.</p>
<p class="mce-root">Suppose we're trying to get an algorithm to excel at the game of Pong. We have input frames that we will run through a model to get it to produce some random output actions, just as we would in a supervised learning setting. The difference, however, is that in the case of RL, we ourselves do not know what the target labels are, and so we don't tell the machine what's better to do in every specific situation. Instead, we apply something called a <strong class="calibre3">policy gradients</strong> method.</p>
<p class="mce-root">So, we start with a random network and feed to it an input frame so it produces a random output action to react to that frame. This action is then sent back to the game engine, which makes it produce another frame. This loop continues over and over. The only feedback it will give is the game's scoreboard. Whenever our agent does something right – that is, it produces some successful sequence – it will get a point, generally termed as a <strong class="calibre3">reward</strong>. Whenever it produces a failing sequence, it will get a point removed—this is a <strong class="calibre3">penalty</strong>.</p>
<p class="mce-root">The ultimate goal the agent is pursuing is to keep updating its policy to get as much rewards as possible. So, over time, it will figure out how to beat a human at the game.</p>
<p class="mce-root">RL is not quick. The agent is going to lose a lot at first. But we will keep feeding it frames so it keeps producing random output actions, and it will stumble upon actions that are successful. It will keep accumulating knowledge about what moves are successful and, after a while, will become invincible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparison of RL with other ML algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">RL involves an <strong class="calibre3">environment</strong>, which is the problem set to be solved, and an <strong class="calibre3">agent</strong>, which is simply the AI algorithm. The agent will perform a certain action and the result of the action will be a change in the <strong class="calibre3">state</strong> of the agent. The change leads to the agent getting a reward, which is a positive reward, or a penalty, which is a negative reward for having performed an incorrect action. By repeating the action and reward process, the agent learns the environment. It understands the various states and the various actions that are desirable and undesirable. This process of performing actions and learning from the rewards is RL. The following diagram is an illustration showing the relationship between the agent and the environment in RL:</p>
<p class="CDPAlignCenter1"><img class="aligncenter122" src="assets/e0363e4a-eefb-47da-aeff-5e949994c980.png"/></p>
<div class="packtfigref">Relationship between the agent and environment in RL</div>
<p class="mce-root">RL, <strong class="calibre3">deep learning</strong> (<strong class="calibre3">DL</strong>), and ML all support automation in one way or another. All of them involve some kind of learning from the given data. However, what separates RL from the others is that RL learns the right actions by trail and error, whereas the others are focused on learning by finding patterns in the existing data. Another key difference is that for DL and ML algorithms to learn better, we will need to give them large labeled datasets, whereas this is not the case with RL.</p>
<p class="mce-root">Let's understand RL better by taking the analogy of training pets at home. Imagine we are teaching our pet dog, Santy, some new tricks. Santy, unfortunately, does not understand English; therefore, we need to find an alternative way to train him. We emulate a situation, and Santy tries to respond in many different ways. We reward Santy with a bone treat for any desirable responses. What this inculcates in the pet dog is that the next time he encounters a similar situation, he will perform the desired behavior as he knows that there is a reward. So, this is learning from positive responses; if he is treated with negative responses, such as frowning, he will be discouraged from undesirable behavior.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Terminology of RL</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's understand the RL key terms—agent, environment, state, policy, reward, and penalty—with our pet dog training analogy:</p>
<ul class="calibre9">
<li class="calibre10">Our pet dog, Santy, is the agent that is exposed to the environment.</li>
<li class="calibre10">The environment is a house or play area, depending on what we want to teach to Santy.</li>
<li class="calibre10">Each situation encountered is called the state. For example, Santy crawling under the bed or running can be interpreted as states.</li>
<li class="calibre10">Santy, the agent, reacts by performing actions to change from one state to another.</li>
<li class="calibre10">After changes in states, we give the agent either a reward or a penalty, depending on the action that is performed.</li>
<li class="calibre10">The policy refers to the strategy of choosing an action for finding better outcomes.</li>
</ul>
<p class="mce-root">Now that we understand each of the RL terms, let's define the terms more formally and visualize the agent's behavior in the diagram that follows:</p>
<ul class="calibre9">
<li class="calibre10"><strong class="calibre1">States</strong>: The complete description of the world is known as the states. We do not abstract any information that is present in the world. States can be a position, a constant, or a dynamic. States are generally recorded in arrays, matrices, or higher order tensors.</li>
<li class="calibre10"><strong class="calibre1">Actions</strong><span>: The environment generally defines the possible actions; that is, different environments lead to different actions, based on the agent. The valid actions for an agent are recorded in a space called an action space. The possible valid actions in an environment are finite in number.</span></li>
<li class="calibre10"><strong class="calibre1">Environment</strong><span>: This is the space where the agent lives and with which the agent interacts. For different types of environments, we use different rewards and policies.</span></li>
<li class="calibre10"><strong class="calibre1">Reward and return</strong><span>: The reward function is the one that must be kept track of at all times in RL. It plays a vital role in tuning, optimizing the algorithm, and stopping the training of the algorithm. The reward is computed based on the current state of the world, the action just taken, and the next state of the world.</span></li>
<li class="calibre10"><strong class="calibre1">Policies</strong><span>: A policy in RL is a rule that's used by an agent for choosing the next action; the policy is also known as the agent's brain.</span></li>
</ul>
<p class="mce-root">Take a look at the following flowchart to understand the process better:</p>
<p class="CDPAlignCenter1"><img class="aligncenter123" src="assets/4bc9d744-0c4c-4e35-a4c6-d917cc652880.png"/></p>
<div class="packtfigref">Agent behavior in RL</div>
<p class="mce-root">At each step, <em class="calibre15">t</em>, the agent performs the following tasks:</p>
<ol class="calibre12">
<li class="calibre10">Executes action <em class="calibre22">a<sub class="calibre40">t</sub></em></li>
<li class="calibre10">Receives observation <em class="calibre22">s<sub class="calibre40">t</sub></em></li>
<li class="calibre10">Receives scalar reward <em class="calibre22">r<sub class="calibre40">t</sub></em></li>
</ol>
<p class="mce-root">The environment implements the following tasks:</p>
<ol class="calibre12">
<li class="calibre10">Changes upon action <em class="calibre22">a<sub class="calibre40">t</sub></em></li>
<li class="calibre10">Emits observation <em class="calibre22">s<sub class="calibre40">t+1</sub></em></li>
<li class="calibre10">Emits scalar reward <em class="calibre22">r<sub class="calibre40">t+1</sub></em></li>
</ol>
<p class="mce-root">Time step <em class="calibre15">t</em> is incremented after each iteration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The multi-arm bandit problem</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let me start with an analogy to understand this topic better. Do you like pizza? I like it a lot! My favorite restaurant in Bangalore serves delicious pizzas. I go to this place almost every time I feel like eating a pizza, and I am almost sure that I will get the best pizza. However, going to the same restaurant every time worries me that I am missing out on pizzas that are even better and served elsewhere in the town!</p>
<p class="mce-root">One alternative available is to try out restaurants one by one and sample the pizzas there, but this means that there is a very high probability that I will end up eating pizzas that aren't very nice. However, this is the one way for me to find a restaurant that serves better pizzas than the one I am currently aware of. I am aware you must be wondering why am I talking about pizzas when I am supposed to be talking about RL. Let me get to the point.</p>
<p class="mce-root">The dilemma with this task arises from incomplete information. In other words, to solve this task, it is essential to gather enough information to formulate the best overall strategy and then explore new actions. This will eventually lead to a minimization of overall bad experiences. This situation can otherwise be termed as the <strong class="calibre3">exploration</strong> versus <strong class="calibre3">exploitation</strong> dilemma:</p>
<p class="CDPAlignCenter1"><img class="aligncenter124" src="assets/edcd5dd5-b248-4187-a01c-ebeb54d077f3.png"/></p>
<div class="packtfigref">Exploration versus exploitation dilemma</div>
<p class="mce-root">The preceding diagram aptly summarizes my best-pizza problem.</p>
<p class="mce-root">The <strong class="calibre3">multi-arm bandit problem</strong> (<strong class="calibre3">MABP</strong>) is a simplified form of the pizza analogy. It is used to represent similar kinds of problems, and finding a good strategy to solve them is already helping a lot of industries.</p>
<p class="mce-root">A <strong class="calibre3">bandit</strong> is defined as someone who steals your money! A one-armed bandit is a simple slot machine. We find this sort of machine in a casino: you insert a coin into the slot machine, pull a lever, and pray to the luck god to get an immediate reward. But the million-dollar question is why is a slot machine called a bandit? It turns out that all casinos configure the slot machines in such a way that all gamblers end up losing money!</p>
<p class="mce-root">A multi-arm bandit is a hypothetical but complicated slot machine where we have more than one slot machine lined up in a row. A gambler can pull several levers, with each lever giving a different return. The following diagram depicts the probability distribution for the corresponding <span class="calibre4">reward</span> that is different to each layer and unknown to the gambler:</p>
<p class="CDPAlignCenter1"><img class="aligncenter125" src="assets/b18bda36-4411-4f1b-94bd-ac6ea8271f7b.png"/></p>
<div class="packtfigref">Multi-arm bandit</div>
<p class="mce-root">Given these slot machines and after a set of initial trials, the task is to identify what lever to pull to get the maximum reward. In other words, pulling any one of the arms gives us a stochastic reward of either R=+1 for success, or R=0 for failure; this is called an <strong class="calibre3">immediate reward</strong>. A multi-arm bandit that issues a reward of 1 or 0 is called a <strong class="calibre3">Bernoulli</strong>. The objective is to pull the arms one-by-one in a sequence while gathering information to maximize the total payout over the long run. Formally, a Bernoulli MABP can be described as a tuple of (A,R), where the following applies:</p>
<ul class="calibre9">
<li class="calibre10">We have KK machines with reward probabilities, {θ1,…,θK}.</li>
<li class="calibre10">At each time step, <em class="calibre22">t</em>, we take an action, <em class="calibre22">a</em>, on one slot machine and receive a reward, <em class="calibre22">r</em>.</li>
<li class="calibre10"><em class="calibre22">A</em> is a set of actions, each referring to the interaction with one slot machine. The value of action <em class="calibre22">a</em> is the expected reward, <sub class="calibre40"><img class="fm-editor-equation17" src="assets/508d27c3-9820-4d02-8bda-d5b6ac02d4ef.png"/></sub>. If action <em class="calibre22">a</em> at time step <em class="calibre22">t</em> is on the <em class="calibre22">i</em>-th machine, then <sub class="calibre40"><img class="fm-editor-equation18" src="assets/7b8aad37-eaa2-408a-81b6-4c361ebbf469.png"/></sub>.  Q(a) is generally referred to as the action-value function.</li>
<li class="calibre10"><em class="calibre22">R</em> is a reward function. In the case of the Bernoulli bandit, we observe a reward, <em class="calibre22">r</em>, in a stochastic fashion. At time step <em class="calibre22">t</em>, <sub class="calibre40"><img class="fm-editor-equation19" src="assets/7a7913f0-0999-4f43-a251-161a5ddd8fc6.png"/></sub> may return reward 1 with a probability of <sub class="calibre40"><img class="fm-editor-equation20" src="assets/84f9c0d3-29d8-44d3-9fcf-e0813cabb2e5.png"/>,</sub> or 0 otherwise.</li>
</ul>
<p class="mce-root">We can solve the MABP with multiple strategies. We will review some of the strategies shortly in this section. To decide on the best strategy and to compare the different strategies, we need a quantitative method. One method is to directly compute the cumulative rewards after a certain predefined number of trials. Comparing the cumulative rewards from each of the strategies gives us an opportunity to identify the best strategies for the problem.</p>
<p class="mce-root">At times, we may already know the best action for the given bandit problem. In those cases, it may be interesting to look at the concept of regret.</p>
<p class="mce-root">Let's imagine that we know of the details of the best arm to pull for the given bandit problem. Assume that by repeatedly pulling this best arm, we get a maximum expected reward, which is shown as a horizontal line in the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter126" src="assets/b1997e98-45a3-4ac4-8558-5b81dbbab53f.png"/></p>
<div class="packtfigref">Maximum reward obtained by pulling the best arm for a MABP</div>
<p class="mce-root">As per the problem statement, we need to make repeated trials by pulling different arms of <span class="calibre4"><span class="calibre4">the multi-arm bandit </span></span>until we are approximately sure of the arm to pull for the maximum average return at time <em class="calibre15">t</em>. There are a number of rounds involved while we explore and decide upon the best arm. The number of rounds, otherwise called <strong class="calibre3">trials</strong>, also incurs some loss, and this is called <strong class="calibre3">regret</strong>. In other words, we want to maximize the reward even during the learning phase. Regret can be summarized as a quantification of exactly how much we regret not picking the optimal arm.</p>
<p class="mce-root">The following diagram is an illustration showing the regret due to trials done to find the best arm:</p>
<p class="CDPAlignCenter1"><img class="aligncenter127" src="assets/7c7058b4-f051-4e70-84da-58a1531f8c4e.png"/></p>
<div class="packtfigref">Concept of regret in MAB</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Strategies for solving MABP</h1>
                </header>
            
            <article>
                
<p class="mce-root">Based on how the exploration is done, the strategies to solve the MABP can be classified into the following types:</p>
<ul class="calibre9">
<li class="calibre10">No exploration</li>
<li class="calibre10">Exploration at random</li>
<li class="calibre10">Exploration smartly with preference to uncertainty</li>
</ul>
<p class="mce-root">Let's delve into the details of some of the algorithms that fall under each of the strategy types.</p>
<p class="mce-root">Let's consider one very naive approach that involves playing just one slot machine for a long time. Here, we do no exploration at all and just randomly pick one arm to repeatedly pull to maximize the long-term rewards. You must be wondering how this works! Let's explore.</p>
<p class="mce-root">In probability theory, the law of large numbers is a theorem that describes the result of performing the same experiment a large number of times. According to this law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.</p>
<p class="mce-root">We can just play with one machine for a large number of rounds so as to eventually estimate the true reward probability according to the law of large numbers.</p>
<p class="mce-root">However, there are some problems with this strategy. First and foremost, we do not know the value of a large number of rounds. Second, it is super resource intensive to play the same slot repeatedly for large number of times. And, most importantly, there is no guarantee that we will obtain the best long-term reward with this strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The epsilon-greedy algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">The greedy algorithm in RL is a complete exploitation algorithm, which does not care for exploration. Greedy algorithms always select the action with the highest estimated action value. The action value is estimated according to past experience by averaging the rewards associated with the target action that have been observed so far.</p>
<p class="mce-root">However, use of a greedy algorithm can be a smart approach if we are able to successfully estimate the action value to the expected action value; if we know the true distribution, we can just select the best actions. An epsilon-greedy algorithm is a simple combination of the greedy and random approaches.</p>
<p class="mce-root">Epsilon helps to do this estimate. It adds exploration as part of the greedy algorithm. In order to counter the logic of always selecting the best action, as per the estimated action value, occasionally, the epsilon probability selects a random action for the sake of exploration; the rest of the time, it behaves as the original greedy algorithm and select the best known action.</p>
<p class="mce-root">The epsilon in this algorithm is an adjustable parameter that determines the probability of taking a random, rather than principled, action. It is also possible to adjust the epsilon value during training. Generally, at the start of the training process, the epsilon value is often initialized to a large probability. As the environment is unknown, the large epsilon value encourages exploration. The value is then gradually reduced to a small constant (often set to 0.1). This will increase the rate of exploitation selection.</p>
<div class="packtinfobox">Due to the simplicity of the algorithm, the approach has become the de facto technique for most recent RL algorithms.<br class="title-page-name"/>
Despite the common usage that the algorithm enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Boltzmann or softmax exploration</h1>
                </header>
            
            <article>
                
<p class="mce-root">Boltzmann exploration is also called <strong class="calibre3">softmax exploration</strong>. As opposed to either taking the optimal action all the time or taking a random action all the time, this exploration favors both through weighted probabilities. This is done through a softmax over the network's estimates of values for each action. In this case, although not guaranteed, the action that the agent estimates to be optimal is most likely to be chosen.</p>
<p class="mce-root">Boltzmann exploration has the biggest advantage over epsilon greedy. This method has information about the likely values of the other actions. In other words, let's imagine that there are five actions available to an agent. Generally, in the epsilon-greedy method, four actions are estimated as non-optimal and they are all considered equally. However, in Boltzmann exploration, the four sub-optimal choices are weighed by their relative value. This enables the agent to ignore actions that are estimated to be largely sub-optimal and give more attention to potentially promising, but not necessarily ideal, actions.</p>
<p class="mce-root">The temperature parameter (<em class="calibre15">τ</em>) controls the spread of the softmax distribution, so that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training. The parameter is annealed over time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decayed epsilon greedy</h1>
                </header>
            
            <article>
                
<p class="mce-root">The value of epsilon is key in determining how well the epsilon-greedy algorithm works for a given problem. Instead of setting this value at the start and then decreasing it, we can make epsilon dependent on time. For example, epsilon can be kept equal to 1 / log(t + 0.00001). As time passes, the epsilon value will keep reducing. This method works as over the time that epsilon is reduced, we become more confident of the optimal action and less exploring is required.</p>
<p class="mce-root">The problem with the random selection of actions is that after sufficient time steps, even if we know that some arm is bad, this algorithm will keep choosing that with probability <em class="calibre15">epsilon/n</em>. Essentially, we are exploring a bad action, which does not sound very efficient. The approach to get around this could be to favor exploration of arms with strong potential in order to get an optimal value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The upper confidence bound algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <strong class="calibre3">upper confidence bound</strong> (<strong class="calibre3">UCB</strong>) algorithm is the most popular and widely used solution for MABPs. This algorithm is based on the principle of optimism in the face of uncertainty. This essentially means, the less uncertain we are about an arm, the more important it becomes to explore that arm.</p>
<p class="mce-root">Assume that we have two arms that can be tried out. If we have tried out the first arm 100 times but the second arm only once, then we are probably reasonably confident about the payoff of the first arm. However, we are very uncertain about the payoff of the second arm. This gives rise to the family of UCB algorithms. This can be further explained through the following diagram:</p>
<p class="CDPAlignCenter1"><img class="aligncenter128" src="assets/08208bea-abf0-4d4d-b37e-886510e89950.png"/></p>
<div class="packtfigref">Illustration to explain upper confident bound algorithm</div>
<p class="mce-root">In the preceding diagram, each bar represents a different arm or an action. The red dot is the true expected reward and the center of the bar represents the observed average reward. The width of the bar represents the confidence interval. We are already aware that, by the law of large numbers, the more samples we have, the closer the observed average gets to the true average, and the more the bar shrinks.</p>
<p class="mce-root">The idea behind UCB algorithms is to always pick the arm or action with the highest upper bound, which is the sum of the observed average and the one-sided width of the confidence interval. This balances the exploration of arms that have not been tried many times with the exploitation of arms that have.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thompson sampling</h1>
                </header>
            
            <article>
                
<p class="mce-root">Thompson sampling is one of the oldest heuristics for MABPs. It is a randomized algorithm based on Bayesian ideas, and has recently generated significant interest after several studies demonstrated it to have better empirical performance compared to other methods.</p>
<div class="packtinfobox">There is a beautiful explanation I found on <a href="https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms" class="calibre39">https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms</a>. I do not think I can do  better job at explaining Thompson sampling than this. You can refer to this for further reference.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-arm bandit – real-world use cases</h1>
                </header>
            
            <article>
                
<p class="mce-root">We encounter so many situations in the real world that are similar to that of the MABP we reviewed in this chapter. We could apply RL strategies to all these situations. The following are some of the real-world use cases similar to that of the MABP:</p>
<ul class="calibre9">
<li class="calibre10">Finding the best medicine/s among many alternatives</li>
<li class="calibre10">Identifying the best product to launch among possible products</li>
<li class="calibre10">Deciding the amount of traffic (users) that we need to allocate for each website</li>
<li class="calibre10">Identifying the best marketing strategy for launching a product</li>
<li class="calibre10">Identifying the best stocks portfolio to maximize profit</li>
<li class="calibre10">Finding out the best stock to invest in</li>
<li class="calibre10">Figuring out the shortest path in a given map</li>
<li class="calibre10">Click-through rate prediction for ads and articles</li>
<li class="calibre10">Predicting the most beneficial content to be cached at a router based upon the content of articles</li>
<li class="calibre10">Allocation of funding for different departments of an organization</li>
<li class="calibre10">Picking best-performing athletes out of a group of students given limited time and an arbitrary selection threshold</li>
</ul>
<p class="mce-root">So far, we have covered almost all of the basic details that we need to know to progress to the practical implementation of RL to the MABP. Let's kick-start coding solutions to the MABP in our next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Solving the MABP with UCB and Thompson sampling algorithms</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this project, we will use upper confidence limits and Thompson sampling algorithms to solve the MABP. We will compare their performance and strategy in three different situations—standard rewards, standard but more volatile rewards, and somewhat chaotic rewards. Let's prepare the simulation data, and once the data is prepared, we will view the simulated data using the following code:</p>
<pre class="calibre16"># loading the required packages<br class="title-page-name"/>library(ggplot2)<br class="title-page-name"/>library(reshape2)<br class="title-page-name"/><span># distribution of arms or actions having normally distributed<br class="title-page-name"/></span># rewards with small variance<br class="title-page-name"/># The data represents a standard, ideal situation i.e.<br class="title-page-name"/># normally distributed rewards, well seperated from each other.<br class="title-page-name"/>mean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)<br class="title-page-name"/>reward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[2], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[3], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[4], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[5], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[6], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[7], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[8], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[9], sd = 2.5),<br class="title-page-name"/>                function(n) rnorm(n= n, mean = mean_reward[10], sd = 2.5))<br class="title-page-name"/>#preparing simulation data<br class="title-page-name"/>dataset = matrix(nrow = 10000, ncol = 10)<br class="title-page-name"/>for(i in 1:10){<br class="title-page-name"/>  dataset[, i] = reward_dist[[i]](n = 10000)<br class="title-page-name"/>}<br class="title-page-name"/># assigning column names<br class="title-page-name"/>colnames(dataset) &lt;- 1:10<br class="title-page-name"/># viewing the dataset that is just created with simulated data<br class="title-page-name"/>View(dataset)</pre>
<p class="mce-root">This will give the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter129" src="assets/af7c8009-6639-486b-aa13-f7b31d4a198e.png"/></p>
<p class="mce-root">Now, create a melted dataset with an arm and reward combination, and then convert the arm column to the nominal type using the following code:</p>
<pre class="calibre16"># creating a melted dataset with arm and reward combination<br class="title-page-name"/>dataset_p = melt(dataset)[, 2:3]<br class="title-page-name"/>colnames(dataset_p) &lt;- c("Bandit", "Reward")<br class="title-page-name"/># converting the arms column in the dataset to nominal type<br class="title-page-name"/>dataset_p$Bandit = as.factor(dataset_p$Bandit)<br class="title-page-name"/># viewing the dataset that is just melted<br class="title-page-name"/>View(dataset_p)</pre>
<p class="mce-root">This will give us the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter130" src="assets/6773aaa3-4263-436a-abda-d6523137ce4c.png"/></p>
<p class="mce-root">Now, plot the distributions of rewards from bandits using the following code:</p>
<pre class="calibre16">#ploting the distributions of rewards from bandits<br class="title-page-name"/>ggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +<br class="title-page-name"/>  geom_density(alpha = 0.3) +<br class="title-page-name"/>  labs(title = "Reward from different bandits")</pre>
<p class="mce-root">This will give us the following output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter131" src="assets/98af8da3-8b26-4f28-8670-55ecd4975f42.png"/></p>
<p class="mce-root">Now let's implement the UCB algorithm on the hypothesized arm with a normal distribution using the following code:</p>
<pre class="calibre16"># implementing upper confidence bound algorithm<br class="title-page-name"/>UCB &lt;- function(N = 1000, reward_data){<br class="title-page-name"/>  d = ncol(reward_data)<br class="title-page-name"/>  bandit_selected = integer(0)<br class="title-page-name"/>  numbers_of_selections = integer(d)<br class="title-page-name"/>  sums_of_rewards = integer(d)<br class="title-page-name"/>  total_reward = 0<br class="title-page-name"/>  for (n in 1:N) {<br class="title-page-name"/>    max_upper_bound = 0<br class="title-page-name"/>    for (i in 1:d) {<br class="title-page-name"/>      if (numbers_of_selections[i] &gt; 0){<br class="title-page-name"/>        average_reward = sums_of_rewards[i] / numbers_of_selections[i]<br class="title-page-name"/>        delta_i = sqrt(2 * log(1 + n * log(n)^2) /<br class="title-page-name"/>numbers_of_selections[i])<br class="title-page-name"/>        upper_bound = average_reward + delta_i<br class="title-page-name"/>      } else {<br class="title-page-name"/>        upper_bound = 1e400<br class="title-page-name"/>      }<br class="title-page-name"/>      if (upper_bound &gt; max_upper_bound){<br class="title-page-name"/>        max_upper_bound = upper_bound<br class="title-page-name"/>        bandit = i<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/>    bandit_selected = append(bandit_selected, bandit)<br class="title-page-name"/>    numbers_of_selections[bandit] = numbers_of_selections[bandit] + 1<br class="title-page-name"/>    reward = reward_data[n, bandit]<br class="title-page-name"/>    sums_of_rewards[bandit] = sums_of_rewards[bandit] + reward<br class="title-page-name"/>    total_reward = total_reward + reward<br class="title-page-name"/>  }<br class="title-page-name"/>  return(list(total_reward = total_reward, bandit_selected bandit_selected, numbers_of_selections = numbers_of_selections, sums_of_rewards = sums_of_rewards))<br class="title-page-name"/>}<br class="title-page-name"/># running the UCB algorithm on our<br class="title-page-name"/># hypothesized arms with normal distributions<br class="title-page-name"/>UCB(N = 1000, reward_data = dataset)</pre>
<p class="mce-root">You will get the following as the resultant output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>       1<br class="title-page-name"/>25836.91<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]   1   1   1   1   1   1   2   1  23 968<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]     4.149238    10.874230     5.998070    11.951624    18.151797    21.004781    44.266832    19.370479   563.001692<br class="title-page-name"/>[10] 25138.139942</pre>
<p class="mce-root">Next, we will implement the Thompson sampling algorithm using a <strong class="calibre3">normal-gamma</strong> prior and normal likelihood to estimate posterior distributions using the following code:</p>
<pre class="calibre16"># Thompson sampling algorithm<br class="title-page-name"/>rnormgamma &lt;- function(n, mu, lambda, alpha, beta){<br class="title-page-name"/>  if(length(n) &gt; 1)<br class="title-page-name"/>    n &lt;- length(n)<br class="title-page-name"/>  tau &lt;- rgamma(n, alpha, beta)<br class="title-page-name"/>  x &lt;- rnorm(n, mu, 1 / (lambda * tau))<br class="title-page-name"/>  data.frame(tau = tau, x = x)<br class="title-page-name"/>}<br class="title-page-name"/>T.samp &lt;- function(N = 500, reward_data, mu0 = 0, v = 1, alpha = 2,<br class="title-page-name"/>beta = 6){<br class="title-page-name"/>  d = ncol(reward_data)<br class="title-page-name"/>  bandit_selected = integer(0)<br class="title-page-name"/>  numbers_of_selections = integer(d)<br class="title-page-name"/>  sums_of_rewards = integer(d)<br class="title-page-name"/>  total_reward = 0<br class="title-page-name"/>  reward_history = vector("list", d)<br class="title-page-name"/>  for (n in 1:N){<br class="title-page-name"/>    max_random = -1e400<br class="title-page-name"/>    for (i in 1:d){<br class="title-page-name"/>      if(numbers_of_selections[i] &gt;= 1){<br class="title-page-name"/>        rand = rnormgamma(1,<br class="title-page-name"/>                          (v * mu0 + numbers_of_selections[i] * mean(reward_history[[i]])) / (v + numbers_of_selections[i]),<br class="title-page-name"/>                          v + numbers_of_selections[i],<br class="title-page-name"/>                          alpha + numbers_of_selections[i] / 2,<br class="title-page-name"/>                          beta + (sum(reward_history[[i]] - mean(reward_history[[i]])) ^ 2) / 2 + ((numbers_of_selections[i] * v) / (v + numbers_of_selections[i])) * (mean(reward_history[[i]]) - mu0) ^ 2 / 2)$x<br class="title-page-name"/>      }else {<br class="title-page-name"/>        rand = rnormgamma(1, mu0, v, alpha, beta)$x<br class="title-page-name"/>      }<br class="title-page-name"/>      if(rand &gt; max_random){<br class="title-page-name"/>        max_random = rand<br class="title-page-name"/>        bandit = i<br class="title-page-name"/>      }<br class="title-page-name"/>    }<br class="title-page-name"/>    bandit_selected = append(bandit_selected, bandit)<br class="title-page-name"/>    numbers_of_selections[bandit] = numbers_of_selections[bandit] + 1<br class="title-page-name"/>    reward = reward_data[n, bandit]<br class="title-page-name"/>    sums_of_rewards[bandit] = sums_of_rewards[bandit] + reward<br class="title-page-name"/>    total_reward = total_reward + reward<br class="title-page-name"/>    reward_history[[bandit]] = append(reward_history[[bandit]], reward)<br class="title-page-name"/>  }<br class="title-page-name"/>  return(list(total_reward = total_reward, bandit_selected = bandit_selected, numbers_of_selections = numbers_of_selections, sums_of_rewards = sums_of_rewards))<br class="title-page-name"/>}<br class="title-page-name"/># Applying Thompson sampling using normal-gamma prior and Normal likelihood to estimate posterior distributions<br class="title-page-name"/>T.samp(N = 1000, reward_data = dataset, mu0 = 40)</pre>
<p class="mce-root">You will get the following as the resultant output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>      10<br class="title-page-name"/>24434.24<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]  16  15  15  14  14  17  16  19  29 845<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]    80.22713   110.09657   141.14346   171.41301   212.86899   293.30138   311.12230   423.93256   713.54105 21976.59855</pre>
<p class="mce-root">From the results, we can infer that the UCB algorithm quickly identified that the 10<sup class="calibre21">th</sup> arm yields the most reward. We also observe that Thompson sampling tried the worst arms a lot more times before finding the best one.</p>
<p class="mce-root">Now, let's simulate the data of bandits with normally distributed rewards with large variance and plot the distributions of rewards by using the following code:</p>
<pre class="calibre16"># Distribution of bandits / actions having normally distributed rewards with large variance<br class="title-page-name"/># This data represents an ideal but more unstable situation: normally distributed rewards with much larger variance,<br class="title-page-name"/># thus not well separated from each other.<br class="title-page-name"/>mean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)<br class="title-page-name"/>reward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[2], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[3], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[4], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[5], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[6], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[7], sd = 20),]<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[8], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[9], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[10], sd = 20))<br class="title-page-name"/>#preparing simulation data<br class="title-page-name"/>dataset = matrix(nrow = 10000, ncol = 10)<br class="title-page-name"/>for(i in 1:10){<br class="title-page-name"/>  dataset[, i] = reward_dist[[i]](n = 10000)<br class="title-page-name"/>}<br class="title-page-name"/>colnames(dataset) &lt;- 1:10<br class="title-page-name"/>dataset_p = melt(dataset)[, 2:3]<br class="title-page-name"/>colnames(dataset_p) &lt;- c("Bandit", "Reward")<br class="title-page-name"/>dataset_p$Bandit = as.factor(dataset_p$Bandit)<br class="title-page-name"/>#plotting the distributions of rewards from bandits<br class="title-page-name"/>ggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +<br class="title-page-name"/>  geom_density(alpha = 0.3) +<br class="title-page-name"/>  labs(title = "Reward from different bandits")</pre>
<p class="mce-root">You will get the following graph as the resultant output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter132" src="assets/f3110523-de32-48fd-8998-a20d758ca85e.png"/></p>
<p class="mce-root">Apply UCB on rewards with higher variance using the following code:</p>
<pre class="calibre16"># Applying UCB on rewards with higher variance<br class="title-page-name"/>UCB(N = 1000, reward_data = dataset)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>       1<br class="title-page-name"/>25321.39<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]   1   1   1   3   1   1   2   6 903  81<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]     2.309649    -6.982907   -24.654597    49.186498     8.367174   -16.211632    31.243270   104.190075 23559.216706  1614.725305</pre>
<p class="mce-root">Next, apply Thompson sampling on rewards with higher variance by using the following code:</p>
<pre class="calibre16"># Applying Thompson sampling on rewards with higher variance<br class="title-page-name"/>T.samp(N = 1000, reward_data = dataset, mu0 = 40)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>       2<br class="title-page-name"/>24120.94<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]  16  15  14  15  15  17  20  21 849  18<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]    94.27878    81.42390   212.00717   181.46489   140.43908   249.82014   368.52864   397.07629 22090.20740 305.69191</pre>
<p class="mce-root">From the results, we can infer that when the fluctuation of rewards is greater, the UCB algorithm is more susceptible to being stuck at a suboptimal choice and never finds the optimal bandit. Thompson sampling is generally more robust and is able to find the optimal bandit in all kinds of situations.</p>
<p class="mce-root">Now let's simulate the more chaotic distribution bandit data and plot the distribution of rewards from bandits by using the following code:</p>
<pre class="calibre16"># Distribution of bandits / actions with rewards of different distributions<br class="title-page-name"/># This data represents an more chaotic (possibly more realistic) situation:<br class="title-page-name"/># rewards with different distribution and different variance.<br class="title-page-name"/>mean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)<br class="title-page-name"/>reward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 20),<br class="title-page-name"/>                function(n) rgamma(n = n, shape = mean_reward[2] / 2, rate<br class="title-page-name"/>                 = 0.5),<br class="title-page-name"/>                function(n) rpois(n = n, lambda = mean_reward[3]),<br class="title-page-name"/>                function(n) runif(n = n, min = mean_reward[4] - 20, max = mean_reward[4] + 20),<br class="title-page-name"/>                function(n) rlnorm(n = n, meanlog = log(mean_reward[5]) - 0.25, sdlog = 0.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[6], sd = 20),<br class="title-page-name"/>                function(n) rexp(n = n, rate = 1 / mean_reward[7]),<br class="title-page-name"/>                function(n) rbinom(n = n, size = mean_reward[8] / 0.5, prob = 0.5),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[9], sd = 20),<br class="title-page-name"/>                function(n) rnorm(n = n, mean = mean_reward[10], sd = 20))<br class="title-page-name"/>#preparing simulation data<br class="title-page-name"/>dataset = matrix(nrow = 10000, ncol = 10)<br class="title-page-name"/>for(i in 1:10){<br class="title-page-name"/>  dataset[, i] = reward_dist[[i]](n = 10000)<br class="title-page-name"/>}<br class="title-page-name"/>colnames(dataset) &lt;- 1:10<br class="title-page-name"/>dataset_p = melt(dataset)[, 2:3]<br class="title-page-name"/>colnames(dataset_p) &lt;- c("Bandit", "Reward")<br class="title-page-name"/>dataset_p$Bandit = as.factor(dataset_p$Bandit)<br class="title-page-name"/>#plotting the distributions of rewards from bandits<br class="title-page-name"/>ggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +<br class="title-page-name"/>  geom_density(alpha = 0.3) +<br class="title-page-name"/>  labs(title = "Reward from different bandits")</pre>
<p class="mce-root">You will get the following graph as the resultant output:</p>
<p class="CDPAlignCenter1"><img class="aligncenter133" src="assets/e2dc6b43-3e5f-4697-a714-ac73f64ed9f6.png"/></p>
<p class="mce-root">Apply UCB on rewards with different distributions by using the following code:</p>
<pre class="calibre16"># Applying UCB on rewards with different distributions<br class="title-page-name"/>UCB(N = 1000, reward_data = dataset)</pre>
<p class="mce-root">You will get the following output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>       1<br class="title-page-name"/>22254.18<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]   1   1   1   1   1   1   1 926  61   6<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]     6.810026     3.373098     8.000000    12.783859    12.858791    11.835287     1.616978 20755.000000 1324.564987   117.335467</pre>
<p class="mce-root">Next, apply Thompson sampling on rewards with different distributions by using the following code:</p>
<pre class="calibre16"># Applying Thompson sampling on rewards with different distributions<br class="title-page-name"/>T.samp(N = 1000, reward_data = dataset, mu0 = 40)</pre>
<p class="mce-root">You will get the following as the resultant output:</p>
<pre class="calibre16">$total_reward<br class="title-page-name"/>       2<br class="title-page-name"/>24014.36<br class="title-page-name"/>$numbers_of_selections<br class="title-page-name"/> [1]  16  14  14  14  14  15  14  51 214 634<br class="title-page-name"/>$sums_of_rewards<br class="title-page-name"/> [1]    44.37095   127.57153   128.00000   142.66207   191.44695   169.10430   150.19486  1168.00000  5201.69130 16691.32118</pre>
<p class="mce-root">From the preceding results, we see that the performance of the two algorithms is similar. A major reason for the Thompson sampling algorithm trying all bandits several times before choosing the one it considers best is because we chose a prior distribution with a relatively high mean in this project. With the prior having a larger mean, the algorithm favors <strong class="calibre3">exploration over exploitation</strong> at the beginning. Only when the algorithm becomes very confident that it has found the best choice does it value exploitation over exploration. If we decrease the mean of the prior, exploitation would have a higher value and the algorithm would stop exploring faster. By altering the prior distribution used, you can adjust the relative importance of exploration over exploitation to suit the specific problem at hand. This is more evidence highlighting the flexibility of the Thompson sampling algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we learned about RL. We started the chapter by defining RL and its difference when compared with other ML techniques. We then reviewed the details of the MABP and looked at the various strategies that can be used to solve this problem. Use cases that are similar to the MABP were discussed. Finally, a project was implemented with UCB and Thompson sampling algorithms to solve the MABP using three different simulated datasets.</p>
<p class="mce-root">We have almost reached the end of this book. The appendix of this book, <em class="calibre15">The Road Ahead</em>, as the name reflects, is a guidance chapter suggesting details on what's next from here to become a better R data scientist. I am super excited that I am at the last leg of this R projects journey. Are you with me on this as well?</p>


            </article>

            
        </section>
    </body></html>