<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Boosting Model Performance with Boosting</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will cover the following recipes:</p>
<ul class="calibre10">
<li class="calibre11">Introduction to boosting</li>
<li class="calibre11">Implementing AdaBoost for disease risk prediction using scikit-learn</li>
<li class="calibre11">Implementing gradient boosting for d<span>isease risk prediction using scikit-learn</span></li>
<li class="calibre11"><span>Implementing extreme gradient boosting for glass identification using XGBoost with scikit-learn </span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to boosting</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">A boosting algorithm is an ensemble technique that helps to improve model performance and accuracy by taking a group of weak learners and combining them to form a strong learner. </span><span class="calibre5">The idea behind boosting is that predictors should learn from mistakes that have been made by previous predictors. </span></p>
<p class="calibre2">Boosting algorithms have two key characteristics:</p>
<ul class="calibre10">
<li class="calibre11">First, they undergo multiple iterations</li>
<li class="calibre11">Second, each iteration focuses on the instances that were wrongly classified by previous iterations</li>
</ul>
<p class="calibre2"><span class="calibre5">When an input is misclassified by a hypothesis, its weight is altered in the next iteration so that the next hypothesis can classify it correctly. More weight will be given to those that provide better performance on the training data. This process, through multiple iterations, converts weak learners into a collection of strong learners, thereby improving the model's performance.</span></p>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">In bagging, no bootstrap sample depends on any other bootstrap, so they all run in parallel. Boosting works in a sequential manner and does not involve bootstrap sampling. </span><span class="calibre5">Both bagging and boosting reduce the variance of a single estimate by combining several estimates from different models into a single estimate. However, it is important to note that boosting does not help significantly if the single model is overfitting. Bagging would be a better option if the model overfits. On the other hand, boosting tries to reduce bias, while bagging rarely improves bias.</span></p>
<p class="calibre2"><span class="calibre5">In this chapter, we will introduce different boosting algorithms such as <strong class="calibre4">Adaptive Boosting</strong> (<strong class="calibre4">AdaBoost</strong>), gradient boosting, and <strong class="calibre4">e</strong></span><span class="calibre5"><strong class="calibre4">xtreme gradient boosting</strong> (<strong class="calibre4">XGBoost</strong>).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing AdaBoost for disease risk prediction using scikit-learn</h1>
                </header>
            
            <article>
                
<p class="calibre2">AdaBoost is one of the earliest boosting algorithms that was used for binary classification. It was proposed by <span class="calibre5">Freund and Schapire in 1996</span>. Many other boosting-based algorithms have since been developed on top of AdaBoost.</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="packtinfobox">
<p class="calibre2">Another variation of adaptive boosting is known as <strong class="calibre4">AdaBoost-abstain</strong>. AdaBoost-abstain allows each baseline classifier to abstain from voting if its dependent feature is missing.</p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p class="calibre2">AdaBoost focuses on combining a set of weak learners into a strong learner. The process of an AdaBoost classifier is as follows:</p>
<ol class="calibre14">
<li class="calibre11">Initially, a short decision tree classifier is fitted onto the data. The decision tree can just have a single split, which is known as a <strong class="calibre1">decision stump</strong>. The overall errors are evaluated. This is the first iteration.</li>
<li class="calibre11">In the second iteration, whatever data is correctly classified will be given lowerweights, while higher weights will be given to the misclassified classes.</li>
<li class="calibre11">In the third iteration, another decision stump will be fitted to the data and the weights will be changed again in the next iteration.</li>
<li class="calibre11">Once these iterations are over, the weights are automatically calculated for each classifier at each iteration based on the error rates to come up with a strong classifier.</li>
</ol>
<p class="calibre2">The following screenshot shows how AdaBoost works:</p>
<p class="CDPAlignCenter"><img src="assets/659ca5ea-7b32-4ead-9a6f-6c082501bb48.png" class="calibre38"/></p>
<p class="CDPAlignLeft3"><span class="calibre5">The concept behind this algorithm is to distribute the weights to the training example and select the classifier with the lowest weighted error. Finally, it c</span>onstructs a strong classifier as a linear combination of these weak learners.</p>
<p class="calibre2">The general equation for an AdaBoost as follows:</p>
<p class="CDPAlignCenter"><img class="fm-editor-equation65" src="assets/6ad74a62-127d-4778-897c-2841ac0d91e3.png"/></p>
<p class="CDPAlignLeft3">Here, <strong class="calibre4">F(x)</strong><span class="calibre5"> </span>represents a strong classifier, <strong class="calibre4"><img class="fm-editor-equation21" src="assets/141fe725-e4a9-44d6-93f6-b61bfb296f09.png"/></strong> represent the weights, and<span class="calibre5"> </span><strong class="calibre4">f(x)</strong><span class="calibre5"> </span>represents a weak classifier.</p>
<p class="calibre2"/>
<p class="calibre2"/>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2"><span class="calibre5">The AdaBoost classifier takes various parameters. The important ones are explained as follows:</span></p>
</div>
</div>
</div>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">base_estimator</kbd>: <span>The learning algorithm that is used to train the models.</span> If a value is not provided for this parameter, the base estimator is <kbd class="calibre12">DecisionTreeClassifier (max_depth=1)</kbd>.</li>
<li class="calibre11"><kbd class="calibre12">n_estimators</kbd>: The <span>number of models to iteratively train.</span></li>
<li class="calibre11"><kbd class="calibre12">learning_rate</kbd>:<strong class="calibre1"> </strong><span>The contribution of each model to the weights. By default, <kbd class="calibre12">learning_rate</kbd> has a value of <kbd class="calibre12">1</kbd>. A lower value for the learning rate forces the model to train slower but might result in better performance scores.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">To start with, import the<span class="calibre5"> </span><kbd class="calibre12">os</kbd><span class="calibre5"> </span>and the <kbd class="calibre12">pandas</kbd> packages and set your working directory according to your requirements:</p>
<pre class="calibre15"># import required packages<br class="title-page-name"/>import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>from sklearn.ensemble import AdaBoostClassifier<br class="title-page-name"/>from sklearn.model_selection import GridSearchCV<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.tree import DecisionTreeClassifier<br class="title-page-name"/><br class="title-page-name"/>from sklearn.metrics import roc_auc_score, roc_curve, auc<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/># Set working directory as per your need<br class="title-page-name"/>os.chdir(".../.../Chapter 8")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2"><span class="calibre5">Download the <kbd class="calibre12">breastcancer.csv</kbd> </span><span class="calibre5">dataset</span><span class="calibre5"> </span><span class="calibre5">from GitHub and copy it to your working directory. Read the dataset:</span></p>
<pre class="calibre15">df_breastcancer = pd.read_csv("breastcancer.csv")</pre>
<p class="calibre2"/>
<p class="calibre2"><span class="calibre5">Take a look at the first few rows with the <kbd class="calibre12">head()</kbd> function:</span></p>
<pre class="calibre15">df_breastcancer.head(5)</pre>
<p class="calibre2">Notice that the <kbd class="calibre12">diagnosis</kbd> <span class="calibre5">variable </span><span class="calibre5">has values such as M and B, representing Malign and Benign, respectively. We will perform label encoding on the</span> <kbd class="calibre12">diagnosis</kbd> <span class="calibre5">variable so that we can convert the M and B values into numeric values.</span></p>
<p class="calibre2"><span class="calibre5">We use</span> <kbd class="calibre12">head()</kbd> <span class="calibre5">to see the changes:</span></p>
<pre class="calibre15"># import LabelEncoder from sklearn.preprocessing<br class="title-page-name"/>from sklearn.preprocessing import LabelEncoder<br class="title-page-name"/><br class="title-page-name"/>lb = LabelEncoder()<br class="title-page-name"/>df_breastcancer['diagnosis'] =lb.fit_transform(df_breastcancer['diagnosis']) <br class="title-page-name"/>df_breastcancer.head(5)</pre>
<p class="calibre2">We then check whether the dataset has any null values:</p>
<pre class="calibre15">df_breastcancer.isnull().sum()</pre>
<p class="calibre2">We check the shape of the dataset with <kbd class="calibre12">shape()</kbd>:</p>
<pre class="calibre15">df_breastcancer.shape</pre>
<p class="calibre2">We now separate our target and feature set. We also split our dataset into training and testing subsets:</p>
<pre class="calibre15"># Create feature &amp; response variables<br class="title-page-name"/># Drop the response var and id column as it'll not make any sense to the analysis<br class="title-page-name"/>X = df_breastcancer.iloc[:,2:31]<br class="title-page-name"/><br class="title-page-name"/># Target<br class="title-page-name"/>Y = df_breastcancer.iloc[:,0]<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0, stratify= Y)</pre>
<p class="calibre2">Now, we will move on to building our model using the <kbd class="calibre12">AdaBoost</kbd> algorithm.</p>
<div class="packtinfobox">It is important to note that the accuracy and AUC scores may differ because of random splits and other randomness factors.</div>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">We will now look at how to use an AdaBoost to train our model:</p>
<ol class="calibre14">
<li class="calibre11">Before we build our first <kbd class="calibre12">AdaBoost</kbd> model, let's train our model using the <kbd class="calibre12">DecisionTreeClassifier</kbd>:</li>
</ol>
<pre class="calibre18">dtree = DecisionTreeClassifier(max_depth=3, random_state=0)<br class="title-page-name"/>dtree.fit(X_train, Y_train)</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We can see our accuracy and <strong class="calibre1">Area Under the Curve</strong> (<strong class="calibre1">AUC</strong>) with the following code:</li>
</ol>
<pre class="calibre18"># Mean accuracy<br class="title-page-name"/>print('The mean accuracy is: ',(dtree.score(X_test,Y_test))*100,'%')<br class="title-page-name"/><br class="title-page-name"/>#AUC score<br class="title-page-name"/>y_pred_dtree = dtree.predict_proba(X_test)<br class="title-page-name"/>fpr_dtree, tpr_dtree, thresholds = roc_curve(Y_test, y_pred_dtree[:,1])<br class="title-page-name"/>auc_dtree = auc(fpr_dtree, tpr_dtree)<br class="title-page-name"/>print ('AUC Value: ', auc_dtree)</pre>
<p class="calibre20">We get an accuracy score and an AUC value of 91.81% and 0.91, respectively. Note that these values might be different for different users due to randomness.</p>
<ol start="3" class="calibre14">
<li class="calibre11">Now, we will build our AdaBoost model using the scikit-learn library. We will use<span> the </span><kbd class="calibre12">AdaBoostClassifier</kbd><span> </span>to build our<span> </span><kbd class="calibre12">AdaBoost</kbd><span> </span>model. <kbd class="calibre12">AdaBoost</kbd> uses <kbd class="calibre12">dtree</kbd> as the base classifier by default:</li>
</ol>
<pre class="calibre18">AdaBoost = AdaBoostClassifier(n_estimators=100, base_estimator=dtree, learning_rate=0.1, random_state=0)<br class="title-page-name"/>AdaBoost.fit(X_train, Y_train)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">We check the accuracy and AUC value of the model on our test data:</li>
</ol>
<pre class="calibre18"># Mean accuracy<br class="title-page-name"/>print('The mean accuracy is: ',(AdaBoost.score(X_test,Y_test))*100,'%')<br class="title-page-name"/><br class="title-page-name"/>#AUC score<br class="title-page-name"/>y_pred_adaboost = AdaBoost.predict_proba(X_test)<br class="title-page-name"/>fpr_ab, tpr_ab, thresholds = roc_curve(Y_test, y_pred_adaboost[:,1])<br class="title-page-name"/>auc_adaboost = auc(fpr_ab, tpr_ab)<br class="title-page-name"/>print ('AUC Value: ', auc_adaboost)</pre>
<p class="calibre2"/>
<p class="calibre20">We notice that we get an accuracy score of 92.82% and an AUC value of 0.97. Both of these metrics are higher than the decision tree model we built in <em class="calibre13">Step 1</em>.</p>
<ol start="5" class="calibre14">
<li class="calibre11">Then, we must fine-tune our hyperparameters. We set <kbd class="calibre12">n_estimators</kbd> to <kbd class="calibre12">100</kbd> and <kbd class="calibre12">learning_rate</kbd> to <kbd class="calibre12">0.4</kbd>:</li>
</ol>
<pre class="calibre18"># Tuning the hyperparams<br class="title-page-name"/>AdaBoost_with_tuning = AdaBoostClassifier(n_estimators=100, base_estimator=dtree, learning_rate=0.4, random_state=0)<br class="title-page-name"/>AdaBoost_with_tuning.fit(X_train, Y_train)</pre>
<ol start="6" class="calibre14">
<li class="calibre11"><span>Now, we will </span>check the accuracy and AUC values of our new model on our test data:</li>
</ol>
<pre class="calibre18"># Mean accuracy<br class="title-page-name"/>print('The mean accuracy is: ',(AdaBoost_with_tuning.score(X_test,Y_test))*100,'%')<br class="title-page-name"/><br class="title-page-name"/>#AUC score<br class="title-page-name"/>y_pred_adaboost_tune = AdaBoost.predict_proba(X_test)<br class="title-page-name"/>fpr_ab_tune, tpr_ab_tune, thresholds = roc_curve(Y_test, y_pred_adaboost_tune[:,1])<br class="title-page-name"/>auc_adaboost_tune = auc(fpr_ab_tune, tpr_ab_tune)<br class="title-page-name"/>print ('AUC Value: ', auc_adaboost_tune)</pre>
<p class="calibre2">We notice the accuracy drops to 92.39%, but that we get an improved AUC value of 0.98.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we used the <kbd class="calibre12">DecisionTreeClassifier</kbd> to <span class="calibre5">build our model. In <em class="calibre13">Step 2</em>, we noticed that our mean accuracy and the AUC score were 91.81% and 0.91, respectively. We aimed to improve this using the <kbd class="calibre12">AdaBoost</kbd> algorithm.</span></p>
<p class="calibre2">Note that the <kbd class="calibre12">AdaBoost</kbd> algorithm uses a decision tree as the base classifier by default. In <em class="calibre13">Step 3</em>, we trained our model using <kbd class="calibre12">AdaBoost</kbd> with the default base learner. We set <kbd class="calibre12">n_estimators</kbd> to <kbd class="calibre12">100</kbd> and the <kbd class="calibre12">learning_rate</kbd> to <kbd class="calibre12">0.1</kbd>. We checked our mean accuracy and AUC value in <em class="calibre13">Step 4</em>. We noticed that we got a decent improvement in the mean accuracy and the AUC as they jumped to 93.57% and 0.977, respectively.</p>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we fine-tuned some of the hyperparameters for our <kbd class="calibre12">AdaBoost</kbd> algorithm, which used a <span class="calibre5">decision tree as the base classifier. We set the <kbd class="calibre12">n_estimators</kbd> to <kbd class="calibre12">100</kbd> and the <kbd class="calibre12">learning_rate</kbd> to <kbd class="calibre12">0.4</kbd>. <em class="calibre13">Step 6</em> gave us the accuracy and AUC values for the model we built in <em class="calibre13">Step 5</em>. We saw that the accuracy dropped to 93.56% and that the AUC stayed similar at 0.981.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">Here, we will <span class="calibre5">showcase training a model using AdaBoost with a </span><strong class="calibre4">support vector machine</strong><span class="calibre5"> (</span><strong class="calibre4">SVM</strong><span class="calibre5">) as the base learner. </span></p>
<p class="calibre2">By default, AdaBoost uses a decision tree as the base learner. We can use different base learners as well. In the following example, we have used an SVM as our base learner with the <kbd class="calibre12">AdaBoost</kbd> algorithm. We use <kbd class="calibre12">SVC</kbd> with <kbd class="calibre12">rbf</kbd> as the kernel:</p>
<pre class="calibre15">from sklearn.svm import SVC<br class="title-page-name"/><br class="title-page-name"/>Adaboost_with_svc_rbf = AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='rbf'), learning_rate=1, random_state=0)<br class="title-page-name"/>Adaboost_with_svc_rbf.fit(X_train, Y_train)</pre>
<p class="calibre2">We can check the accuracy and the AUC values of our AdaBoost model with <strong class="calibre4">support vector classifier</strong> (<strong class="calibre4">SVC</strong>) as the base learner:</p>
<pre class="calibre15"># Mean accuracy<br class="title-page-name"/>print('The mean accuracy is: ',(Adaboost_with_svc_rbf.score(X_test,Y_test))*100,'%') <br class="title-page-name"/><br class="title-page-name"/>#AUC score<br class="title-page-name"/>y_pred_svc_rbf = Adaboost_with_svc_rbf.predict_proba(X_test)<br class="title-page-name"/>fpr_svc_rbf, tpr_svc_rbf, thresholds = roc_curve(Y_test, y_pred_svc_rbf[:,1])<br class="title-page-name"/>auc_svc_rbf = auc(fpr_svc_rbf, tpr_svc_rbf)<br class="title-page-name"/>print ('AUC Value: ', auc_svc_rbf)</pre>
<p class="calibre2">We noticed that the accuracy and AUC values fall to 62.57 and 0.92, respectively.</p>
<p class="calibre2">Now, we will rebuild our AdaBoost model with SVC. This time, we will use a linear kernel:</p>
<pre class="calibre15">Adaboost_with_svc_linear =AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='linear'), learning_rate=1, random_state=0)<br class="title-page-name"/>Adaboost_with_svc_linear.fit(X_train, Y_train)</pre>
<p class="calibre2"><span class="calibre5">We now get a mean accuracy of 90.64% and a decent AUC value of 0.96.</span></p>
<p class="calibre2">We will now plot a graph to compare the AUC value of each model using the following code:</p>
<pre class="calibre15">import matplotlib.pyplot as plt<br class="title-page-name"/>% matplotlib inline<br class="title-page-name"/>plt.figure(figsize=(8,8))<br class="title-page-name"/><br class="title-page-name"/>plt.plot(fpr_dtree, tpr_dtree,label="Model1: Dtree, auc="+str(auc_dtree))<br class="title-page-name"/>plt.plot(fpr_ab, tpr_ab,label="Model2: Adaboost, auc="+str(auc_adaboost))<br class="title-page-name"/>plt.plot(fpr_ab_tune,tpr_ab_tune,label="Model3: Adaboost with Tuning, auc="+str(auc_adaboost_tune))<br class="title-page-name"/>plt.plot(fpr_svc_rbf, tpr_svc_rbf, label="Model4: Adaboost with SVC (RBF Kernel), auc="+str(auc_svc_rbf))<br class="title-page-name"/>plt.plot(fpr_svc_lin, tpr_svc_lin, label="Model5: Adaboost with SVC (Linear Kernel), auc="+str(auc_svc_linear))<br class="title-page-name"/><br class="title-page-name"/>plt.legend(loc=5)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre2">This gives us the following plot:</p>
<p class="CDPAlignCenter"><img src="assets/ccb24245-d84c-46b3-a7e0-46377e766863.png" class="calibre39"/></p>
<p class="calibre2">We can also plot the accuracy of all the models with the following code:</p>
<pre class="calibre15">import matplotlib.pyplot as plt<br class="title-page-name"/>% matplotlib inline<br class="title-page-name"/>plt.figure(figsize=(8,8))<br class="title-page-name"/><br class="title-page-name"/>label = ['Decison Tree', 'Adaboost', 'Adaboost with Tuning', 'Adaboost with SVC (RBF)', 'Adaboost with SVC (Linear)']<br class="title-page-name"/><br class="title-page-name"/>values = [dtree.score(X_test,Y_test),<br class="title-page-name"/>        AdaBoost.score(X_test,Y_test),<br class="title-page-name"/>        AdaBoost_with_tuning.score(X_test,Y_test),<br class="title-page-name"/>        Adaboost_with_svc_rbf.score(X_test,Y_test),<br class="title-page-name"/>        Adaboost_with_svc_linear.score(X_test,Y_test)]<br class="title-page-name"/><br class="title-page-name"/>def plot_bar_accuracy():<br class="title-page-name"/>    # this is for plotting purpose<br class="title-page-name"/>    index = np.arange(len(label))<br class="title-page-name"/>    plt.bar(index, values)<br class="title-page-name"/>    plt.xlabel('Algorithms', fontsize=10)<br class="title-page-name"/>    plt.ylabel('Accuracy', fontsize=10)<br class="title-page-name"/>    plt.xticks(index, label, fontsize=10, rotation=90)<br class="title-page-name"/>    plt.title('Model Accuracies')<br class="title-page-name"/>    plt.show()<br class="title-page-name"/><br class="title-page-name"/>plot_bar_accuracy()</pre>
<p class="calibre2">This gives us the following output:</p>
<p class="CDPAlignCenter"><img src="assets/3c274f0f-7286-4468-b1a2-07b246aa9cd5.png" class="calibre40"/></p>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p class="calibre2">We can also use grid search with AdaBoost:</p>
<pre class="calibre15">#grid search using svm<br class="title-page-name"/>Adaboost_with_svc = AdaBoostClassifier(n_estimators=100, base_estimator=SVC(probability=True, kernel='linear'), learning_rate=1, algorithm= 'SAMME')<br class="title-page-name"/><br class="title-page-name"/>Ada_Grid = {'n_estimators': [10,30,40,100],<br class="title-page-name"/>           'learning_rate': [0.1, 0.2, 0.3]}<br class="title-page-name"/><br class="title-page-name"/>estimator = Adaboost_with_svc<br class="title-page-name"/>Adaboost_with_grid_search = GridSearchCV(estimator,Ada_Grid).fit(X_train, Y_train)<br class="title-page-name"/>print(Adaboost_with_grid_search.best_params_)<br class="title-page-name"/>print(Adaboost_with_grid_search.best_score_)</pre>
<p class="calibre2">In the preceding code, we performed a grid search with the <kbd class="calibre12">n_estimators</kbd> set to <kbd class="calibre12">10</kbd>, <kbd class="calibre12">30</kbd>, <kbd class="calibre12">40</kbd>, and <kbd class="calibre12">100</kbd>, and <kbd class="calibre12">learning_rate</kbd> set to <kbd class="calibre12">0.1</kbd>, <kbd class="calibre12">0.2</kbd>, and <kbd class="calibre12">0.3</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing a gradient boosting machine for disease risk prediction using scikit-learn</h1>
                </header>
            
            <article>
                
<p class="calibre2">Gradient boosting is a machine learning technique that works on the principle of boosting, where weak learners iteratively shift their focus toward error observations that were difficult to predict in previous iterations and <span class="calibre5"><span class="calibre5">create </span></span>an ensemble of weak learners, typically decision trees.</p>
<p class="calibre2"><span class="calibre5">Gradient boosting trains models in a sequential manner, and</span> involves the following steps:</p>
<ol class="calibre14">
<li class="calibre11">Fitting a model to the data</li>
<li class="calibre11">Fitting a model to the residuals</li>
<li class="calibre11">Creating a new model</li>
</ol>
<p class="calibre2"><span class="calibre5">While the AdaBoost model identifies errors by using weights that have been assigned to the data points, gradient boosting does the same by calculating the gradients in the loss function. </span><span class="calibre5">The loss function is a measure of how a model is able to fit the data on which it is trained and</span> generally depends on the type of problem being solved. If we are talking about regression problems, mean squared error may be used, while in classification problems, the logarithmic loss can be used. The gradient descent procedure is used to minimize loss when adding trees one at a time. Existing trees in the model remain the same.</p>
<p class="calibre2"/>
<p class="calibre2">There are a handful of hyperparameters that may be tuned for this:</p>
<ul class="calibre10">
<li class="front-matter">
<ul class="calibre41">
<li class="calibre11"><kbd class="calibre12">N_estimators</kbd>:<strong class="calibre1"> </strong>This represents the number of trees in the model. Usually, the higher it is, the better the model learns the data.</li>
<li class="calibre11"><kbd class="calibre12">max_depth</kbd>: This signifies how deep our tree is. It is used to control overfitting.</li>
<li class="calibre11"><kbd class="calibre12">min_samples_split</kbd>: This is the minimum number of samples required to split an internal node. Values that are too high can prevent the model from learning relations.</li>
<li class="calibre11"><kbd class="calibre12">learning_rate</kbd>: This controls the magnitude of change in the estimates. Lower values with a higher number of trees are generally preferred.</li>
<li class="calibre11"><kbd class="calibre12">loss</kbd>: This refers to the loss function that is minimized in each split. <kbd class="calibre12">deviance</kbd> is used in the algorithm as the default parameter, while the other is <kbd class="calibre12">exponential</kbd>.</li>
<li class="calibre11"><kbd class="calibre12">max_features</kbd>: This represents the number of features we have to consider when looking for the best split.</li>
<li class="calibre11"><kbd class="calibre12">criterion</kbd>:<strong class="calibre1"> </strong>This function measures the quality of the split and supports <kbd class="calibre12">friedman_mse</kbd> and <kbd class="calibre12">mae</kbd> to evaluate the performance of the model.</li>
<li class="calibre11"><kbd class="calibre12">subsample</kbd>:<strong class="calibre1"> </strong>This represents the fraction of samples to be used for fitting the individual base learners. Choosing a subsample that is less than 1.0 leads to a reduction of variance and an increase in bias.</li>
<li class="calibre11"><kbd class="calibre12">min_impurity_split</kbd>: This is represented as a threshold to stop tree growth early.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">We will take the same dataset that we used for training our AdaBoost model. In this example, we will see how we can train our model using gradient boosting machines. We will also look at a handful of hyperparameters that can be tuned to improve the model's performance.</span></p>
<p class="calibre2"/>
<p class="calibre2">First, we must import all the required libraries:</p>
<pre class="calibre15">import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/><br class="title-page-name"/>from sklearn.ensemble import GradientBoostingClassifier <br class="title-page-name"/>from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score<br class="title-page-name"/>from sklearn.preprocessing import MinMaxScaler<br class="title-page-name"/><br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import itertools</pre>
<p class="calibre2">Then, we read our data and label encode our target variables to 1 and 0:</p>
<pre class="calibre15"># Read the Dataset<br class="title-page-name"/>df_breastcancer = pd.read_csv("breastcancer.csv")<br class="title-page-name"/><br class="title-page-name"/>from sklearn.preprocessing import LabelEncoder<br class="title-page-name"/>lb = LabelEncoder()<br class="title-page-name"/>df_breastcancer['diagnosis'] = lb.fit_transform(df_breastcancer['diagnosis']) <br class="title-page-name"/>df_breastcancer.head(5)</pre>
<p class="calibre2">Then, separate our target and feature variables. We split our data into train and test subsets:</p>
<pre class="calibre15"># create feature &amp; response variables<br class="title-page-name"/># drop the response var and id column as it'll not make any sense to the analysis<br class="title-page-name"/>X = df_breastcancer.iloc[:,2:31]<br class="title-page-name"/><br class="title-page-name"/># Target variable<br class="title-page-name"/>Y = df_breastcancer.iloc[:,0]<br class="title-page-name"/><br class="title-page-name"/># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=0, stratify= Y)</pre>
<div class="packtinfobox">This is the same code that we used in the <em class="calibre23">Getting ready</em> section of the <kbd class="calibre19">AdaBoost</kbd> example. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">We will now look at how to use a Gradient Boosting Machines to train our model:</span></p>
<ol class="calibre14">
<li class="calibre11">We imported <kbd class="calibre12">GradientBoostingClassifier</kbd> from <kbd class="calibre12">sklearn.ensemble</kbd> in the last section,<em class="calibre23"> Getting ready</em>. <span>We trained our model using <kbd class="calibre12">GradieBoostingClassfier</kbd>:</span></li>
</ol>
<pre class="calibre18">GBM_model = GradientBoostingClassifier() <br class="title-page-name"/>GBM_model.fit(X_train, Y_train)</pre>
<ol start="2" class="calibre14">
<li class="calibre11">Here, we must pass our test data to the <kbd class="calibre12">predict()</kbd> function to make the predictions using the model we built in <em class="calibre23">Step 1</em>:</li>
</ol>
<pre class="calibre18">Y_pred_gbm = GBM_model.predict(X_test)</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Now, we use  <kbd class="calibre12"><span>classification_report</span></kbd> to see the following metrics:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, Y_pred_gbm))</pre>
<p class="calibre20"><kbd class="calibre12">classification_report</kbd> gives us the following output:</p>
<p class="CDPAlignCenter"><img src="assets/ad04d7fa-7696-4168-bd2f-8bee27c56d4a.png" class="calibre42"/></p>
<ol start="4" class="calibre14">
<li class="calibre11">We will use <kbd class="calibre12">confusion_matrix()</kbd> to generate the confusion matrix. We then pass the output of the <kbd class="calibre12">confusion_matrix</kbd> to our predefined function, that is, <kbd class="calibre12">plot_confusion_matrix()</kbd>, to plot the matrix:</li>
</ol>
<p class="CDPAlignCenter"><img src="assets/905c7c1c-03c9-453b-b2d6-2931f578c79f.png" class="calibre43"/></p>
<ol start="5" class="calibre14">
<li class="calibre11">We can check the test accuracy and the AUC value with <kbd class="calibre12">accuracy_score()</kbd> and <kbd class="calibre12">roc_auc_score()</kbd>.</li>
</ol>
<p class="calibre2">Note that <span class="calibre5"><kbd class="calibre12">accuracy_score</kbd> and <kbd class="calibre12">roc_auc_score</kbd> have been imported from <kbd class="calibre12">sklearn.metrics</kbd>:</span></p>
<p class="CDPAlignCenter"><img src="assets/e2061b05-4ac1-4159-a970-4e4c9f32c13c.png" class="calibre44"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we trained a gradient boosting classifier model. In <em class="calibre13">Step 2</em>, we used the <kbd class="calibre12">predict()</kbd> method to make predictions on our test data.</p>
<p class="calibre2">In <em class="calibre13">Step 3</em>, we used <kbd class="calibre12">classification_report()</kbd> to see various metrics such as <kbd class="calibre12">precision</kbd>, <kbd class="calibre12">recall</kbd>, and <kbd class="calibre12">f1-score</kbd> for each class, as well as the average of each of the metrics. The <kbd class="calibre12"><span>classification_report()</span></kbd> <span class="calibre5">reports the averages for the total true positives, false negatives, false positives, unweighted mean per label, and support-weighted mean per label. It also reports a sample average for multi-label classification.</span></p>
<p class="calibre2"/>
<div class="packtinfobox"><span>Precision refers to the classifier's ability not to label an instance that is negative as positive, </span><span>while recall refers to the ability of the classifier to find all positive instances. The f<sub class="calibre45">1</sub> score is a weighted harmonic mean of precision and recall. The best <kbd class="calibre19">f<sub class="calibre46">1</sub> score</kbd> is 1.0 and the worst is 0.0. The support is the number of observations of each class. </span></div>
<p class="calibre2">In <em class="calibre13">Step 4</em>, we used <kbd class="calibre12">confusion_matrix()</kbd> to generate the confusion matrix to see the true positives, true negatives, false positives, and false negatives. </p>
<p class="calibre2">In <em class="calibre13">Step 5</em>, we looked at the accuracy and the AUC values of our test data using the <span class="calibre5"><kbd class="calibre12">accuracy_score()</kbd> and </span><span class="calibre5"><kbd class="calibre12">roc_auc_score()</kbd> functions.</span></p>
<p class="calibre2">In the next section, we will tune our hyperparameters using a grid search to find the optimal model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">We will now look at how to fine-tune the hyperparameters for gradient boosting machines:</p>
<ol class="calibre14">
<li class="calibre11">First, we import <kbd class="calibre12">GridSearchCV</kbd> from <kbd class="calibre12">sklearn.model_selection</kbd>:</li>
</ol>
<pre class="calibre18">from sklearn.model_selection import GridSearchCV</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We set the grid parameters to a variable:</li>
</ol>
<pre class="calibre18">parameters = {<br class="title-page-name"/>    "n_estimators":[100,150,200],<br class="title-page-name"/>    "loss":["deviance"],<br class="title-page-name"/>    "learning_rate": [0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],<br class="title-page-name"/>    "min_samples_split":np.linspace(0.1, 0.5, 4),<br class="title-page-name"/>    "min_samples_leaf": np.linspace(0.1, 0.5, 4),<br class="title-page-name"/>    "max_depth":[3, 5, 8],<br class="title-page-name"/>    "max_features":["log2","sqrt"],<br class="title-page-name"/>    "criterion": ["friedman_mse", "mae"],<br class="title-page-name"/>    "subsample":[0.3, 0.6, 1.0]<br class="title-page-name"/>    }</pre>
<ol start="3" class="calibre14">
<li class="calibre11">We use <kbd class="calibre12">GridSeacrhCV</kbd>, which lets us combine an estimator with a grid search to tune the hyperparameters. <span>The <kbd class="calibre12">GridSeacrhCV</kbd> method selects the optimal </span><span>parameter from the grid values and uses it with the estimator:</span></li>
</ol>
<pre class="calibre18">grid = GridSearchCV(GradientBoostingClassifier(), parameters, cv=3, n_jobs=-1) <br class="title-page-name"/>grid.fit(X_train, Y_train)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">Then, we can view the optimal parameters:</li>
</ol>
<pre class="calibre18">grid.best_estimator_</pre>
<p class="calibre20">Take a look at the following screenshot:</p>
<p class="CDPAlignCenter"><img src="assets/e01f81c8-036d-4083-b101-61505564fa5e.png" class="calibre47"/></p>
<ol start="5" class="calibre14">
<li class="calibre11">We pass our test data to the <kbd class="calibre12">predict</kbd> method to get the predictions:</li>
</ol>
<pre class="calibre18">grid_predictions = grid.predict(X_test)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">Again, we can see the metrics that are provided by <kbd class="calibre12">classification_report</kbd>:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, grid_predictions))</pre>
<p class="calibre20">This gives us the following output. We notice that the average <kbd class="calibre12">precision</kbd> and <kbd class="calibre12">f1-score</kbd> improved from the previous case:</p>
<p class="CDPAlignCenter"><img src="assets/ee2e913d-849e-4077-991b-93d071a4080f.png" class="calibre48"/></p>
<ol start="7" class="calibre14">
<li class="calibre11">Now, we will take a look at the confusion matrix and plot it, like we did earlier:</li>
</ol>
<pre class="calibre18">cnf_matrix = confusion_matrix(Y_test, grid_predictions)<br class="title-page-name"/>plot_confusion_matrix(cnf_matrix,classes=[0,1])<br class="title-page-name"/></pre>
<p class="CDPAlignLeft1">We get the following plot from the preceding code:</p>
<p class="CDPAlignCenter"><img src="assets/77d312c3-89a3-4a46-b046-920e8dcfd943.png" class="calibre49"/></p>
<ol start="8" class="calibre14">
<li class="calibre11">Now, we will look at the accuracy and AUC values again:</li>
</ol>
<p class="calibre2"> </p>
<pre class="calibre18">print("Accuracy score = {:0.2f}".format(accuracy_score(Y_test, grid_predictions)))<br class="title-page-name"/>print("Area under ROC curve = {:0.2f}".format(roc_auc_score(Y_test, grid_predictions)))</pre>
<p class="calibre20">We notice that the accuracy remains the same but that the AUC improves from 0.96 to 0.97:</p>
<p class="CDPAlignCenter"><img src="assets/8df0c134-5624-4f39-afce-466c64cbff69.png" class="calibre50"/></p>
<p class="calibre2"/>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the extreme gradient boosting method for glass identification using XGBoost with scikit-learn </h1>
                </header>
            
            <article>
                
<p class="calibre2">XGBoost stands for <span class="calibre5">extreme gradient boosting. It </span>is a variant of the gradient boosting machine that aims to improve performance and speed. The XGBoost <span class="calibre5">library in Python implements the gradient boosting decision tree algorithm. </span><span class="calibre5">The name gradient boosting comes from its us of the gradient descent algorithm to minimize loss when adding new models. XGBoost can handle both regression and classification tasks.</span></p>
<p class="calibre2">XGBoost is the algorithm of choice among those participating in Kaggle competitions because of its performance and speed of execution in difficult machine learning problems.</p>
<p class="calibre2"><span class="calibre5">Some of the important parameters that are used in XGBoost are as follows:</span></p>
<ul class="calibre10">
<li class="calibre11"><kbd class="calibre12">n_estimators</kbd>/<kbd class="calibre12">ntrees</kbd>:<strong class="calibre1"> </strong>This specifies the number of trees to build. <span>The default value is</span> 50<span>.</span></li>
<li class="calibre11"><kbd class="calibre12">max_depth</kbd>:<strong class="calibre1"> </strong>This specifies the maximum tree depth. The default value is 6. <span>Higher values will make the model more complex and may lead to overfitting. Setting this value to 0 specifies no limit. </span></li>
<li class="calibre11"><kbd class="calibre12">min_rows</kbd>: This<strong class="calibre1"> </strong>specifies the minimum number of observations for a leaf. The default value is 1.</li>
<li class="calibre11"><kbd class="calibre12">learn_rate</kbd>: This s<span>pecifies the learning rate by which to shrink the feature weights. Shrinking feature weights after each boosting step makes the boosting process more conservative and prevents overfitting. The range is 0.0 to 1.0. The default value is 0.3.</span></li>
<li class="calibre11"><kbd class="calibre12">sample_rate</kbd>:<strong class="calibre1"> </strong>This s<span>pecifies the row sampling ratio of the training instance (the <em class="calibre23">x</em> <em class="calibre23">axis</em>). For example, setting this value to 0.5 tells XGBoost to randomly collect half of the data instances to grow trees. The default value is 1 and the range is 0.0 to 1.0. Higher values may improve training accuracy. </span></li>
<li class="calibre11"><kbd class="calibre12">col_sample_rate</kbd>: This s<span>pecifies the column sampling rate (the <em class="calibre23">y axis</em>) for each split in each level. The default value is 1.0</span><span> and the range is from 0 to 1.0. Higher values may improve training accuracy. </span></li>
</ul>
<p class="calibre2"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready...</h1>
                </header>
            
            <article>
                
<p class="calibre2">You will need the <kbd class="calibre12">XGBoost</kbd> library installed to continue with this recipe. You can use the <kbd class="calibre12">pip</kbd> command to install the <kbd class="calibre12"><span>XGBoost</span></kbd> library as follows:</p>
<pre class="calibre15"><strong class="calibre1">!pip install xgboost</strong></pre>
<p class="calibre2">Import the required libraries:</p>
<pre class="calibre15"># Import required libraries<br class="title-page-name"/>import os<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import numpy as np<br class="title-page-name"/><br class="title-page-name"/>from numpy import sort<br class="title-page-name"/><br class="title-page-name"/>from xgboost import XGBClassifier<br class="title-page-name"/>from xgboost import plot_tree<br class="title-page-name"/>from xgboost import plot_importance<br class="title-page-name"/><br class="title-page-name"/>from sklearn.feature_selection import SelectFromModel<br class="title-page-name"/>from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold<br class="title-page-name"/><br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>from sklearn.metrics import accuracy_score, confusion_matrix<br class="title-page-name"/><br class="title-page-name"/>import itertools</pre>
<p class="calibre2">Set your working folder and read your data:</p>
<pre class="calibre15">os.chdir("/.../Chapter 7")<br class="title-page-name"/>os.getcwd()<br class="title-page-name"/><br class="title-page-name"/>df_glassdata = pd.read_csv('glassdata.csv')<br class="title-page-name"/>df_glassdata.shape</pre>
<p class="calibre2">This data has been taken from the UCI ML repository. The column names have been changed according to the data description that's provided at the following link: <a href="https://bit.ly/2EZX6IC" class="calibre9">https://bit.ly/2EZX6IC</a>.</p>
<p class="calibre2">We take a look at the data:</p>
<pre class="calibre15">df_glassdata.head()</pre>
<p class="calibre2">We split our data into a target and feature set, and verify it. Note that we ignore the ID column:</p>
<pre class="calibre15"># split data into X and Y<br class="title-page-name"/>X = df_glassdata.iloc[:,1:10]<br class="title-page-name"/>Y = df_glassdata.iloc[:,10]<br class="title-page-name"/><br class="title-page-name"/>print(X.shape)<br class="title-page-name"/>print(Y.shape)</pre>
<p class="calibre2">We confirm that there are no missing values:</p>
<pre class="calibre15">df_glassdata.isnull().sum()</pre>
<p class="calibre2">We split our dataset into train and test subsets:</p>
<pre class="calibre15"># Create train &amp; test sets<br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=0)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Now, we will proceed to build our first XGBoost model:</span></p>
<ol class="calibre14">
<li class="calibre11">First, we fit our train data into the XGBoost classifier:</li>
</ol>
<pre class="calibre18">xg_model = XGBClassifier()<br class="title-page-name"/>xg_model.fit(X_train, Y_train)</pre>
<ol start="2" class="calibre14">
<li class="calibre11">We can visualize a single XGBoost decision tree from our trained model. <span>Visualizing decision trees can provide insight into the gradient boosting process:</span></li>
</ol>
<pre class="calibre18">plot_tree(xg_model, num_trees=0, rankdir='LR')<br class="title-page-name"/>fig = pyplot.gcf()<br class="title-page-name"/>fig.set_size_inches(30, 30)</pre>
<p class="calibre2"/>
<p class="calibre20">This gives us the following output:</p>
<p class="CDPAlignCenter"><img src="assets/76e980da-6fd8-4498-9b40-d45f6a6996ac.png" class="calibre51"/></p>
<p class="calibre20">With <kbd class="calibre12">num_trees=0</kbd>, we get the first boosted tree. We can view the other boosted trees by setting the index value to the <kbd class="calibre12">num_trees</kbd> parameter.</p>
<ol start="3" class="calibre14">
<li class="calibre11">We set <kbd class="calibre12">num_trees=5</kbd> in the following example:</li>
</ol>
<pre class="calibre18">plot_tree(xg_model, num_trees=5, rankdir='LR')<br class="title-page-name"/>fig = pyplot.gcf()<br class="title-page-name"/>fig.set_size_inches(30, 30)</pre>
<p class="calibre20">The following screenshot shows us the 6th boosted tree:</p>
<p class="CDPAlignCenter"><img src="assets/21e57f28-d9e9-47d4-8583-92582c2b6d1c.png" class="calibre52"/></p>
<p class="calibre2"/>
<p class="calibre2"/>
<div class="packtinfobox">You will need the <kbd class="calibre19">graphviz</kbd> library installed on your system to plot the boosted trees.</div>
<ol start="4" class="calibre14">
<li class="calibre11">We will now use <kbd class="calibre12">predict()</kbd> on our test data to get the predicted values. We can see our test accuracy with <kbd class="calibre12">accuracy_score()</kbd>:</li>
</ol>
<pre class="calibre18">test_predictions = xg_model.predict(X_test)<br class="title-page-name"/>test_accuracy = accuracy_score(Y_test, test_predictions)<br class="title-page-name"/><br class="title-page-name"/>print("Test Accuracy: %.2f%%" % (test_accuracy * 100.0))</pre>
<p class="calibre20">By executing this code, we can see the test accuracy is 69.23%. </p>
<ol start="5" class="calibre14">
<li class="calibre11">We can see our confusion matrix by using the following code:</li>
</ol>
<pre class="calibre18">confusion_matrix(Y_test, predictions)</pre>
<ol start="6" class="calibre14">
<li class="calibre11">We can then use a predefined function, <kbd class="calibre12">plot_confusion_matrix()</kbd>, which we have sourced from <a href="https://scikit-learn.org" class="calibre9">https://scikit-learn.org</a>:</li>
</ol>
<pre class="calibre18">def plot_confusion_matrix(cm, classes,<br class="title-page-name"/>                          normalize=False,<br class="title-page-name"/>                          title='Confusion matrix',<br class="title-page-name"/>                          cmap=plt.cm.Blues):<br class="title-page-name"/>    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br class="title-page-name"/>    plt.title(title)<br class="title-page-name"/>    plt.colorbar()<br class="title-page-name"/>    tick_marks = np.arange(len(classes))<br class="title-page-name"/>    plt.xticks(tick_marks, classes, rotation=45)<br class="title-page-name"/>    plt.yticks(tick_marks, classes)<br class="title-page-name"/><br class="title-page-name"/>    fmt = '.2f' if normalize else 'd'<br class="title-page-name"/>    thresh = cm.max() / 2.<br class="title-page-name"/>    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br class="title-page-name"/>        plt.text(j, i, format(cm[i, j], fmt),<br class="title-page-name"/>                 horizontalalignment="center",<br class="title-page-name"/>                 color="white" if cm[i, j] &gt; thresh else "black")<br class="title-page-name"/><br class="title-page-name"/>    plt.ylabel('True label')<br class="title-page-name"/>    plt.xlabel('Predicted label')<br class="title-page-name"/>    plt.tight_layout()</pre>
<ol start="7" class="calibre14">
<li class="calibre11">We then look at the <kbd class="calibre12">unique</kbd> values of our target variable to set the names of each level of our target variable:</li>
</ol>
<pre class="calibre18">Y.unique()</pre>
<p class="calibre20">In the following code block, we can see the <kbd class="calibre12">target_names</kbd> values as <kbd class="calibre12">1</kbd>, <kbd class="calibre12">2</kbd>, <kbd class="calibre12">3</kbd>, <kbd class="calibre12">5</kbd>, <kbd class="calibre12">6</kbd>, and <kbd class="calibre12">7</kbd>. We set the names to each level of our target variable accordingly:</p>
<pre class="calibre18"># Set names to each level of our target variable<br class="title-page-name"/>target_names = [ '1', '2', '3', '5', '6', '7']<br class="title-page-name"/><br class="title-page-name"/># Pass Actual &amp; Predicted values to confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, predictions)<br class="title-page-name"/><br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">We can now visualize the confusion matrix, as shown in the following screenshot:</p>
<p class="CDPAlignCenter"><img src="assets/94cf908f-30aa-49c6-ad21-e09a5d42b239.png" class="calibre53"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In <em class="calibre13">Step 1</em>, we fit the <kbd class="calibre12">XGBoostClassfier</kbd> to our train data. In <em class="calibre13">Step 2</em> and <em class="calibre13"><span class="calibre5">Step </span>3</em>, we visualized the individual boosted trees. To do this, we used the <span class="calibre5"><kbd class="calibre12">plot_tree()</kbd> function. We passed our <kbd class="calibre12">XGBoost</kbd> model to the <kbd class="calibre12">plot_tree()</kbd></span> and <span class="calibre5">set the index of the tree by setting the <kbd class="calibre12">num_trees</kbd> parameter. The</span> <span class="calibre5"><kbd class="calibre12">rankdir='LR'</kbd> </span><span class="calibre5">parameter </span><span class="calibre5">plotted the tree from left to right. Setting</span> <kbd class="calibre12">rankdir</kbd> <span class="calibre5">to UT would plot a vertical tree. </span></p>
<p class="calibre2">In <em class="calibre13">Step 4</em>, we passed our test subset to <kbd class="calibre12">predict()</kbd> to get the test accuracy. <em class="calibre13">Step 5</em> gave us the confusion matrix. In <em class="calibre13">Step 6</em>, we sourced a predefined function, <span class="calibre5"><kbd class="calibre12">plot_confusion_matrix()</kbd>, from <a href="https://scikit-learn.org/stable/" class="calibre9">scikit-learn.org</a>. We used this function to plot our confusion matrix. </span>In <em class="calibre13">Step 7</em>, we looked at the unique values of our target variable so that we could set the names for each class of our confusion matrix plot. We then plotted our confusion matrix to evaluate our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">In this section, we will look at how we can check feature importance and perform feature selection based on that. We will also look at how we can evaluate the performance of our XGBoost model using cross-validation. </span></p>
<p class="calibre2">We can check feature importance with <kbd class="calibre12">model.feature_importances_</kbd>:</p>
<pre class="calibre15">print(xg_model.feature_importances_)</pre>
<p class="calibre2">We can also visualize feature importance using <kbd class="calibre12">plot_importance()</kbd>:</p>
<div class="packtinfobox">Note that we have imported <kbd class="calibre19">plot_importance</kbd> <span>from the <kbd class="calibre19">xgboost</kbd></span><span> library.</span></div>
<pre class="calibre15">plot_importance(xg_model)</pre>
<p class="calibre2"/>
<p class="calibre2">After executing the preceding code, we get to see the following chart, which shows feature importance in descending order of importance:</p>
<p class="CDPAlignCenter"><img src="assets/c9e25012-3c0d-4e66-b74a-1b944c24b59c.png" class="calibre54"/></p>
<p class="calibre2">Feature importance can be used for feature selection using <span class="calibre5"><kbd class="calibre12">SelectFromModel</kbd>.</span></p>
<div class="packtinfobox">The <kbd class="calibre19">SelectFromModel</kbd> class is imported from <kbd class="calibre19">sklearn.feature_selection</kbd>.</div>
<p class="calibre2">In the following example, the <kbd class="calibre12">SelectFromModel</kbd> takes the pretrained <kbd class="calibre12">XGBoost</kbd> model and provides a subset from our dataset with the selected features. It decides on the selected features based on a threshold value. </p>
<p class="calibre2"><span class="calibre5">Features that have an importance that is greater than or equal to the threshold value are kept, while any others are discarded:</span></p>
<pre class="calibre15"># The threshold value to use for feature selection. <br class="title-page-name"/>feature_importance = sort(xg_model.feature_importances_)<br class="title-page-name"/><br class="title-page-name"/># select features using threshold<br class="title-page-name"/>for each_threshold in feature_importance:<br class="title-page-name"/>    selection = SelectFromModel(xg_model, threshold=each_threshold, prefit=True)<br class="title-page-name"/>    <br class="title-page-name"/>    # Reduce X_train only to the selected feature<br class="title-page-name"/>    selected_feature_X_train = selection.transform(X_train)<br class="title-page-name"/>    <br class="title-page-name"/>    # Train the model<br class="title-page-name"/>    selection_model = XGBClassifier()<br class="title-page-name"/>    selection_model.fit(selected_feature_X_train, Y_train)<br class="title-page-name"/>    <br class="title-page-name"/>    # Reduce X_test only to the selected feature<br class="title-page-name"/>    selected_feature_X_test = selection.transform(X_test)<br class="title-page-name"/>    <br class="title-page-name"/>    # Predict using the test value of the selected feature<br class="title-page-name"/>    predictions = selection_model.predict(selected_feature_X_test)<br class="title-page-name"/>    <br class="title-page-name"/>    accuracy = accuracy_score(Y_test, predictions)<br class="title-page-name"/>    print("Threshold=%.5f, Number of Features=%d, Model Accuracy: %.2f%%" % (each_threshold, selected_feature_X_train.shape[1],accuracy*100))</pre>
<p class="calibre2">From the preceding code, we get to see the following output:</p>
<p class="CDPAlignCenter"><img src="assets/2ad16bc9-18cf-45ec-88ca-7fb8be176527.png" class="calibre55"/></p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2"><span class="calibre5">We notice that the performance of the model fluctuates with the number of selected features. Based on the preceding output, we decide to opt for five features that give us an accuracy value of 72%. Also, if we use the Occam's razor principle, we can probably opt for a simpler model with four features that gives us a slightly lower accuracy of 71%.</span></p>
</div>
<p class="calibre2">We can also evaluate our models using cross-validation. To perform k-fold cross-validation, we must import the <kbd class="calibre12">KFold</kbd> class from <kbd class="calibre12">sklearn.model_selection</kbd>. </p>
<p class="calibre2">First, we create the <kbd class="calibre12">KFold</kbd> object and mention the number of splits that we would like to have:</p>
<pre class="calibre15">kfold = KFold(n_splits=40, random_state=0)<br class="title-page-name"/>xg_model_with_kfold = XGBClassifier()<br class="title-page-name"/><br class="title-page-name"/>cv_results = cross_val_score(xg_model_with_kfold, X_train, Y_train, cv=kfold, verbose=True)<br class="title-page-name"/>print("Mean Accuracy: %.2f%% Standard Deviation %.2f%%" % (cv_results.mean()*100, cv_results.std()*100))</pre></div>
</div>
<p class="calibre2">With <kbd class="calibre12">cross_val_score()</kbd>, we evaluate our model, which gives us the <span class="calibre5">mean and standard deviation classification accuracy. We notice that we get a </span>mean accuracy of 77.92% and a standard deviation of 22.33%.</p>
<p class="calibre2">In our case, we have a target variable with six classes.</p>
<p class="calibre2">If you have many classes for a multi-class classification task, you may use stratified folds when performing cross-validation:</p>
<pre class="calibre15">Stratfold = StratifiedKFold(n_splits=40, random_state=0)<br class="title-page-name"/>xg_model_with_stratfold = XGBClassifier()<br class="title-page-name"/><br class="title-page-name"/>sf_results = cross_val_score(xg_model_with_stratfold, X_train, Y_train, cv=Stratfold, verbose=True)<br class="title-page-name"/>print("Mean Accuracy: %.2f%% Standard Deviation %.2f%%" % (sf_results.mean()*100, sf_results.std()*100))</pre>
<p class="calibre2">With <kbd class="calibre12">StratifiedKFold()</kbd>, we get an improved mean accuracy of 81.18% and a reduced standard deviation of 21.37%.</p>
<div class="packttip">Note that <kbd class="calibre19">n_splits</kbd> cannot be greater than the number of members in each class.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul class="calibre10">
<li class="calibre11">LightGBM is an open source software for the gradient boosting framework that was developed by Microsoft. It uses the tree-based algorithm differently to other <strong class="calibre1">Gradient Boosting Machines</strong> (<strong class="calibre1">GBMs</strong>): <a href="https://bit.ly/2QW53jH" class="calibre9">https://bit.ly/2QW53jH</a></li>
</ul>


            </article>

            
        </section>
    </body></html>