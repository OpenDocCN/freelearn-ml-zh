- en: Chapter 10. Estimating Projective Relations in Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the fundamental matrix of an image pair
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching images using random sample consensus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing a homography between two images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting a planar target in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Images are generally produced using a digital camera, which captures a scene
    by projecting light going through its lens onto an image sensor. The fact that
    an image is formed by the projection of a 3D scene onto a 2D plane implies the
    existence of important relationships both between a scene and its image and between
    different images of the same scene. Projective geometry is the tool that is used
    to describe and characterize, in mathematical terms, the process of image formation.
    In this chapter, we will introduce you to some of the fundamental projective relations
    that exist in multi-view imagery and explain how these can be used in computer
    vision programming. But, before we start the recipes, let's explore the basic
    concepts related to scene projection and image formation.
  prefs: []
  type: TYPE_NORMAL
- en: Image formation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fundamentally, the process used to produce images has not changed since the
    beginning of photography. The light coming from an observed scene is captured
    by a camera through a frontal aperture, and the captured light rays hit an image
    plane (or an image sensor) located at the back of the camera. Additionally, a
    lens is used to concentrate the rays coming from the different scene elements.
    This process is illustrated by the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/image_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, `do` is the distance from the lens to the observed object, `di` is the
    distance from the lens to the image plane, and `f` is the focal length of the
    lens. These quantities are related by the so-called **thin lens equation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/B05388_10_15-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In computer vision, this camera model can be simplified in a number of ways.
    Firstly, we can neglect the effect of the lens by considering that we have a camera
    with an infinitesimal aperture since, in theory, this does not change the image
    appearance. (However, by doing so, we ignore the focusing effect by creating an
    image with an infinite depth of field.) In this case, therefore, only the central
    ray is considered. Secondly, since most of the time we have `do>>di`, we can assume
    that the image plane is located at the focal distance. Finally, we can note from
    the geometry of the system that the image on the plane is inverted. We can obtain
    an identical but upright image by simply positioning the image plane in front
    of the lens. Obviously, this is not physically feasible, but from a mathematical
    point of view, this is completely equivalent. This simplified model is often referred
    to as the **pinhole camera ** **model**, and it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/image_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From this model, and using the law of similar triangles, we can easily derive
    the basic projective equation that relates a photographed object with its image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/B05388_10_16-2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The size (`hi`) of the image of an object (of physical height `ho`) is therefore
    inversely proportional to its distance (`do`) from the camera, which is naturally
    true. In general, this relation describes where a 3D scene point will be projected
    on the image plane given the geometry of the camera. More specifically, if we
    assume that the reference frame is positioned at the focal point, then a 3D scene
    point located at position `(X,Y,Z)` will be projected onto the image plane at
    `(x,y)=(fX/Z,fY/Z)`. Here, the `Z` coordinate corresponds with the depth of the
    point (or distance to camera, denoted by `do` in the previous equation). This
    relation can be rewritten in a simple matrix form through the introduction of
    homogeneous coordinates, in which 2D points are represented by 3-vectors, and
    3D points are represented by 4-vectors (the extra coordinate is simply an arbitrary
    scale factor `s` that needs to be removed when a 2D coordinate needs to be extracted
    from a homogeneous 3-vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/B05388_10_17-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This `3x4` matrix is called the projection matrix. In cases where the reference
    frame is not aligned with the focal point, then rotation `r` and translation `t` matrices
    must be introduced. The role of these ones is simply to express the projected
    3D point into a camera-centric reference frame, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image formation](img/B05388_10_18-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The first matrix of this equation is said to contain the intrinsic parameters
    of the camera (here, only the focal length, but the next chapter will introduce
    a few more intrinsic parameters). The second matrix contains the extrinsic parameters
    that are the parameters that relate the camera to the exterior world. It should
    be noted that, in practice, image coordinates are expressed in pixels while 3D
    coordinates are expressed in world measurements (for example, meters). This aspect
    will be explored in [Chapter 11](ch11.html "Chapter 11. Reconstructing 3D Scenes")
    , *Reconstructing 3D Scenes*.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the fundamental matrix of an image pair
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The introductory section of this chapter presented the projective equation,
    describing how a scene point projects onto the image plane of a single camera.
    In this recipe, we will explore the projective relationship that exists between
    two images that display the same scene. These two images could have been obtained
    by moving a camera to two different locations to take pictures from two viewpoints,
    or by using two cameras, each of them taking a different picture of the scene.
    When those two cameras are separated by a rigid baseline, we use the term **stereovision**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now consider two pinhole cameras observing a given scene point, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/image_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We learned that we can find the image `x` of a 3D point `X` by tracing a line
    joining this 3D point with the camera's center. Conversely, the scene point that
    has its image at position **x** on the image plane can be located anywhere on
    this line in the 3D space. This implies that, if we want to find the corresponding
    point of a given image point in another image, we need to search along the projection
    of this line onto the second image plane. This imaginary line is called the **epipolar
    line** of point `x`. It defines a fundamental constraint that must satisfy two
    corresponding points; that is, the match of a given point must lie on the epipolar
    line of this point in the other view, and the exact orientation of this epipolar
    line depends on the respective position of the two cameras. In fact, the configuration
    of the set of possible epipolar lines characterizes the geometry of a two-view
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Another observation that can be made from the geometry of this two-view system
    is that all the epipolar lines pass through the same point. This point corresponds
    to the projection of one camera's center onto the other camera (points `e` and
    `e'` in the above figure). This special point is called an **epipole**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the relationship between an image point and its corresponding
    epipolar line can be expressed using a `3x3` matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_10_19-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In projective geometry, a 2D line is also represented by a 3-vector. It corresponds
    to the set of 2D points `(x',y')`, that satisfy the equation `l1'x'+ l2'y'+ l3'=
    0` (the prime superscript denotes that this line belongs to the second image).
    Consequently, the matrix `F`, called the fundamental matrix, maps a 2D image point
    in one view to an epipolar line in the other view.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fundamental matrix of an image pair can be estimated by solving a set of
    equations that involve a certain number of known matched points between the two
    images. The minimum number of such matches is seven. In order to illustrate the
    fundamental matrix estimation process, we selected seven good matches from the
    matching results of SIFT features, as presented in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'These matches will be used to compute the fundamental matrix using the `cv::findFundamentalMat`
    OpenCV function. The image pair with its selected matches is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These matches are stored in a `cv::DMatch` vector pointing to indexes of `cv::keypoint`
    instances. These keypoints first need to be converted into `cv::Point2f` in order
    to be used with `cv::findFundamentalMat`. An OpenCV function can be used to this
    end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The two resulting vectors `selPoints1` and `selPoints2` contain the corresponding
    point coordinates in the two images. The `pointIndexes1` and `pointIndexes2` vectors
    contain the indexes of the keypoints to be converted. The call to the `cv::findFundamentalMat`
    function is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'One way to visually verify the validity of the fundamental matrix is to draw
    the epipolar lines of some selected points. Another OpenCV function allows the
    epipolar lines of a given set of points to be computed. Once these have been computed,
    they can be drawn using the `cv::line` function. The following lines of code accomplish
    these two steps (that is, computing and drawing epipolar lines on the image on
    the right from the points in the image on the left):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The epipolar lines of the left image are obtained in a similar way. The following
    image shows these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Remember that the epipole of one image is at the intersection of all its epipolar
    lines. This one is the projection of the other camera's center. Note that the
    epipolar lines can intersect (and often do) outside of the image boundaries. In
    the case of our example, the epipole of the second image is at the location where
    the first camera would be visible if the two images were taken at the same instant.
    Note also that the results can be quite unstable when the fundamental matrix is
    computed from only seven matches. Indeed, substituting one match for another could
    lead to a significantly different set of epipolar lines.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We previously explained that, for a point in one image, the fundamental matrix
    gives the equation of the line on which its corresponding point in the other view
    should be found. If the corresponding point of a point `(x,y)` is `(x'',y'')`,
    suppose we have `F`, the fundamental matrix, between the two views. Since `(x'',y'')`
    lies on the epipolar line given by multiplying `F` by `(x,y)` expressed in homogenous
    coordinates, we must then have the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/B05388_10_20-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This equation expresses the relationship between two corresponding points and
    is known as the **epipolar constraint**. Using this equation, it becomes possible
    to estimate the entries of the matrix using known matches. Since the entries of
    the `F` matrix are given up to a scale factor, there are only eight entries to
    be estimated (the ninth one can be arbitrarily set to `1`). Each match contributes
    to one equation. Therefore, with eight known matches, the matrix can be fully
    estimated by solving the resulting set of linear equations. This is what is done
    when you use the `cv::FM_8POINT` flag with the `cv::findFundamentalMat` function.
    Note that, in this case, it is possible (and preferable) to input more than eight
    matches. The obtained over-determined system of linear equations can then be solved
    in a mean-square sense.
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the fundamental matrix, an additional constraint can also be exploited.
    Mathematically, the `F` matrix maps a 2D point to a 1D pencil of lines (that is,
    lines that intersect at a common point). The fact that all these epipolar lines
    pass through this unique point (that is, the epipole) imposes a constraint on
    the matrix. This constraint reduces the number of matches required to estimate
    the fundamental matrix to seven. In mathematical terms, we say that the fundamental
    matrix has 7 degrees of freedom and is therefore of rank-2\. Unfortunately, in
    this case, the set of equations becomes nonlinear, with up to three possible solutions
    (in this case, `cv::findFundamentalMat` will return a fundamental matrix of the
    size `9x3`, that is, three `3x3` matrices stacked up). The seven-match solution
    of the `F` matrix estimation can be invoked in OpenCV by using the `cv::FM_7POINT`
    flag. This is what we did in the example in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it should be mentioned that the choice of an appropriate set of matches
    in the image is important to obtain an accurate estimation of the fundamental
    matrix. In general, the matches should be well distributed across the images and
    include points at different depths in the scene. Otherwise, the solution will
    become unstable. In particular, the selected scene points should not be coplanar,
    as the fundamental matrix (in this case) becomes degenerated.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Multiple View Geometry in Computer Vision*, Cambridge University Press, 2004,
    *R. Hartley* and *A. Zisserma*n, is the most complete reference on projective
    geometry in computer vision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Matching images using random sample **consensus* recipe explains how a
    fundamental matrix can be robustly estimated from a larger match set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Computing a homography between two images* recipe explains why a fundamental
    matrix cannot be computed when the matched points are coplanar, or are the result
    of a pure rotation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matching images using random sample consensus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When two cameras observe the same scene, they see the same elements but under
    different viewpoints. We have already studied the feature point matching problem
    in the previous chapter. In this recipe, we come back to this problem, and we
    will learn how to exploit the epipolar constraint introduced in the previous recipe
    to match image features more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle that we will follow is simple: when we match feature points between
    two images, we only accept those matches that fall on corresponding epipolar lines.
    However, to be able to check this condition, the fundamental matrix must be known,
    but we need good matches to estimate this matrix. This seems to be a chicken-and-egg
    problem. However, in this recipe, we propose a solution in which the fundamental
    matrix and a set of good matches will be jointly computed.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The objective is to be able to compute a fundamental matrix and a set of good
    matches between two views. To do so, all the found feature point correspondences
    will be validated using the epipolar constraint introduced in the previous recipe.
    To this end, we have created a class that encapsulates the different steps of
    the proposed robust matching process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Users of this class simply supply the feature detector and descriptor instances
    of their choice. These ones can also be specified using the defined `setFeatureDetector`
    and `setDescriptorExtractor` setter methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main method is the match method, which returns matches, detected keypoints,
    and the estimated fundamental matrix. The method proceeds in four distinct steps
    (explicitly identified in the comments of the following code), which we will now
    explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first two steps simply detect the feature points and compute their descriptors.
    Next, we proceed to feature matching using the `cv::BFMatcher` class, as we did
    in the previous chapter. We use the crosscheck flag to obtain matches of better
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth step is the new concept introduced in this recipe. It consists of
    an additional filtering test that will this time use the fundamental matrix in
    order to reject matches that do not obey the epipolar constraint. This test is
    based on the `RANSAC` method that can compute the fundamental matrix even when
    outliers are present in the match set (this method will be explained in the next
    section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using our `RobustMatcher` class, the robust matching of an image pair is then
    easily accomplished by the following calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in `54` matches that are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Most of the time, the resulting matches will be good matches. However, a few
    false matches might remain; these are ones that accidently fell on the corresponding
    epipolar lines of the computed fundamental matrix.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding recipe, we learned that it is possible to estimate the fundamental
    matrix associated with an image pair from a number of feature point matches. Obviously,
    to be exact, this match set must be made up of only good matches. However, in
    a real context, it is not possible to guarantee that a match set obtained by comparing
    the descriptors of the detected feature points will be completely exact. This
    is why a fundamental matrix estimation method based on the **RANSAC** (**RANdom
    SAmpling Consensus**) strategy has been introduced.
  prefs: []
  type: TYPE_NORMAL
- en: The RANSAC algorithm aims to estimate a given mathematical entity from a data
    set that may contain a number of outliers. The idea is to randomly select some
    data points from the set and perform the estimation with only those. The number
    of selected points should be the minimum number of points required to estimate
    the mathematical entity. In the case of the fundamental matrix, eight matched
    pairs is the minimum number (in fact, the real minimum is seven matches, but the
    8-point linear algorithm is faster to compute). Once the fundamental matrix is
    estimated from these eight random matches, all the other matches in the match
    set are tested against the epipolar constraint that derives from this matrix.
    All the matches that fulfill this constraint (that is, matches for which the corresponding
    feature is at a short distance from its epipolar line) are identified. These matches
    form the support set of the computed fundamental matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The central idea behind the RANSAC algorithm is that, the larger the support
    set, the higher the probability that the computed matrix is the right one. Conversely,
    if one (or more) of the randomly selected matches is an incorrect match, then
    the computed fundamental matrix will also be incorrect, and its support set will
    be expected to be small. This process is repeated a number of times and, in the
    end, the matrix with the largest support will be retained as the most probable
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, our objective is to pick eight random matches several times so that
    eventually we select eight good ones, which should give us a large support set.
    Depending on the proportion of incorrect matches in the entire data set, the probability
    of selecting a set of eight correct matches will differ. However, we know that,
    the more selections we make, the higher our confidence will be that we have at
    least one good match set among those selections. More precisely, if we assume
    that the match set is made of `w%` inliers (good matches), then the probability
    that we select eight good matches is `w⁸.` Consequently, the probability that
    a selection contains at least one incorrect match is `(1-w⁸)`. If we make `k`
    selections, the probability of having one random set that contains good matches
    only is `1-(1-w⁸)^k`.
  prefs: []
  type: TYPE_NORMAL
- en: This is the confidence probability `c`, and we want this probability to be as
    high as possible, since we need at least one good set of matches in order to obtain
    the correct fundamental matrix. Therefore, when running the RANSAC algorithm,
    one needs to determine the number of `k` selections that need to be made in order
    to obtain a given confidence level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use of the RANSAC method to estimate the fundamental matrix is done inside
    the `ransacTest` method of our `RobustMatcher` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code is a bit long because the keypoints need to be converted into `cv::Point2f`
    before the F matrix computation. When using the `cv::findFundamentalMat` function
    with the `cv::FM_RANSAC` method, two extra parameters are provided. One of these
    extra parameters is the confidence level, which determines the number of iterations
    to be made (by default, it is `0.99`). The other parameter is the maximum distance
    to the epipolar line for a point to be considered as an inlier. All of the matched
    pairs in which a point is at a greater distance from its epipolar line than the
    distance specified will be reported as an outlier. The function also returns `std::vector`
    of the character value, indicating that the corresponding match in the input set
    has been identified either as an outlier (`0`) or as an inlier (`1`). This explains
    the last loop of our method that extracts the good matches from the original match
    set.
  prefs: []
  type: TYPE_NORMAL
- en: The more good matches you have in your initial match set, the higher the probability
    that RANSAC will give you the correct fundamental matrix. This is why we applied
    the crosscheck filter when matching the feature points. You could have also used
    the ratio test presented in the previous recipe in order to further improve the
    quality of the final match set. It is just a question of balancing the computational
    complexity, the final number of matches, and the required level of confidence
    that the obtained match set will contain only exact matches.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The result of the robust matching process presented in this recipe is: 1) an
    estimate of the fundamental matrix computed using the eight selected matches that
    have the largest support and 2) the match set included in this support set. Using
    this information, it is possible to refine these results in two ways.'
  prefs: []
  type: TYPE_NORMAL
- en: Refining the fundamental matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since we now have a match set of good quality, as a last step, it might be
    a good idea to use all of them to re-estimate the fundamental matrix. We already
    mentioned that a linear 8-point algorithm to estimate this matrix exists. We can,
    therefore, obtain an over-determined system of equations that will solve the fundamental
    matrix in a least-squares sense. This step can be added the end of our `ransacTest`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `cv::findFundamentalMat` function does indeed accept more than `8` matches
    by solving the linear system of equations using singular value decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Refining the matches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that in a two-view system, every point must lie on the epipolar
    line of its corresponding point. This is the epipolar constraint expressed by
    the fundamental matrix. Consequently, if you have a good estimate of a fundamental
    matrix, you can use this epipolar constraint to correct the obtained matches by
    forcing them to lie on their epipolar lines. This can be easily done by using
    the `cv::correctMatches` OpenCV function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This function proceeds by modifying the position of each corresponding point
    so that it satisfies the epipolar constraint while minimizing the cumulative (squared)
    displacement.
  prefs: []
  type: TYPE_NORMAL
- en: Computing a homography between two images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first recipe of this chapter showed you how to compute the fundamental matrix
    of an image pair from a set of matches. In projective geometry, another very useful
    mathematical entity also exists. This one can be computed from multi-view imagery
    and, as we will see, is a matrix with special properties.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, let''s consider the projective relation between a 3D point and its image
    on a camera, which we presented in the introduction section of this chapter. Basically,
    we learned that this equation relates a 3D point to its image using the intrinsic
    properties of the camera and the position of that camera (specified with a rotation
    and a translation component). If we now carefully examine this equation, we realize
    that there are two special situations of particular interest. The first situation
    is when two views of a scene are separated by a pure rotation. We can then observe
    that the fourth column of the extrinsic matrix will be made up of `0`s (that is,
    the translation is null):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_10_21-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As a result, the projective relation in this special case becomes a `3x3` matrix.
    A similarly interesting situation also occurs when the object we observe is a
    plane. In this specific case, we can assume without loss of generality that the
    points on this plane will be located at `Z=0`. As a result, we obtain the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_10_22-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This zero coordinate of the scene points will then cancel the third column
    of the projective matrix, which will then again become a `3x3` matrix. This special
    matrix is called a **homography**, and it implies that, under special circumstances
    (here, a pure rotation or a planar object), a world point can be related to its
    image by a linear relation. In addition, because this matrix is invertible, you
    can also relate an image point on one view directly to its corresponding point
    on the other view, given that these two views are separated by a pure rotation,
    or are imaging a planar object. The homographic relation is then of the following
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/B05388_10_23-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, `H` is a `3x3` matrix. This relation holds up to a scale factor represented
    here by the `s` scalar value. Once this matrix is estimated, all the points in
    one view can be transferred to a second view using this relation. This is the
    property that will be exploited in this recipe and the next one. Note that, as
    a side effect of the homography relation, the fundamental matrix becomes undefined
    in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose that we have two images separated by a pure rotation. This happens,
    for example, when you take pictures of a building or a landscape by rotating yourself;
    as you are sufficiently far away from your subject, the translational component
    is, in this case, negligible. These two images can be matched using the features
    of your choice and the `cv::BFMatcher` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, as we did in the previous recipe, we will apply a RANSAC step that will
    this time involve the estimation of a homography based on a match set (which obviously
    contains a good number of outliers). This is done by using the `cv::findHomography`
    function, which is very similar to the `cv::findFundamentalMat` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that a homography exists (instead of a fundamental matrix) because our
    two images are separated by a pure rotation. We display here the inlier keypoints
    as identified by the `inliers` argument of the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The homography is a `3x3` invertible matrix. Therefore, once it has been computed,
    you can transfer image points from one image to the other. In fact, you can do
    this for every pixel of an image. Consequently, you can transfer a complete image
    to the point of view of a second image. This process is called image **mosaicing**
    or image **stitching** and is often used to build a large panorama from multiple
    images. An OpenCV function that does exactly this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this new image is obtained, it can be appended to the other image in order
    to expand the view (since the two images are now from the same point of view):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When two views are related by a homography, it becomes possible to determine
    where a given scene point on one image is found on the other image. This property
    becomes particularly interesting for the points in one image that fall outside
    the image boundaries of the other. Indeed, since the second view shows a portion
    of the scene that is not visible in the first image, you can use the homography
    in order to expand the image by reading the color value of the additional pixels
    in the other image. That's how we were able to create a new image that is an expansion
    of our second image in which extra columns were added to the right-hand side.
  prefs: []
  type: TYPE_NORMAL
- en: The homography computed by `cv::findHomography` is the one that maps the points
    in the first image to the points in the second image. This homography can be computed
    from a minimum of four matches and the RANSAC algorithm is again used here. Once
    the homography with the best support is found, the `cv::findHomography` method
    refines it using all the identified inliers.
  prefs: []
  type: TYPE_NORMAL
- en: Now, in order to transfer the points of image `1` to image `2`, what we need
    is, in fact, the inverse homography. This is exactly what the `cv::warpPerspective`
    function is doing by default; that is, it uses the inverse of the homography provided
    as the input to get the color value of each point of the output image (this is
    what we called backward mapping in [Chapter 2](ch02.html "Chapter 2. Manipulating
    Pixels") , *Manipulating Pixels*). When an output pixel is transferred to a point
    outside the input image, a black value (`0`) is simply assigned to this pixel.
    Note that a `cv::WARP_INVERSE_MAP` flag can be specified as the optional fifth
    argument in `cv::warpPerspective` if you want to use direct homography instead
    of the inverted one during the pixel transfer process.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `contrib` package of OpenCV offers a complete stitching solution that can
    produce high-quality panoramas from multiple images.
  prefs: []
  type: TYPE_NORMAL
- en: Generating image panoramas with the cv::Stitcher module
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The mosaic we obtained in this recipe is good but still contains some defects.
    The alignment of the images is not perfect and we can clearly see the cut between
    the two images because the brightness and contrast in the two images are not the
    same. Fortunately, there is now a stitching solution in OpenCV that looks at all
    these aspects and tries to produce a panorama of optimal quality. This solution
    is quite complex and elaborated but, at its core, it relies on the principles
    learned in this recipe. That is, matching feature points in images and robustly
    estimating a homography. In addition, the solution estimates the intrinsic and
    extrinsic camera parameters to ensure a better alignment. It also nicely blends
    the images together by compensating for the difference in exposure conditions.
    The high-level call of this function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Numerous parameters in the instance can be adjusted to obtain high-quality
    results. Interested readers should explore this package in more depth in order
    to learn more about it. In our case, the result obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating image panoramas with the cv::Stitcher module](img/image_10_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, in general, an arbitrary number of input images can be used to compose
    a large panorama.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Remapping an image* recipe in [Chapter 2](ch02.html "Chapter 2. Manipulating
    Pixels") , *Manipulating Pixels*, discusses the concept of backward mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Automatic panoramic image stitching using invariant* features article by
    *M. Brown* and *D. Lowe* in *International Journal of Computer Vision,*74, 1,
    2007, describes a complete method for building panoramas from multiple images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting a planar target in images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we explained how homographies can be used to stitch
    together images separated by a pure rotation to create a panorama. In this recipe,
    we also learned that different images of a plane also generate homographies between
    views. We will now see how we can make use of this fact to recognize a planar
    object in an image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you want to detect the occurrence of a planar object in an image. This
    object could be a poster, painting, logo, signage, and so on. Based on what we
    have learned in this chapter, the strategy would consist of detecting feature
    points on this planar object and to try to match them with the feature points
    in the image. These matches would then be validated using a robust matching scheme
    similar to the one we used previously, but this time based on a homography. If
    the number of valid matches is high, then this must mean that our planar object
    is visible in the current image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, our mission is to detect the occurrence of the first edition
    of our book in an image, more specifically, the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_020-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s define a `TargetMatcher` class that is very similar to our `RobustMatcher`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The reference image of the planar object to be matched is held by the `target`
    attribute. As it will be explained in the next section, feature points will be
    detected in a pyramid of images of the target successively down-sampled. The matching
    methods are similar to the ones of the `RobustMatcher` class, except that they
    include `cv::findHomography` instead of `cv::findFundamentalMat` in the `ransacTest`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `TargetMatcher` class, a specific feature point detector and descriptor
    must be instantiated and passed to the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we selected the FAST detector in conjunction with the BRISK descriptor
    because they are quick to compute. Then, you must specify the target to be detected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, this is the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'You can detect this target in an image by calling the `detectTarget` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This method returns the position of the four corners of the target in the image
    (if found). Lines can then be drawn to visually validate the detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/image_10_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we do not know what the size of the target in the image is, we have decided
    to build a pyramid made of the target image in different sizes. Another option
    would have been to use scale-invariant features. At each level of our pyramid,
    the size of the target image is reduced by a certain factor (attribute `scaleFactor`,
    `0.9` by default) and the pyramid is made of a number of levels (attribute `numberOfLevels`,
    `8` by default). Feature points are detected for each level of the pyramid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `detectTarget` method then proceeds onto three steps. Firstly, interest
    points are detected in the input image. Secondly, this image is robustly matched
    with each image of the target pyramid. The level with the highest number of inliers
    is retained. If this one has a sufficiently high number of surviving matches,
    then we have found the target. The third step consists of reprojecting the four
    corners of the target to the correct scale onto the input image using the found
    homography and the `cv::getPerspectiveTransform` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the matching results obtained in the case of our
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/image_10_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Fast and robust homography scheme for real-time planar target detection*
    article by *H. Bazargani*, *O. Bilaniuk* and *R. Laganière* in *Journal of Real-Time
    Image Processing*, May 2015, describes a method to detecting a planar target in
    real-time. It also describes the `cv::RHO` method for the `cv::findHomography`
    function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
