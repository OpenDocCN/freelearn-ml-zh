- en: Chapter 8. Developing Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The year 2017 was all about chatbots, and that continues in 2018\. Chatbots
    are not new at all. The concept of chatbots has been around since the 1970s. Sometimes,
    a chatbot application is also referred to as a question-answering system. This
    is a more specific technical term for a chatbot. Let's take a step into history.
    Lunar was the first rule-based question-answering system. Using this system, geologists
    could ask questions regarding the moon rock from the Apollo missions. In order
    to improvise the rule-based system that was used in the Apollo mission, we had
    to find out a way to encode pattern-based question and answers. For this purpose,
    **Artificial Intelligence Markup Language** was used, also called **AIML**. This
    helps the programmer code less lines of code in order to achieve the same result
    that we generated by using a hardcoded pattern-based system. With recent advances
    in the field of **Machine Learning** (**ML**), we can build a chatbot without
    hardcoded responses.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots are now used in apps because of the numerous benefits they have; for
    example, users don't need to install different varieties of apps on their mobile.
    If there is a chatbot that provides you the news, then you can ask for news that
    is on CNN or The Economic Times. Big tech giants such as Facebook, Hike, WeChat,
    Snapchat, Slack, and so on provide chatbots for better customer engagement. They
    achieve this by making a chatbot that one can guide their customers in order to
    perform some operations; it also provides useful information about the product
    and its platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots provide different services. By using the Facebook chatbot platform
    you can order flowers and see the news as well. Doesn't it sound cool? Technically,
    these chatbots are the new apps of the current era. I have briefly discussed the
    benefits of the chatbot, but we will look at them in detail in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building the basic version of chatbots:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding rule-based systems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding architecture
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the rule-based system of chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing rule-based chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem with the existing approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding key concepts for optimizing the approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implementing the revised approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the sequence-to-sequence (seq2seq) model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Testing the revised approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing metrics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised version of chatbots
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problem with the revised approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding key concepts for solving the existing problems
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best approach:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, our primary goal is to understand how to build a chatbot. Chatbots,
    or **question-answering systems** (**QA systems**), are really helpful. Let's
    consider a fun example. Suppose you are a student and you have five books to read.
    Reading and understanding five books may take time. What if you could feed the
    content of all these five books to the computer and ask only relevant questions?
    By doing this, students could learn the concepts and new information faster. As
    we all know, major internet product companies are arranging information so it
    is easy to access. Chatbots or QA systems will help us understand the meaning
    behind this information. This is the main reason why chatbots are the buzzword
    for the year 2017\. Whatever application you can think of, you can make a chatbot
    for it. Many messaging platforms now host chatbots built by developers, including
    Facebook Messenger, Slack, WeChat, and so on. Chatbots are the new app because
    they already live inside the installed apps that you probably use a dozen times
    in a day. Chatbots developed using Facebook chatbot APIs are inside the Facebook
    Messenger app. You might use Messenger a dozen times a day. So, users don't need
    to install a separate app for a specific functionality. This will help companies
    engage their customers even better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we proceed, I want to introduce a couple of important terms that can
    help us understand which kind of chatbot development we are targeting in this
    chapter. First of all, let''s understand what the different approaches for developing
    a chatbot are:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-based approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative-based approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieval-based approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a retrieval-based approach, we need to define the set of predefined responses,
    and we will apply some kind of heuristics on predefined responses so that the
    chatbot can generate the best possible answers for the given questions. The answers
    are very dependent on the input question and the context of that input question.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, during the development of the retrieval-based model, we used only expression
    matching, which can help us get the appropriate answer, but using only expression
    matching won't help us here. So recently, researchers and programmers have started
    using expression matching along with advanced machine learning (ML) classifier
    techniques. Let's take an example to understand how the machine learning classifier
    will be useful in order to build retrieval-based chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose Alice needs to send flowers to her friends on their birthdays. One of
    her friends, Emma, likes roses and another friend, Lucy, likes lilies. Alice uses
    a flower-ordering chatbot to book her order. She writes, *I want to book one multicolor
    rose flower bouquet and one with lilies*. So, in this case, if we implement a
    basic ML classifier, the chatbot can easily identify that there are two different
    orders Alice is booking, and it is also able to interpret the quantity of each
    of them. The chatbot will also ask for the different addresses and so on. By using
    ML techniques, we can code more complex heuristics, which can help us to generate
    more appropriate chatbot answers. The Facebook messenger chatbot API is an example
    of this.
  prefs: []
  type: TYPE_NORMAL
- en: There is another interesting example that can be solved by using ML heuristics.
    Say, you ask a chatbot, *what day is it today? Or*, *today is what day?* If we
    have implemented advanced ML techniques, then it can recognize that both questions
    are worded differently but have the same intent. During the development of the
    chatbot, intent and context detection are more complex tasks, which can be implemented
    by ML techniques and using some heuristics.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's move on to the harder approach, which is the generative-based approach.
  prefs: []
  type: TYPE_NORMAL
- en: Generative-based approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the generative-based approach, there aren''t any predefined responses given
    to the chatbot. The chatbot generates the responses from scratch. In order to
    build the generative-based chatbot, we need to provide a lot of data and the machine
    will learn how to answer the questions asked by users just by seeing the data.
    In 2015, Google researchers Oriol Vinyals and Quoc V. Le proposed an approach
    called A *Neural Conversational Network.* You can refer to the paper at: [https://arxiv.org/pdf/1506.05869v2.pdf](https://arxiv.org/pdf/1506.05869v2.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, researchers have used a Cornell movie dialog dataset. This dataset
    has been fed to the machines so that it can learn the basic English language.
    For this, they have used the **Sequence-to-sequence** (**seq2seq**) neural network
    architecture. After that, they used the IT support dataset so that the machines
    have domain knowledge. Once a machine is trained on that, they have tested the
    chatbot in the IT support department, this chatbot will be able to answer questions
    with great accuracy. In the upcoming section, we will build our own Neural Conversational
    Network. This approach is less time consuming and overcomes the challenges we
    face in the retrieval-based model, such as intent identification, context identification,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some other important terms that we need to discuss here. There are
    some important constraints that we need to think about before developing a chatbot.
    The first one is related to the conversation domain:'
  prefs: []
  type: TYPE_NORMAL
- en: Open domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closed domain
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let's understand what an open domain is. Conversations are fuzzy and
    uncertain sometimes. Let me give you an example. Suppose you meet an old school
    friend that you haven't seen for many years. During the course of the conversation,
    you don't know which particular topic you both are going to talk about. In this
    situation, the conversation can go anywhere. So, the domain of the conversation
    is not fixed. You can talk about life, jobs, travelling, family, and so on. There
    is an infinite number of topics that you can talk about. This kind of conversation,
    where we can't restrict the areas we are talking about, is called an open domain.
    Developing an open domain chatbot is difficult because ideally, this kind of chatbot
    can answer every question from any kind of domain with human-level accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, these kinds of chatbots are not made. When we are able to make this
    kind of chatbot, it will have to pass the Turing Test. Let me give you a glimpse
    of the Turing Test so that you can understand the explanation better. This experiment
    was created by the great computer scientist Alan Turing in 1950\. In this experiment,
    a person, called a judge, asks a series of questions to a person and a machine.
    Now, the judge won't know which answer is from the human and which one is from
    the machine. But after seeing or hearing the answers, if the judge can't differentiate
    which answers are coming from the human and which answers are coming from the
    machine, then the machine passes the Turing Test, and we can say that the machine
    exhibited human-level intelligence because it behaves as intelligently as humans.
    So far, there is not a single chatbot that has passed the Turing Test with human-level
    accuracy. You can read more about the Turing Test by visiting [https://en.wikipedia.org/wiki/Turing_test](https://en.wikipedia.org/wiki/Turing_test).
    This segment of technology is growing rapidly, so the next five years could be
    exciting.
  prefs: []
  type: TYPE_NORMAL
- en: Google has been quite aggressive in making an open domain chatbot. It is building
    this product in the form of Google Assistance, but the accuracy levels and functionality
    in passing the Turing Test are still limited. Now let's understand the second
    type of domain.
  prefs: []
  type: TYPE_NORMAL
- en: Closed domain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A closed domain is the opposite of an open domain. For a closed domain, we
    need to restrict the conversation topics. Let''s take an example: in the office,
    we sometimes have meetings. Before the meeting, the participants know the topics
    on which there''s going to be a discussion. So during the meeting, we just focus
    on those topics. Here, we won''t have an infinite number of topics and domains
    to talk about. This kind of conversation, where we have restricted the areas we
    can talk about, is called a closed domain.'
  prefs: []
  type: TYPE_NORMAL
- en: If a financial institute such as a bank launches a chatbot for their customers,
    then the developed chatbot cannot answer questions such as *can you tell me* *what
    the weather in Singapore is today?* But it helps you check the procedure of applying
    for a credit card, and this is because a chatbot can understand questions related
    to a specific domain. A chatbot for a closed domain is definitely possible, and
    there are many companies that are building domain-specific chatbots as it is good
    for engaging with the customer base. So during the chapter, we will be focusing
    on the closed domain chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try to understand the last constraint; the conversation length, which
    means the length of the answers we will be getting from the chatbot. Based on
    that, we need to understand the following terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Short conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short conversation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of chatbot can generate short answers. During the development of
    the chatbot, we need to ask ourselves whether we expect a short conversation or
    not. If we expect a short answer then you should be glad because this short conversation-based
    chatbot can be easily built. An example of a short conversation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: Hi'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine: Hello'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: How are you?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine: I''m fine'
  prefs: []
  type: TYPE_NORMAL
- en: This example indicates short conversations generated by a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Long conversation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This type of chatbot can generate long answers. It is hard for a machine to
    learn long conversations, so building a chatbot that can generate long conversations
    is difficult. Let''s look at an example of a long conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: I want to tell you a story.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine: Please go on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human: Here you go. John went to the market. Daniel is travelling to India.
    Siri has an apple. Siri is in the kitchen. So my question is, where is Siri?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine: Based on your story, I think Siri is in the kitchen.'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in this example, in order to generate the right answer, the machine
    should also store and process the given facts so that it can generate the right
    answer. Therefore, long conversation and reasoning-based chatbots are a bit hard
    to develop.
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned a lot of terms. Now let's see how they're going to
    affect us when we develop a chatbot. Based on the approaches and domain type,
    we can build different types of chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Open domain and generative-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We want to build a chatbot using the generative-based approach, which operates
    on the open domain. This means that the chatbot needs to learn how to answer the
    questions from any domain from scratch. The conversation can go in any direction
    here. This type of chatbot is an example of **Artificial General Intelligence**
    (**AGI**), and we are not quite there yet.
  prefs: []
  type: TYPE_NORMAL
- en: So, developing this type of chatbot is not a part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Open domain and retrieval-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we want to build a chatbot that can operate on an open domain using a retrieval-based
    approach, then as coders, we need to hardcode pretty much all the responses and
    possible questions as well as variations. This approach consumes a hell of a lot
    of time, so this type of chatbot is also not a part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Closed domain and retrieval-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have understood that we can't operate on the open domain, but what about
    the closed domain? We can surely work on the closed domain as there is a finite
    number of questions that can be asked by a user to a chatbot and that a chatbot
    can answer. If we use the retrieval-based approach for a closed domain, then we
    can code questions that are relatively easy. We can integrate some NLP tools,
    such as a parser, **Name Entity Recognition** (**NER**), and so on in order to
    generate the most accurate answer.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. Suppose we want to build a chatbot that can give us real-time
    weather information for any location. If we build the chatbot using a retrieval-based
    approach, then the user will definitely get an accurate answer for questions such
    as *What is the weather in Mumbai? What is the weather in California? Are there
    any chances of rainfall today?* The chatbot will give you answers to the first
    two questions really well, but during the third question, it will be confused
    because we don't provide a location for the chances of rainfall. If the chatbot
    has used some heuristics, then there will be a chance that you may get a response.
    Chatbot may ask you about the location for which you want to know the chances
    of rainfall, but mostly, this won't happen. The chatbot directly tells you the
    chances of rainfall in, say, California. In reality, I want to know the chances
    of rainfall in Mumbai. So, these kinds of context-related problems are common
    to the retrieval-based approach. We need to implement the generative-based approach
    to overcome context-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: Closed domain and generative-based approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we use the generative-based approach for the closed domain, the development
    of this kind of chatbot takes less coding time, and the quality of the answers
    improves as well. If we want our chatbot to understand long contexts and intents
    over a series of questions from the user, then the generative-based approach is
    the right choice. After training on large corpus and optimization, the chatbot
    can understand the context and intent of questions as well as be able to ask reasoning
    types of questions. This space of chatbot development is exciting and interesting
    for research and implementing new ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example. Suppose we have built a chatbot to apply for a home
    loan from a bank. When the user runs this chatbot, it may ask these questions:
    what is the status of my home loan application? Are there any documents remaining
    from my side that I should upload? Will I get approval in the next 2 days or not?
    Have you received my tax sheets and salary slips? The context of the last question
    is dependent on the second question. These kinds of questions and their answers
    can be easily generated with the generative-based approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure, which will help us summarize all the preceding
    discussions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Closed domain and generative-based approach](img/B08394_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: Pictorial representation of the approach to develop a chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will be building a chatbot that will be based on the closed
    domain and that uses retrieval-based and generative-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the dataset that we will be using in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to develop a chatbot, we are using two datasets. These datasets are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cornell Movie-Dialogs dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bAbI dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cornell Movie-Dialogs dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset has been widely used for developing chatbots. You can download
    the Cornell Movie-Dialogs corpus from this link: [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
    This corpus contains a large metadata-rich collection of fictional conversations
    extracted from raw movie scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This corpus has 220,579 conversational exchanges between 10,292 pairs of movie
    characters. It involves 9,035 characters from 617 movies. In total, it has 304,713
    utterances. This dataset also contains movie metadata. There are the following
    types of metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Movie-related metadata includes the following details:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genre of the movie
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Release year
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: IMDb rating
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Character-related metadata includes the following details:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender of 3,774 characters
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of characters in movies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: When you download this dataset, you'll notice that there are two files we will
    be using throughout this chapter. The names of the files are `movie_conversations.txt`
    and `movie_lines.txt`. Let's look at the content details of each file.
  prefs: []
  type: TYPE_NORMAL
- en: Content details of movie_conversations.txt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file contains `line_id` for the `movie_lines.txt` file. You can see the
    content of `movie_conversations.txt` in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Content details of movie_conversations.txt](img/B08394_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Sample content of the movie_conversations.txt file'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, this file contains line numbers, and
    the actual content of the conversation is present in *movie_lines.txt*. *+++$+++*
    acts as a separator. You must definitely be eager to know how to process this
    dataset; just bear with me for a while and we will cover this aspect in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the content of the next file.
  prefs: []
  type: TYPE_NORMAL
- en: Content details of movie_lines.txt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file contents the actual movie dialogs. You can see the sample content
    of `movie_lines.txt` in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Content details of movie_lines.txt](img/B08394_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'figure 8.3: Sample content of movie_lines.txt'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, each line has a unique conversation
    line ID. This `line_id` refers to the `movie_conversations.txt` file. This file
    contains the same line separator and the names of the characters involved in the
    conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the both files together, then it is might make more sense to you.
    In the `movie_conversations.txt` file, refer to the conversations on *line_id
    194, 195, 196*, and *197*. All these conversations can be found in `movie_lines.txt.`
    In the preceding image, you can see that *line_id* *194* contains this question:
    *Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly
    horrendous public break- up on the quad. Again.* On the other hand, *line_id*
    *195* contains the answer: *Well, I thought we''d start with pronunciation, if
    that''s okay with you.*'
  prefs: []
  type: TYPE_NORMAL
- en: We need to prepare the dataset in the form of a question-answer format before
    feeding it to the machine. We will implement the data preparation step before
    using it for training.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the bAbI dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The bAbI dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This dataset is built by Facebook AI Research (FAIR), where AI stands for artificial
    intelligence. This dataset belongs to the bAbI project. You can download the dataset
    from [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/).
    It is a well-maintained dataset. The goal of the bAbI project is to try to build
    an automatic text understanding and reasoning system. This dataset consists of
    the following sub datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: The (20) QA bAbI tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The (6) dialog bAbI tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Children's Book Test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Movie Dialog dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The WikiMovies dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Dialog-based Language Learning dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SimpleQuestions dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HITL Dialogue Simulator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will be using only one subset here, which is the (20) QA bAbI tasks because
    it is the one that's most useful for building the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: The (20) QA bAbI tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s look at this subdataset in detail. Here, 20 different tasks have been
    performed using this (20) QA bAbI dataset. Let''s see what these tasks are. These
    tasks give machines the capacity to perform some reasoning, and based on that,
    the machine can answer a question. You can refer to the task name given in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The (20) QA bAbI tasks](img/B08394_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: (20) QA bAbI task details'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: http://www.thespermwhale.com/jaseweston/babi/abordes-ICLR.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: Facebook researchers Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M.
    Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov published a paper
    in which they proposed an interesting AI-based QA system. You can refer to their
    research paper by visiting [https://arxiv.org/abs/1502.05698](https://arxiv.org/abs/1502.05698).
    In this chapter, we will be attempting to achieve the results for task T1, and
    we will regenerate its result.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset contains the corpus in two languages, English and Hindi. There
    are two types of folders here: the folder with the name *en* has 1,000 training
    examples, whereas *en-10K* has 10,000 training examples. The format for each of
    the task datasets is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The (20) QA bAbI tasks](img/B08394_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Format of Single supporting QA bAbI task'
  prefs: []
  type: TYPE_NORMAL
- en: The supporting facts are called a story. Based on the story, the user can ask
    questions to the machine, and the machine should give the logically correct answer
    that can be derived from the provided supporting text. This is a hard task because
    in this case, the machine should remember the long context that it can use as
    and when needed. We will use this interesting dataset soon.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's start building the chatbot baseline version.
  prefs: []
  type: TYPE_NORMAL
- en: Building the basic version of a chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be building the basic version of a chatbot. Getting
    data is not an issue for any company nowadays but getting a domain-specific conversational
    dataset is challenging.
  prefs: []
  type: TYPE_NORMAL
- en: There are so many companies out there whose goal is to make an innovative domain-specific
    chatbot, but their major challenge is getting the right data. If you are facing
    the same issue, then this basic approach can help you in that. This basic version
    of a chatbot is based on the closed domain and the retrieval-based approach, which
    uses the rule-based system. So, let's start understanding each aspect of the rule-based
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Why does the rule-based system work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I mentioned earlier, a rule-based system is the way to implementing a retrieval-based
    approach. Now, you may wonder why we need a rule-based system. Considering that
    we are living in the era of Machine Learning (ML), doesn't it sound old? Let me
    share my personal experience with you. I closely collaborate with many start-ups.
    Some of them operate in the financial domain, some in the human resource domain,
    and some in the legal domain. In this era of chatbots, start-ups are really keen
    on developing domain-specific chatbots that can help users. Initially, they work
    on some general dataset so that the machine can learn the language and generate
    the logical casual answers for them, but they soon realize that they don't have
    enough domain-specific data to help them build a good chatbot. Let me give you
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'I collaborated with a fintech start-up where we needed to build a chatbot.
    The specific requirement for the chatbot was that it should help customers who
    want to apply for a home loan as well as those who have already applied and need
    some assistance. Now, this fintech just started 1 and a half years ago. So they
    don''t have large chat logs about the kind of queries customers may have. In short,
    the company doesn''t have enough domain-specific data, such as what kind of queries
    a home loan applicant may ask and how to synchronize these customer queries to
    the loan procedure this fintech company follows. In this case, there are two main
    things that we need to focus on:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to build a minimum viable chatbot that can help customers with basic
    FAQs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of this minimum viable chatbot, you can also come to learn what
    kind of questions people are asking, and based on these questions, the chatbot
    can be tweaked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In these kind of situations, where we don't have a domain-specific dataset,
    the rule-based or retrieval-based model will work for us. From the next section
    onward, we will explore the rule-based system and the approach of developing a
    basic chatbot and its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the rule-based system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will cover the rule-based system so that you don''t feel
    left out when we start developing the retrieval-based chatbot. The **rule**-**based**
    (**RB**) system is defined as follows: using available knowledge or rules, we
    develop a system that uses the rules, apply the available system rules on the
    corpus, and try to generate or infer the results. From the perspective of the
    chatbot, the RB system has all possible questions and answers and they''re hardcoded.
    We can definitely use regular expressions and fuzzy logic to implement some kind
    of heuristics in order to make the RB system more accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following figure, which will give you an idea about the workflow
    of the chatbot using the retrieval-based approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the rule-based system](img/B08394_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: Workflow of rule-based chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the preceding figure, you know that in an RB system we will manually
    hand-code all possible questions and answers as well as implement regular expressions
    and fuzzy logic, which will give the chatbot the ability to generate the appropriate
    answers. Based on business requirements, questions can be added and deleted from
    this system. Now let's discuss our approach, and based on this approach we will
    build a basic version of the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at the steps that will help us implement the
    basic version of the chatbot. Here, I''m building a chatbot for the finance domain,
    which will help users apply for home loans. We will code some questions so that
    you know how a rule-based chatbot can be developed. We need to perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Listing down possible questions and answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deciding standard messages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Listing down possible questions and answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we need to list all the questions that we can think of on behalf
    of the users. Once we decide on the questions, then one by one we need to decide
    the answers to these questions. Suppose we ask the user to provide their full
    name, email ID, phone number, and loan amount so in case the user drops in between,
    the customer executive can call them back. After this, we ask the user what kind
    of assistance they require and then they can ask their questions. They may ask
    for the eligibility criteria, application status, document requirements, and so
    on. During the first iteration, you need to add the bare minimum questions that
    are frequently asked by users. Once we decide the questions and answers, it will
    be easy for us to code them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, I include the following questions in this basic version of the financial
    domain chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: Please let me know the eligibility criteria for getting a home loan
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please let me know what the status of my loan application is
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let me know the list of documents I need to submit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The answers to each of these questions will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We need a minimum of 3 years of job experience, 3 years of IT returns, and a
    minimum income of more than 3.5 lakh INR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your application is with our credit risk management team
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to submit salary slips for the last 6 months, a proof of identity,
    3 years of IT returns, and the lease documents for your house.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to decide some standard messages, which we will cover in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding standard messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to decide the standard message, for example, a welcome message from
    the chatbot. If the user asks a question that the chatbot cannot answer, then
    what message should pop up? We also need to decide the message when the user ends
    the chat.
  prefs: []
  type: TYPE_NORMAL
- en: These standard messages help users understand what they can and cannot ask the
    chatbot. Now let's look at the architectural part of the basic version of the
    chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, let''s talk about architecture. When we are building a domain-specific
    rule-based chatbot, then we need to store whatever questions the users ask. We
    also need to build a chatbot that is fast and scalable. In this approach, we build
    the web services. The web service REST APIs will be easily integrated with the
    website and the frontend. We need a database that can store the conversations
    of the users. This conversation data will be helpful when we try to improvise
    the chatbot or use it for ML training. The libraries that I''m using are given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Flask for implementing web services and REST APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MongoDB for storing the conversations. The reason behind choosing the NoSQL
    database is that conversations don't have a specific format. NoSQL is a good option
    to store schema-less data. We need to store the raw conversations, so NoSQL is
    a good option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the following figure, which will help you understand the entire
    architecture and process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the architecture](img/B08394_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Architectural design for the basic version of a chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this architecture, you will find that the process flow of the basic
    version of a chatbot is quite simple. This flow involves seven simple steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The user will ask their questions to the chatbot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rule-based engine of the chatbot will process the question. Here, the REST
    API has been called to generate the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the question that's asked is available to the RB system, then the user will
    get an appropriate answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the question that's asked is not available to the RB system, then the user
    will not get the answer but a standard error message.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The conversation of the user will be stored in the MongoDB database. This response
    is in the JSON format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The same JSON response is sent by the REST API to the frontend. At the frontend,
    a JavaScript parses this response and pops up the appropriate answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the user gets their answer, they may end the chat or ask another question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Another major point that I want to highlight is that before storing the data
    to MongoDB, we need to finalize the attributes of the JSON response that will
    actually help us when we parse the JSON response using JavaScript. You can refer
    to the following screenshot, which will help you learn which kind of JSON schema
    I have decided on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the architecture](img/B08394_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Understanding the JSON response attribute'
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of each of the JSON attributes is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`current_form_action:` This attribute indicates which REST API is currently
    being invoked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`message_bot:` This field carries the answer from the bot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`message_human:` This field carries the query of the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_field_type:` If we need to populate the textbox or button in the next
    question, this is useful for generating dynamic HTML components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`next_form_action:` This attribute indicates which REST API we should invoke
    in the upcoming request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`placeholder_text:` If you want to put watermark text in the textbox, then
    this attribute helps you with the HTML functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`previous_field_type:` This attribute keeps track of what the last field type
    was.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`previous_form_action:` This attribute keeps track of what the last REST API
    we invoked was.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`suggestion_message:` Sometimes, we need a message to invoke a specific rule.
    This is the same as when you say, *OK Google* and the Google home assistance is
    invoked. This attribute basically guides the user as to what they need to expect
    when asking their queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's start the implementation of the rule-based chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the rule-based chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will understand the implementation of the chatbot. This
    implementation is divided into two parts. You can find this code by visiting:
    [https://github.com/jalajthanaki/Chatbot_Rule_Based](https://github.com/jalajthanaki/Chatbot_Rule_Based):'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the conversation flow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing RESTful APIs using flask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the conversation flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement the conversation logic, we are writing a separate Python
    script, so that whenever we need to add or delete some logic it will be easy for
    us. Here, we create one Python package in which we put this conversation logic.
    The name of the file is *conversationengine.py* and it uses JSON, BSON, and re
    as Python dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this file, we have implemented each conversation in the form of a function.
    When the user opens the chatbot for the first time, a welcome message should pop
    up. You can refer to the code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the conversation flow](img/B08394_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: Code snippet for the welcome message'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the users need to type in **Hi** in order to start a conversation. When
    the user types **hi**, the `start_coversation_action` function will be invoked
    and the chatbot will ask for some information so that it can give the user a more
    accurate, personalized answer. First, it asks the user their name, and then it
    asks for their email ID and phone number. You can refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the conversation flow](img/B08394_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: Code snippet for asking basic user information'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, there are the `borrowers_name_asking`, `borrowers_email_id_asking`,
    and `mobilenumber_asking` functions, which ask the user to provide their name,
    email ID, and phone number. Apart from this, there are questions that can help
    users learn what the status of their loan application is. If the customer is new,
    then they can ask questions such as *what kind of documents are needed in order
    to apply for a home loan?* You can find these status- and document-related questions
    inside the `other_cases` function. You can refer to the code for this function
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the conversation flow](img/B08394_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: Code snippet for question related to loan application status and
    documents needed for applying for a home loan'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding figure*,* we have used a regular expression
    here so that the chatbot can answer status- and document-related questions. This
    is coded purely using keyword-based logic.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at how to build the web service with this function using flask.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RESTful APIs using flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have only coded the function that takes the user input query and
    invokes the appropriate function based on that query. For better maintenance and
    easy integration, we need to implement RESTful APIs using flask. In order to implement
    this, we use the `flask`, `json`, `os`, `uuid`, `datetime`, `pytz`, and `flsk_pymongo`
    libraries. Flask is an easy-to-use web framework. You can find the code snippet
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing RESTful APIs using flask](img/B08394_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Code snippet for making a RESTful API for the chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the preceding figure, each route calls a different method
    that is part of the `conversationengine.py` file we covered earlier. In order
    to run this flask engine, we need to use the flask `app.run ()` command. You can
    find all APIs and their functions by visiting: [https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py](https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's test this rule-based chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the rule-based chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will test the basic version of the chatbot. Let's begin
    with basic personal information that the chatbot asks for from the user. Here,
    I will generate the JSON response generated by the flask RESTful API. We need
    a JavaScript to parse this JSON response if we are integrating these APIs with
    the frontend. I won't explain the frontend integration part here, so let's analyze
    the JSON responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the welcome message, refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the rule-based chatbot](img/B08394_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: JSON response for the welcome message'
  prefs: []
  type: TYPE_NORMAL
- en: 'The JSON response when the chatbot is asking for the name of a user is given
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the rule-based chatbot](img/B08394_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: JSON response for asking the name of the user'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the user asks for the status of his application, then they will get the
    JSON response given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the rule-based chatbot](img/B08394_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: JSON response to get status-related information'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the user asks status-related questions with a blend of Hindi-English (Hinglish)
    and if they use the word *status* in their query, then the chatbot will generate
    the response. You can see the response in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the rule-based chatbot](img/B08394_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: json response to get status-related information for the Hindi-English
    (Hinglish) language'
  prefs: []
  type: TYPE_NORMAL
- en: 'If the user asks queries that are not coded, then it will generate the following
    json response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the rule-based chatbot](img/B08394_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: JSON response for an unknown question'
  prefs: []
  type: TYPE_NORMAL
- en: After testing, we come to learn that queries that have been coded are working
    fine, but this basic version of a chatbot is not working properly for questions
    that we haven't coded. I want to point out some advantages after testing the rule-based
    chatbot. However, there are various disadvantages of this approach too, which
    we will discuss in an upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of the rule-based chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the following advantages of the rule-based chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy to code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needs less computation power.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uses the pattern matching approach so if users use English and other languages
    in their conversation, they will still get an answer. This is because the chatbot
    identifies keywords that the user provides in their question. Suppose the user
    asks in English, *can you provide me a list of documents that I need to submit?*
    And another user may ask a question in the Hindi language: *Kya aap mujhe bata
    sakte hain mujhe kaun se documents submit karne hain?* For this question, the
    chatbot will generate the answer because it finds specific keywords from user
    queries, and if those keywords are present, then the chatbot generates an answer
    irrespective of the language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's look at the problems related to this approach that we need to solve
    in order to improve the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the problems with the basic version of our
    chatbot. As we already know, for unseen queries this approach doesn't work, which
    means that the basic approach is not able to generalize the user's questions properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have listed down some of the problems here:'
  prefs: []
  type: TYPE_NORMAL
- en: Time consuming because we need to hardcode each and every scenario, which is
    not feasible at all
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It cannot work for unseen use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user should process the rigid flow of conversation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It cannot understand the long context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of these problems can be solved using the generative-based approach. Let's
    look at the key concepts that will help us improvise this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts for optimizing the approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be discussing the key concepts that can help us improvise
    the chatbot basic version. The problems that we have listed down previously can
    be solved by using **Deep Learning** (**DL**) techniques, which can help us build
    a more generalized chatbot in less time.
  prefs: []
  type: TYPE_NORMAL
- en: Before proceeding ahead, we need to decide which DL technique we will use for
    our revised approach. DL helps us achieve great results. Here, we need to use
    the End-to-End DL approach that makes no assumptions about data, the structure
    of the dialog, and use cases. This is what we want. In order to achieve this,
    we will be using **Recurrent Neural Nets** (**RNN**). Now you may ask why RNN
    is useful. Let me explain this by way of an example. Suppose we want to classify
    the temperature in the hot or cold category; to do that, we will be using a feed
    forward neural net to classify the temperature into hot or cold, but the conversation
    isn't a fixed size. A conversation is a sequence of words. We need to use a neural
    net that can help us process the sequences of words. RNN is best for processing
    these kinds of sequences. In RNN, we feed data back into the input while training
    it in a recurring loop.
  prefs: []
  type: TYPE_NORMAL
- en: In the revised approach, we are going to use the sequence-to-sequence (seq2seq)
    model from TensorFlow. So, let's discuss the sequence model for a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the seq2seq model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The good part of using the seq2seq model is that we don't need to perform feature
    engineering. Like most of the DL techniques, it generates features by its own.
    We will discuss the seq2seq model briefly. The seq2seq model consists of two **Long
    Short Term Memory** (**LSTM**) recurrent neural networks. The first neural net
    is an *encoder*. It processes the input. The second neural net is a *decoder*.
    It generates the output. Usually, the DL algorithm needs a dimensionality of the
    inputs and outputs to be a fixed size, but here, we are accepting a sequence of
    words in a sentence and outputting a new sequence of words. So, we need a sequence
    model that can learn data with long range memory dependencies. The LSTM architecture
    is best suited for this. The encoder LSTM turns the input sentence of variable
    length into a fixed dimensional vector representation. We can think of this as
    a *thought vector* or a *context vector*. The reason we are using LSTM is that
    it can remember words from far back in the sequence; here, we are dealing with
    large sequence attention mechanisms of the seq2seq model, which helps the decoder
    selectively look at the parts of the sequence that are most relevant for more
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the architecture of the seq2seq model in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the seq2seq model](img/B08394_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Architecture of the seq2seq model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: [http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png](http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png)'
  prefs: []
  type: TYPE_NORMAL
- en: When we provide a large enough dataset of questions and responses, it will recognize
    the closeness of the set of questions and represent them as a single thought vector.
    This representation helps the machine to understand the intent of the questions
    irrespective of the structure of the sentence so the machine can recognize the
    questions such as "what time it is?" and "what's the time?" have the same intent,
    so they will fall into a single thought vector. After training, we will have a
    huge set of not just synapse weights but thought vectors as well. After that,
    we need to use additional hyper parameters along with appropriate loss functions
    when we train the model. Once we train the model, we can chat with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more details about the seq2seq model, then you should refer
    to a research paper published by Google researchers titled *A Neural Conversational
    Model*. You can also refer to this paper at: [https://arxiv.org/pdf/1506.05869v3.pdf](https://arxiv.org/pdf/1506.05869v3.pdf)
    and this amazing article if you want to learn more about LSTM at: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's implement the chatbot using the seq2seq model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover each part of the implementation. You can find
    the code by using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow](https://github.com/jalajthanaki/Chatbot_tensorflow).
    Note that here, I''m using TensorFlow version 0.12.1\. I perform training on a
    GeForce GTX 1060 6GB GPU for a few hours. In this implementation, we don''t need
    to generate features because the seq2seq model generates its internal representation
    for sequences of words given in a sentence. Our implementation part has the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the seq2seq model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin our coding.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During this implementation, we will be using the Cornell movie-dialogs dataset.
    First of all, we need to prepare data in a format that we can use for training.
    There is a Python script that is used to perform data preparation. You can find
    the script at: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data preparation can be subdivided into the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating question-answer pairs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the dataset into the training dataset and the testing dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a vocabulary for the training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating question-answer pairs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to generate question-answer pairs from the Cornell movie-dialogs dataset,
    we are using the `movie_lines.txt` and `movie_conversations.txt` files. The `movie_lines.txt`
    files give us information about *line_id* of each conversation along with the
    real conversation, whereas `movie_conversations.txt` has *line_ids* only. In this
    situation, we need to generate the appropriate pair of conversations of question
    and answer from the dataset. For that, we will combine these two files. In Python
    script, there are some functions that help us combine these files. The details
    related to functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_id2line():` This function helps us spilt the data using the +++$+++ pattern.
    We perform splitting on the `movie_lines.txt` file. After splitting, with the
    help of this function, we create a dictionary in which we put *line_id* as the
    key and the movie dialog as the value. So, *key = line_id* and *value = text*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_conversations():` This function splits the data given in the `movie_conversations.txt`
    file. This will help us create a list. This list contains list of *line_ids*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gather_dataset():` This function actually generates question-answer pairs.
    In this function, a simple logic is applied. We take the list of *line_ids* and
    we know that the last element indicates the answer. So, we separate the questions
    and answers. With the help of the `get_id2line()` function, we search the questions
    and their corresponding answers. Here, we are using the value of the key to search
    questions and answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the following screenshot to see the actual coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generating question-answer pairs](img/B08394_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.19: Functions used for generating question-answer pairs'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's explore the data preprocessing section.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some preprocessing and filtering steps are involved here. As a part of preprocessing,
    we perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We convert the conversation into lowercase using the inbuilt string function,
    `lower()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also remove the junk characters and too short or too long conversations.
    For this, we use a list-based approach to remove junk characters and the `filter_data()`
    function to remove too short or too long conversations. When we apply the `filter_data()`
    function on our dataset, *28%* of dataset is filtered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also filter out conversations with so many unknowns. Here, *2%* of the dataset
    has been affected. For this, we have used the `filter_unk()` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also tokenize the sentences. In this process, we convert *list of [line of
    text]* into *list of [line of words]*. This tokenization is helpful because during
    training, the machine can process individual words of the sentence, and with the
    help of the word ID, data retrieval becomes much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the code given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing the dataset](img/B08394_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.20: Code snippet for preprocessing'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the dataset into the training dataset and the testing dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After preprocessing, we will split the data into the training dataset and the
    testing dataset and for that, we will use the following functions:'
  prefs: []
  type: TYPE_NORMAL
- en: We can save the training and testing datasets using the `prepare_seq2seq_files()`
    function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access the `train.enc`, `train.dec`, `test.enc`, and `test.dec` data
    files directly from this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a vocabulary for the training and testing datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now it''s time to generate the vocabulary from the dataset. For vocabulary
    generation, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `prepare_custom_data()` function of the `data_utils.py` file, we can
    generate the vocabulary that we will feed into the seq2seq model while training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access the `data_uti``ls.py` file using this link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that vocabulary files are generated when we start training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filenames of vocabulary files are `train.enc.ids20000`, `train.dec.ids20000`,
    `test.enc.ids20000`, and `test.dec.id``s20000`. Here, 20000 indicates the size
    of the vocabulary we have provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can access this file at: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the code for the `prepare_custom_data()` function in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a vocabulary for the training and testing datasets](img/B08394_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.21: Code snippet for generating the vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's actually implement the seq2seq model using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the seq2seq model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we chatbot development:seq2seq model, building" will be performing
    the actual training using the seq2seq model. We will be using TensorFlow to implement
    the seq2seq model. Before getting into training, let''s look into the hyper parameters
    configuration file, which you can access using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Duringchatbot development:seq2seq model, building" training, our script uses
    these files and their parameters. The following parameters are in this configuration
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Mode`: This can be either train or test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_enc:` This contains the path of the training dataset for the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_dec:` This contains the path of the training dataset for the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_enc:` This contains the path of the testing dataset for the encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_dec:` This contains the path of the testing dataset for the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Working_directory:` This is the folder where we can store our checkpoints,
    vocabulary, and temporary data files'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`enc_vocab_size:` This number defines the vocabulary size for the encoder.
    We set 20,000 as the vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dec_vocab_size:` This number defines the vocabulary size for the decoder.
    We set 20,000 as the vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_layers:` This indicates the number of LSTM layers. Here, we set it as
    3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layer_size:` This indicates the number of layers in the seq2seq model. We
    set it as 256.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps_per_checkpoint:` At a checkpoint, the model''s parameters are saved,
    the model is evaluated, and results are printed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate:` This indicates how fast or how slow we train our model. We
    set the value to 0.5 for now.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most of the preceding parameters can be changed in order to get the best possible
    results. During training, we need to set the Mode as train and run the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now it''s time to understand what is inside the execute.py file. You can access
    this file using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this script, we call the TensorFlow API. This script can be divided into
    the following parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We are using the `Seq2SeqModel()` function from TensorFlow here. This function
    reads the configuration file and uses the values defined in the configuration
    file. In order to store the train model, we use the `saver.restore()` function,
    and to get the status of the checkpoints we use the `get_checkpoint_state()` function.
    You can refer to the code snippet given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the model](img/B08394_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.22: Code snippet for creating seq2seq model'
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We defined the `train()`method inside the `execute.py` file. This function
    initializes the TensorFlow session and begins training. You can refer to the code
    snippet given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the model](img/B08394_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.23: Code snippet for training the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now it''s time to train the model. When we execute the `python execute.py`
    command, you will see the output given in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the model](img/B08394_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.24: Training of the seq2seq model using TensorFlow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, training has been performed on a GPU. I trained this model for 3 hours.
    I have trained this model for 15,000 checkpoints. You can refer to the following
    screesnhot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the model](img/B08394_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.25: Output of the seq2seq training'
  prefs: []
  type: TYPE_NORMAL
- en: 'On a CPU, training will take a lot of time, so I have also uploaded pre-trained
    models for you to use. You can download them by using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir).'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to understand the testing metrics that help us evaluate the trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will perform testing of the revised approach. Before performing
    actual testing and seeing how good or bad the chatbot conversation is, we need
    to understand the basic testing metrics that we will be using for this approach
    and for the best approach. These testing metrics help us evaluate the model accuracy.
    Let's understand the testing metrics first, and then we will move on to the testing
    of the revised approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we need to understand the following testing metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Perplexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perplexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the NLP domain, perplexity is also referred to as per-word perplexity. Perplexity
    is a measurement of how well a trained model predicts the output for unseen data.
    It is also used to compare probability models. A low perplexity indicates that
    the probability distribution is good at predicting the sample. Even during training,
    you can see that for each checkpoint, perplexity is decreasing. Ideally, when
    there is no change in perplexity, we need to stop the training. During the training
    of the seq2seq model, I stopped training after 3 hours, so when you train the
    model from your end you can wait till the perplexity stops decreasing further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perplexity is using the concept of entropy. If you want to know about perplexity,
    then you can refer to [https://www.youtube.com/watch?v=BAN3NB_SNHY](https://www.youtube.com/watch?v=BAN3NB_SNHY).
    Per-word perplexity is based on entropy. So, in order to understand entropy, you
    can refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=Bd15qhUrKCI](https://www.youtube.com/watch?v=Bd15qhUrKCI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=K-rQ8KnmmH8](https://www.youtube.com/watch?v=K-rQ8KnmmH8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH](https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you understand entropy, it will be easy for you to understand the equation
    of perplexity. Refer to the equation given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Perplexity](img/B08394_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.26: Equation of perplexity'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://www.tensorflow.org/tutorials/recurrent'
  prefs: []
  type: TYPE_NORMAL
- en: Here, N is the number of samples and P is a probability function. We are calculating
    entropy using the natural logarithm function. Now let's look at another testing
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Training loss indicates the direction in which the training progresses. Usually,
    when we start training, the value of loss is high and training accuracy is low,
    but during the training, the value of loss goes down and the training accuracy
    goes up. There are many error functions that are used in DL algorithms. Here,
    we are using cross-entropy as a loss function. Cross-entropy and log loss are
    slightly different depending on the context, but in machine learning, when calculating
    error rates between 0 and 1, they are the same thing. You can refer to the equation
    given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loss](img/B08394_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.27: Equation for cross-entropy'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore the cross-entropy loss function, then you can refer
    to: [http://neuralnetworksanddeeplearning.com/chap3.html](http://neuralnetworksanddeeplearning.com/chap3.html).'
  prefs: []
  type: TYPE_NORMAL
- en: We haven't gone into great mathematical detail here because just by tracking
    the training process, you will get to know whether the value of loss is increasing
    or decreasing. If it is decreasing over the period of training time, then the
    training is moving in the right direction. This is applicable to perplexity as
    well. Initially, the perplexity value is huge, but during training it gradually
    falls down and at some point it neither increases nor decreases. At that time,
    we need to stop the training. Now let's test the revised chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised version of the chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be performing testing of the revised chatbot. I have
    trained it on a GPU for only 3 hours; now let's check how much our chatbot can
    tell us. For testing, we need to make a small change in the `seq2seq.ini` configuration
    file. We need to set *value of mode as test* and then execute the `python execute.py`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the given command, you will get the output given in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised version of the chatbot](img/B08394_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.28: Output of testing revised approach'
  prefs: []
  type: TYPE_NORMAL
- en: If you train for a longer period of time, then you will get a more impressive
    result. I feel that DL algorithms help us if we want to build the chatbot using
    the generative-based approach. Now let's discuss how we can improvise this revised
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be discussing what the problems with the revised approach
    are. Is there any way in which we can optimize the revised approach? So first
    of all, let's discuss the area of improvement so that during the upcoming approach,
    we can focus on that particular point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the points that I want to highlight are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous version of our chatbot there was a lack of reasoning, which
    means the chatbot couldn't answer the question by applying basic reasoning to
    it. This is what we need to improve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let me give you an example. Suppose I tell chatbot a story: *John is in the
    kitchen. Daniel is in the bathroom*. After that, say, I ask the chatbot this question:
    *Where is John?* The chatbot that we have built so far will not be able to answer
    this simple question. We as humans answer these kinds of questions well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We try to implement this kind of functionality in our next approach so that
    we can enable some features of AI in the chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the important concepts that can help us build the AI-enabled chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts to solve existing problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Facebook AI research group published a paper that proposed a Memory Network
    that can prove that the machine can also answer questions that are based on reasoning.
    You can certainly refer to this paper titled, Towards AI-Complete question answering:
    A set of prerequisite toy tasks. The link is: [https://arxiv.org/pdf/1502.05698.pdf](https://arxiv.org/pdf/1502.05698.pdf).
    You can also refer to this paper on the memory network at: [https://arxiv.org/pdf/1410.3916.pdf](https://arxiv.org/pdf/1410.3916.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be using the bAbI dataset and train the model based on the improvised
    memory network. Once training is done, we will check whether our chatbot can answer
    questions based on using the simple reasoning ability or not. We will be recreating
    the result of the Facebook research paper. Before we move to the implementation
    part, we need to understand what memory networks are and how we can build a system
    that will use logic to answer questions. So, let's look at memory networks briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Memory networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will explore the memory network so that we can understand
    what is actually happening behind the scenes when we implement it in the upcoming
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, in the LSTM network, memory is encoded by using hidden states and
    weights. These hidden states and weights are too small for extremely long sequences
    of data, be that a book or a movie. So, in a language translation application,
    multiple LSTM stats are used along with the attention mechanism in order to choose
    the appropriate work for translation that fits the context. Facebook researchers
    have developed another strategy called a memory network that outperforms LSTMs
    for the question-answer system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind the memory network was to allow the neural network to
    use an external data structure as memory storage, and it learns where to retrieve
    the required memory from this external memory structure in a supervised way. You
    can refer to the architecture of the memory network given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/B08394_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.29: Architecture of memory network'
  prefs: []
  type: TYPE_NORMAL
- en: When it came to answering questions from the little data that was generated,
    the information was pretty easy to handle with the memory network, but in the
    real world, having the data handle long dependency relations is a challenging
    task. On Kaggle, there was a competition called The Allen AI Science Challenge
    in which the winner used a special variation of the memory network called a dynamic
    memory network (DMN), and that is what we are using to build our chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic memory network (DMN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of DMN is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic memory network (DMN)](img/B08394_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.30: Architecture of DMN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/dmn-high-level.png'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of DMN defines two types of memory, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic memory: We are using the pre-trained glove model that will generate
    vectors for input data. These vectors are the input to our DMN model and are used
    as semantic memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Episodic memory: This memory contains other knowledge. The inspiration for
    this memory came from the hippocampus function of our brain. It''s able to retrieve
    temporal states that are triggered by a response, such as an image or a sound.
    We will see the usage of this episodic memory in a bit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are some important modules that we need to understand:'
  prefs: []
  type: TYPE_NORMAL
- en: Input module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Episodic memory module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before I start explaining the modules, please refer to the following figure
    for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic memory network (DMN)](img/B08394_08_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.31: Details on each DMN module'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/dmn-details.png'
  prefs: []
  type: TYPE_NORMAL
- en: Input module
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The input module is a GRU (Gated Recurrent Unit) that runs on a sequence of
    word vectors. A GRU cell is kind of like an LSTM cell, but it''s more computationally
    efficient since it has only two gates and doesn''t use a memory unit. The two
    gates control when the content is updated and when it''s erased. There are only
    two tasks GRU is performing: one is *Update* and the other is *Reset*. You can
    refer to the following figure depicting LSTM and GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Input module](img/B08394_08_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.32: Illustration of LSTM and GRU'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://cdn-images-1.medium.com/max/1200/0*1udenjz1XCZ5cHU4'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden stats of the input module represent the input process in the form
    of a vector so far. It outputs hidden states after every sentence, and these outputs
    are called facts in the paper because they represent the essence of what is fed.
    You might want to know how the hidden state is calculated in GRU. For that, you
    can refer to the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: Ht = GRU(Xt, ht-1)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, Ht is the current timestep, ht-1 is the previous timestep, and Xt is
    the given word vector. The preceding equation is the simple format of the GRU
    hidden stat calculation. You can see more detailed and complex equations in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Input module](img/B08394_08_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.33: Equation for calculating the hidden state in GRU'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/gru.png'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, with the help of the given word vector and the previous timestep
    vector, we compute the current timestep vector. The update gave us a single layer
    neural network. We sum up the matrix multiplications and add bias terms to it.
    Then, the sigmoid squashes it to a list of values between 0 and 1, and that is
    our output vector. We do this twice with different sets of weights, and then we
    use the reset gate that will learn to ignore the past timesteps when necessary.
    For example, if the next sentence has nothing to do with those that came before
    it, the update gate is similar in that it can learn to ignore the current timestep
    entirely. Maybe the current sentence has nothing to do with the answer whereas
    previous ones did.
  prefs: []
  type: TYPE_NORMAL
- en: Question module
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This module processes the question word by word and outputs a vector using the
    same GRU as the input module and with the same weight. We need to code for input
    statements (input data) and for the question that we will ask. We can code them
    by implementing embedding layers for them. Now we need to create an episodic memory
    representation for both.
  prefs: []
  type: TYPE_NORMAL
- en: Episodic memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As I've described earlier, the concept of episodic memory derives from the hippocampus
    function of our brain. Both fact and question vectors extracted from the input
    and enter to the episodic memory module. It is composed of two nested GRUs. The
    inner GRU generates what are called episodes. It does this by passing over the
    facts from the input module. When updating its inner state, it takes into account
    the output of an attention function on the current fact. The attention function
    gives a score between 0 and 1 to each fact and so the GRU ignores facts with low
    scores. During training, after each full pass on all the available facts, the
    inner GRU outputs an episode, which is then fed to the outer GRU. We need multiple
    episodes so that our model can learn what part of a sentence it should pay attention
    to. During the second pass, the GRU realizes that something else is also important
    in the sentence. With the help of multiple passes, we can gather increasingly
    relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a brief explanation of DMN. You can refer to this great article as
    well: [https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/](https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the implementation for this.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered the entire concept that can help us implement the DMN-based
    chatbot. In order to implement this approach, we will be using Keras with the
    TensorFlow backend. Without wasting any time, we will jump to the implementation
    section. You can refer to the code for this approach using this GitHub link: [https://github.com/jalajthanaki/Chatbot_based_on_bAbI_dataset_using_Keras](https://github.com/jalajthanaki/Chatbot_based_on_bAbI_dataset_using_Keras).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will train our model on the given bAbI task 1 dataset. First of all,
    we need to parse the stories and build the vocabulary. You can refer to the code
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.34: Code snippet for parsing stories and build vocabulary'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can initialize our model and set its loss function as a categorical cross-entropy
    with stochastic gradient descent implementation using RMSprop in Keras. You can
    refer to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.35: Code snippet for building the model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training, we need to set a hyperparameter. With the help of the value
    of the hyperparameter script, we will decide whether to run the script in the
    training mode or the testing mode. You can see all the hyperparameters that we
    need to set during training in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.36: Hyperparameter values for training'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have used three hyperparameters. We can experiment with them. Let''s
    discuss them for a minute:'
  prefs: []
  type: TYPE_NORMAL
- en: 'train_epochs: This parameter indicates the number of times the training examples
    complete a forward pass and backward pass in a neural network. One epoch means
    one forward pass and one backward pass of the training example. Here we are setting
    train_epochs 100 times. You can increase it but then the training time also increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'batch_size: This parameter indicates the number of training examples in one
    forward and backward pass. The higher batch size needs more memory so we have
    set this value to 32\. If you have more memory available then you can increase
    the batch size. Please see the simple example given in the following information
    box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'lstm_size: This parameter indicates the number of LSTM cells present in our
    neural network. You can decrease and increase the number of LSTM cells. In our
    case, less than 64 LSTM cells will not give us good output so I have set lstm_size
    to 64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have 1000 training examples and your batch size is 500 then it will take
    2 iterations to complete 1 epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have trained this model on a GPU. If you are not using a GPU then it may
    take a lot of time. You can start training by executing this command: `python
    main.py`. The output of the training is given in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.37: Code snippet for the training output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we train the model, we can load and test it. There are two testing modes
    available:'
  prefs: []
  type: TYPE_NORMAL
- en: Random testing mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User interactive testing mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random testing mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this mode, the script itself will load a random story and give you its answer.
    You can see the value of the hyperparameters in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random testing mode](img/B08394_08_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: figure 8.38, Value of hyperparameters for random testing mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing, execute the `python main.py` command and you can see the testing
    results. These results have been shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Random testing mode](img/B08394_08_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.39: Result of a random testing mode'
  prefs: []
  type: TYPE_NORMAL
- en: User interactive testing mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this mode, if the testing user can give their own story and ask their own
    question, the chatbot will generate the answer to that question. You just need
    to remember that before every word, you need to provide space. You can refer to
    the value of the hyperparameter for the user interactive testing mode in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User interactive testing mode](img/B08394_08_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.40: Values of hyperparameters for the user interactive testing mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing, execute the `python main.py` command and you can see the testing
    results. These results are shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![User interactive testing mode](img/B08394_08_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.41: Result of the user interactive testing mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test all other tasks, then you can use this web application:
    [https://ethancaballero.pythonanywhere.com/](https://ethancaballero.pythonanywhere.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: This approach gives us up to 92 to 95% accuracy. This approach helps us build
    AI-enabled chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing the hybrid approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a real-life scenario, in order to build the chatbot we can also combine some
    of the techniques described here. As per the business needs we can use a hybrid
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. Suppose you are building a chatbot for the finance domain.
    If a user asks for the available balance in his account then we just need a rule-based
    system, which can query the database and generate the account balance details
    for that user. If a user asks how he can transfer money from one account to the
    other account, the chatbot can help the user by generating step-by-step information
    on how to transfer money. Here, we will use the deep learning-based generative
    approach. We should have one system that includes a rule-based engine as well
    as a deep learning algorithm to generate the best possible output. In this system,
    a user's question first goes to the rule-based system. If that question's answer
    can be generated by the rule-based system, then the answer will be passed to the
    end user. If the answer is not generated by a rule-based system, then the question
    will pass further on to the deep learning algorithm and it will generate the answer.
    Finally, the end-user will see the response to his question.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we referred to a different dataset in order to make a chatbot.
    You learned about the rule-based approach that can be used if you don''t have
    any datasets. You also learned about the open and closed domains. After that,
    we used the retrieval-based approach in order to build the basic version of a
    chatbot. In the revised approach, we used TensorFlow. This revised approach is
    great for us because it saves time compared to the basic approach. We implemented
    Google''s neural Conversational Model paper on the Cornell Movie-Dialogs dataset.
    For the best approach, we built a model that used the Facebook bAbI dataset and
    built the basic reasoning functionality that helped us generate good results for
    our chatbot. Although the training time for the revised and best approaches are
    really long, those who want to train the model on the cloud platform can choose
    to do so. So far, I like Amazon Web Services (AWS) and the Google Cloud platform.
    I also uploaded a pre-trained model to my GitHub repository so you could recreate
    the results. If you are a beginner and want to make a really good chatbot, then
    Google''s API.AI is a good chatbot development platform. It is now known as Dialogflow
    and is available at: [https://dialogflow.com/](https://dialogflow.com/). You can
    also refer to the IBM Watson API at: [https://www.ibm.com/watson/how-to-build-a-chatbot/](https://www.ibm.com/watson/how-to-build-a-chatbot/).
    These APIs can help you a great deal in building a chatbot; plus it requires less
    coding knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building a computer vision-based application
    that will help us identify named objects present in images and videos. This application
    will detect objects in real time. The object-detection application is used to
    build self-driving cars, robots, and so on, so keep reading!
  prefs: []
  type: TYPE_NORMAL
