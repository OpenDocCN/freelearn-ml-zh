- en: Chapter 8. Developing Chatbots
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 开发聊天机器人
- en: The year 2017 was all about chatbots, and that continues in 2018\. Chatbots
    are not new at all. The concept of chatbots has been around since the 1970s. Sometimes,
    a chatbot application is also referred to as a question-answering system. This
    is a more specific technical term for a chatbot. Let's take a step into history.
    Lunar was the first rule-based question-answering system. Using this system, geologists
    could ask questions regarding the moon rock from the Apollo missions. In order
    to improvise the rule-based system that was used in the Apollo mission, we had
    to find out a way to encode pattern-based question and answers. For this purpose,
    **Artificial Intelligence Markup Language** was used, also called **AIML**. This
    helps the programmer code less lines of code in order to achieve the same result
    that we generated by using a hardcoded pattern-based system. With recent advances
    in the field of **Machine Learning** (**ML**), we can build a chatbot without
    hardcoded responses.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年是关于聊天机器人的年份，这一趋势在2018年继续。聊天机器人根本不是什么新鲜事物。聊天机器人的概念自20世纪70年代以来就已经存在。有时，聊天机器人应用程序也被称为问答系统。这是一个更具体的聊天机器人技术术语。让我们回顾一下历史。Lunar是第一个基于规则的问答系统。使用这个系统，地质学家可以就阿波罗任务中的月球岩石提出问题。为了改进在阿波罗任务中使用的基于规则的系统，我们必须找到一种方法来编码基于模式的问题和答案。为此，我们使用了**人工智能标记语言**，也称为**AIML**。这有助于程序员用更少的代码行来实现我们通过使用硬编码的基于模式系统所生成相同的结果。随着机器学习（**ML**）领域的最新进展，我们可以构建一个无需硬编码响应的聊天机器人。
- en: Chatbots are now used in apps because of the numerous benefits they have; for
    example, users don't need to install different varieties of apps on their mobile.
    If there is a chatbot that provides you the news, then you can ask for news that
    is on CNN or The Economic Times. Big tech giants such as Facebook, Hike, WeChat,
    Snapchat, Slack, and so on provide chatbots for better customer engagement. They
    achieve this by making a chatbot that one can guide their customers in order to
    perform some operations; it also provides useful information about the product
    and its platforms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聊天机器人具有许多优势，它们现在被用于应用程序中；例如，用户不需要在他们的手机上安装不同种类的应用程序。如果有聊天机器人可以提供新闻，那么你可以要求CNN或《经济学人》上的新闻。像Facebook、Hike、微信、Snapchat、Slack等大型科技公司提供聊天机器人以实现更好的客户互动。他们通过创建一个可以引导客户执行某些操作的聊天机器人来实现这一点；它还提供了有关产品及其平台的有用信息。
- en: Chatbots provide different services. By using the Facebook chatbot platform
    you can order flowers and see the news as well. Doesn't it sound cool? Technically,
    these chatbots are the new apps of the current era. I have briefly discussed the
    benefits of the chatbot, but we will look at them in detail in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人提供不同的服务。通过使用Facebook聊天机器人平台，你可以订购鲜花并查看新闻。这听起来酷吗？从技术上讲，这些聊天机器人是当前时代的应用程序。我简要讨论了聊天机器人的好处，但我们将在本章中详细探讨它们。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'Building the basic version of chatbots:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建聊天机器人的基本版本：
- en: Understanding rule-based systems
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于规则的系统
- en: Understanding the approach
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解方法
- en: Understanding architecture
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解架构
- en: Implementing the rule-based system of chatbots
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施聊天机器人的基于规则系统
- en: Testing rule-based chatbots
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基于规则的聊天机器人
- en: 'Problem with the existing approach:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有方法的缺点：
- en: Understanding key concepts for optimizing the approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解优化方法的关键概念
- en: 'Implementing the revised approach:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施修订的方法：
- en: Data preparation
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Implementing the sequence-to-sequence (seq2seq) model
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施序列到序列（seq2seq）模型
- en: 'Testing the revised approach:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试修订的方法：
- en: Understanding the testing metrics
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试指标
- en: Testing the revised version of chatbots
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试聊天机器人的修订版本
- en: 'Problem with the revised approach:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修订方法的缺点：
- en: Understanding key concepts for solving the existing problems
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解解决现有问题的关键概念
- en: 'The best approach:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法：
- en: Implementing the best approach
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Summary
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Introducing the problem statement
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: In this chapter, our primary goal is to understand how to build a chatbot. Chatbots,
    or **question-answering systems** (**QA systems**), are really helpful. Let's
    consider a fun example. Suppose you are a student and you have five books to read.
    Reading and understanding five books may take time. What if you could feed the
    content of all these five books to the computer and ask only relevant questions?
    By doing this, students could learn the concepts and new information faster. As
    we all know, major internet product companies are arranging information so it
    is easy to access. Chatbots or QA systems will help us understand the meaning
    behind this information. This is the main reason why chatbots are the buzzword
    for the year 2017\. Whatever application you can think of, you can make a chatbot
    for it. Many messaging platforms now host chatbots built by developers, including
    Facebook Messenger, Slack, WeChat, and so on. Chatbots are the new app because
    they already live inside the installed apps that you probably use a dozen times
    in a day. Chatbots developed using Facebook chatbot APIs are inside the Facebook
    Messenger app. You might use Messenger a dozen times a day. So, users don't need
    to install a separate app for a specific functionality. This will help companies
    engage their customers even better.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的主要目标是了解如何构建一个聊天机器人。聊天机器人，或**问答系统**（**QA系统**），非常有用。让我们考虑一个有趣的例子。假设你是一名学生，你有五本书要读。阅读和理解五本书可能需要时间。如果你能将这些五本书的内容输入到电脑中，只提出相关的问题呢？通过这样做，学生可以更快地学习概念和新信息。众所周知，主要的互联网产品公司正在整理信息，使其易于访问。聊天机器人或QA系统将帮助我们理解这些信息背后的含义。这就是为什么聊天机器人是2017年的热门词汇。无论你能想到什么应用，你都可以为它制作一个聊天机器人。现在许多消息平台都托管了开发者构建的聊天机器人，包括Facebook
    Messenger、Slack、微信等等。聊天机器人是新的应用程序，因为它们已经存在于你每天可能使用十几次的已安装应用程序中。使用Facebook聊天机器人API开发的聊天机器人位于Facebook
    Messenger应用程序中。你可能每天会使用Messenger十几次。因此，用户不需要为特定的功能安装一个单独的应用程序。这将帮助公司更好地与客户互动。
- en: 'Before we proceed, I want to introduce a couple of important terms that can
    help us understand which kind of chatbot development we are targeting in this
    chapter. First of all, let''s understand what the different approaches for developing
    a chatbot are:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我想介绍一些重要的术语，这些术语可以帮助我们了解在本章中我们针对的是哪种聊天机器人开发。首先，让我们了解开发聊天机器人的不同方法：
- en: Retrieval-based approach
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于检索的方法
- en: Generative-based approach
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于生成的方法
- en: Retrieval-based approach
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于检索的方法
- en: In a retrieval-based approach, we need to define the set of predefined responses,
    and we will apply some kind of heuristics on predefined responses so that the
    chatbot can generate the best possible answers for the given questions. The answers
    are very dependent on the input question and the context of that input question.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于检索的方法中，我们需要定义一组预定义的响应，并将某种启发式方法应用于预定义的响应，以便聊天机器人可以为给定的问题生成最佳可能的答案。这些答案非常依赖于输入问题和该输入问题的上下文。
- en: Earlier, during the development of the retrieval-based model, we used only expression
    matching, which can help us get the appropriate answer, but using only expression
    matching won't help us here. So recently, researchers and programmers have started
    using expression matching along with advanced machine learning (ML) classifier
    techniques. Let's take an example to understand how the machine learning classifier
    will be useful in order to build retrieval-based chatbots.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于检索的模型的过程中，我们之前只使用了表达式匹配，这可以帮助我们得到适当的答案，但仅使用表达式匹配在这里不会有所帮助。因此，最近研究人员和程序员开始使用表达式匹配和高级机器学习（ML）分类器技术。让我们通过一个例子来了解机器学习分类器如何有助于构建基于检索的聊天机器人。
- en: Suppose Alice needs to send flowers to her friends on their birthdays. One of
    her friends, Emma, likes roses and another friend, Lucy, likes lilies. Alice uses
    a flower-ordering chatbot to book her order. She writes, *I want to book one multicolor
    rose flower bouquet and one with lilies*. So, in this case, if we implement a
    basic ML classifier, the chatbot can easily identify that there are two different
    orders Alice is booking, and it is also able to interpret the quantity of each
    of them. The chatbot will also ask for the different addresses and so on. By using
    ML techniques, we can code more complex heuristics, which can help us to generate
    more appropriate chatbot answers. The Facebook messenger chatbot API is an example
    of this.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设Alice需要在朋友的生日时送花。她的一个朋友Emma喜欢玫瑰，另一个朋友Lucy喜欢百合。Alice使用一个花订购聊天机器人来预订她的订单。她写道：“我想预订一束多色的玫瑰花束和一束百合花。”在这种情况下，如果我们实现一个基本的机器学习分类器，聊天机器人可以轻松地识别Alice正在预订的两个不同的订单，并且它还能够解释每个订单的数量。聊天机器人还会要求提供不同的地址等。通过使用机器学习技术，我们可以编写更复杂的启发式算法，这有助于我们生成更合适的聊天机器人回答。Facebook
    messenger聊天机器人API就是这样一个例子。
- en: There is another interesting example that can be solved by using ML heuristics.
    Say, you ask a chatbot, *what day is it today? Or*, *today is what day?* If we
    have implemented advanced ML techniques, then it can recognize that both questions
    are worded differently but have the same intent. During the development of the
    chatbot, intent and context detection are more complex tasks, which can be implemented
    by ML techniques and using some heuristics.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有另一个有趣的例子可以通过使用机器学习启发式算法来解决。比如说，你问聊天机器人：“今天是什么日子？”或者“今天是星期几？”如果我们实现了高级机器学习技术，那么它可以识别出这两个问题虽然措辞不同，但意图相同。在聊天机器人的开发过程中，意图和上下文检测是更复杂的任务，可以通过机器学习技术和一些启发式算法来实现。
- en: Now let's move on to the harder approach, which is the generative-based approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转向更困难的方法，即基于生成的方法。
- en: Generative-based approach
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于生成的方法
- en: 'In the generative-based approach, there aren''t any predefined responses given
    to the chatbot. The chatbot generates the responses from scratch. In order to
    build the generative-based chatbot, we need to provide a lot of data and the machine
    will learn how to answer the questions asked by users just by seeing the data.
    In 2015, Google researchers Oriol Vinyals and Quoc V. Le proposed an approach
    called A *Neural Conversational Network.* You can refer to the paper at: [https://arxiv.org/pdf/1506.05869v2.pdf](https://arxiv.org/pdf/1506.05869v2.pdf).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于生成的方法中，没有为聊天机器人提供任何预定义的响应。聊天机器人从头开始生成响应。为了构建基于生成的聊天机器人，我们需要提供大量数据，机器将仅通过观察数据来学习如何回答用户提出的问题。2015年，谷歌研究人员Oriol
    Vinyals和Quoc V. Le提出了一种称为A *神经对话网络*的方法。您可以参考以下论文：[https://arxiv.org/pdf/1506.05869v2.pdf](https://arxiv.org/pdf/1506.05869v2.pdf)。
- en: In this paper, researchers have used a Cornell movie dialog dataset. This dataset
    has been fed to the machines so that it can learn the basic English language.
    For this, they have used the **Sequence-to-sequence** (**seq2seq**) neural network
    architecture. After that, they used the IT support dataset so that the machines
    have domain knowledge. Once a machine is trained on that, they have tested the
    chatbot in the IT support department, this chatbot will be able to answer questions
    with great accuracy. In the upcoming section, we will build our own Neural Conversational
    Network. This approach is less time consuming and overcomes the challenges we
    face in the retrieval-based model, such as intent identification, context identification,
    and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，研究人员使用了康奈尔电影对白数据集。这个数据集被输入到机器中，以便它能够学习基本的英语语言。为此，他们使用了**序列到序列**（**seq2seq**）神经网络架构。之后，他们使用了IT支持数据集，以便机器获得领域知识。一旦机器在这个数据集上训练完毕，他们就在IT支持部门测试了聊天机器人，这个聊天机器人将能够以极高的准确性回答问题。在接下来的章节中，我们将构建自己的神经对话网络。这种方法耗时较少，并克服了我们在基于检索的模型中面临的挑战，如意图识别、上下文识别等。
- en: 'There are some other important terms that we need to discuss here. There are
    some important constraints that we need to think about before developing a chatbot.
    The first one is related to the conversation domain:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在此我们还需要讨论一些其他的重要术语。在开发聊天机器人之前，我们需要考虑一些重要的约束条件。第一个与对话领域相关：
- en: Open domain
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开放域
- en: Closed domain
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 封闭域
- en: Open domain
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放域
- en: First, let's understand what an open domain is. Conversations are fuzzy and
    uncertain sometimes. Let me give you an example. Suppose you meet an old school
    friend that you haven't seen for many years. During the course of the conversation,
    you don't know which particular topic you both are going to talk about. In this
    situation, the conversation can go anywhere. So, the domain of the conversation
    is not fixed. You can talk about life, jobs, travelling, family, and so on. There
    is an infinite number of topics that you can talk about. This kind of conversation,
    where we can't restrict the areas we are talking about, is called an open domain.
    Developing an open domain chatbot is difficult because ideally, this kind of chatbot
    can answer every question from any kind of domain with human-level accuracy.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解什么是开放域。有时候对话是模糊和不确定的。让我给你举一个例子。假设你遇到了一个多年未见的老同学。在对话的过程中，你不知道你们将要谈论哪个特定的话题。在这种情况下，对话可以随意展开。因此，对话的领域是不固定的。你可以谈论生活、工作、旅行、家庭等等。你可以谈论无数的话题。这种我们无法限制谈话领域的对话，被称为开放域。开发一个开放域聊天机器人是困难的，因为理想情况下，这种聊天机器人能够以人类水平的准确性回答来自任何领域的每一个问题。
- en: Currently, these kinds of chatbots are not made. When we are able to make this
    kind of chatbot, it will have to pass the Turing Test. Let me give you a glimpse
    of the Turing Test so that you can understand the explanation better. This experiment
    was created by the great computer scientist Alan Turing in 1950\. In this experiment,
    a person, called a judge, asks a series of questions to a person and a machine.
    Now, the judge won't know which answer is from the human and which one is from
    the machine. But after seeing or hearing the answers, if the judge can't differentiate
    which answers are coming from the human and which answers are coming from the
    machine, then the machine passes the Turing Test, and we can say that the machine
    exhibited human-level intelligence because it behaves as intelligently as humans.
    So far, there is not a single chatbot that has passed the Turing Test with human-level
    accuracy. You can read more about the Turing Test by visiting [https://en.wikipedia.org/wiki/Turing_test](https://en.wikipedia.org/wiki/Turing_test).
    This segment of technology is growing rapidly, so the next five years could be
    exciting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这类聊天机器人尚未被制造。当我们能够制造这种聊天机器人时，它将必须通过图灵测试。让我给你一个图灵测试的简要介绍，以便你能更好地理解解释。这个实验是由伟大的计算机科学家艾伦·图灵在1950年创造的。在这个实验中，一个被称为裁判的人向一个人和一个机器提出一系列问题。现在，裁判不知道哪个答案是来自人类，哪个答案是来自机器。但是，在看到或听到答案后，如果裁判无法区分哪些答案是来自人类，哪些答案是来自机器，那么机器就通过了图灵测试，我们可以说机器表现出了人类水平的智能，因为它表现得像人类一样聪明。到目前为止，还没有任何一个聊天机器人能够以人类水平的准确性通过图灵测试。你可以通过访问[https://en.wikipedia.org/wiki/Turing_test](https://en.wikipedia.org/wiki/Turing_test)了解更多关于图灵测试的信息。这一段技术正在快速发展，所以接下来的五年可能会非常激动人心。
- en: Google has been quite aggressive in making an open domain chatbot. It is building
    this product in the form of Google Assistance, but the accuracy levels and functionality
    in passing the Turing Test are still limited. Now let's understand the second
    type of domain.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在开发开放域聊天机器人方面非常积极。它正以谷歌助手的形式构建这个产品，但通过图灵测试的准确性和功能性仍然有限。现在让我们了解第二种类型的领域。
- en: Closed domain
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 封闭域
- en: 'A closed domain is the opposite of an open domain. For a closed domain, we
    need to restrict the conversation topics. Let''s take an example: in the office,
    we sometimes have meetings. Before the meeting, the participants know the topics
    on which there''s going to be a discussion. So during the meeting, we just focus
    on those topics. Here, we won''t have an infinite number of topics and domains
    to talk about. This kind of conversation, where we have restricted the areas we
    can talk about, is called a closed domain.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 封闭域是开放域的对立面。对于封闭域，我们需要限制对话话题。让我们举一个例子：在办公室，我们有时会有会议。在会议之前，参与者知道将要讨论的话题。因此，在会议期间，我们只关注那些话题。在这里，我们不会有无限的话题和领域可以谈论。这种我们限制了可以谈论的领域和话题的对话，被称为封闭域。
- en: If a financial institute such as a bank launches a chatbot for their customers,
    then the developed chatbot cannot answer questions such as *can you tell me* *what
    the weather in Singapore is today?* But it helps you check the procedure of applying
    for a credit card, and this is because a chatbot can understand questions related
    to a specific domain. A chatbot for a closed domain is definitely possible, and
    there are many companies that are building domain-specific chatbots as it is good
    for engaging with the customer base. So during the chapter, we will be focusing
    on the closed domain chatbot.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一家金融机构，如银行，为他们的客户推出聊天机器人，那么开发的聊天机器人不能回答诸如“你能告诉我今天新加坡的天气吗？”这样的问题。但它可以帮助你检查申请信用卡的程序，这是因为聊天机器人可以理解与特定领域相关的提问。一个针对封闭领域的聊天机器人是肯定可能的，而且有很多公司正在构建特定领域的聊天机器人，因为这对与客户群互动是有益的。所以在本章中，我们将重点关注封闭领域聊天机器人。
- en: 'Let''s try to understand the last constraint; the conversation length, which
    means the length of the answers we will be getting from the chatbot. Based on
    that, we need to understand the following terms:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解最后一个约束条件；对话长度，这意味着我们将从聊天机器人那里得到的答案的长度。基于这一点，我们需要理解以下术语：
- en: Short conversation
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简短对话
- en: Long conversation
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长对话
- en: Short conversation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简短对话
- en: 'This type of chatbot can generate short answers. During the development of
    the chatbot, we need to ask ourselves whether we expect a short conversation or
    not. If we expect a short answer then you should be glad because this short conversation-based
    chatbot can be easily built. An example of a short conversation is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的聊天机器人可以生成简短答案。在聊天机器人的开发过程中，我们需要问自己是否期望简短对话。如果我们期望简短答案，那么你应该很高兴，因为基于简短对话的聊天机器人可以很容易地构建。以下是一个简短对话的例子：
- en: 'Human: Hi'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 人：嗨
- en: 'Machine: Hello'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 机器：你好
- en: 'Human: How are you?'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人：你好吗？
- en: 'Machine: I''m fine'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 机器：我很好
- en: This example indicates short conversations generated by a chatbot.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子表明了聊天机器人生成的简短对话。
- en: Long conversation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长对话
- en: 'This type of chatbot can generate long answers. It is hard for a machine to
    learn long conversations, so building a chatbot that can generate long conversations
    is difficult. Let''s look at an example of a long conversation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的聊天机器人可以生成长答案。机器学习长对话是困难的，因此构建能够生成长对话的聊天机器人是困难的。让我们看看一个长对话的例子：
- en: 'Human: I want to tell you a story.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人：我想给你讲一个故事。
- en: 'Machine: Please go on.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 机器：请继续。
- en: 'Human: Here you go. John went to the market. Daniel is travelling to India.
    Siri has an apple. Siri is in the kitchen. So my question is, where is Siri?'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 人：给你。约翰去了市场。丹尼尔正在去印度的路上。Siri有一个苹果。Siri在厨房。所以我的问题是，Siri在哪里？
- en: 'Machine: Based on your story, I think Siri is in the kitchen.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 机器：根据你的故事，我认为Siri在厨房。
- en: As you can see in this example, in order to generate the right answer, the machine
    should also store and process the given facts so that it can generate the right
    answer. Therefore, long conversation and reasoning-based chatbots are a bit hard
    to develop.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本例中看到的，为了生成正确的答案，机器也应该存储和处理给定的信息，以便它能生成正确的答案。因此，长对话和基于推理的聊天机器人开发起来有点困难。
- en: So far, you have learned a lot of terms. Now let's see how they're going to
    affect us when we develop a chatbot. Based on the approaches and domain type,
    we can build different types of chatbots.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了很多术语。现在让我们看看它们在我们开发聊天机器人时将如何影响我们。根据方法和领域类型，我们可以构建不同类型的聊天机器人。
- en: Open domain and generative-based approach
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开放领域和基于生成的方法
- en: We want to build a chatbot using the generative-based approach, which operates
    on the open domain. This means that the chatbot needs to learn how to answer the
    questions from any domain from scratch. The conversation can go in any direction
    here. This type of chatbot is an example of **Artificial General Intelligence**
    (**AGI**), and we are not quite there yet.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用基于生成的方法来构建聊天机器人，这种方法在开放领域上运行。这意味着聊天机器人需要从头开始学习如何回答来自任何领域的问题。这里的对话可以朝任何方向发展。这种类型的聊天机器人是**通用人工智能**（**AGI**）的一个例子，我们还没有达到那个阶段。
- en: So, developing this type of chatbot is not a part of this chapter.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，开发这种类型的聊天机器人不是本章的一部分。
- en: Open domain and retrieval-based approach
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开放领域和基于检索的方法
- en: If we want to build a chatbot that can operate on an open domain using a retrieval-based
    approach, then as coders, we need to hardcode pretty much all the responses and
    possible questions as well as variations. This approach consumes a hell of a lot
    of time, so this type of chatbot is also not a part of this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要构建一个可以使用基于检索的方法在开放域上运行的聊天机器人，那么作为编码者，我们需要硬编码几乎所有的响应以及可能的问题和变体。这种方法消耗了大量的时间，因此这种类型的聊天机器人也不是本章的一部分。
- en: Closed domain and retrieval-based approach
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 封闭域和基于检索的方法
- en: We have understood that we can't operate on the open domain, but what about
    the closed domain? We can surely work on the closed domain as there is a finite
    number of questions that can be asked by a user to a chatbot and that a chatbot
    can answer. If we use the retrieval-based approach for a closed domain, then we
    can code questions that are relatively easy. We can integrate some NLP tools,
    such as a parser, **Name Entity Recognition** (**NER**), and so on in order to
    generate the most accurate answer.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到我们无法在开放域进行操作，但关于封闭域呢？我们当然可以在封闭域工作，因为用户可以向聊天机器人提出有限数量的问题，而聊天机器人可以回答。如果我们为封闭域使用基于检索的方法，那么我们可以编写相对简单的问题。我们可以集成一些NLP工具，例如解析器、**命名实体识别**（NER）等，以便生成最准确的答案。
- en: Let's take an example. Suppose we want to build a chatbot that can give us real-time
    weather information for any location. If we build the chatbot using a retrieval-based
    approach, then the user will definitely get an accurate answer for questions such
    as *What is the weather in Mumbai? What is the weather in California? Are there
    any chances of rainfall today?* The chatbot will give you answers to the first
    two questions really well, but during the third question, it will be confused
    because we don't provide a location for the chances of rainfall. If the chatbot
    has used some heuristics, then there will be a chance that you may get a response.
    Chatbot may ask you about the location for which you want to know the chances
    of rainfall, but mostly, this won't happen. The chatbot directly tells you the
    chances of rainfall in, say, California. In reality, I want to know the chances
    of rainfall in Mumbai. So, these kinds of context-related problems are common
    to the retrieval-based approach. We need to implement the generative-based approach
    to overcome context-related problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子。假设我们想要构建一个可以为我们提供任何位置的实时天气信息的聊天机器人。如果我们使用基于检索的方法构建聊天机器人，那么用户将肯定能够得到关于“孟买的天气如何？加利福尼亚的天气如何？今天有降雨的可能吗？”等问题准确的答案。聊天机器人会很好地回答前两个问题，但在第三个问题时，它会感到困惑，因为我们没有提供降雨可能性的位置。如果聊天机器人使用了某些启发式方法，那么你可能会得到一个响应。聊天机器人可能会询问你想要了解降雨可能性的位置，但大多数情况下，这种情况不会发生。聊天机器人会直接告诉你，例如，加利福尼亚的降雨可能性。实际上，我想知道孟买的降雨可能性。所以，这类与上下文相关的问题在基于检索的方法中很常见。我们需要实施基于生成的方法来克服与上下文相关的问题。
- en: Closed domain and generative-based approach
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 封闭域和基于生成的方法
- en: When we use the generative-based approach for the closed domain, the development
    of this kind of chatbot takes less coding time, and the quality of the answers
    improves as well. If we want our chatbot to understand long contexts and intents
    over a series of questions from the user, then the generative-based approach is
    the right choice. After training on large corpus and optimization, the chatbot
    can understand the context and intent of questions as well as be able to ask reasoning
    types of questions. This space of chatbot development is exciting and interesting
    for research and implementing new ideas.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为封闭域使用基于生成的方法时，这种类型聊天机器人的开发所需编码时间更少，答案的质量也得到了提高。如果我们想让我们的聊天机器人理解用户一系列问题中的长上下文和意图，那么基于生成的方法是正确的选择。经过在大语料库上的训练和优化后，聊天机器人可以理解问题的上下文和意图，并且能够提出推理类型的问题。这个聊天机器人开发空间对于研究和实施新想法来说既令人兴奋又有趣。
- en: 'Let''s take an example. Suppose we have built a chatbot to apply for a home
    loan from a bank. When the user runs this chatbot, it may ask these questions:
    what is the status of my home loan application? Are there any documents remaining
    from my side that I should upload? Will I get approval in the next 2 days or not?
    Have you received my tax sheets and salary slips? The context of the last question
    is dependent on the second question. These kinds of questions and their answers
    can be easily generated with the generative-based approach.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子。假设我们构建了一个聊天机器人来向银行申请住房贷款。当用户运行这个聊天机器人时，它可能会问这些问题：我的住房贷款申请状态如何？我这边还有哪些文件需要上传？我会在接下来的2天内获得批准吗？您收到我的税务报表和工资条了吗？最后一个问题的上下文取决于第二个问题。这些类型的问题及其答案可以很容易地通过基于生成的方法生成。
- en: 'Refer to the following figure, which will help us summarize all the preceding
    discussions:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图，它将帮助我们总结前面的讨论：
- en: '![Closed domain and generative-based approach](img/B08394_08_01.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![封闭域和基于生成的方法](img/B08394_08_01.jpg)'
- en: 'Figure 8.1: Pictorial representation of the approach to develop a chatbot'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：开发聊天机器人的方法示意图
- en: In this chapter, we will be building a chatbot that will be based on the closed
    domain and that uses retrieval-based and generative-based approaches.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个基于封闭域的聊天机器人，它将使用基于检索和基于生成的方法。
- en: Now let's look at the dataset that we will be using in this chapter.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看本章我们将使用的数据集。
- en: Understanding datasets
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'In order to develop a chatbot, we are using two datasets. These datasets are
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发聊天机器人，我们使用了两个数据集。这些数据集如下：
- en: Cornell Movie-Dialogs dataset
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 康奈尔电影对话数据集
- en: bAbI dataset
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bAbI数据集
- en: Cornell Movie-Dialogs dataset
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 康奈尔电影对话数据集
- en: 'This dataset has been widely used for developing chatbots. You can download
    the Cornell Movie-Dialogs corpus from this link: [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
    This corpus contains a large metadata-rich collection of fictional conversations
    extracted from raw movie scripts.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集已被广泛用于开发聊天机器人。您可以从以下链接下载康奈尔电影对话语料库：[https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)。这个语料库包含从原始电影剧本中提取的大量丰富元数据的虚构对话集合。
- en: 'This corpus has 220,579 conversational exchanges between 10,292 pairs of movie
    characters. It involves 9,035 characters from 617 movies. In total, it has 304,713
    utterances. This dataset also contains movie metadata. There are the following
    types of metadata:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语料库包含10,292对电影角色之间的220,579个对话交换。它涉及来自617部电影中的9,035个角色。总共有304,713个话语。这个数据集还包含电影元数据。以下是一些元数据类型：
- en: 'Movie-related metadata includes the following details:'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与电影相关的元数据包括以下细节：
- en: Genre of the movie
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影类型
- en: Release year
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布年份
- en: IMDb rating
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMDb评分
- en: 'Character-related metadata includes the following details:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与角色相关的元数据包括以下细节：
- en: Gender of 3,774 characters
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3,774个角色的性别
- en: Total number of characters in movies
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影中的角色总数
- en: When you download this dataset, you'll notice that there are two files we will
    be using throughout this chapter. The names of the files are `movie_conversations.txt`
    and `movie_lines.txt`. Let's look at the content details of each file.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当您下载这个数据集时，您会注意到我们将在这个章节中使用两个文件。这两个文件的名称是`movie_conversations.txt`和`movie_lines.txt`。让我们看看每个文件的内容细节。
- en: Content details of movie_conversations.txt
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: movie_conversations.txt的内容细节
- en: 'This file contains `line_id` for the `movie_lines.txt` file. You can see the
    content of `movie_conversations.txt` in the following figure:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件包含`movie_lines.txt`文件的`line_id`。您可以在以下图中看到`movie_conversations.txt`的内容：
- en: '![Content details of movie_conversations.txt](img/B08394_08_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![movie_conversations.txt的内容细节](img/B08394_08_02.jpg)'
- en: 'Figure 8.2: Sample content of the movie_conversations.txt file'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：movie_conversations.txt文件的内容示例
- en: As you can see in the preceding figure, this file contains line numbers, and
    the actual content of the conversation is present in *movie_lines.txt*. *+++$+++*
    acts as a separator. You must definitely be eager to know how to process this
    dataset; just bear with me for a while and we will cover this aspect in the upcoming
    sections.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的图中可以看到，这个文件包含行号，实际对话内容位于*movie_lines.txt*中。*+++$+++*充当分隔符。您肯定非常想知道如何处理这个数据集；请稍等片刻，我们将在接下来的章节中介绍这一方面。
- en: Now let's look at the content of the next file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看下一个文件的内容。
- en: Content details of movie_lines.txt
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`movie_lines.txt` 的内容细节'
- en: 'This file contents the actual movie dialogs. You can see the sample content
    of `movie_lines.txt` in the following figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件包含实际的电影对白。你可以在下面的图像中看到 `movie_lines.txt` 的样本内容：
- en: '![Content details of movie_lines.txt](img/B08394_08_03.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![`movie_lines.txt` 的内容细节](img/B08394_08_03.jpg)'
- en: 'figure 8.3: Sample content of movie_lines.txt'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：`movie_lines.txt` 的样本内容
- en: As you can see in the preceding figure, each line has a unique conversation
    line ID. This `line_id` refers to the `movie_conversations.txt` file. This file
    contains the same line separator and the names of the characters involved in the
    conversation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前面的图像中可以看到的，每一行都有一个唯一的对话行 ID。这个 `line_id` 指的是 `movie_conversations.txt`
    文件。这个文件包含相同的行分隔符和参与对话的角色名称。
- en: 'If you see the both files together, then it is might make more sense to you.
    In the `movie_conversations.txt` file, refer to the conversations on *line_id
    194, 195, 196*, and *197*. All these conversations can be found in `movie_lines.txt.`
    In the preceding image, you can see that *line_id* *194* contains this question:
    *Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly
    horrendous public break- up on the quad. Again.* On the other hand, *line_id*
    *195* contains the answer: *Well, I thought we''d start with pronunciation, if
    that''s okay with you.*'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你同时看到这两个文件，那么可能对你来说更有意义。在 `movie_conversations.txt` 文件中，参考 *行号 194, 195, 196*
    和 *197* 上的对话。所有这些对话都可以在 `movie_lines.txt` 文件中找到。在前面的图像中，你可以看到 *行号* *194* 包含这个问题：*我们能快点吗？Roxanne
    Korrine 和 Andrew Barrett 在广场上正在进行一场可怕的公开分手。再次。* 另一方面，*行号* *195* 包含这个答案：*嗯，我想如果我们从发音开始，如果你觉得可以的话。*
- en: We need to prepare the dataset in the form of a question-answer format before
    feeding it to the machine. We will implement the data preparation step before
    using it for training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数据集以问答格式准备并输入到机器之前，我们需要准备这个数据集。我们将在使用它进行训练之前实现数据准备步骤。
- en: Now let's look at the bAbI dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 bAbI 数据集。
- en: The bAbI dataset
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: bAbI 数据集
- en: 'This dataset is built by Facebook AI Research (FAIR), where AI stands for artificial
    intelligence. This dataset belongs to the bAbI project. You can download the dataset
    from [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/).
    It is a well-maintained dataset. The goal of the bAbI project is to try to build
    an automatic text understanding and reasoning system. This dataset consists of
    the following sub datasets:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集是由 Facebook AI Research (FAIR) 构建的，其中 AI 代表人工智能。这个数据集属于 bAbI 项目。你可以从 [https://research.fb.com/downloads/babi/](https://research.fb.com/downloads/babi/)
    下载这个数据集。这是一个维护良好的数据集。bAbI 项目的目标是尝试构建一个自动文本理解和推理系统。这个数据集包括以下子数据集：
- en: The (20) QA bAbI tasks
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) QA bAbI 任务
- en: The (6) dialog bAbI tasks
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) 对话 bAbI 任务
- en: The Children's Book Test
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 儿童书籍测试
- en: The Movie Dialog dataset
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影对话数据集
- en: The WikiMovies dataset
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WikiMovies 数据集
- en: The Dialog-based Language Learning dataset
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于对话的语言学习数据集
- en: The SimpleQuestions dataset
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单问题数据集
- en: HITL Dialogue Simulator
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HITL 对话模拟器
- en: We will be using only one subset here, which is the (20) QA bAbI tasks because
    it is the one that's most useful for building the chatbot.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里将只使用一个子集，即 (20) QA bAbI 任务，因为它对于构建聊天机器人最有用。
- en: The (20) QA bAbI tasks
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: (20) QA bAbI 任务
- en: 'Let''s look at this subdataset in detail. Here, 20 different tasks have been
    performed using this (20) QA bAbI dataset. Let''s see what these tasks are. These
    tasks give machines the capacity to perform some reasoning, and based on that,
    the machine can answer a question. You can refer to the task name given in the
    following figure:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这个子数据集。在这里，使用这个 (20) QA bAbI 数据集执行了 20 个不同的任务。让我们看看这些任务是什么。这些任务赋予机器进行某些推理的能力，基于这些推理，机器可以回答问题。你可以参考以下图像中给出的任务名称：
- en: '![The (20) QA bAbI tasks](img/B08394_08_04.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![ (20) QA bAbI 任务](img/B08394_08_04.jpg)'
- en: 'Figure 8.4: (20) QA bAbI task details'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4： (20) QA bAbI 任务细节
- en: 'Image source: http://www.thespermwhale.com/jaseweston/babi/abordes-ICLR.pdf'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：http://www.thespermwhale.com/jaseweston/babi/abordes-ICLR.pdf
- en: Facebook researchers Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M.
    Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov published a paper
    in which they proposed an interesting AI-based QA system. You can refer to their
    research paper by visiting [https://arxiv.org/abs/1502.05698](https://arxiv.org/abs/1502.05698).
    In this chapter, we will be attempting to achieve the results for task T1, and
    we will regenerate its result.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook的研究员Jason Weston、Antoine Bordes、Sumit Chopra、Alexander M. Rush、Bart
    van Merriënboer、Armand Joulin和Tomas Mikolov发表了一篇论文，在论文中他们提出了一种有趣的基于AI的QA系统。你可以通过访问[https://arxiv.org/abs/1502.05698](https://arxiv.org/abs/1502.05698)来参考他们的研究论文。在本章中，我们将尝试实现任务T1的结果，并将重新生成其结果。
- en: 'This dataset contains the corpus in two languages, English and Hindi. There
    are two types of folders here: the folder with the name *en* has 1,000 training
    examples, whereas *en-10K* has 10,000 training examples. The format for each of
    the task datasets is given in the following figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含两种语言的语料库，英语和印地语。这里有两种类型的文件夹：名为*en*的文件夹有1,000个训练示例，而*en-10K*有10,000个训练示例。每个任务数据集的格式如下图所示：
- en: '![The (20) QA bAbI tasks](img/B08394_08_05.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![（20）QA bAbI任务](img/B08394_08_05.jpg)'
- en: 'Figure 8.5: Format of Single supporting QA bAbI task'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：单个支持QA bAbI任务的格式
- en: The supporting facts are called a story. Based on the story, the user can ask
    questions to the machine, and the machine should give the logically correct answer
    that can be derived from the provided supporting text. This is a hard task because
    in this case, the machine should remember the long context that it can use as
    and when needed. We will use this interesting dataset soon.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 支持的事实被称为故事。基于故事，用户可以向机器提问，机器应该给出可以从提供的支持文本中推导出的逻辑上正确的答案。这是一个困难的任务，因为在这种情况下，机器应该记住它可以随时使用的长上下文。我们很快就会使用这个有趣的数据集。
- en: Now let's start building the chatbot baseline version.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始构建聊天机器人的基本版本。
- en: Building the basic version of a chatbot
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建聊天机器人的基本版本
- en: In this section, we will be building the basic version of a chatbot. Getting
    data is not an issue for any company nowadays but getting a domain-specific conversational
    dataset is challenging.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建聊天机器人的基本版本。对于任何公司来说，获取数据都不是问题，但获取特定领域的对话数据集具有挑战性。
- en: There are so many companies out there whose goal is to make an innovative domain-specific
    chatbot, but their major challenge is getting the right data. If you are facing
    the same issue, then this basic approach can help you in that. This basic version
    of a chatbot is based on the closed domain and the retrieval-based approach, which
    uses the rule-based system. So, let's start understanding each aspect of the rule-based
    system.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有那么多公司的目标是要创建一个创新的特定领域聊天机器人，但他们的主要挑战是获取正确的数据。如果你面临同样的问题，那么这个基本方法可以帮助你。这个聊天机器人的基本版本是基于封闭领域和基于检索的方法，它使用基于规则的系统。所以，让我们开始了解基于规则的系统的每个方面。
- en: Why does the rule-based system work?
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么基于规则的系统有效？
- en: As I mentioned earlier, a rule-based system is the way to implementing a retrieval-based
    approach. Now, you may wonder why we need a rule-based system. Considering that
    we are living in the era of Machine Learning (ML), doesn't it sound old? Let me
    share my personal experience with you. I closely collaborate with many start-ups.
    Some of them operate in the financial domain, some in the human resource domain,
    and some in the legal domain. In this era of chatbots, start-ups are really keen
    on developing domain-specific chatbots that can help users. Initially, they work
    on some general dataset so that the machine can learn the language and generate
    the logical casual answers for them, but they soon realize that they don't have
    enough domain-specific data to help them build a good chatbot. Let me give you
    an example.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前提到的，基于规则的系统是实现基于检索的方法的方式。现在，你可能想知道为什么我们需要基于规则的系统。考虑到我们生活在机器学习（ML）的时代，这难道听起来不古老吗？让我与你分享我的个人经历。我密切合作了许多初创公司。其中一些在金融领域运营，一些在人力资源领域运营，还有一些在法律领域运营。在这个聊天机器人的时代，初创公司非常热衷于开发特定领域的聊天机器人，以帮助用户。最初，他们使用一些通用数据集，以便机器可以学习语言并为它们生成逻辑因果答案，但他们很快意识到他们没有足够的特定领域数据来帮助他们构建一个好的聊天机器人。让我给你举一个例子。
- en: 'I collaborated with a fintech start-up where we needed to build a chatbot.
    The specific requirement for the chatbot was that it should help customers who
    want to apply for a home loan as well as those who have already applied and need
    some assistance. Now, this fintech just started 1 and a half years ago. So they
    don''t have large chat logs about the kind of queries customers may have. In short,
    the company doesn''t have enough domain-specific data, such as what kind of queries
    a home loan applicant may ask and how to synchronize these customer queries to
    the loan procedure this fintech company follows. In this case, there are two main
    things that we need to focus on:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我与一家金融科技初创公司合作，我们需要构建一个聊天机器人。聊天机器人的具体要求是它应该帮助想要申请住房贷款的客户，以及那些已经申请并需要一些帮助的客户。现在，这家金融科技初创公司刚刚开始运营1年半。因此，他们没有关于客户可能提出的问题的大量聊天记录。简而言之，公司没有足够的特定领域数据，例如住房贷款申请人可能提出的问题类型，以及如何将这些客户查询同步到这家金融科技公司遵循的贷款程序。在这种情况下，我们需要关注两个主要方面：
- en: We need to build a minimum viable chatbot that can help customers with basic
    FAQs
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要构建一个最小可行聊天机器人，以帮助客户处理基本的常见问题解答（FAQs）。
- en: With the help of this minimum viable chatbot, you can also come to learn what
    kind of questions people are asking, and based on these questions, the chatbot
    can be tweaked
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过这个最小可行聊天机器人的帮助，你还可以了解人们提出的问题类型，并根据这些问题，调整聊天机器人。
- en: In these kind of situations, where we don't have a domain-specific dataset,
    the rule-based or retrieval-based model will work for us. From the next section
    onward, we will explore the rule-based system and the approach of developing a
    basic chatbot and its architecture.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这类情况下，当我们没有特定领域的数据集时，基于规则的或基于检索的模型将为我们工作。从下一节开始，我们将探讨基于规则的系统以及开发基本聊天机器人和其架构的方法。
- en: Understanding the rule-based system
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基于规则的系统
- en: 'In this section, we will cover the rule-based system so that you don''t feel
    left out when we start developing the retrieval-based chatbot. The **rule**-**based**
    (**RB**) system is defined as follows: using available knowledge or rules, we
    develop a system that uses the rules, apply the available system rules on the
    corpus, and try to generate or infer the results. From the perspective of the
    chatbot, the RB system has all possible questions and answers and they''re hardcoded.
    We can definitely use regular expressions and fuzzy logic to implement some kind
    of heuristics in order to make the RB system more accurate.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍基于规则的系统，这样当你我们开始开发基于检索的聊天机器人时，你不会感到被排除在外。**基于规则**（**RB**）系统定义为：使用可用的知识或规则，我们开发一个使用规则的系统，在语料库上应用可用的系统规则，并尝试生成或推断结果。从聊天机器人的角度来看，RB系统包含所有可能的问题和答案，并且它们是硬编码的。我们绝对可以使用正则表达式和模糊逻辑来实现某种启发式方法，以使RB系统更加准确。
- en: 'Refer to the following figure, which will give you an idea about the workflow
    of the chatbot using the retrieval-based approach:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图示，它将给你一个关于使用基于检索方法的聊天机器人工作流程的思路：
- en: '![Understanding the rule-based system](img/B08394_08_06.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![理解基于规则的系统](img/B08394_08_06.jpg)'
- en: 'Figure 8.6: Workflow of rule-based chatbot'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：基于规则的聊天机器人工作流程
- en: Based on the preceding figure, you know that in an RB system we will manually
    hand-code all possible questions and answers as well as implement regular expressions
    and fuzzy logic, which will give the chatbot the ability to generate the appropriate
    answers. Based on business requirements, questions can be added and deleted from
    this system. Now let's discuss our approach, and based on this approach we will
    build a basic version of the chatbot.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的图示，你知道在一个基于规则的（RB）系统中，我们将手动编写所有可能的问题和答案，以及实现正则表达式和模糊逻辑，这将使聊天机器人具备生成适当答案的能力。根据业务需求，可以从这个系统中添加和删除问题。现在让我们讨论我们的方法，并基于这个方法，我们将构建聊天机器人的基本版本。
- en: Understanding the approach
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解方法
- en: 'In this section, we will look at the steps that will help us implement the
    basic version of the chatbot. Here, I''m building a chatbot for the finance domain,
    which will help users apply for home loans. We will code some questions so that
    you know how a rule-based chatbot can be developed. We need to perform the following
    steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看有助于我们实现聊天机器人基本版本的步骤。在这里，我正在为金融领域构建一个聊天机器人，它将帮助用户申请住房贷款。我们将编写一些问题，以便你知道如何开发基于规则的聊天机器人。我们需要执行以下步骤：
- en: Listing down possible questions and answers.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出可能的问题和答案。
- en: Deciding standard messages.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定标准消息。
- en: Understanding the architecture.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解架构。
- en: Listing down possible questions and answers
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列出可能的问题和答案
- en: First of all, we need to list all the questions that we can think of on behalf
    of the users. Once we decide on the questions, then one by one we need to decide
    the answers to these questions. Suppose we ask the user to provide their full
    name, email ID, phone number, and loan amount so in case the user drops in between,
    the customer executive can call them back. After this, we ask the user what kind
    of assistance they require and then they can ask their questions. They may ask
    for the eligibility criteria, application status, document requirements, and so
    on. During the first iteration, you need to add the bare minimum questions that
    are frequently asked by users. Once we decide the questions and answers, it will
    be easy for us to code them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要列出所有我们可以代表用户想到的问题。一旦我们决定了这些问题，然后我们需要逐一决定这些问题的答案。假设我们要求用户提供全名、电子邮件ID、电话号码和贷款金额，以便在用户中途退出时，客户代表可以回电。之后，我们询问用户他们需要什么样的帮助，然后他们可以提出问题。他们可能会询问资格标准、申请状态、文件要求等等。在第一次迭代中，您需要添加用户经常提出的最基本问题。一旦我们决定了问题和答案，编码它们就会变得容易。
- en: 'Say, I include the following questions in this basic version of the financial
    domain chatbot:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我在这个金融领域聊天机器人的基本版本中包含以下问题：
- en: Please let me know the eligibility criteria for getting a home loan
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请告知我获得住房贷款的资格标准
- en: Please let me know what the status of my loan application is
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请告知我的贷款申请状态
- en: Let me know the list of documents I need to submit
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 告知我需要提交的文件清单
- en: 'The answers to each of these questions will be as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 每个问题的答案如下：
- en: We need a minimum of 3 years of job experience, 3 years of IT returns, and a
    minimum income of more than 3.5 lakh INR
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要至少3年的工作经验、3年的IT税单以及至少35万卢比的收入
- en: Your application is with our credit risk management team
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的申请已提交至我们的信用风险管理团队
- en: You need to submit salary slips for the last 6 months, a proof of identity,
    3 years of IT returns, and the lease documents for your house.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要提交过去6个月的工资条、身份证明、3年的IT税单以及您房屋的租赁文件。
- en: We also need to decide some standard messages, which we will cover in the next
    section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要决定一些标准消息，这些内容将在下一节中介绍。
- en: Deciding standard messages
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定标准消息
- en: We need to decide the standard message, for example, a welcome message from
    the chatbot. If the user asks a question that the chatbot cannot answer, then
    what message should pop up? We also need to decide the message when the user ends
    the chat.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要决定标准消息，例如聊天机器人发出的欢迎信息。如果用户提出聊天机器人无法回答的问题，那么应该弹出什么信息？我们还需要决定用户结束聊天时的消息。
- en: These standard messages help users understand what they can and cannot ask the
    chatbot. Now let's look at the architectural part of the basic version of the
    chatbot.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标准消息帮助用户了解他们可以向聊天机器人提出什么问题以及不能提出什么问题。现在让我们看看聊天机器人基本版本中的架构部分。
- en: Understanding the architecture
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解架构
- en: 'In this section, let''s talk about architecture. When we are building a domain-specific
    rule-based chatbot, then we need to store whatever questions the users ask. We
    also need to build a chatbot that is fast and scalable. In this approach, we build
    the web services. The web service REST APIs will be easily integrated with the
    website and the frontend. We need a database that can store the conversations
    of the users. This conversation data will be helpful when we try to improvise
    the chatbot or use it for ML training. The libraries that I''m using are given
    as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们谈谈架构。当我们构建一个特定领域的基于规则的聊天机器人时，我们需要存储用户提出的所有问题。我们还需要构建一个快速且可扩展的聊天机器人。在这种方法中，我们构建了Web服务。Web服务的REST
    API将很容易与网站和前端集成。我们需要一个可以存储用户对话的数据库。这些对话数据将有助于我们改进聊天机器人或用于机器学习训练。我使用的库如下所示：
- en: Flask for implementing web services and REST APIs
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Flask实现Web服务和REST API
- en: MongoDB for storing the conversations. The reason behind choosing the NoSQL
    database is that conversations don't have a specific format. NoSQL is a good option
    to store schema-less data. We need to store the raw conversations, so NoSQL is
    a good option.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MongoDB来存储对话。选择NoSQL数据库的原因是对话没有特定的格式。NoSQL是存储无模式数据的良好选择。我们需要存储原始对话，因此NoSQL是一个很好的选择。
- en: 'You can refer to the following figure, which will help you understand the entire
    architecture and process:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图表，它将帮助您理解整个架构和流程：
- en: '![Understanding the architecture](img/B08394_08_07.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![理解架构](img/B08394_08_07.jpg)'
- en: 'Figure 8.7: Architectural design for the basic version of a chatbot'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：聊天机器人基本版本的架构设计
- en: 'Based on this architecture, you will find that the process flow of the basic
    version of a chatbot is quite simple. This flow involves seven simple steps:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个架构，您会发现聊天机器人基本版本的处理流程相当简单。这个流程涉及七个简单的步骤：
- en: The user will ask their questions to the chatbot.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户将向聊天机器人提问。
- en: The rule-based engine of the chatbot will process the question. Here, the REST
    API has been called to generate the response.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聊天机器人的基于规则的引擎将处理这个问题。在这里，已经调用了REST API来生成响应。
- en: If the question that's asked is available to the RB system, then the user will
    get an appropriate answer.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提出的问题对RB系统可用，那么用户将得到一个适当的答案。
- en: If the question that's asked is not available to the RB system, then the user
    will not get the answer but a standard error message.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提出的问题对RB系统不可用，那么用户将不会得到答案，而是收到一个标准错误信息。
- en: The conversation of the user will be stored in the MongoDB database. This response
    is in the JSON format.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户的对话将被存储在MongoDB数据库中。这个响应是JSON格式的。
- en: The same JSON response is sent by the REST API to the frontend. At the frontend,
    a JavaScript parses this response and pops up the appropriate answer.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: REST API将发送相同的JSON响应到前端。在前端，JavaScript解析这个响应并弹出适当的答案。
- en: When the user gets their answer, they may end the chat or ask another question.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当用户得到他们的答案后，他们可能会结束聊天或提出另一个问题。
- en: 'Another major point that I want to highlight is that before storing the data
    to MongoDB, we need to finalize the attributes of the JSON response that will
    actually help us when we parse the JSON response using JavaScript. You can refer
    to the following screenshot, which will help you learn which kind of JSON schema
    I have decided on:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想强调的一个主要观点是，在将数据存储到MongoDB之前，我们需要最终确定JSON响应的属性，这些属性实际上在用JavaScript解析JSON响应时对我们有帮助。您可以参考以下截图，这将帮助您了解我决定采用哪种类型的JSON模式：
- en: '![Understanding the architecture](img/B08394_08_08.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![理解架构](img/B08394_08_08.jpg)'
- en: 'Figure 8.8: Understanding the JSON response attribute'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：理解JSON响应属性
- en: 'The usage of each of the JSON attributes is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 每个JSON属性的使用方法如下：
- en: '`current_form_action:` This attribute indicates which REST API is currently
    being invoked.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`current_form_action:` 这个属性指示当前正在调用的REST API。'
- en: '`message_bot:` This field carries the answer from the bot.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message_bot:` 这个字段携带来自机器人的答案。'
- en: '`message_human:` This field carries the query of the user.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`message_human:` 这个字段携带用户的查询。'
- en: '`next_field_type:` If we need to populate the textbox or button in the next
    question, this is useful for generating dynamic HTML components.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_field_type:` 如果我们需要在下一个问题中填充文本框或按钮，这个属性对于生成动态HTML组件很有用。'
- en: '`next_form_action:` This attribute indicates which REST API we should invoke
    in the upcoming request.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`next_form_action:` 这个属性指示在即将到来的请求中我们应该调用哪个REST API。'
- en: '`placeholder_text:` If you want to put watermark text in the textbox, then
    this attribute helps you with the HTML functionality.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`placeholder_text:` 如果您想在文本框中放置水印文本，那么这个属性可以帮助您实现HTML功能。'
- en: '`previous_field_type:` This attribute keeps track of what the last field type
    was.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`previous_field_type:` 这个属性跟踪最后一个字段类型。'
- en: '`previous_form_action:` This attribute keeps track of what the last REST API
    we invoked was.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`previous_form_action:` 这个属性跟踪我们最后调用的REST API。'
- en: '`suggestion_message:` Sometimes, we need a message to invoke a specific rule.
    This is the same as when you say, *OK Google* and the Google home assistance is
    invoked. This attribute basically guides the user as to what they need to expect
    when asking their queries.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`suggestion_message:` 有时，我们需要一条消息来调用特定的规则。这就像您说“OK Google”并调用Google Home助手一样。这个属性基本上指导用户在提问时可以期待什么。'
- en: Now let's start the implementation of the rule-based chatbot.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始实现基于规则的聊天机器人。
- en: Implementing the rule-based chatbot
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基于规则的聊天机器人
- en: 'In this section, we will understand the implementation of the chatbot. This
    implementation is divided into two parts. You can find this code by visiting:
    [https://github.com/jalajthanaki/Chatbot_Rule_Based](https://github.com/jalajthanaki/Chatbot_Rule_Based):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解聊天机器人的实现。这个实现分为两部分。您可以通过访问以下链接找到此代码：[https://github.com/jalajthanaki/Chatbot_Rule_Based](https://github.com/jalajthanaki/Chatbot_Rule_Based)：
- en: Implementing the conversation flow
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现对话流程
- en: Implementing RESTful APIs using flask
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用flask实现RESTful API
- en: Implementing the conversation flow
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现对话流程
- en: In order to implement the conversation logic, we are writing a separate Python
    script, so that whenever we need to add or delete some logic it will be easy for
    us. Here, we create one Python package in which we put this conversation logic.
    The name of the file is *conversationengine.py* and it uses JSON, BSON, and re
    as Python dependencies.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现对话逻辑，我们编写了一个单独的Python脚本，这样我们每次需要添加或删除一些逻辑时都会很容易。在这里，我们创建了一个Python包，我们将这个对话逻辑放入其中。文件名为
    *conversationengine.py*，它使用 JSON、BSON 和 re 作为Python依赖项。
- en: 'In this file, we have implemented each conversation in the form of a function.
    When the user opens the chatbot for the first time, a welcome message should pop
    up. You can refer to the code given in the following screenshot:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，我们将每个对话以函数的形式实现。当用户第一次打开聊天机器人时，应该会弹出一条欢迎信息。你可以在下面的屏幕截图中查看给出的代码：
- en: '![Implementing the conversation flow](img/B08394_08_09.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![实现对话流程](img/B08394_08_09.jpg)'
- en: 'Figure 8.9: Code snippet for the welcome message'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：欢迎信息的代码片段
- en: 'Now the users need to type in **Hi** in order to start a conversation. When
    the user types **hi**, the `start_coversation_action` function will be invoked
    and the chatbot will ask for some information so that it can give the user a more
    accurate, personalized answer. First, it asks the user their name, and then it
    asks for their email ID and phone number. You can refer to the following screenshot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用户需要输入 **Hi** 来开始对话。当用户输入 **hi** 时，`start_coversation_action` 函数将被调用，聊天机器人将询问一些信息，以便它能给出更准确、个性化的答案。首先，它会询问用户的姓名，然后询问他们的电子邮件ID和电话号码。你可以在下面的屏幕截图中查看：
- en: '![Implementing the conversation flow](img/B08394_08_10.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![实现对话流程](img/B08394_08_10.jpg)'
- en: 'Figure 8.10: Code snippet for asking basic user information'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：询问基本用户信息的代码片段
- en: 'In the same way, there are the `borrowers_name_asking`, `borrowers_email_id_asking`,
    and `mobilenumber_asking` functions, which ask the user to provide their name,
    email ID, and phone number. Apart from this, there are questions that can help
    users learn what the status of their loan application is. If the customer is new,
    then they can ask questions such as *what kind of documents are needed in order
    to apply for a home loan?* You can find these status- and document-related questions
    inside the `other_cases` function. You can refer to the code for this function
    in the following screenshot:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，还有 `borrowers_name_asking`、`borrowers_email_id_asking` 和 `mobilenumber_asking`
    函数，它们要求用户提供他们的姓名、电子邮件ID和电话号码。除此之外，还有一些问题可以帮助用户了解他们的贷款申请状态。如果客户是新的，他们可以提出诸如 *为了申请住房贷款需要哪些文件？*
    之类的问题。你可以在 `other_cases` 函数中找到这些状态和文件相关的问题。你可以在下面的屏幕截图中查看该函数的代码：
- en: '![Implementing the conversation flow](img/B08394_08_11.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![实现对话流程](img/B08394_08_11.jpg)'
- en: 'Figure 8.11: Code snippet for question related to loan application status and
    documents needed for applying for a home loan'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：与贷款申请状态和申请住房贷款所需文件相关的代码片段
- en: As you can see in the preceding figure*,* we have used a regular expression
    here so that the chatbot can answer status- and document-related questions. This
    is coded purely using keyword-based logic.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们在这里使用了一个正则表达式，以便聊天机器人能够回答状态和文件相关的问题。这是纯粹使用基于关键字的逻辑编写的。
- en: Now let's look at how to build the web service with this function using flask.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 Flask 构建这个函数的Web服务。
- en: Implementing RESTful APIs using flask
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Flask 实现RESTful API
- en: 'So far, we have only coded the function that takes the user input query and
    invokes the appropriate function based on that query. For better maintenance and
    easy integration, we need to implement RESTful APIs using flask. In order to implement
    this, we use the `flask`, `json`, `os`, `uuid`, `datetime`, `pytz`, and `flsk_pymongo`
    libraries. Flask is an easy-to-use web framework. You can find the code snippet
    in the following figure:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只编写了接收用户输入查询并基于该查询调用相应函数的功能。为了更好的维护和易于集成，我们需要使用 Flask 实现RESTful API。为了实现这一点，我们使用了
    `flask`、`json`、`os`、`uuid`、`datetime`、`pytz` 和 `flsk_pymongo` 库。Flask 是一个易于使用的Web框架。你可以在下面的图中找到代码片段：
- en: '![Implementing RESTful APIs using flask](img/B08394_08_12.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Flask 实现RESTful API](img/B08394_08_12.jpg)'
- en: 'Figure 8.12: Code snippet for making a RESTful API for the chatbot'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：为聊天机器人创建RESTful API的代码片段
- en: 'As you can see in the preceding figure, each route calls a different method
    that is part of the `conversationengine.py` file we covered earlier. In order
    to run this flask engine, we need to use the flask `app.run ()` command. You can
    find all APIs and their functions by visiting: [https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py](https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，每个路由调用一个不同的方法，这些方法都是我们之前提到的`conversationengine.py`文件的一部分。为了运行这个Flask引擎，我们需要使用Flask的`app.run()`命令。你可以通过访问以下链接找到所有API及其功能：[https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py](https://github.com/jalajthanaki/Chatbot_Rule_Based/blob/master/flaskengin.py)。
- en: Now let's test this rule-based chatbot.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来测试这个基于规则的聊天机器人。
- en: Testing the rule-based chatbot
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试基于规则的聊天机器人
- en: In this section, we will test the basic version of the chatbot. Let's begin
    with basic personal information that the chatbot asks for from the user. Here,
    I will generate the JSON response generated by the flask RESTful API. We need
    a JavaScript to parse this JSON response if we are integrating these APIs with
    the frontend. I won't explain the frontend integration part here, so let's analyze
    the JSON responses.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将测试聊天机器人的基本版本。让我们从聊天机器人从用户那里询问的基本个人信息开始。在这里，我将生成由Flask RESTful API生成的JSON响应。如果我们将这些API与前端集成，我们需要JavaScript来解析这个JSON响应。我不会在这里解释前端集成部分，所以让我们分析JSON响应。
- en: 'For the welcome message, refer to the following screenshot:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于欢迎信息，请参考以下截图：
- en: '![Testing the rule-based chatbot](img/B08394_08_13.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![测试基于规则的聊天机器人](img/B08394_08_13.jpg)'
- en: 'Figure 8.13: JSON response for the welcome message'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13：欢迎信息的JSON响应
- en: 'The JSON response when the chatbot is asking for the name of a user is given
    in the following figure:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当聊天机器人询问用户姓名时的JSON响应如下所示：
- en: '![Testing the rule-based chatbot](img/B08394_08_14.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![测试基于规则的聊天机器人](img/B08394_08_14.jpg)'
- en: 'Figure 8.14: JSON response for asking the name of the user'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14：询问用户姓名的JSON响应
- en: 'If the user asks for the status of his application, then they will get the
    JSON response given in the following figure:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户询问其应用程序的状态，那么他们将得到以下图中给出的JSON响应：
- en: '![Testing the rule-based chatbot](img/B08394_08_15.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![测试基于规则的聊天机器人](img/B08394_08_15.jpg)'
- en: 'Figure 8.15: JSON response to get status-related information'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15：获取状态相关信息的JSON响应
- en: 'If the user asks status-related questions with a blend of Hindi-English (Hinglish)
    and if they use the word *status* in their query, then the chatbot will generate
    the response. You can see the response in the following figure:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户用印地语-英语（Hinglish）混合提问并使用查询中的单词*状态*，那么聊天机器人将生成响应。你可以在以下图中看到响应：
- en: '![Testing the rule-based chatbot](img/B08394_08_16.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![测试基于规则的聊天机器人](img/B08394_08_16.jpg)'
- en: 'Figure 8.16: json response to get status-related information for the Hindi-English
    (Hinglish) language'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16：获取印地语-英语（Hinglish）语言相关信息的JSON响应
- en: 'If the user asks queries that are not coded, then it will generate the following
    json response:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户提出未编码的查询，它将生成以下JSON响应：
- en: '![Testing the rule-based chatbot](img/B08394_08_17.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![测试基于规则的聊天机器人](img/B08394_08_17.jpg)'
- en: 'Figure 8.17: JSON response for an unknown question'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17：未知问题的JSON响应
- en: After testing, we come to learn that queries that have been coded are working
    fine, but this basic version of a chatbot is not working properly for questions
    that we haven't coded. I want to point out some advantages after testing the rule-based
    chatbot. However, there are various disadvantages of this approach too, which
    we will discuss in an upcoming section.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 经过测试，我们了解到已经编码的查询运行良好，但这个聊天机器人的基本版本对于未编码的问题处理不正确。我想在测试基于规则的聊天机器人后指出一些优点。然而，这种方法也有各种缺点，我们将在下一节中讨论。
- en: Advantages of the rule-based chatbot
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的聊天机器人的优点
- en: 'You can refer to the following advantages of the rule-based chatbot:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下基于规则的聊天机器人的优点：
- en: Easy to code.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码简单。
- en: Needs less computation power.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算能力需求较低。
- en: 'Uses the pattern matching approach so if users use English and other languages
    in their conversation, they will still get an answer. This is because the chatbot
    identifies keywords that the user provides in their question. Suppose the user
    asks in English, *can you provide me a list of documents that I need to submit?*
    And another user may ask a question in the Hindi language: *Kya aap mujhe bata
    sakte hain mujhe kaun se documents submit karne hain?* For this question, the
    chatbot will generate the answer because it finds specific keywords from user
    queries, and if those keywords are present, then the chatbot generates an answer
    irrespective of the language.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模式匹配方法，因此如果用户在对话中使用英语和其他语言，他们仍然会得到答案。这是因为聊天机器人识别出用户在问题中提供的特定关键词。假设用户用英语问：“你能提供我需要提交的文档列表吗？”另一个用户可能用印地语提问：“Kya
    aap mujhe bata sakte hain mujhe kaun se documents submit karne hain?”对于这个问题，聊天机器人将生成答案，因为它从用户查询中找到特定的关键词，如果这些关键词存在，那么聊天机器人将生成答案，而不考虑语言。
- en: Now let's look at the problems related to this approach that we need to solve
    in order to improve the chatbot.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看与这种方法相关的问题，我们需要解决这些问题来改进聊天机器人。
- en: Problems with the existing approach
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有方法的问题
- en: In this section, we will discuss the problems with the basic version of our
    chatbot. As we already know, for unseen queries this approach doesn't work, which
    means that the basic approach is not able to generalize the user's questions properly.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论我们聊天机器人的基本版本存在的问题。正如我们所知，对于未见过的查询，这种方法不起作用，这意味着基本方法无法正确地泛化用户的问题。
- en: 'I have listed down some of the problems here:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里列出了一些问题：
- en: Time consuming because we need to hardcode each and every scenario, which is
    not feasible at all
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 耗时，因为我们需要为每个场景硬编码，这根本不可行
- en: It cannot work for unseen use cases
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法处理未见过的用例
- en: The user should process the rigid flow of conversation
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户应该处理对话的严格流程
- en: It cannot understand the long context
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法理解长上下文
- en: Most of these problems can be solved using the generative-based approach. Let's
    look at the key concepts that will help us improvise this approach.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些问题可以通过基于生成的方法来解决。让我们看看将帮助我们改进这种方法的关键概念。
- en: Understanding key concepts for optimizing the approach
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解优化方法的关键概念
- en: In this section, we will be discussing the key concepts that can help us improvise
    the chatbot basic version. The problems that we have listed down previously can
    be solved by using **Deep Learning** (**DL**) techniques, which can help us build
    a more generalized chatbot in less time.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些关键概念，这些概念可以帮助我们改进聊天机器人的基本版本。我们之前列出的问题可以通过使用**深度学习**（**DL**）技术来解决，这些技术可以帮助我们在更短的时间内构建一个更通用的聊天机器人。
- en: Before proceeding ahead, we need to decide which DL technique we will use for
    our revised approach. DL helps us achieve great results. Here, we need to use
    the End-to-End DL approach that makes no assumptions about data, the structure
    of the dialog, and use cases. This is what we want. In order to achieve this,
    we will be using **Recurrent Neural Nets** (**RNN**). Now you may ask why RNN
    is useful. Let me explain this by way of an example. Suppose we want to classify
    the temperature in the hot or cold category; to do that, we will be using a feed
    forward neural net to classify the temperature into hot or cold, but the conversation
    isn't a fixed size. A conversation is a sequence of words. We need to use a neural
    net that can help us process the sequences of words. RNN is best for processing
    these kinds of sequences. In RNN, we feed data back into the input while training
    it in a recurring loop.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续前进之前，我们需要决定我们将使用哪种深度学习技术来改进我们的方法。深度学习帮助我们取得很好的成果。在这里，我们需要使用端到端深度学习方法，它不对数据、对话结构和用例结构做出任何假设。这正是我们想要的。为了实现这一点，我们将使用**循环神经网络**（**RNN**）。现在你可能想知道为什么RNN是有用的。让我通过一个例子来解释这一点。假设我们想要将温度分类为热或冷；为了做到这一点，我们将使用前馈神经网络将温度分类为热或冷，但对话并不是固定大小的。对话是一系列单词。我们需要使用一个能够帮助我们处理单词序列的神经网络。RNN最适合处理这类序列。在RNN中，我们在训练过程中将数据反馈到输入中，形成一个循环。
- en: In the revised approach, we are going to use the sequence-to-sequence (seq2seq)
    model from TensorFlow. So, let's discuss the sequence model for a bit.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在改进的方法中，我们将使用TensorFlow中的序列到序列（seq2seq）模型。因此，让我们先讨论一下序列模型。
- en: Understanding the seq2seq model
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解seq2seq模型
- en: The good part of using the seq2seq model is that we don't need to perform feature
    engineering. Like most of the DL techniques, it generates features by its own.
    We will discuss the seq2seq model briefly. The seq2seq model consists of two **Long
    Short Term Memory** (**LSTM**) recurrent neural networks. The first neural net
    is an *encoder*. It processes the input. The second neural net is a *decoder*.
    It generates the output. Usually, the DL algorithm needs a dimensionality of the
    inputs and outputs to be a fixed size, but here, we are accepting a sequence of
    words in a sentence and outputting a new sequence of words. So, we need a sequence
    model that can learn data with long range memory dependencies. The LSTM architecture
    is best suited for this. The encoder LSTM turns the input sentence of variable
    length into a fixed dimensional vector representation. We can think of this as
    a *thought vector* or a *context vector*. The reason we are using LSTM is that
    it can remember words from far back in the sequence; here, we are dealing with
    large sequence attention mechanisms of the seq2seq model, which helps the decoder
    selectively look at the parts of the sequence that are most relevant for more
    accuracy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用seq2seq模型的好处是我们不需要执行特征工程。像大多数深度学习技术一样，它通过自身生成特征。我们将简要讨论seq2seq模型。seq2seq模型由两个**长短期记忆**（**LSTM**）循环神经网络组成。第一个神经网络是一个*编码器*。它处理输入。第二个神经网络是一个*解码器*。它生成输出。通常，深度学习算法需要输入和输出的维度是固定大小的，但在这里，我们接受一个句子中的单词序列作为输入，并输出一个新的单词序列。因此，我们需要一个能够学习具有长距离记忆依赖性的数据序列模型。LSTM架构最适合这种情况。编码器LSTM将可变长度的输入句子转换为一个固定维度的向量表示。我们可以将其视为一个*思维向量*或*上下文向量*。我们使用LSTM的原因是它可以记住序列中远处的单词；在这里，我们处理seq2seq模型的大序列注意力机制，这有助于解码器选择性地查看与提高准确性最相关的序列部分。
- en: 'You can refer to the architecture of the seq2seq model in the following figure:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中seq2seq模型的架构：
- en: '![Understanding the seq2seq model](img/B08394_08_18.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![理解seq2seq模型](img/B08394_08_18.jpg)'
- en: 'Figure 8.18: Architecture of the seq2seq model'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18：seq2seq模型的架构
- en: 'Image source: [http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png](http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png](http://suriyadeepan.github.io/img/seq2seq/seq2seq2.png)
- en: When we provide a large enough dataset of questions and responses, it will recognize
    the closeness of the set of questions and represent them as a single thought vector.
    This representation helps the machine to understand the intent of the questions
    irrespective of the structure of the sentence so the machine can recognize the
    questions such as "what time it is?" and "what's the time?" have the same intent,
    so they will fall into a single thought vector. After training, we will have a
    huge set of not just synapse weights but thought vectors as well. After that,
    we need to use additional hyper parameters along with appropriate loss functions
    when we train the model. Once we train the model, we can chat with it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提供一个足够大的问题和回答的数据集时，它将识别问题集的相似性，并将它们表示为一个单独的思维向量。这种表示有助于机器理解问题的意图，而不管句子的结构如何，因此机器可以识别“现在几点了？”和“现在几点？”这样的问题具有相同的意图，因此它们将落入一个单独的思维向量中。训练后，我们将拥有一个巨大的集合，不仅包括突触权重，还有思维向量。之后，在训练模型时，我们需要使用额外的超参数以及适当的损失函数。一旦训练了模型，我们就可以与它聊天。
- en: 'If you want to know more details about the seq2seq model, then you should refer
    to a research paper published by Google researchers titled *A Neural Conversational
    Model*. You can also refer to this paper at: [https://arxiv.org/pdf/1506.05869v3.pdf](https://arxiv.org/pdf/1506.05869v3.pdf)
    and this amazing article if you want to learn more about LSTM at: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于seq2seq模型的信息，那么您应该参考由谷歌研究人员发表的标题为*一个神经对话模型*的研究论文。您也可以参考这篇论文：[https://arxiv.org/pdf/1506.05869v3.pdf](https://arxiv.org/pdf/1506.05869v3.pdf)以及这篇关于LSTM的精彩文章：[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: Now let's implement the chatbot using the seq2seq model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用seq2seq模型来实现聊天机器人。
- en: Implementing the revised approach
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现改进的方法
- en: 'In this section, we will cover each part of the implementation. You can find
    the code by using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow](https://github.com/jalajthanaki/Chatbot_tensorflow).
    Note that here, I''m using TensorFlow version 0.12.1\. I perform training on a
    GeForce GTX 1060 6GB GPU for a few hours. In this implementation, we don''t need
    to generate features because the seq2seq model generates its internal representation
    for sequences of words given in a sentence. Our implementation part has the following
    steps:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍实现的每个部分。您可以通过使用此 GitHub 链接找到代码：[https://github.com/jalajthanaki/Chatbot_tensorflow](https://github.com/jalajthanaki/Chatbot_tensorflow)。请注意，在这里，我使用
    TensorFlow 版本 0.12.1。我在 GeForce GTX 1060 6GB GPU 上训练了几小时。在这个实现中，我们不需要生成特征，因为 seq2seq
    模型会为句子中给出的单词序列生成其内部表示。我们的实现部分有以下步骤：
- en: Data preparation
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Implementing the seq2seq model
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 seq2seq 模型
- en: Let's begin our coding.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始编码。
- en: Data preparation
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'During this implementation, we will be using the Cornell movie-dialogs dataset.
    First of all, we need to prepare data in a format that we can use for training.
    There is a Python script that is used to perform data preparation. You can find
    the script at: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现过程中，我们将使用康奈尔电影对白数据集。首先，我们需要以我们可以用于训练的格式准备数据。有一个 Python 脚本用于执行数据准备。您可以在以下位置找到脚本：[https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data/prepare_data_script/data.py)。
- en: 'Data preparation can be subdivided into the following steps:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备可以细分为以下步骤：
- en: Generating question-answer pairs
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成问答对
- en: Preprocessing the dataset
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: Splitting the dataset into the training dataset and the testing dataset
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分割为训练数据集和测试数据集
- en: Building a vocabulary for the training and testing datasets
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练和测试数据集构建词汇表
- en: Generating question-answer pairs
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成问答对
- en: 'In order to generate question-answer pairs from the Cornell movie-dialogs dataset,
    we are using the `movie_lines.txt` and `movie_conversations.txt` files. The `movie_lines.txt`
    files give us information about *line_id* of each conversation along with the
    real conversation, whereas `movie_conversations.txt` has *line_ids* only. In this
    situation, we need to generate the appropriate pair of conversations of question
    and answer from the dataset. For that, we will combine these two files. In Python
    script, there are some functions that help us combine these files. The details
    related to functions are as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从康奈尔电影对白数据集中生成问答对，我们使用 `movie_lines.txt` 和 `movie_conversations.txt` 文件。`movie_lines.txt`
    文件提供了关于每个对话的 *line_id* 以及实际对话的信息，而 `movie_conversations.txt` 只包含 *line_ids*。在这种情况下，我们需要从数据集中生成适当的问答对。为此，我们将这两个文件结合起来。在
    Python 脚本中，有一些函数帮助我们合并这些文件。有关函数的详细信息如下：
- en: '`get_id2line():` This function helps us spilt the data using the +++$+++ pattern.
    We perform splitting on the `movie_lines.txt` file. After splitting, with the
    help of this function, we create a dictionary in which we put *line_id* as the
    key and the movie dialog as the value. So, *key = line_id* and *value = text*'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_id2line():` 这个函数帮助我们使用 +++$+++ 模式来分割数据。我们在 `movie_lines.txt` 文件上执行分割。分割后，借助这个函数，我们创建了一个字典，我们将
    *line_id* 作为键，将电影对白作为值。所以，*键 = line_id* 和 *值 = 文本*'
- en: '`get_conversations():` This function splits the data given in the `movie_conversations.txt`
    file. This will help us create a list. This list contains list of *line_ids*.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_conversations():` 这个函数分割 `movie_conversations.txt` 文件中给出的数据。这将帮助我们创建一个列表。这个列表包含
    *line_ids* 的列表。'
- en: '`gather_dataset():` This function actually generates question-answer pairs.
    In this function, a simple logic is applied. We take the list of *line_ids* and
    we know that the last element indicates the answer. So, we separate the questions
    and answers. With the help of the `get_id2line()` function, we search the questions
    and their corresponding answers. Here, we are using the value of the key to search
    questions and answers.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gather_dataset():` 这个函数实际上生成问答对。在这个函数中，应用了一个简单的逻辑。我们取 *line_ids* 的列表，我们知道最后一个元素表示答案。因此，我们分离问题和答案。借助
    `get_id2line()` 函数，我们搜索问题和相应的答案。在这里，我们使用键的值来搜索问题和答案。'
- en: 'You can refer to the following screenshot to see the actual coding:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下截图查看实际的编码：
- en: '![Generating question-answer pairs](img/B08394_08_19.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![生成问答对](img/B08394_08_19.jpg)'
- en: 'Figure 8.19: Functions used for generating question-answer pairs'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19：用于生成问答对的函数
- en: Now let's explore the data preprocessing section.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索数据预处理部分。
- en: Preprocessing the dataset
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: 'Some preprocessing and filtering steps are involved here. As a part of preprocessing,
    we perform the following steps:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这里涉及一些预处理和过滤步骤。作为预处理的一部分，我们执行以下步骤：
- en: We convert the conversation into lowercase using the inbuilt string function,
    `lower()`.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用内置的字符串函数`lower()`将对话转换为小写。
- en: We also remove the junk characters and too short or too long conversations.
    For this, we use a list-based approach to remove junk characters and the `filter_data()`
    function to remove too short or too long conversations. When we apply the `filter_data()`
    function on our dataset, *28%* of dataset is filtered.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还移除了垃圾字符以及过短或过长的对话。为此，我们使用基于列表的方法移除垃圾字符，并使用`filter_data()`函数移除过短或过长的对话。当我们对数据集应用`filter_data()`函数时，*28%*的数据集被过滤。
- en: We also filter out conversations with so many unknowns. Here, *2%* of the dataset
    has been affected. For this, we have used the `filter_unk()` method.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还过滤掉了包含大量未知词汇的对话。在这里，*2%*的数据集受到了影响。为此，我们使用了`filter_unk()`方法。
- en: We also tokenize the sentences. In this process, we convert *list of [line of
    text]* into *list of [line of words]*. This tokenization is helpful because during
    training, the machine can process individual words of the sentence, and with the
    help of the word ID, data retrieval becomes much faster.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将对句子进行分词。在这个过程中，我们将*文本行列表*转换为*单词行列表*。这种分词对于训练过程很有帮助，因为机器可以处理句子中的单个单词，借助单词ID，数据检索变得更快。
- en: 'You can refer to the code given in the following screenshot:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下屏幕截图中给出的代码：
- en: '![Preprocessing the dataset](img/B08394_08_20.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![预处理数据集](img/B08394_08_20.jpg)'
- en: 'Figure 8.20: Code snippet for preprocessing'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20：预处理代码片段
- en: Splitting the dataset into the training dataset and the testing dataset
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据集分为训练数据集和测试数据集
- en: 'After preprocessing, we will split the data into the training dataset and the
    testing dataset and for that, we will use the following functions:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理完成后，我们将数据集分为训练数据集和测试数据集，为此，我们将使用以下函数：
- en: We can save the training and testing datasets using the `prepare_seq2seq_files()`
    function
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`prepare_seq2seq_files()`函数保存训练和测试数据集。
- en: 'You can access the `train.enc`, `train.dec`, `test.enc`, and `test.dec` data
    files directly from this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以直接从以下GitHub链接访问`train.enc`、`train.dec`、`test.enc`和`test.dec`数据文件：[https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)
- en: Building a vocabulary for the training and testing datasets
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为训练和测试数据集构建词汇表
- en: 'Now it''s time to generate the vocabulary from the dataset. For vocabulary
    generation, we will perform the following steps:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候从数据集中生成词汇表了。对于词汇表的生成，我们将执行以下步骤：
- en: Using the `prepare_custom_data()` function of the `data_utils.py` file, we can
    generate the vocabulary that we will feed into the seq2seq model while training.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`data_utils.py`文件中的`prepare_custom_data()`函数，我们可以在训练时生成要输入到seq2seq模型中的词汇表。
- en: 'You can access the `data_uti``ls.py` file using this link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用此链接访问`data_uti``ls.py`文件：[https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/data_utils.py)
- en: Note that vocabulary files are generated when we start training.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，当开始训练时，会生成词汇表文件。
- en: The filenames of vocabulary files are `train.enc.ids20000`, `train.dec.ids20000`,
    `test.enc.ids20000`, and `test.dec.id``s20000`. Here, 20000 indicates the size
    of the vocabulary we have provided.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇表文件的文件名是`train.enc.ids20000`、`train.dec.ids20000`、`test.enc.ids20000`和`test.dec.id``s20000`。这里的20000表示我们提供的词汇表大小。
- en: 'You can access this file at: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过此链接访问该文件：[https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/data)
- en: 'You can see the code for the `prepare_custom_data()` function in the following
    screenshot:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下屏幕截图中看到`prepare_custom_data()`函数的代码：
- en: '![Building a vocabulary for the training and testing datasets](img/B08394_08_21.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![为训练和测试数据集构建词汇表](img/B08394_08_21.jpg)'
- en: 'Figure 8.21: Code snippet for generating the vocabulary'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21：生成词汇的代码片段
- en: Now let's actually implement the seq2seq model using TensorFlow.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实际上将使用TensorFlow实现seq2seq模型。
- en: Implementing the seq2seq model
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现seq2seq模型
- en: 'In this section, we chatbot development:seq2seq model, building" will be performing
    the actual training using the seq2seq model. We will be using TensorFlow to implement
    the seq2seq model. Before getting into training, let''s look into the hyper parameters
    configuration file, which you can access using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用seq2seq模型进行实际训练。我们将使用TensorFlow来实现seq2seq模型。在开始训练之前，让我们看看超参数配置文件，您可以通过以下GitHub链接访问：[https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/seq2seq.ini)。
- en: 'Duringchatbot development:seq2seq model, building" training, our script uses
    these files and their parameters. The following parameters are in this configuration
    file:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天机器人开发过程中：seq2seq模型，构建"训练，我们的脚本使用这些文件及其参数。以下参数位于此配置文件中：
- en: '`Mode`: This can be either train or test'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Mode`: 这可以是train或test'
- en: '`train_enc:` This contains the path of the training dataset for the encoder.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_enc:` 这包含了解码器训练数据集的路径。'
- en: '`train_dec:` This contains the path of the training dataset for the decoder.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dec:` 这包含了解码器训练数据集的路径。'
- en: '`test_enc:` This contains the path of the testing dataset for the encoder.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_enc:` 这包含了解码器测试数据集的路径。'
- en: '`test_dec:` This contains the path of the testing dataset for the decoder.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_dec:` 这包含了解码器测试数据集的路径。'
- en: '`Working_directory:` This is the folder where we can store our checkpoints,
    vocabulary, and temporary data files'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Working_directory:` 这是我们可以存储我们的检查点、词汇和临时数据文件的文件夹'
- en: '`enc_vocab_size:` This number defines the vocabulary size for the encoder.
    We set 20,000 as the vocabulary size.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`enc_vocab_size:` 这个数字定义了解码器的词汇量大小。我们将其设置为20,000。'
- en: '`dec_vocab_size:` This number defines the vocabulary size for the decoder.
    We set 20,000 as the vocabulary size.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dec_vocab_size:` 这个数字定义了解码器的词汇量大小。我们将其设置为20,000。'
- en: '`num_layers:` This indicates the number of LSTM layers. Here, we set it as
    3.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers:` 这表示LSTM层的数量。在这里，我们将其设置为3。'
- en: '`layer_size:` This indicates the number of layers in the seq2seq model. We
    set it as 256.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`layer_size:` 这表示seq2seq模型中的层数。我们将其设置为256。'
- en: '`steps_per_checkpoint:` At a checkpoint, the model''s parameters are saved,
    the model is evaluated, and results are printed.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps_per_checkpoint:` 在检查点，模型的参数被保存，模型被评估，并打印结果。'
- en: '`learning_rate:` This indicates how fast or how slow we train our model. We
    set the value to 0.5 for now.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate:` 这表示我们训练模型的速度有多快或多慢。我们目前将其值设置为0.5。'
- en: 'Most of the preceding parameters can be changed in order to get the best possible
    results. During training, we need to set the Mode as train and run the following
    command:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数前面的参数都可以更改以获得最佳结果。在训练期间，我们需要将模式设置为train并运行以下命令：
- en: '[PRE0]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now it''s time to understand what is inside the execute.py file. You can access
    this file using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候了解execute.py文件内部的内容了。您可以通过以下GitHub链接访问此文件：[https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py](https://github.com/jalajthanaki/Chatbot_tensorflow/blob/master/execute.py)。
- en: 'In this script, we call the TensorFlow API. This script can be divided into
    the following parts:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们调用TensorFlow API。这个脚本可以分为以下几部分：
- en: Creating the model
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建模型
- en: Training the model
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Creating the model
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建模型
- en: 'We are using the `Seq2SeqModel()` function from TensorFlow here. This function
    reads the configuration file and uses the values defined in the configuration
    file. In order to store the train model, we use the `saver.restore()` function,
    and to get the status of the checkpoints we use the `get_checkpoint_state()` function.
    You can refer to the code snippet given in the following figure:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用TensorFlow的`Seq2SeqModel()`函数。这个函数读取配置文件并使用配置文件中定义的值。为了存储训练模型，我们使用`saver.restore()`函数，为了获取检查点的状态，我们使用`get_checkpoint_state()`函数。您可以参考以下图中的代码片段：
- en: '![Creating the model](img/B08394_08_22.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![创建模型](img/B08394_08_22.jpg)'
- en: 'Figure 8.22: Code snippet for creating seq2seq model'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22：创建seq2seq模型的代码片段
- en: Training the model
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We defined the `train()`method inside the `execute.py` file. This function
    initializes the TensorFlow session and begins training. You can refer to the code
    snippet given in the following screenshot:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`execute.py`文件中定义了`train()`方法。此函数初始化TensorFlow会话并开始训练。您可以参考以下截图中的代码片段：
- en: '![Training the model](img/B08394_08_23.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型](img/B08394_08_23.jpg)'
- en: 'Figure 8.23: Code snippet for training the model'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23：训练模型的代码片段
- en: 'Now it''s time to train the model. When we execute the `python execute.py`
    command, you will see the output given in the following screenshot:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是训练模型的时候了。当我们执行`python execute.py`命令时，您将看到以下截图中的输出：
- en: '![Training the model](img/B08394_08_24.jpg)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型](img/B08394_08_24.jpg)'
- en: 'Figure 8.24: Training of the seq2seq model using TensorFlow'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24：使用TensorFlow训练seq2seq模型
- en: 'Here, training has been performed on a GPU. I trained this model for 3 hours.
    I have trained this model for 15,000 checkpoints. You can refer to the following
    screesnhot:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，训练是在GPU上进行的。我训练了这个模型3个小时。我已经训练了这个模型15,000个检查点。您可以参考以下截图：
- en: '![Training the model](img/B08394_08_25.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![训练模型](img/B08394_08_25.jpg)'
- en: 'Figure 8.25: Output of the seq2seq training'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25：seq2seq训练的输出
- en: 'On a CPU, training will take a lot of time, so I have also uploaded pre-trained
    models for you to use. You can download them by using this GitHub link: [https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，训练将花费很多时间，因此我还上传了预训练模型供您使用。您可以通过以下GitHub链接下载它们：[https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir](https://github.com/jalajthanaki/Chatbot_tensorflow/tree/master/working_dir)。
- en: Now it's time to understand the testing metrics that help us evaluate the trained
    model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是了解帮助我们评估训练模型的测试指标的时候了。
- en: Testing the revised approach
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试改进的方法
- en: In this section, we will perform testing of the revised approach. Before performing
    actual testing and seeing how good or bad the chatbot conversation is, we need
    to understand the basic testing metrics that we will be using for this approach
    and for the best approach. These testing metrics help us evaluate the model accuracy.
    Let's understand the testing metrics first, and then we will move on to the testing
    of the revised approach.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对改进的方法进行测试。在执行实际测试并查看聊天机器人对话的好坏之前，我们需要了解我们将为此方法和最佳方法使用的测试指标。这些测试指标帮助我们评估模型精度。让我们首先了解测试指标，然后我们将继续对改进的方法进行测试。
- en: Understanding the testing metrics
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解测试指标
- en: 'In this section, we need to understand the following testing metrics:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们需要了解以下测试指标：
- en: Perplexity
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆度
- en: Loss
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失
- en: Perplexity
  id: totrans-354
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆度
- en: In the NLP domain, perplexity is also referred to as per-word perplexity. Perplexity
    is a measurement of how well a trained model predicts the output for unseen data.
    It is also used to compare probability models. A low perplexity indicates that
    the probability distribution is good at predicting the sample. Even during training,
    you can see that for each checkpoint, perplexity is decreasing. Ideally, when
    there is no change in perplexity, we need to stop the training. During the training
    of the seq2seq model, I stopped training after 3 hours, so when you train the
    model from your end you can wait till the perplexity stops decreasing further.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域，混淆度也被称为每词混淆度。混淆度是衡量训练模型预测未见数据输出好坏的度量。它也用于比较概率模型。低混淆度表明概率分布擅长预测样本。即使在训练过程中，您也可以看到对于每个检查点，混淆度都在下降。理想情况下，当混淆度没有变化时，我们需要停止训练。在seq2seq模型的训练过程中，我在3小时后停止了训练，因此当您从您的端点训练模型时，您可以等待混淆度不再进一步下降。
- en: 'Perplexity is using the concept of entropy. If you want to know about perplexity,
    then you can refer to [https://www.youtube.com/watch?v=BAN3NB_SNHY](https://www.youtube.com/watch?v=BAN3NB_SNHY).
    Per-word perplexity is based on entropy. So, in order to understand entropy, you
    can refer to the following links:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆度是使用熵的概念。如果您想了解混淆度，可以参考[https://www.youtube.com/watch?v=BAN3NB_SNHY](https://www.youtube.com/watch?v=BAN3NB_SNHY)。每词混淆度基于熵。因此，为了理解熵，您可以参考以下链接：
- en: '[https://www.youtube.com/watch?v=Bd15qhUrKCI](https://www.youtube.com/watch?v=Bd15qhUrKCI)'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=Bd15qhUrKCI](https://www.youtube.com/watch?v=Bd15qhUrKCI)'
- en: '[https://www.youtube.com/watch?v=K-rQ8KnmmH8](https://www.youtube.com/watch?v=K-rQ8KnmmH8)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=K-rQ8KnmmH8](https://www.youtube.com/watch?v=K-rQ8KnmmH8)'
- en: '[https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH](https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH](https://www.youtube.com/watch?v=ICKBWIkfeJ8&list=PLAwxTw4SYaPkQXg8TkVdIvYv4HfLG7SiH)'
- en: 'Once you understand entropy, it will be easy for you to understand the equation
    of perplexity. Refer to the equation given in the following figure:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了熵，理解困惑度方程就会变得容易。参考以下图中给出的方程：
- en: '![Perplexity](img/B08394_08_26.jpg)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![困惑度](img/B08394_08_26.jpg)'
- en: 'Figure 8.26: Equation of perplexity'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.26：困惑度方程
- en: 'Image source: https://www.tensorflow.org/tutorials/recurrent'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：https://www.tensorflow.org/tutorials/recurrent
- en: Here, N is the number of samples and P is a probability function. We are calculating
    entropy using the natural logarithm function. Now let's look at another testing
    metric.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，N是样本数量，P是一个概率函数。我们使用自然对数函数来计算熵。现在让我们看看另一个测试指标。
- en: Loss
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失
- en: 'Training loss indicates the direction in which the training progresses. Usually,
    when we start training, the value of loss is high and training accuracy is low,
    but during the training, the value of loss goes down and the training accuracy
    goes up. There are many error functions that are used in DL algorithms. Here,
    we are using cross-entropy as a loss function. Cross-entropy and log loss are
    slightly different depending on the context, but in machine learning, when calculating
    error rates between 0 and 1, they are the same thing. You can refer to the equation
    given in the following figure:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失表示训练进展的方向。通常，当我们开始训练时，损失值较高，训练准确率较低，但在训练过程中，损失值会下降，训练准确率会上升。在深度学习算法中使用了多种错误函数。在这里，我们使用交叉熵作为损失函数。交叉熵和对数损失在上下文中略有不同，但在机器学习中，当计算0到1之间的错误率时，它们是同一回事。你可以参考以下图中给出的方程：
- en: '![Loss](img/B08394_08_27.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![损失](img/B08394_08_27.jpg)'
- en: 'Figure 8.27: Equation for cross-entropy'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27：交叉熵方程
- en: 'Image source: http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy
- en: 'If you want to explore the cross-entropy loss function, then you can refer
    to: [http://neuralnetworksanddeeplearning.com/chap3.html](http://neuralnetworksanddeeplearning.com/chap3.html).'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要探索交叉熵损失函数，可以参考：[http://neuralnetworksanddeeplearning.com/chap3.html](http://neuralnetworksanddeeplearning.com/chap3.html)。
- en: We haven't gone into great mathematical detail here because just by tracking
    the training process, you will get to know whether the value of loss is increasing
    or decreasing. If it is decreasing over the period of training time, then the
    training is moving in the right direction. This is applicable to perplexity as
    well. Initially, the perplexity value is huge, but during training it gradually
    falls down and at some point it neither increases nor decreases. At that time,
    we need to stop the training. Now let's test the revised chatbot.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里没有深入数学细节，因为仅仅通过跟踪训练过程，你就可以知道损失值是增加还是减少。如果在训练时间内损失值在下降，那么训练就是在正确的方向上。这也适用于困惑度。最初，困惑度值很大，但在训练过程中它会逐渐下降，在某个时刻既不增加也不减少。那时，我们需要停止训练。现在让我们测试修改后的聊天机器人。
- en: Testing the revised version of the chatbot
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试聊天机器人的修改版本
- en: In this section, we will be performing testing of the revised chatbot. I have
    trained it on a GPU for only 3 hours; now let's check how much our chatbot can
    tell us. For testing, we need to make a small change in the `seq2seq.ini` configuration
    file. We need to set *value of mode as test* and then execute the `python execute.py`
    command.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对修改后的聊天机器人进行测试。我仅在GPU上训练了3小时；现在让我们看看我们的聊天机器人能告诉我们多少。为了测试，我们需要在`seq2seq.ini`配置文件中进行一些小的更改。我们需要将*模式值设置为测试*，然后执行`python
    execute.py`命令。
- en: 'After executing the given command, you will get the output given in the following
    figure:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 执行给定的命令后，你将得到以下图中给出的输出：
- en: '![Testing the revised version of the chatbot](img/B08394_08_28.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![测试聊天机器人的修改版本](img/B08394_08_28.jpg)'
- en: 'Figure 8.28: Output of testing revised approach'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28：修改后方法的输出
- en: If you train for a longer period of time, then you will get a more impressive
    result. I feel that DL algorithms help us if we want to build the chatbot using
    the generative-based approach. Now let's discuss how we can improvise this revised
    approach.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Problems with the revised approach
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be discussing what the problems with the revised approach
    are. Is there any way in which we can optimize the revised approach? So first
    of all, let's discuss the area of improvement so that during the upcoming approach,
    we can focus on that particular point.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the points that I want to highlight are as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: In the previous version of our chatbot there was a lack of reasoning, which
    means the chatbot couldn't answer the question by applying basic reasoning to
    it. This is what we need to improve.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let me give you an example. Suppose I tell chatbot a story: *John is in the
    kitchen. Daniel is in the bathroom*. After that, say, I ask the chatbot this question:
    *Where is John?* The chatbot that we have built so far will not be able to answer
    this simple question. We as humans answer these kinds of questions well.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We try to implement this kind of functionality in our next approach so that
    we can enable some features of AI in the chatbot
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the important concepts that can help us build the AI-enabled chatbot.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts to solve existing problems
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Facebook AI research group published a paper that proposed a Memory Network
    that can prove that the machine can also answer questions that are based on reasoning.
    You can certainly refer to this paper titled, Towards AI-Complete question answering:
    A set of prerequisite toy tasks. The link is: [https://arxiv.org/pdf/1502.05698.pdf](https://arxiv.org/pdf/1502.05698.pdf).
    You can also refer to this paper on the memory network at: [https://arxiv.org/pdf/1410.3916.pdf](https://arxiv.org/pdf/1410.3916.pdf).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be using the bAbI dataset and train the model based on the improvised
    memory network. Once training is done, we will check whether our chatbot can answer
    questions based on using the simple reasoning ability or not. We will be recreating
    the result of the Facebook research paper. Before we move to the implementation
    part, we need to understand what memory networks are and how we can build a system
    that will use logic to answer questions. So, let's look at memory networks briefly.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Memory networks
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will explore the memory network so that we can understand
    what is actually happening behind the scenes when we implement it in the upcoming
    section.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Basically, in the LSTM network, memory is encoded by using hidden states and
    weights. These hidden states and weights are too small for extremely long sequences
    of data, be that a book or a movie. So, in a language translation application,
    multiple LSTM stats are used along with the attention mechanism in order to choose
    the appropriate work for translation that fits the context. Facebook researchers
    have developed another strategy called a memory network that outperforms LSTMs
    for the question-answer system.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind the memory network was to allow the neural network to
    use an external data structure as memory storage, and it learns where to retrieve
    the required memory from this external memory structure in a supervised way. You
    can refer to the architecture of the memory network given in the following figure:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![Memory networks](img/B08394_08_29.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.29: Architecture of memory network'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: When it came to answering questions from the little data that was generated,
    the information was pretty easy to handle with the memory network, but in the
    real world, having the data handle long dependency relations is a challenging
    task. On Kaggle, there was a competition called The Allen AI Science Challenge
    in which the winner used a special variation of the memory network called a dynamic
    memory network (DMN), and that is what we are using to build our chatbot.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic memory network (DMN)
  id: totrans-395
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The architecture of DMN is given in the following figure:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic memory network (DMN)](img/B08394_08_30.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.30: Architecture of DMN'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/dmn-high-level.png'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of DMN defines two types of memory, which are as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic memory: We are using the pre-trained glove model that will generate
    vectors for input data. These vectors are the input to our DMN model and are used
    as semantic memory.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Episodic memory: This memory contains other knowledge. The inspiration for
    this memory came from the hippocampus function of our brain. It''s able to retrieve
    temporal states that are triggered by a response, such as an image or a sound.
    We will see the usage of this episodic memory in a bit.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are some important modules that we need to understand:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Input module
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question module
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Episodic memory module
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before I start explaining the modules, please refer to the following figure
    for better understanding:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![Dynamic memory network (DMN)](img/B08394_08_31.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.31: Details on each DMN module'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/dmn-details.png'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Input module
  id: totrans-411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The input module is a GRU (Gated Recurrent Unit) that runs on a sequence of
    word vectors. A GRU cell is kind of like an LSTM cell, but it''s more computationally
    efficient since it has only two gates and doesn''t use a memory unit. The two
    gates control when the content is updated and when it''s erased. There are only
    two tasks GRU is performing: one is *Update* and the other is *Reset*. You can
    refer to the following figure depicting LSTM and GRU:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![Input module](img/B08394_08_32.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.32: Illustration of LSTM and GRU'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://cdn-images-1.medium.com/max/1200/0*1udenjz1XCZ5cHU4'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'The hidden stats of the input module represent the input process in the form
    of a vector so far. It outputs hidden states after every sentence, and these outputs
    are called facts in the paper because they represent the essence of what is fed.
    You might want to know how the hidden state is calculated in GRU. For that, you
    can refer to the following equation:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Ht = GRU(Xt, ht-1)
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, Ht is the current timestep, ht-1 is the previous timestep, and Xt is
    the given word vector. The preceding equation is the simple format of the GRU
    hidden stat calculation. You can see more detailed and complex equations in the
    following figure:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '![Input module](img/B08394_08_33.jpg)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.33: Equation for calculating the hidden state in GRU'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'Image source: https://yerevann.github.io/public/2016-02-06/gru.png'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, with the help of the given word vector and the previous timestep
    vector, we compute the current timestep vector. The update gave us a single layer
    neural network. We sum up the matrix multiplications and add bias terms to it.
    Then, the sigmoid squashes it to a list of values between 0 and 1, and that is
    our output vector. We do this twice with different sets of weights, and then we
    use the reset gate that will learn to ignore the past timesteps when necessary.
    For example, if the next sentence has nothing to do with those that came before
    it, the update gate is similar in that it can learn to ignore the current timestep
    entirely. Maybe the current sentence has nothing to do with the answer whereas
    previous ones did.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Question module
  id: totrans-423
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This module processes the question word by word and outputs a vector using the
    same GRU as the input module and with the same weight. We need to code for input
    statements (input data) and for the question that we will ask. We can code them
    by implementing embedding layers for them. Now we need to create an episodic memory
    representation for both.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Episodic memory
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As I've described earlier, the concept of episodic memory derives from the hippocampus
    function of our brain. Both fact and question vectors extracted from the input
    and enter to the episodic memory module. It is composed of two nested GRUs. The
    inner GRU generates what are called episodes. It does this by passing over the
    facts from the input module. When updating its inner state, it takes into account
    the output of an attention function on the current fact. The attention function
    gives a score between 0 and 1 to each fact and so the GRU ignores facts with low
    scores. During training, after each full pass on all the available facts, the
    inner GRU outputs an episode, which is then fed to the outer GRU. We need multiple
    episodes so that our model can learn what part of a sentence it should pay attention
    to. During the second pass, the GRU realizes that something else is also important
    in the sentence. With the help of multiple passes, we can gather increasingly
    relevant information.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a brief explanation of DMN. You can refer to this great article as
    well: [https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/](https://yerevann.github.io/2016/02/05/implementing-dynamic-memory-networks/).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the implementation for this.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have covered the entire concept that can help us implement the DMN-based
    chatbot. In order to implement this approach, we will be using Keras with the
    TensorFlow backend. Without wasting any time, we will jump to the implementation
    section. You can refer to the code for this approach using this GitHub link: [https://github.com/jalajthanaki/Chatbot_based_on_bAbI_dataset_using_Keras](https://github.com/jalajthanaki/Chatbot_based_on_bAbI_dataset_using_Keras).'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will train our model on the given bAbI task 1 dataset. First of all,
    we need to parse the stories and build the vocabulary. You can refer to the code
    in the following figure:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_34.jpg)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.34: Code snippet for parsing stories and build vocabulary'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'We can initialize our model and set its loss function as a categorical cross-entropy
    with stochastic gradient descent implementation using RMSprop in Keras. You can
    refer to the following screenshot:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_35.jpg)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.35: Code snippet for building the model'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training, we need to set a hyperparameter. With the help of the value
    of the hyperparameter script, we will decide whether to run the script in the
    training mode or the testing mode. You can see all the hyperparameters that we
    need to set during training in the following figure:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_36.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.36: Hyperparameter values for training'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have used three hyperparameters. We can experiment with them. Let''s
    discuss them for a minute:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: 'train_epochs: This parameter indicates the number of times the training examples
    complete a forward pass and backward pass in a neural network. One epoch means
    one forward pass and one backward pass of the training example. Here we are setting
    train_epochs 100 times. You can increase it but then the training time also increases.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'batch_size: This parameter indicates the number of training examples in one
    forward and backward pass. The higher batch size needs more memory so we have
    set this value to 32\. If you have more memory available then you can increase
    the batch size. Please see the simple example given in the following information
    box.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'lstm_size: This parameter indicates the number of LSTM cells present in our
    neural network. You can decrease and increase the number of LSTM cells. In our
    case, less than 64 LSTM cells will not give us good output so I have set lstm_size
    to 64.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have 1000 training examples and your batch size is 500 then it will take
    2 iterations to complete 1 epoch.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 'I have trained this model on a GPU. If you are not using a GPU then it may
    take a lot of time. You can start training by executing this command: `python
    main.py`. The output of the training is given in the following figure:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the best approach](img/B08394_08_37.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.37: Code snippet for the training output'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we train the model, we can load and test it. There are two testing modes
    available:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Random testing mode
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User interactive testing mode
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random testing mode
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this mode, the script itself will load a random story and give you its answer.
    You can see the value of the hyperparameters in the following figure:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: '![Random testing mode](img/B08394_08_38.jpg)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
- en: figure 8.38, Value of hyperparameters for random testing mode.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing, execute the `python main.py` command and you can see the testing
    results. These results have been shown in the following figure:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '![Random testing mode](img/B08394_08_39.jpg)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.39: Result of a random testing mode'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: User interactive testing mode
  id: totrans-460
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this mode, if the testing user can give their own story and ask their own
    question, the chatbot will generate the answer to that question. You just need
    to remember that before every word, you need to provide space. You can refer to
    the value of the hyperparameter for the user interactive testing mode in the following
    figure:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '![User interactive testing mode](img/B08394_08_40.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.40: Values of hyperparameters for the user interactive testing mode'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing, execute the `python main.py` command and you can see the testing
    results. These results are shown in the following figure:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '![User interactive testing mode](img/B08394_08_41.jpg)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.41: Result of the user interactive testing mode'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to test all other tasks, then you can use this web application:
    [https://ethancaballero.pythonanywhere.com/](https://ethancaballero.pythonanywhere.com/).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: This approach gives us up to 92 to 95% accuracy. This approach helps us build
    AI-enabled chatbots.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: Discussing the hybrid approach
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a real-life scenario, in order to build the chatbot we can also combine some
    of the techniques described here. As per the business needs we can use a hybrid
    approach.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example. Suppose you are building a chatbot for the finance domain.
    If a user asks for the available balance in his account then we just need a rule-based
    system, which can query the database and generate the account balance details
    for that user. If a user asks how he can transfer money from one account to the
    other account, the chatbot can help the user by generating step-by-step information
    on how to transfer money. Here, we will use the deep learning-based generative
    approach. We should have one system that includes a rule-based engine as well
    as a deep learning algorithm to generate the best possible output. In this system,
    a user's question first goes to the rule-based system. If that question's answer
    can be generated by the rule-based system, then the answer will be passed to the
    end user. If the answer is not generated by a rule-based system, then the question
    will pass further on to the deep learning algorithm and it will generate the answer.
    Finally, the end-user will see the response to his question.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we referred to a different dataset in order to make a chatbot.
    You learned about the rule-based approach that can be used if you don''t have
    any datasets. You also learned about the open and closed domains. After that,
    we used the retrieval-based approach in order to build the basic version of a
    chatbot. In the revised approach, we used TensorFlow. This revised approach is
    great for us because it saves time compared to the basic approach. We implemented
    Google''s neural Conversational Model paper on the Cornell Movie-Dialogs dataset.
    For the best approach, we built a model that used the Facebook bAbI dataset and
    built the basic reasoning functionality that helped us generate good results for
    our chatbot. Although the training time for the revised and best approaches are
    really long, those who want to train the model on the cloud platform can choose
    to do so. So far, I like Amazon Web Services (AWS) and the Google Cloud platform.
    I also uploaded a pre-trained model to my GitHub repository so you could recreate
    the results. If you are a beginner and want to make a really good chatbot, then
    Google''s API.AI is a good chatbot development platform. It is now known as Dialogflow
    and is available at: [https://dialogflow.com/](https://dialogflow.com/). You can
    also refer to the IBM Watson API at: [https://www.ibm.com/watson/how-to-build-a-chatbot/](https://www.ibm.com/watson/how-to-build-a-chatbot/).
    These APIs can help you a great deal in building a chatbot; plus it requires less
    coding knowledge.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will be building a computer vision-based application
    that will help us identify named objects present in images and videos. This application
    will detect objects in real time. The object-detection application is used to
    build self-driving cars, robots, and so on, so keep reading!
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
