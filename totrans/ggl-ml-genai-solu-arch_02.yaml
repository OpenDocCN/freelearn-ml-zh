- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI/ML Concepts, Real-World Applications, and Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce basic concepts that will be explored in more detail
    throughout the rest of the book. We understand that readers of this book may be
    starting from different stages in their **artificial intelligence/machine learning**
    (**AI/ML**) journey, whereby some readers may already be advanced practitioners
    who are familiar with running AI/ML workloads while others may be newer to AI/ML
    in general. For this reason, we will briefly describe important fundamental concepts
    as required throughout the book to ensure that all readers have a common baseline
    upon which to build their understanding of the topics we discuss. Readers who
    are newer to AI/ML will benefit from learning the important underlying concepts
    rather than diving straight into the deep end of each topic without a baseline
    context, and advanced practitioners should find them to be useful knowledge refreshers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Terminology—AI, ML, **deep learning** (**DL**), and **generative** **AI** (**GenAI**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief history of AI/ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML approaches and use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief discussion of ML basic concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common challenges in developing ML applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the common types of AI/ML approaches
    and their real-world applications, as well as some historical background on the
    development of AI/ML concepts. Finally, you will learn about common types of challenges
    and pitfalls that companies encounter when they begin to implement AI/ML workloads.
    This is a particularly important part of the book, especially for the solutions
    architect role, and it provides real-world insights that are not found in academic
    courses; these insights come from years of experience in the field, working on
    large-scale AI/ML projects with many different companies.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology – AI, ML, DL, and GenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we describe how the terms *AI* and *ML* relate to each other. It should
    be noted that these terms are often used interchangeably, as well as the abbreviated
    term, *AI/ML*, which serves as an umbrella term to encapsulate both AI and ML.
    We also describe how the terms *DL* and *GenAI* fit in under the umbrella of AI/ML.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll begin by briefly including officially-accepted definitions of the terms
    *AI* and *ML*. We have chosen to include definitions from the *Collins English
    Dictionary*, in which AI is defined as “*a type of computer technology concerned
    with making machines work in an intelligent way, similar to the way that the human
    mind works*” and ML is defined as “*a branch of artificial intelligence in which
    a computer generates rules underlying or based on raw data that has been fed into
    it.*” The term *DL* has not yet been officially included as a dictionary term,
    but the *Collins English Dictionary* lists it as a new word suggestion, with the
    proposed definition of “*a type of machine learning concerned with artificial
    neural networks allowing advanced pattern recognition.*” We understand that official
    dictionary definitions don’t always explain the concepts completely, but it’s
    important to include them for reference, and we will cover these concepts in more
    detail as we progress through the book. All of those terms constitute what we
    are now beginning to refer to as “Traditional AI”, to distinguish it from GenAI,
    which is a much newer, and quite different concept. There will be an entire section
    of this book dedicated to GenAI, so this distinction will become clearer.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general rule, DL is considered to be a sub-field of ML, and ML is considered
    to be a sub-field of AI. GenAI can be seen as a sub-field within DL, because it
    uses Deep Neural Networks and concepts from Natural Language Processing in its
    application. You will often see them graphically represented in literature as
    a set of concentric circles, whereby AI is the broadest field, ML is nested as
    a sub-field within AI, and DL is nested as a sub-field within ML. I’m adding to
    this conceptual representation by including GenAI within the field of DL, although
    it is more of an association rather than a strict sub-category:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Depicting the relationship between the terms AI, ML, DL, and
    GenAI](img/B18143_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Depicting the relationship between the terms AI, ML, DL, and GenAI'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered some basic terminology regarding AI/ML, let’s briefly
    discuss its history and understand how the AI/ML industry has developed so far.
  prefs: []
  type: TYPE_NORMAL
- en: A brief history of AI/ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we traveled back in time by only a few years — to the year 2015 — and compared
    the state of the AI/ML industry to what it is today, we would see that relatively
    few companies had commercially implemented large-scale AI/ML use cases at that
    point. Although we would find academic research being performed in this space,
    we wouldn’t regularly hear AI/ML being discussed in mainstream media, and successful
    commercial or industrial implementations had mainly been achieved only by some
    of the world’s largest, industry-leading technology or niche companies. Jumping
    forward by just 2 years, we find that by the end of 2017, the tech industry is
    abuzz with discussions of AI/ML, and it seems to be the main topic—or at least
    one of the main topics—on everybody’s mind.
  prefs: []
  type: TYPE_NORMAL
- en: Based on our time-traveling adventure, one would not be faulted for believing
    that AI/ML is a brand-new term that suddenly emerged only in the past few years.
    In reality, however, these concepts have been developing over many decades. As
    the next step in our time-traveling journey, we travel further back in time to
    the 1950s. The first use of the term *artificial intelligence* is credited to
    Professor John McCarthy in 1955 (McCarthy et al. 1955), and a number of other
    important developments that contributed significantly to this field of science
    took place during the 1950s, such as Alan Turing’s 1950 paper, *Computing Machinery
    and Intelligence*, in which he posed the question, “*Can machines think?*” (Turing,
    1950), and Frank Rosenblatt’s work on the “Perceptron” (Rosenblatt, 1957), which
    we’ll look at in more detail later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: As an extension of our time-traveling journey, it should be noted that AI/ML
    algorithms today use mathematical concepts that were originally discovered and
    formulated centuries or millennia ago. For example, many of the algorithms we
    will explore in this book use concepts from linear algebra and calculus, which
    have been in use for centuries, and when we learn about cost functions, training,
    and evaluation, we will be using concepts from Euclidean geometry, such as the
    Pythagorean theorem, which dates back millennia. Interestingly, although Pythagoras
    is believed to have lived around 2,500 years ago, there is some evidence that
    concepts from the “Pythagorean theorem” were already understood and used by previous
    civilizations such as the Babylonians of Mesopotamia more than 1,000 years before
    his birth (Götze, 1945, 37-38). How fascinating it is that some of our most cutting-edge
    DL algorithms today use the same mathematical constructs that were used by ancient
    civilizations in the Bronze Age! In the 1960’s and 1970’s, the practice of using
    computers to perform statistical analysis and modeling on data began to grow significantly,
    and specialized software such as **Statistical Analysis System** (**SAS**) and
    **Statistical Package for the Social Sciences** (**SPSS**) emerged for these purposes.
    These tools were generally used for **in-memory** processing, meaning that all
    of the data used with these tools would be loaded into memory on a single computing
    machine. In the next section, you will see why it’s important to call this out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we travel forward in time, to the present day, and one of the questions
    that come to mind is this: If all of this started back in the 1950s, why does
    it seem that AI and ML are concepts that everybody has suddenly become enthusiastic
    about only in the past few years? Why did we not see such widespread adoption
    and success of AI/ML implementations before now? A number of factors have contributed
    to the gap in time that has been experienced between when these concepts originally
    began to be researched and when they finally started to gain publicly noticeable
    traction in the industry during the past few years (see the “AI Winter,” for example).
    As we will see in later chapters of this book, one such factor is that AI/ML use
    cases generally require large amounts of data and extensive computing resources.
    This is one of the reasons why—until recently—AI/ML research was usually being
    performed only by entities that could afford to amass these required resources,
    such as large technology companies, well-established research institutions, and
    government bodies.'
  prefs: []
  type: TYPE_NORMAL
- en: What has changed in recent years to help AI/ML break out beyond the exclusive
    realm of large corporations and research institutions? How have smaller companies,
    and even amateur hobbyists, suddenly obtained access to the resources required
    to train, host, and evaluate ML models and experiment with new ideas on how to
    apply AI/ML to an ever-increasing plethora of interesting new use cases? One of
    the primary contributors to this sudden revolution has been “cloud computing,”
    as well as iterative tooling development for building and running AI/ML workloads,
    and advances in DL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: AI/ML and cloud computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand in more detail how cloud computing has helped to suddenly revolutionize
    AI/ML research and real-world application, let’s consider the types of resources
    that are required to train, host, evaluate, and manage ML models. While we could
    use a laptop or a home computer to train and evaluate a relatively simple model
    on a small dataset, as we want to scale out our research and use cases, we would
    quickly find that the computing resources on our personal computer would not be
    sufficient to train larger models and the hard drive(s) in our personal computer
    would not be large enough to store the required datasets. These were also the
    limitations experienced by the statistical modeling tools such as SAS and SPSS
    that we mentioned in the previous section, which processed data “in-memory” on
    a single machine.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this concept, we’re going to scale up our use cases in a series
    of steps. If we were to scale up only slightly beyond the resources of the most
    powerful personal computer on the market, we would need to run our workload on
    a hardware “server,” which contains more powerful computing resources, and we
    could attach multiple large-capacity hard drives to this server, potentially as
    a **Redundant Array of Independent Disks** (**RAID**) array (formerly **Redundant
    Array of Inexpensive Disks**). This is still something that an individual person
    could perform in their home, but powerful servers—especially the “latest and greatest”
    servers on the market—can be quite expensive to purchase, and it would require
    a bit more technical knowledge to set up the server and configure the RAID array.
    At this point, we are already extending outside the realm of all but the most
    dedicated amateur hobbyists.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling beyond the resources of the most powerful hardware server on the market
    would require us to create a cluster of servers. Apart from the additional expense
    of purchasing multiple servers and their attached disks, this would also require
    more advanced technical knowledge to build and configure a network that links
    the servers together appropriately. This would not be an economically viable approach
    for most hobbyists, but it may still make some sense for a small company.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s scale all the way up to some of today’s most advanced DL use cases,
    which can take weeks or months to train, on hundreds of high-powered and very
    expensive servers. If we wanted to run these kinds of workloads completely by
    ourselves, we would need to build a data center, hire teams of experts to install
    hundreds of servers, build and configure a complex network to link them all together
    appropriately, and perform multiple other supporting activities to get our infrastructure
    set up. Let’s imagine for a moment that we want to create a start-up company that
    will use DL to implement a new breakthrough idea that we’ve devised. Simply building
    the data center could take a couple of years and would cost many millions of dollars,
    before we could ever start experimenting with our idea. This would not be a viable
    option.
  prefs: []
  type: TYPE_NORMAL
- en: With cloud computing, however, we could simply write a script or click some
    links and buttons on a cloud computing provider’s website, and it would spin up
    all of the servers we need within minutes. We could then perform our experiments
    — iteratively train and evaluate our models — and then simply shut down the servers
    when our work is done. As you can imagine, this is infinitely easier, cheaper,
    and more achievable than trying to build and manage our own data centers. This
    now provides small companies and limited-funding researchers or hobbyists with
    access to compute power and resources that were previously only available to very
    large organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Note that it was not only the ability to more easily create and access the required
    hardware infrastructure that helped revolutionize the AI/ML industry. Relevant
    software tools and frameworks have also been evolving over time, as well as the
    amount of data that has become available. While the tools such as SAS and SPSS
    that were developed in the 1960’s and 1970’s were sufficient for performing statistical
    modeling on datasets that could fit in memory on a single machine, the rapid growth
    in popularity of the Internet caused a dramatic increase in the amount of data
    that companies could gather and produce. In parallel, libraries were developed
    in languages such as Python, which made activities such as data processing, analysis,
    and modeling much easier. I can confidently say that it is a lot easier to perform
    many kinds of modeling use cases with libraries such as scikit-learn, PyTorch,
    and Keras, than it was with the earlier tools mentioned above. Still, many of
    today’s machine learning algorithms can be seen as an evolutions of traditional
    statistical modeling techniques, which have been enhanced to handle larger and
    more complex use cases. In addition to this, tools such as Apache Hadoop and Apache
    Spark, which I will discuss in more detail later in this book, made it possible
    to implement use cases that could span beyond the limits of a single machine,
    and therefore handle much larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in our discussion of scaling out our use cases, we have mainly focused
    on training and evaluating ML models, but those activities are only a subset of
    what’s required to create an ML application that is actually used in the real
    world. Throughout this book, we will often use the term *in production* to refer
    to the concept of creating, hosting, and serving AI/ML applications that are used
    in the real world, outside of a laboratory testing environment.
  prefs: []
  type: TYPE_NORMAL
- en: Even large, well-established organizations with teams of experienced data scientists
    often find that successfully hosting an ML application in production can be more
    complex and challenging than the model training and evaluation process. Let’s
    now take a look at how cloud computing can provide additional value for addressing
    these challenges beyond simply spinning up the required compute and storage resources.
    In later chapters in this book, we will discuss all of the steps in a typical
    ML project in great detail, but for now, let’s consider at a high level what kinds
    of resources and infrastructure would be required to host an ML model in production
    if cloud computing did not exist.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the activities required for model training and evaluation, such
    as constructing the data centers, installing the servers, building and configuring
    a complex network, and maintaining all of the hardware over time, we would need
    to perform many other activities to actually host and serve an ML model for production
    usage. For example, it would be necessary to create an interface to expose our
    model to end users or other systems. The most likely approach would be to use
    a web-based interface, in which case we would need to build a cluster of web servers
    and configure and manage those web servers on an ongoing basis. We would need
    to develop and build an application on those web servers to expose our model to
    web clients, and then install and configure load balancers and distribute load
    across our web servers. All of this infrastructure would, of course, need to be
    secured appropriately. *Figure 1**.2* shows an example of the kind of infrastructure
    you may need to set up; bear in mind that you may need to duplicate that infrastructure
    for each layer in your solution—for example, you would need to duplicate that
    infrastructure multiple times for your web server layer, your application server
    layer, and your model serving layer. The diagram shows two load balancers and
    routers for redundancy, in case one of those components fails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2: Example infrastructure for model hosting](img/B18143_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Example infrastructure for model hosting'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, most data scientists are not also networking experts, web server
    configuration experts, and security experts, so even if we had a team of the best
    data scientists who had created a breakthrough model, we would need many other
    teams of dedicated experts just to build and maintain the infrastructure required
    to expose that model to clients. On the other hand, if we wanted to use a service
    such as Vertex AI on Google Cloud, it would automatically build and manage all
    of this infrastructure for us, and we could go from laboratory testing to production
    hosting within minutes.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that without cloud computing, companies wouldn’t just
    find it less convenient to build their AI/ML workloads, but it would also be prohibitive
    – that is, large companies wouldn’t invest in building the required infrastructure
    unless they were sure their application was going to be successful in advance,
    which is a very difficult thing to forecast. Smaller companies wouldn’t be able
    to get started without significant up-front investments in infrastructure expenses,
    which most would not be able to obtain. As such, AI/ML research, experimentation,
    and eventual implementation in the real world would be far less prevalent and
    achievable without cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of implementing AI/ML in the real world, let’s take a look at the different
    kinds of AI/ML approaches and some of their real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: ML approaches and use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI/ML applications are usually intended to make some kind of prediction based
    on input data, with perhaps the exception of Generative AI, because Generative
    AI is intended to generate content rather than simply making predictions. In order
    to make predictions, ML models first need to be **trained**, and how they are
    trained depends on the approach being used. While ML is a broad concept that encompasses
    many different fields of research, with endless new use cases being created almost
    every day, the industry generally groups ML approaches into three high-level categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised** **learning** (**SL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised** **learning** (**UL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement** **learning** (**RL**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SL is the most commonly used type of ML in the industry and perhaps the easiest
    to describe. The term *supervised* indicates that we are informing the ML model
    of the correct answers during the training process. For example, let’s imagine
    that we want to train a model to be able to identify photographs of cats. In this
    case, we would use thousands or millions of photographs in our training set, and
    we would tell the model which photographs contain cats and which ones do not.
    We do this via a process called **labeling**, which we will describe in more detail
    in later chapters. If trained correctly, our model would learn how to distinguish
    input features that identify a cat in each photograph. If we then presented new
    photographs that the model had never seen before (that is, that were not included
    in the training set), our model would be able to identify whether those photographs
    contained cats. More specifically, for each photograph, our model would be able
    to predict the probability that it contains a cat, based on observed features
    in the photograph.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two subcategories of SL: classification and regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our cat-identification model described previously is an example of a classification
    use case in which our model can classify whether our photo contains a cat. Classification
    is further broken down into binary classification or multi-class classification.
    Binary classification provides a “yes” or “no” prediction. For example, in this
    case, we would ask the model, “Does this photograph contain a cat?”, and the model
    would respond with either “Yes” or “No.” If we were to train our model to identify
    many different types of objects, then it would be capable of performing multi-class
    classification, and we could ask a broader question such as, “What do you see
    in this photograph?”. In this case, the model could respond with multiple different
    object classifications, including “cat” (if it sees a cat in the photograph),
    among other objects that it predicts to exist in the photograph (see *Figure 1**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Classification of cat and flowers in a photograph](img/B18143_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Classification of cat and flowers in a photograph'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Of course, classification can be used for much more important use cases than
    identifying pictures of cats. An example of an important real-world classification
    use case is in medical diagnoses, whereby an ML model can predict the presence
    of a medical condition in a patient based on input data such as physical symptoms
    or a radiology image.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While classification is useful when there are discrete answers to our questions,
    regression is used when we deal with “continuous variables,” whereby the answer
    to our question could be any value in a continuum, such as 0.1, 2.3, 9894.6, 105,
    or 0.00000487\. To introduce some terminology, the inputs that we provide to our
    model are referred to as “input variables,” and the variables that we wish to
    predict are referred to as “target variables” or “dependent variables.”
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When we use the term *regression* here, we are referring to linear regression.
    This is not to be confused with logistic regression, which is actually a type
    of classification that we will cover later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aim of linear regression is to define a linear function that maps input
    variables to an output target variable. For example, we may want to predict the
    grade a student will achieve on an exam based on the number of hours they spent
    studying, from data we have regarding previous students’ grades and how many hours
    they spent studying. We could graph the data as follows (see *Figure 1**.4*),
    where the stars represent each student in the dataset; that is, they represent
    each student’s grade and the number of hours they spent studying:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4: Student grades and hours studying](img/B18143_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Student grades and hours studying'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the diagram, there appears to be a relationship or correlation
    between the grades achieved and the number of hours spent studying; that is, students
    who studied for longer hours generally got higher grades.
  prefs: []
  type: TYPE_NORMAL
- en: 'A linear regression model trained on this dataset would try to find the function
    or line that best represents that relationship. You may remember from school that
    a simple line function is often represented by the formula *y = ax + b*, where
    *a* is a multiple of *x*, and *b* is where the line intercepts the *y* axis. To
    find the most accurate function, the linear regression process tries to define
    a line that minimizes the distance between that line and each of the data points
    (stars), which may look something like *Figure 1**.5*. The distances between the
    line and some of the data points are shown in red for reference. In later sections,
    we’ll discuss in more detail how those distances are calculated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5: Linear regression function](img/B18143_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Linear regression function'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this function, we could now estimate or predict future student grades
    based on the number of hours they spend studying. For example, if a student spends
    10 hours studying, we would predict that they would achieve a grade of approximately
    70% based on what we see in *Figure 1**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6: Applying linear regression function](img/B18143_01_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Applying linear regression function'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of linear regression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regression is one of the most widely used types of ML, and it is useful for
    many different business use cases. It’s the quintessential example of “predicting
    the future” that is often attributed to the power of ML. Business leaders often
    want to predict numbers that relate to the performance of their business, such
    as predicting sales for the next quarter, based on historical sales and other
    market data. Whenever you have numerical metrics that you can track, and sufficient
    historical features relating to those metrics, you can try to predict or “forecast”
    the future values of those metrics, from stock market prices and housing prices
    to blood pressure measurements in various medical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: UL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With UL, we do not train the model on a dataset in which the entries are labeled
    with the correct answers. Instead, we ask the model to find unknown or non-predetermined
    patterns in the data. An analogy we could use here is that in SL, we are teaching
    the model about what exists in the data, whereas in UL, the model is teaching
    us about what exists in the data, such as underlying trends among various data
    points in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common type of UL is something known as “clustering,” whereby data
    points are grouped together based on some kinds of similarities that are observed
    by the model. *Figure 1**.7* provides a visual representation of this concept,
    showing the input data on the left and the resulting data clusters on the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7: Clustering](img/B18143_01_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Clustering'
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of UL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An example of a real-world application of clustering would be for categorizing
    groups of customers with similar purchasing preferences. You may have noticed
    this being utilized when you are purchasing an item online and you see recommendations
    for other items that may interest you, accompanied by a message such as “people
    who purchased this item also purchased these other items.”
  prefs: []
  type: TYPE_NORMAL
- en: Another important real-world use case is fraud detection, in which case one
    of the clusters could represent legitimate transactions and another cluster could
    represent unusual or potentially fraudulent transactions, or anything that does
    not match the characteristics of legitimate transactions could be flagged as potentially
    fraudulent. As new transactions occur, the model could group them accordingly
    based on their input characteristics and could trigger a warning response if a
    transaction appears to be fraudulent. Have you ever received notifications or
    questions from your bank when you tried to use your credit card on the first day
    of vacation in a new location? That’s because the bank’s ML models determined
    that the characteristics of the transaction were abnormal in some way; in this
    case, it was a transaction from a location that is far from where you usually
    use your card.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Clustering can actually be considered as a type of unsupervised classification.
  prefs: []
  type: TYPE_NORMAL
- en: RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In RL, the mechanism for training a model is quite different from the previous
    two approaches. To introduce some terminology, we say that the model uses an **agent**
    that has an overall goal it wants to achieve (the desired model output). The agent
    interacts with its **environment** by sending **actions** to the environment.
    The environment evaluates the actions and provides feedback in the form of a **reward**
    signal, which indicates whether or not the actions contribute to achieving the
    overall goal, and **observations**, which describe the current state of the environment.
    See *Figure 1**.8* for a visual representation of this process. To make a very
    broad analogy, this is similar to how we train some animals, such as dogs. For
    example, if the dog performs a desired action, then the trainer rewards it with
    a tasty treat. Conversely, if the dog does something undesirable, the trainer
    may reprimand it in some way. In the case of RL, the model randomly attempts different
    actions in its environment. If an action or set of actions is deemed to contribute
    to achieving the overall goal, then the environment provides a positive reward
    as feedback to the agent, whereas if the action is considered to be detrimental
    to achieving the overall goal, then the environment provides a negative reward
    as feedback to the agent. In this case, the reward is usually just a numeric value,
    such as 0.5 or -0.2, rather than a tasty treat, because unfortunately for ML models,
    they aren’t yet complex enough to enjoy tasty treats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8: RL](img/B18143_01_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: RL'
  prefs: []
  type: TYPE_NORMAL
- en: The model’s environment is the space within which the goal and all possible
    actions exist, and the observations are features of the environment. This can
    be a physical environment, such as when a robot is moving around in a physical
    space, or something abstract, based on the problem that the model is trying to
    address. For example, you could create a model that becomes an expert in playing
    a game such as chess or a video game. The model will begin by trying all kinds
    of random actions, most of which may seem silly or bizarre at first, but based
    on feedback from the environment, the model’s actions will gradually become more
    relevant and may eventually outperform the actions of human experts in that task.
  prefs: []
  type: TYPE_NORMAL
- en: RL could actually be considered a type of SL because feedback is provided to
    the model when it makes a prediction, and the model learns and makes improvements
    based on that feedback. However, it differs from the standard concept of SL, which
    we described previously, because we are not providing labeled correct answers
    as part of the training process. Instead, the model is provided with signals that
    help it understand what kinds of actions it should perform in order to progress
    toward achieving the required goal.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications of RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RL is not yet as widely adopted in the industry as “traditional” SL and UL,
    but some interesting applications are emerging. In addition to the gaming use
    cases mentioned in a previous paragraph, one of the most recognizable applications
    of RL is in robotic navigation and self-driving cars. In this application, the
    car could be considered as the model agent, whereby it performs actions such as
    accelerating, braking, and steering the wheels. Sensors on the car, such as cameras
    and lidar sensors, provide observations on the state of the environment. If the
    actions performed by the car help it to achieve a goal such as navigating a course
    or self-parking without hitting any obstacles, then it would receive positive
    rewards, whereas if it collided with an obstacle, then it would receive a negative
    reward. Over time, it could learn to navigate the course or self-park and avoid
    obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: Another important real-world application of RL is in healthcare, where it has
    shown promising results for use cases such as medical imaging diagnoses (Zhou
    et al., 2021, 1-39) and determining what kinds of medical treatments work for
    individual patients based on their conditions, via mechanisms such as **dynamic
    treatment** **regimes** (**DTRs**).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed the different types of ML approaches and examples of
    their real-world use cases, let’s take a look at some of the underlying concepts
    that form the basis for how these ML implementations work.
  prefs: []
  type: TYPE_NORMAL
- en: A brief discussion of ML basic concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mathematics is the hidden magic behind ML, and pretty much all ML algorithms
    function by using mathematics to find relationships and patterns in data. This
    book focuses on practical implementations of AI/ML on Google Cloud; it is not
    a theoretical academic course, so we will not go into a lot of detail on the mathematical
    equations upon which ML models operate, but we will include mathematical formulae
    for reference where relevant throughout the book, and here we present some basic
    concepts that are widely used in AI/ML algorithms. There are plenty of academic
    materials available for learning each of these concepts in more detail. As an
    architect, understanding the mathematical concepts could be considered an extracurricular
    credit rather than a requirement; you usually would not need to dive into the
    mathematical details of ML algorithms in your day-to-day work, but if you want
    to have a better understanding of how some of the algorithms work, you can review
    these concepts in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Linear algebra
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In ML, we make frequent use of vectors and matrices to store and represent
    information. To briefly cover some definitions, *Collins Dictionary* defines a
    vector as “*a variable quantity, such as force, that has size and direction*”
    and a matrix as “*an arrangement of numbers, symbols, or letters in rows and columns
    which is used in solving mathematical problems.*” Let’s take a look at what this
    really means. Representing information in matrices is most easily demonstrated
    if we use tabular data as an example. Consider the information in *Table 1.1*,
    which represents house sales in King County, Washington (excerpted from a Kaggle
    dataset: [https://www.kaggle.com/datasets/harlfoxem/housesalesprediction](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 1.1: King County house sales](img/B18143_01_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 1.1: King County house sales'
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset depicted in *Table 1.1* has 7 rows (not including the title row)
    and 13 columns, where each row represents a single house sale, which we consider
    a data point or an observation in our dataset, and each column represents individual
    features of the data points. We can consider each row and each column to be vectors.
    Bear in mind that a vector can also be considered as a matrix with one row or
    one column (that is, a one-dimensional vector). So, for each individual house
    purchase in our dataset, we have a vector that contains each of the features of
    that house. Let’s imagine that we want to predict the price of houses based on
    the features (other than the price) of each house. We would want to find the function
    that best describes the relationship between the price and all of the other features,
    and linear regression is one way in which we could do this. In this case, we would
    want to find the set of values by which we should multiply each feature and then
    add all of the results of those multiplications together in order to correctly
    estimate the price of each house. This means that each feature would have a corresponding
    multiplier (or “coefficient”). In order to efficiently compute the multiplications
    of the features and the coefficients and then add all of the results together,
    we could represent all of the coefficients also as a vector and calculate the
    dot product of the feature vector and the coefficient vector. We’ll take a minute
    here to clarify what it means to calculate the dot product. If we have two vectors,
    *A* and *B*, where *A = [a b c]*, and *B = [d e f]*, the dot product is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a*d + b*e +* *c*f*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let’s take the first row of *Table 1.1* (without the price)
    as a feature vector; it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s create an initial vector of random coefficients (we can just create
    random coefficients at first and improve our guesses later during the model training
    process), which needs to have the same number of elements as our preceding feature
    vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When calculating the dot product, there are rules regarding the shapes of each
    vector, but we are omitting those details here for simplicity. We will go deeper
    into those details in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dot product of our feature vector and coefficient vector is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From our first guess at what the coefficients should be, we estimate that the
    price of the house in *row 1* of *Table 1.1* would be $996.663\. However, we can
    see in *Table 1.1* that the actual price of that house was $221,900\. We can now
    calculate the error resulting from our guess as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We often refer to this as the **loss** or the **cost** of our linear function,
    and it is similar to what was represented by the red lines in *Figure 1**.5* earlier,
    where this value represents the “distance” from the correct answer; that is, how
    far away our guess is from the correct answer. This is the first step in the learning
    process, and in later sections, we will want to find coefficients that minimize
    this error.
  prefs: []
  type: TYPE_NORMAL
- en: Calculus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One common use of calculus in ML is in the error minimization process mentioned
    previously. In later chapters, we will define something called a **loss function**
    (or a **cost function**), and we will use mechanisms such as “Gradient descent”
    (described later) to minimize that loss function. In that case, we will use calculus
    to derive the slope at various points on the curve that represents the loss function,
    and we will use that information to work toward minimizing the cost function (see
    *Figure 1**.9*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9: The slope at a point on a function curve (source: https://commons.wikimedia.org/wiki/File:Parabola_tangent.png)](img/B18143_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: The slope at a point on a function curve (source: https://commons.wikimedia.org/wiki/File:Parabola_tangent.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Statistics and probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML models don’t provide answers as definite facts. Instead, the results from
    an ML model are often provided as approximations, probabilities, or inferences.
    We most commonly call the results from an ML model invocation an **inference**.
    Referencing our cat classification model from earlier in this chapter, a model
    that we would use to identify cats in a photograph would usually respond to us
    with the “probability” of a cat existing in the photograph. For example, the model
    may tell us that it is 97.3% sure that it sees a cat in the photograph. One of
    the main goals of ML is to ensure that these probabilities are as accurate as
    possible. A model would not be effective if it says it’s 100% sure it sees a cat,
    but there is actually no cat in the photograph. In the case of binary classification,
    where the response is either true or false, there would generally be a threshold
    above which we consider the probability response to be true or below which we
    would consider it to be false. For example, we could determine that anything above
    72.3% probability is deemed to be positive, and anything below that threshold
    is negative. The threshold value would vary based on the use case and is one of
    the parameters that need to be determined when building such models.
  prefs: []
  type: TYPE_NORMAL
- en: If we break the process down a bit further, in the case of the cat classification
    model, it has observed some features in the photograph, and based on previous
    training in which it has seen those kinds of features (or features similar to
    those), it estimates the probability of a cat being present in the photograph.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also see later in this book that statistical analysis plays an important
    role in the early stages of an ML project when data scientists are exploring how
    datasets can be used to address a business problem. In such data exploration activities,
    data scientists usually analyze the statistical distributions of values for each
    of the variables or features in the dataset. For example, when exploring a dataset,
    data scientists often want to see statistical information regarding each of the
    numeric variables in the data, such as the mean, median, mode, and the minimum
    and maximum range in the values; see *Figure 1**.10*, in which the statistical
    distributions of some features from our house sales dataset are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10: Statistical distributions of dataset features](img/B18143_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Statistical distributions of dataset features'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We also introduce the term *data science* here. While data science is a broad
    scientific field, for the purposes of this book, we use the term *data science*
    to incorporate all of the steps required to create an ML model, including all
    data preparation and processing steps.
  prefs: []
  type: TYPE_NORMAL
- en: Data science and ML are fields in which we constantly strive for improvement,
    whether it’s to improve the accuracy of our models, how quickly they train and
    perform, or how much compute power they use. There’s a well-known saying, “*What
    isn’t measured cannot be improved*” (this is actually an approximation of slightly
    different observations from Peter Drucker and Lord Kelvin), and this saying holds
    a lot of truth; in order to improve something in a methodical way, you need to
    be able to measure some attribute of that thing. For this reason, metrics are
    an essential component of any ML project, and selecting the correct metric to
    monitor can have a critical impact on the success of an ML implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from operational metrics, such as measuring the latency of responses from
    your ML models, there are also various metrics for measuring how accurate an ML
    model’s inferences are.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in linear regression, it’s common to measure the **Mean Absolute
    Error** (**MAE**), **Mean Squared Error** (**MSE**), or **Root Mean Squared Error**
    (**RMSE**), while for classification use-cases, we often use metrics such as Accuracy
    and Precision. We will explore all of these metrics, and many others, in later
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed some of the underlying theory and mathematical concepts that
    are used in ML, let’s bring the discussion back to the real world again, and let’s
    take a look at what kinds of challenges exist for companies when they try to implement
    ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Common challenges in developing ML applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Companies typically run into common kinds of challenges when they embark on
    an AI/ML development journey, and it is often a key requirement of an architect’s
    role to understand common challenges in a given problem space. As an architect,
    if you are not aware of challenges and how to address them, it’s unlikely that
    you will design an appropriate solution. In this section, we introduce the most
    frequently encountered challenges and pitfalls at a high level, and in later sections
    of this book, we discuss ways to address or alleviate some of these hurdles of
    AI/ML development.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering, processing, and labeling data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data is the key ingredient in ML because, in general, ML models cannot function
    without data. There’s an often-quoted adage that data scientists spend up to 80%
    of their time working on finding, cleaning, and processing data before they can
    begin to make use of it for analytical or data science purposes. This is an important
    concept to understand; that is, data scientists are not tasked simply with finding
    relevant data, although that is, in itself, often a difficult task; they also
    need to convert that data to a state that can be used efficiently by ML algorithms.
    Not only may data be unusable for many kinds of ML models in its raw format, but
    a data scientist may also need to combine data from many different sources, each
    with different formats and different problems that need to be addressed in the
    raw data before it can be used by an ML model. Also, the available data may not
    be sufficient to make the kinds of predictions that we’d like to get from an ML
    model, and data scientists often need to invent ways to generate new data by cleverly
    using what’s available from their existing data sources. We will cover this in
    more detail when we discuss a practice known as “feature engineering” later in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality impact on model performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The effectiveness of a data scientist in performing the aforementioned tasks
    can have drastic impacts on how well or poorly the resulting ML models function
    because the data that is fed into ML models usually has a direct influence on
    the model’s output accuracy. Bear in mind that for some business applications,
    a tiny difference in the ML model’s accuracy can result in a difference of millions
    of dollars in revenue for business owners. Another well-known expression that
    describes this process well is “garbage in, garbage out.” The concept is quite
    simple; if the data you feed into the model doesn’t accurately represent what
    you’re trying to predict, the model will not be able to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not just a model’s outputs that are affected by the quality or contents
    of the data. Large ML models can be expensive and time-consuming to train, and
    inadequately prepared data can increase the time and expense required to train
    a model. As an architect or data scientist, these factors play a fundamental role
    in how we design our workloads because an architect’s purpose is not just to design
    solutions that address technical challenges, but often what will be equally or
    even more important will be the cost of implementing the solution. If we designed
    a solution that would be too expensive to implement, then the project may not
    get approval to proceed, or the company may lose money by implementing the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and fairness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another important concept that we will cover in more detail in this book is
    the concept of bias and fairness. The goal and challenge in this regard is to
    ensure that the data we use to train and evaluate ML models represents a fair
    distribution of all relevant classes for our dataset. For example, if our model
    will make predictions that will affect people’s lives, such as approving a loan
    or a credit card application, we need to ensure that the dataset used to train
    the model fairly represents all relevant demographic groups and does not become
    inadvertently biased in relation to any particular demographic groups.
  prefs: []
  type: TYPE_NORMAL
- en: Data labeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to the challenges described previously, another substantial challenge
    exists specifically for **supervised ML** (**SML**) applications. As we discussed
    earlier in this chapter, SML models learn from labels in the data that provide
    the “correct” answer for each data entry. See *Figure 1**.11* for an example,
    in which the dataset contains labels describing whether or not students passed
    their exams, in addition to other details regarding those exams, such as the grade
    received and the number of hours spent studying. However, usually, these datasets
    and the related labels need to be generated or created somehow, and considering
    that some datasets may contain millions of data points, it can be difficult, time-consuming,
    and error-prone to label all of the data accurately:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11: Example of labels (highlighted in green) in a dataset](img/B18143_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: Example of labels (highlighted in green) in a dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Data governance and regulatory compliance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s important to control how data is being stored and processed within your
    company and who has access to the data. Special care must be taken with regard
    to sensitive data — for example, data that contains customers’ personal details
    such as their address, date of birth, or credit card number. There are specific
    regulations that need to be upheld in this regard, such as the **California Consumer
    Privacy Act** (**CCPA**), the **Children’s Online Privacy Protection Act** (**COPPA**),
    the **General Data Protection Regulation** (**GDPR**), and the **Health Insurance
    Portability and Accountability Act** (**HIPAA**), which outline detailed rules
    on how specific types of data must be handled. For companies that operate on an
    international scale, abiding by all of the varying regulations in different countries
    can be quite complicated. When data scientists are gathering, storing, exploring,
    processing, and labeling data, they need to keep these security requirements in
    mind, and as an AI/ML solutions architect, you will need to ensure that the data
    storage and processing infrastructure facilitates adhering to these regulations
    and other important information security practices. We will cover each of Google
    Cloud’s relevant data storage and processing infrastructure options in this book
    and provide additional guidance on data governance concepts where appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Data and model lineage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data science contains the word “science” for a reason. As with most scientific
    fields, it involves iterative experimentation. When data scientists create new
    models, they usually go through a complex process in which they need to experiment
    with different datasets, different transformations on the datasets, different
    algorithms and parameters, and other supporting activities and resources. A team
    of data scientists could try hundreds of different combinations of steps before
    they create the desired model, and each of those steps has inputs and outputs.
    If a data scientist has a breakthrough discovery and creates a killer new model,
    and then they leave the company or something happens to them, we would not be
    able to recreate their work unless they kept detailed notes of all of the steps
    they took to create that model, including all input and output artifacts that
    were used and created by each step.
  prefs: []
  type: TYPE_NORMAL
- en: This is also important during the experimentation process, in which data scientists
    may want to collaborate with other scientists on their team or on other teams.
    If a data scientist gets some promising results from an experiment, they could
    share the details with their peers, who could validate the results or build on
    top of them by combining the outputs of other experiments they had performed.
    This kind of collaboration is fundamental to many kinds of scientific research
    and is often required for significant progress to occur.
  prefs: []
  type: TYPE_NORMAL
- en: Data and model lineage refers to this process of tracking all of the steps and
    their associated inputs and outputs that were required to create a model. It’s
    not only important for collaboration and progress but also for governance purposes
    and fairness in AI/ML development; it’s also important to understand how a model
    was created and which data artifacts, algorithms, and parameters were used along
    the way.
  prefs: []
  type: TYPE_NORMAL
- en: As companies begin to perform AI/ML research, they often do not have robust
    lineage tracking mechanisms in place, and collaboration at scale can be hampered
    as a result. Even worse, companies sometimes find themselves using models for
    which nobody has a good understanding of how they work or how they were created.
    This is not a good position to be in if you want to update those models or they
    need to be audited for compliance reasons. Later in this book, we’ll see how Google
    Cloud’s Vertex AI platform can help to ensure that data and model lineage are
    being tracked appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Organizational challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most large companies have evolved over time and generally consist of multiple
    organizations that are loosely connected to each other. As large companies begin
    to experiment with AI/ML, research often takes place organically in each organization
    without coordination between the different parts of the company. When this happens,
    knowledge and data are often not shared adequately — or at all — across the company,
    and this leads to the formation of silos within each organization, which in turn
    create obstacles that impede the company’s overall success regarding AI/ML solution
    development. As an AI/ML solutions architect, you will need to advise company
    leadership on how to structure their organizations and their corporate policies
    to make their AI/ML journey as successful as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine that we own a large company, and a data science team in one organization
    — let’s call it “Organization A” — in our company has spent the past year gathering,
    cleaning, and experimenting with a large dataset, and they finally had some success
    in training an ML model that is providing promising results. Let’s also imagine
    that — similar to most companies — the organizations that make up our business
    operate mainly independently of each other, with little communication between
    them unless it is required as part of regular business operations. Now, another
    organization in our company, named “Organization B,” starts exploring AI/ML, and
    they have a similar use case to Organization A. Because the organizations operate
    independently and do not regularly communicate with each other, Organization B
    will start from scratch and will spend the next year wasting their time doing
    work that has already been completed elsewhere in the company.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s imagine that our company consists of 20 large organizations, each
    with hundreds of product development teams. Consider how much time would be wasted
    if even just 20% of those product development teams started creating AI/ML workloads
    without communicating with each other. It may be hard to believe, but this is
    how most large companies operate when they begin to experiment with AI/ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are mainly four different types of silos that form in the scenarios described
    previously, and they are related to the following four topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI/ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tooling and development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge silos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This one is pretty straightforward: if organizations are not sharing knowledge
    effectively with each other, teams all across the company will waste time trying
    to solve similar problems from scratch again and again.'
  prefs: []
  type: TYPE_NORMAL
- en: Data silos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve already talked about the importance and difficulty of getting access
    to data; especially getting access to clean, processed data that is ready to be
    used for training ML models. In most companies, each organization (and, possibly,
    each team) will build its own datasets. If a team in Organization B wanted to
    get access to a dataset that was built by Organization A, they would first need
    to learn of the existence of that dataset (which requires some knowledge sharing
    to occur). Then, they would need to request access to the data, which can often
    go through months of escalations through upper management just to get the required
    approvals. Next, a multi-month project would need to be carried out in order to
    actually set up the integration between the Organization A and Organization B
    systems. In an industry where AI/ML use cases and opportunities are evolving so
    quickly, these are the kinds of obstacles and processes that kill a company’s
    ability to rapidly innovate in this space. *Figure 1**.12* shows an example of
    data silos in a company. You will soon learn about ways to effectively and securely
    share datasets between organizations in order to break down data silos:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12: An example of data silos](img/B18143_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: An example of data silos'
  prefs: []
  type: TYPE_NORMAL
- en: Model silos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This is an extension of the knowledge and data silo concepts. Just as with knowledge
    and datasets, some kinds of models can be reused once they’ve been developed.
    If a team in Organization A has created a useful model, and that model could be
    reused by other teams in the company, then we should ensure that such sharing
    is enabled not only by our corporate structure, culture, and policies but also
    by our AI/ML development infrastructure. To understand this in more detail, you
    will learn how to share models, what kinds of requirements that entails, and how
    our AI/ML development tools and infrastructure can help or hinder this process.
  prefs: []
  type: TYPE_NORMAL
- en: Tooling and development silos
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In large companies, various development teams may use different tools and methodologies
    to build their AI/ML workloads. The selection of those tools and methodologies
    is often based on arbitrary factors such as what kinds of tools the employees
    used at previous companies. Employees in each organization or team will install
    their chosen tools on their machines and start developing in an ad hoc manner.
    For example, Employee A in the Organization B organization will install and use
    a tool named `scikit-learn` for their development and will use MySQL databases
    to store their application data, while Employee B in the Organization B organization
    will install and use PyTorch for their development and will use Oracle databases
    to store their application data. Next, Employee A may leave the company, and a
    new employee, Employee C, will be hired, and they will prefer using TensorFlow
    and some other type of database. This “wild west” approach makes it very difficult
    for employees and teams to collaborate and share artifacts at scale across the
    company.
  prefs: []
  type: TYPE_NORMAL
- en: In later chapters, we will go into detail on how to prevent, fix, and design
    around these pitfalls, but for now, it’s critical to highlight the importance
    of standardization. As companies begin to build their data science strategies,
    they should standardize as much as possible. Standardize the toolsets that will
    be used for AI/ML development and the types of data systems and formats that will
    be used. Establish company practices that encourage knowledge sharing and simplify
    data and model sharing across teams and organizations in a secure manner. Without
    these strategies, it will be difficult to collaborate and innovate rapidly at
    scale. One caveat is that you will need to find the balance between standardization
    and flexibility. Lack of standardization leads to the problems mentioned previously,
    but if your standardization strategy is too rigid, it could hinder your developers’
    productivity. For example, it would be too rigid to force all of your developers
    to use only one type of database and only one specific programming language or
    framework. Different tools are best suited to different use cases, and your company
    should provide guidelines to your employees on what tools are recommended for
    which use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Operationalization and ongoing management of AI/ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By now, it’s hopefully pretty clear that AI/ML model development can be complicated
    and challenging. However, even when you’ve successfully created a model that makes
    useful predictions, your work is still not done. Companies often find it difficult
    to bring a model out into the real world even though it has been working well
    in the lab. We already covered some of the infrastructural and logistical activities
    that need to be performed in order to host a model, but what introduces additional
    complexity is that most models need to evolve over time, because the environment
    in which they operate will almost inevitably evolve and change over time. This
    is similar to regular software development, in which we need to update our applications
    in order to provide new functionality or to react to changes in how our customers
    are using our products.
  prefs: []
  type: TYPE_NORMAL
- en: Another important factor is knowing when we may need to update our models. When
    our models are running in the real world, we need to monitor them on an ongoing
    basis in order to determine whether they continue to adequately meet the business
    needs they were created to address.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, you’ll learn about the unique requirements for monitoring and
    updating AI/ML models and how traditional software DevOps mechanisms are not sufficient
    by themselves for these purposes, but how we can build upon those mechanisms to
    suit the needs of AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Edge cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The term *edge cases* is being used as a pun with a double meaning here. In
    traditional software development, edge cases are abnormal or extreme use cases
    that can cause anomalous behavior. However, in this case, we also refer to the
    concept of edge computing, which is a sub-field of cloud computing that focuses
    on providing compute resources as close as possible to customers with low-latency
    requirements (see *Figure 1**.13*). We refer to the locations of these compute
    resources as “edge locations” because they exist outside the core cloud computing
    infrastructure locations, and they generally have limited resources in comparison
    to core cloud computing infrastructure locations.
  prefs: []
  type: TYPE_NORMAL
- en: ML models often require powerful compute resources in order to function, and
    this can present challenges for edge computing use cases due to limited resources
    at edge locations.
  prefs: []
  type: TYPE_NORMAL
- en: However, some ML models need to operate at or close to the “edge.” For example,
    consider a self-driving car, which needs to perform actions to navigate in its
    environment. Before and after each action, it needs to consult an ML model to
    determine the best next actions to perform. In this case, it cannot use a model
    that is hosted in a far-away data center because it cannot wait for an API request
    to travel over the internet to a server in the cloud, and then wait for the server
    to provide a response before it decides what to do next. Instead, it needs to
    make decisions and react to its environment within milliseconds. This is a clear
    use case for edge computing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In later chapters, we explore some of the requirements and solutions for these
    scenarios and how to address them for AI/ML workloads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13: Edge computing](img/B18143_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Edge computing'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced basic terminology related to AI/ML and some background
    information on how AI/ML has developed over time. We also explored different AI/ML
    approaches that exist today and some of their applications in the real world.
    Finally, and perhaps most importantly, we summarized common challenges and pitfalls
    that companies typically run into when they begin to implement AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will dive deeper into the model development process.
  prefs: []
  type: TYPE_NORMAL
