- en: Creating a Facial Expression Platform on IBM Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover a complete IBM Cloud-based expression classification
    solution that will use deep learning machine learning techniques on the IBM Cloud
    platform. The solution will implement a simple, yet efficient, ML model using
    TensorFlow and the ML services available with IBM Watson Studio. The goal is to
    illustrate an end-to-end solution for a complex ML task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will divide this chapter into the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding facial expression classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring expression databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing faces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning the expression classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the expression classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding facial expression classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with a brief discussion leading up to just what we mean when we
    say *facial expression classification*.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](99762d15-664d-4987-82cf-74440dedabb3.xhtml), *Deep Learning Using
    TensorFlow on the IBM Cloud*, we used IBM Watson Studio to perform *object detection*
    within random images. In that use case, we asked our project's model to find or
    detect common or known objects that might be pictured within an image. For example,
    we submitted an image of an animal and the solution correctly detected and identified
    a pinto horse, although no further information about the detected object was produced,
    such as whether the horse was angry or frightened.
  prefs: []
  type: TYPE_NORMAL
- en: Face detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps the next step on this journey (after object detection within an image)
    is face detection. Face detection is a computer technology being applied in a
    variety of applications that strive to identify human faces within digital images.
  prefs: []
  type: TYPE_NORMAL
- en: Facial recognition is a way of detecting and identifying a human face through
    the use of technology. A facial recognition solution utilizes the logic of biometrics
    to map facial features from a photograph (or even video) and then compares the
    information with a database of known faces, looking for a match.
  prefs: []
  type: TYPE_NORMAL
- en: Biometrics is the measurement and statistical examination of people's unique
    physical and behavioral characteristics, based upon the principle that each and
    every individual can be accurately identified by his or her intrinsic physical
    or behavioral traits.
  prefs: []
  type: TYPE_NORMAL
- en: Facial expression analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can get to the point: *facial expression analysis*.'
  prefs: []
  type: TYPE_NORMAL
- en: This concept commonly classifies all facial expressions as relating to one of
    the six universal emotions—joy (happiness), surprise, disgust, sadness, anger,
    and fear as well as neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Emotions are one element that makes us human and (believe it or not) they are
    difficult to hide, since all emotions, suppressed or not, are likely to have a
    noticeable physical effect that can be of value if we can automate the process
    of detecting and then interpreting the physical effects.
  prefs: []
  type: TYPE_NORMAL
- en: Once detected, the interpretation (of facial expressions) process is (perhaps)
    just another classification exercise. In practice, you'll find that facial expression
    classification will be based upon what is known as **TBM** or the **transferable
    belief model** framework.
  prefs: []
  type: TYPE_NORMAL
- en: TBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TBM offers an interesting premise. Without intending to provide a comprehensive
    explanation of the TBM framework, a key point is that it introduces degrees of
    belief and transfer (giving rise to the name of the method: the transferable belief
    model), which allows the model to make the necessary assumptions required to perform
    adequate classification (of the expressions). Basically, this means it scores
    its assumptions, that is, the assumption that the expression is a happy expression
    is determined to have a *n* percentage chance of being correct (we''ll see this
    in action later in this chapter when we review the results of our project).'
  prefs: []
  type: TYPE_NORMAL
- en: Further (and I'm oversimplifying), TBM looks to use quantified beliefs to make
    its classification decisions. Something perhaps more easily understood is that
    facial expression analysis extracts an expression skeleton of facial features
    (the mouth, eyes and eyebrows) and then derives simple distance coefficients from
    facial images. These characteristic distances are then fed to a rule-based decision
    system that relies on TBM in order to assign a facial expression to the face image.
  prefs: []
  type: TYPE_NORMAL
- en: Again, the goal is not to define the theory behind TBM, or even the intimate
    details of a facial expression analysis solution, but more to show a working example
    of such; therefore, we will go on to the next section and our use case example
    and leave to the reader the further work of researching this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring expression databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the core of all facial expression analysis solutions is an expression database.
  prefs: []
  type: TYPE_NORMAL
- en: A (**facial**) **expression database** is a collection of images showing the
    specific facial expressions of a range of emotions. These images must be well
    annotated or emotion-tagged if they are to be useful to expression recognition
    systems and their related algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A major hindrance to new developments in the area of **automatic human behavior
    analysis** is the lack of suitable databases with displays of behavior and affect.
    There have been directed advances in this area, as in the **MMI Facial Expression
    Database** project, which aims to deliver large volumes of **visual data of facial
    expressions** to the facial expression analysis community.
  prefs: []
  type: TYPE_NORMAL
- en: The MMI Facial Expression Database was initially created in 2002 as a resource
    for building and evaluating facial expression recognition algorithms. One significance
    of this database is that others databases focus on the expressions of the six
    basic emotions (which we mentioned earlier), whereas this database contains both
    these prototypical expressions as well as expressions with a single **Facial Action
    Coding System** (**FACS**) or **Action Unit** (**AU**) activated, for all existing
    AUs and many other **Action Descriptors** (**AD**). Recently recordings of naturalistic
    expressions have been added too.
  prefs: []
  type: TYPE_NORMAL
- en: The database is freely available to the scientific community. Find out more
    about the database online at [https://mmifacedb.eu](https://mmifacedb.eu).
  prefs: []
  type: TYPE_NORMAL
- en: In other example projects, we've been able to create our own test data or alter
    existing datasets to use within a project. However, with **expression analysis**
    projects, it is really not realistic to create a reasonably sized database (stating
    with nothing), which would require the collection and processing of literally
    thousands of images, all appropriately documented.
  prefs: []
  type: TYPE_NORMAL
- en: After collection, each (facial) image needs to be reviewed and categorized based
    on the emotion shown into one of seven categories (angry, disgust, fear, happy,
    sad, surprise, and neutral). To further complicate this work, images may not be
    aligned and properly proportioned.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that, even if you have a large number of images, if the images
    are not correctly labeled or simply do not contain detectable facial images, the
    performance of the expression analysis and detection process will be compromised
    (it will perform poorly).
  prefs: []
  type: TYPE_NORMAL
- en: These types of challenges make the classification process more difficult because
    the model is forced to generalize.
  prefs: []
  type: TYPE_NORMAL
- en: Training with the Watson Visual Recognition service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Considering the above mentioned challenges, IBM Watson Studio helps us to get
    started anyway by offering (right out of the box) the **Watson Visual Recognition**
    service.
  prefs: []
  type: TYPE_NORMAL
- en: This valuable service helps with the process of accurately analyzing, classifying,
    and training images using machine learning logic (although, to be sure, it still
    requires reasonable amounts of relevant training data to begin with, but more
    on that in a bit).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, there is a set of built-in models that is available to us to provide
    highly accurate results without endless training. The models are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General model**: General classifier categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Face model:** Locate faces within an image, gender, and age'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit model**: Whether an image is inappropriate for general use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**b model**: Specifically for images of food items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter's project, we will show how to use the Visual Recognition Service
    and the Face model to build an end-to-end working solution that can look at an
    image of a human face, perform expression analysis and simple classification,
    and ultimately, determine whether the individual is feeling happy or sad.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing faces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have just mentioned that building a suitable expression database is a lot
    of work. To be able to build an end-to-end working expression analysis solution
    (and fit it all into a single chapter of this book), we will take some liberties
    with our project:'
  prefs: []
  type: TYPE_NORMAL
- en: We will limit our model's ability to detect and classify only two emotions—happiness
    and sadness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will supply only a limited amount of expression data to train our model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Obviously, in the real world, our second assumption is a risky one; as in any
    ML model, less training data typically produces a less valuable result.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, if we have decided to satisfy only the minimal requirements for using
    the face model and the Visual Recognition service, we can get away with collecting
    only 10 images for each class which we intend our model to train on (10 happy
    faces, 10 sad faces, and 10 negative faces).
  prefs: []
  type: TYPE_NORMAL
- en: 'These individual training files will need to be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Either JPEG (`.jpg`) and PNG (`.png`) formatted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least 32*32 pixels in size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compressed as a group into class ZIP files, that is, 10 happy faces in a `happy.zip`
    file and 10 sad faces in a `sad.zip` file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sampling of our initial happy model training data is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e66f972a-ec25-4ac9-a6aa-3c4865f06efb.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding images are of 10 faces showing what we think can be labeled as
    being representative of happy facial expressions. Notice that the individual files
    have all been added to a compressed (ZIP) file named `happy.zip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sampling of our initial sadmodel training data is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f50f2a2d-d146-4d66-849b-9f761a3f7b29.png)'
  prefs: []
  type: TYPE_IMG
- en: And clearly, as the former group displayed happiness, the later images are of
    faces showing what we think can be labeled as being representative of sad expressions
    (individual files and the zipped file, `sad.zip`).
  prefs: []
  type: TYPE_NORMAL
- en: Negative or non-positive classing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the face model to work correctly, negative images are also required, not
    to be used to create a class (we will cover this in an upcoming section) within
    the created classifier, but to define what the updated classifier is not. Negative
    example files should not contain images that have the subject of any of the positive
    classes (happy and sad). In essence, the face images in this group should be perhaps
    considered to be neutral. You only need to specify one negative example file.
  prefs: []
  type: TYPE_NORMAL
- en: Because you want to give the model examples of what not to look for, you must
    provide the negative class. Providing a ML model with all positive images would
    mean that it would just assume that everything is positive and produce a risky
    result.
  prefs: []
  type: TYPE_NORMAL
- en: 'So finally, our initial negative model training data is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37ef7b79-dd09-413b-a9a1-72864e84c0a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Preparing the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now get moving along with the project model development.
  prefs: []
  type: TYPE_NORMAL
- en: The next step (assuming you have already created a new IBM Watson Studio project)
    is to associate the Watson Visual Recognition Service to the project. We covered
    how to do this in [Chapter 7](99762d15-664d-4987-82cf-74440dedabb3.xhtml)*, Deep
    Learning Using TensorFlow on the IBM Cloud*, so we will assume you have already
    added the service to this new project. If not, review [Chapter 7](99762d15-664d-4987-82cf-74440dedabb3.xhtml)*,
    Deep Learning Using TensorFlow on the IBM Cloud*, or the online Watson Studio
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Project assets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter's project, our assets will primarily be the training images
    we have collected and, indirectly perhaps, classified. These image assets are
    added in a way similar to the process we used to add data assets to Watson Studio
    projects in earlier chapters, but there are a few differences, which we will soon
    see.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, we will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the Assets tab and, under Models, click on New Visual Recognition model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d5b40a59-b895-4128-a930-6e5d8f5b74ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the model is created (it should take only a few moments), you can browse
    to or drag and drop the training (`.zip`) files we collected in the earlier section
    of this chapter to add them to our new project. This will upload the image files
    to **Cloud Object Storage** (**COO**), making them available to be used in our
    project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1f296ff0-2fc6-4adc-ae50-92703f30f9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: You do not have to load each image file independently, just the three zipped
    files (`happy.zip`, `sad.zip`, and `negative.zip`). The ZIP files should then
    be listed as shown in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Although you can upload the ZIP files, Watson will not allow those ZIP files
    to use the Preview feature on the Data Assets page. This is not a problem though,
    as you can still preview the images from the Model page, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Creating classes for our model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, from the Default Custom Model page, we need to create two classes. Perform
    the following steps to create the class for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on Create a class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a class name for it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click the blue Create button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/f960a915-1d9e-477a-8647-387adc7af744.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We actually only need to create two classes for this project: happy and sad,
    since Watson has already created the negative class for us. There is only one
    negative class per model, but you can have as many other classes as required for
    your project''s objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the classes are created, you then need to simply drag and drop the `.zip`
    files into the corresponding classes as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fea40c0-94fa-4332-9571-87ac1e30bf53.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, we dropped each of the three ZIP files onto their corresponding
    class panes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we stated earlier in this chapter, Watson preview doesn''t work with zipped
    image file assets; however, from the Default Custom Model page (shown in the following
    screenshot) we can click on All Images and scroll through what was loaded to see
    the filename, label, and content:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39206f6e-f778-4a52-973a-a95f1fc87364.png)'
  prefs: []
  type: TYPE_IMG
- en: Automatic labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Earlier in this chapter, we pointed out that, after collecting images to be
    used for expression analysis and recognition, each individual image must be annotated
    or labelled as to which emotion group it belongs to. This can be a daunting task.
    Fortunately, when using IBM Watson Studio, you can simply include the appropriate
    images in a ZIP file and drop the ZIP file onto a class and Watson will automatically
    label the image file. For example, in the following screenshot, you can see that
    we can correctly identify those images to include within our **happy** class (shown
    outlined in green) while a single image, `sad05` (shown outlined in red), does
    not belong to and should be excluded from our ZIP file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eba475d9-828e-47e1-956b-5db9bc63cc7c.png)'
  prefs: []
  type: TYPE_IMG
- en: This is a pretty easy process, but it could invite errors. Since it is easy
    and quick, you may mistakenly include images that will dilute the training sample.
    Keep in mind that, even if the image files are named intuitively, such as happy
    or sad, Watson doesn't care about the names, it simply labels all of the images
    in the file as *positive* or *matching* to the class.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there's one more note about the training data. Once you go to the trouble
    of collecting and uploading data as an IBM Watson Studio asset, that data is available
    to any of your projects and, if you want to, you can share it with any other Watson
    Studio user! This promotes the development of assets across projects and users
    and increases the return on your investment.
  prefs: []
  type: TYPE_NORMAL
- en: Learning the expression classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once you notice that the model status (indicated at the upper-right of the
    Default Custom Model page) has changed to Model is ready to train, you can then
    click on the Train Model button to start training the Face model on the training
    images we provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b5eb50d-742a-4330-96e3-285cd285fddc.png)'
  prefs: []
  type: TYPE_IMG
- en: Since we provided only roughly 30 training images, the training process should
    take less than 5 or 10 minutes. During the training, you will not be able to make
    any changes to the model or classes.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the expression classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the model training is complete, you should see the following Training
    successful message:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65975056-898f-47e1-9b9d-47653ea42fc0.png)'
  prefs: []
  type: TYPE_IMG
- en: From this point, you can click on the here hyperlink in the popup to view and
    test the model.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the model training results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After successfully training the model, you will be redirected to a page where
    you can see an overview or Summary (Model ID, Status, and other metadata) of the
    model build (take a note of the Model ID as that will be required during the implementation
    stage):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b28522a-9f8c-461e-be40-3e5b4a1eede1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also see our model''s Classes information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d150d8ec-dac7-4f83-b0ec-0876f0f455cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To test and understand how our model performs (or whether it even works!), you
    can upload images in the Test tab of the previous page view. Let's try it!
  prefs: []
  type: TYPE_NORMAL
- en: 'To test with images, click on Test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/937da7de-c674-410d-99d8-7869e96140e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Like we did to collect our happy and sad images, we can identify several random
    images (without regard to the expression shown in the image) to test our models
    ability to detect expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48473fd5-e379-4919-a66a-fe5496c5d244.png)'
  prefs: []
  type: TYPE_IMG
- en: Test scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea is for our model to interpret the preceding images, perform some expression
    analysis, and then classify each image as either happy or sad facial expressions.
    In addition, the model should generate and display a score for each of the defined
    classes (except for the negative class).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you have seen in our model, we defined just two classes to be classified—happy
    and sad. The model should, for each test image, display a percentage score showing
    the percentage of whether the detected expression is happy or sad. For example,
    the following score indicates that there is approximately 90 percent chance that
    the expression identified is happy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c86b5570-4744-41c9-ad04-b9fbb17b7f75.png)'
  prefs: []
  type: TYPE_IMG
- en: Test the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test the model with the following images, we can simply drag and drop the
    image files onto the preceding page to let the classifier analyze them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d1caeb0-f9a4-499b-bfa1-b248955c13e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the following screenshot as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e944204a-4135-4146-8942-516dfbbe06c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Success! It appears that all four of our random faces have been evaluated and
    scored by our Face model correctly. We can see that, in the first two images,
    the model indicates 11 and 90 percent that the individuals are happy and sad,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though it appears that our little solution is working correctly, we still
    have to keep in mind that the model has been trained on a very small amount of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve the model, from the Default Custom Model page, you can click on
    the blue button labeled Edit and Retrain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c4fd313-60c5-456b-a5a7-51edef621695.png)'
  prefs: []
  type: TYPE_IMG
- en: This will make our project editable.
  prefs: []
  type: TYPE_NORMAL
- en: More training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some improvements to our solution would include adding additional images to
    the happy and sad groups. To do this, you can create a new ZIP file with new and
    additional images and upload it to IBM Watson Studio (in the same fashion as we
    did earlier in this chapter), upload the file, and drop the new ZIP file into
    the respective class. Watson will add the new images (and not overwrite what''s
    already been defined):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9978cb0-9da4-40f7-85cd-e7cf8b0f8f04.png)'
  prefs: []
  type: TYPE_IMG
- en: Adding more classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another great improvement for our solution would be to add additional classes.
    This is to allow our model to support the detection of, perhaps, a third emotion
    or expression other than happy and sad. Let''s try to add anger as our third (not
    counting negative) class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step, of course, is to collect and compress (zip up) our angry images
    training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b9d06d43-eb94-4e57-a5a5-e24e389a3b18.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that it is not important what the individual image files are named,
    but it is important that they all represent the same emotion, *anger*.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we upload the `angry.zip` file (as another data asset available to our
    project), we can then go ahead and click on Create a class, enter `angry` for
    the class name, and then click on Create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/300d414b-72c5-4892-939d-f02cc5d135d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After a moment or two, our new `angry` class is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8afbac0c-9176-4595-b1b6-2424ad008523.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can once more click on the Train Model button to start retraining the
    Face model on the training images we provided, along with our new angry class.
    After a few moments, we should see the Model Trained message again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cf224e13-b4d5-47ef-a5fa-d053b03d6814.png)'
  prefs: []
  type: TYPE_IMG
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once again, we can go to the Default Custom Model page, click on the Test tab,
    and drop some new test images for the model to evaluate and classify:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a4801df-ec7f-4ac1-a3eb-aaaddd42ac7b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that the model has correctly classified the first image as 0.77
    angry. We also retested a previous image as a bit of a regression test and the
    model again correctly classified it (as 0.66 happy).
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that now our model provides three scores for each test image: angry,
    happy, and sad, corresponding to each of our model''s defined classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/780287b9-88da-4316-9564-9ee67aa16bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the concepts behind expression analysis and detection
    and used IBM Watson Studio, the Watson Visual Recognition service, and the default
    face model to build, train, and test an end-to-end, working visual expression
    classification solution with almost zero programming!
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover automated classification of lithofacies
    formation using ML on the IBM Cloud platform.
  prefs: []
  type: TYPE_NORMAL
