- en: More Classification Techniques - K-Nearest Neighbors and Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Statistical thinking will one day be as necessary for efficient citizenship
    as the ability to read and write."'
  prefs: []
  type: TYPE_NORMAL
- en: '- H.G. Wells'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 3](d5d39222-b2f8-4c80-9348-34e075893e47.xhtml), *Logistic Regression
    and Discriminant Analysis*, we discussed using logistic regression to determine
    the probability that a predicted observation belongs to a categorical response
    what we refer to as a classification problem. Logistic regression was just the
    beginning of classification methods, with a number of techniques that we can use
    to improve our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will delve into two nonlinear techniques: **K-Nearest Neighbors**
    (**KNN**) and **Support Vector Machines** (**SVM**). These techniques are more
    sophisticated than what we''ve discussed earlier because the assumptions on linearity
    can be relaxed, which means a linear combination of the features in order to define
    the decision boundary is not needed. Be forewarned though, that this does not
    always equal superior predictive ability. Additionally, these models can be a
    bit problematic to interpret for business partners and they can be computationally
    inefficient. When used wisely, they provide a powerful complement to the other
    tools and techniques discussed in this book. They can be used for continuous outcomes
    in addition to classification problems; however, for the purposes of this chapter,
    we will focus only on the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: After a high-level background on the techniques, we will lay out the business
    case and then put both of them to the test in order to determine the best method
    of the two, starting with KNN.
  prefs: []
  type: TYPE_NORMAL
- en: K-nearest neighbors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous efforts, we built models that had coefficients or, said another
    way, parameter estimates for each of our included features. With KNN, we have
    no parameters as the learning method is the so-called instance-based learning.
    In short, *The labeled examples (inputs and corresponding output labels) are stored
    and no action is taken until a new input pattern demands an output value*. (Battiti
    and Brunato, 2014, p. 11). This method is commonly called **lazy learning**, as
    no specific model parameters are produced. The `train` instances themselves represent
    the knowledge. For the prediction of any new instance (a new data point), the
    `train` data is searched for an instance that most resembles the new instance
    in question. KNN does this for a classification problem by looking at the closest
    points-the nearest neighbors to determine the proper class. The *k* comes into
    play by determining how many neighbors should be examined by the algorithm, so
    if *k=5*, it will examine the five nearest points. A weakness of this method is
    that all five points are given equal weight in the algorithm even if they are
    less relevant in learning. We will look at the methods using R and try to alleviate
    this issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best way to understand how this works is with a simple visual example of
    a binary classification learning problem. In the following figure, we have a plot
    of whether a tumor is **benign** or **malignant** based on two predictive features.
    The **X** in the plot indicates a new observation that we would like to predict.
    If our algorithm considers **K=3**, the circle encompasses the three observations
    that are nearest to the one that we want to score. As the most commonly occurring
    classifications are **malignant**, the **X** data point is classified as **malignant,**
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06473_05_01.png)'
  prefs: []
  type: TYPE_IMG
- en: Even from this simple example, it is clear that the selection of *k* for the
    nearest neighbors is critical. If *k* is too small, you may have a high variance
    on the `test` set observations even though you have a low bias. On the other hand,
    as *k* grows, you may decrease your variance but the bias may be unacceptable.
    Cross-validation is necessary to determine the proper *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to point out the calculation of the distance or the nearness
    of the data points in our feature space. The default distance is **Euclidean Distance**.
    This is simply the straight-line distance from point `A` to point `B`-as the crow
    flies-or you can utilize the formula that it is equivalent to the square root
    of the sum of the squared differences between the corresponding points. The formula
    for `Euclidean Distance`, given point `A` and `B` with coordinates `p1`, `p2`,
    ... `pn` and `q1`, `q2`,... `qn` respectively, would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_02-2.png)'
  prefs: []
  type: TYPE_IMG
- en: This distance is highly dependent on the scale that the features were measured
    on, so it is critical to standardize them. Other distance calculations as well
    as weights, can be used depending on the distance. We will explore this in the
    upcoming example.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first time I heard of support vector machines, I have to admit that I was
    scratching my head, thinking that this was some form of an academic obfuscation
    or inside joke. However, my open-minded review of SVM has replaced this natural
    skepticism with a healthy respect for the technique.
  prefs: []
  type: TYPE_NORMAL
- en: '*SVMs have been shown to perform well in a variety of settings and are often
    considered one of the best "out-of-the-box" classifiers *(James, G., 2013). To
    get a practical grasp of the subject, let''s look at another simple visual example.
    In the following figure, you will see that the classification task is linearly
    separable. However, the dotted line and solid line are just two among an infinite
    number of possible linear solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You would have separating hyperplanes in a problem that has more than two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_02-1.png)'
  prefs: []
  type: TYPE_IMG
- en: So many solutions can be problematic for generalization because whatever solution
    you choose, any new observation to the right of the line will be classified as
    **benign**, and to the left of the line, it will be classified as **malignant**.
    Therefore, either line has no bias on the `train` data but may have a widely divergent
    error on any data to test. This is where the support vectors come into play. The
    probability that a point falls on the wrong side of the linear separator is higher
    for the dotted line than the solid line, which means that the solid line has a
    higher margin of safety for classification. Therefore, as Battiti and Brunato
    say, *SVMs are linear separators with the largest possible margin and the support
    vectors the ones touching the safety margin region on both sides*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates this idea. The thin solid line is the optimal
    linear separator to create the aforementioned largest possible margin, thus increasing
    the probability that a new observation will fall on the correct side of the separator.
    The thicker black lines correspond to the safety margin, and the shaded data points
    constitute the support vectors. If the support vectors were to move, then the
    margin and, subsequently, the decision boundary would change. The distance between
    the separators is known as the **margin**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06473_05_03.png)'
  prefs: []
  type: TYPE_IMG
- en: This is all fine and dandy, but the real-world problems are not so clear cut.
  prefs: []
  type: TYPE_NORMAL
- en: In data that is not linearly separable, many observations will fall on the wrong
    side of the margin (the so-called slack variables), which is a misclassification.
    The key to building an SVM algorithm is to solve for the optimal number of support
    vectors via cross-validation. Any observation that lies directly on the wrong
    side of the margin for its class is known as a **support vector**.
  prefs: []
  type: TYPE_NORMAL
- en: If the tuning parameter for the number of errors is too large, which means that
    you have many support vectors, you will suffer from a high bias and low variance.
    On the other hand, if the tuning parameter is too small, the opposite might occur.
    According to James et al., who refer to the tuning parameter as `C`, as `C` decreases,
    the tolerance for observations being on the wrong side of the margin decreases
    and the margin narrows. This `C`, or rather, the cost function, simply allows
    for observations to be on the wrong side of the margin. If `C` were set to zero,
    then we would prohibit a solution where any observation violates the margin.
  prefs: []
  type: TYPE_NORMAL
- en: Another important aspect of SVM is the ability to model nonlinearity with quadratic
    or higher order polynomials of the input features. In SVMs, this is known as the
    **kernel trick**. These can be estimated and selected with cross-validation. In
    the example, we will look at the alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: As with any model, you can expand the number of features using polynomials to
    various degrees, interaction terms, or other derivations. In large datasets, the
    possibilities can quickly get out of control. The kernel trick with SVMs allows
    us to efficiently expand the feature space, with the goal that you achieve an
    approximate linear separation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check out how this is done, first look at the SVM optimization problem and
    its constraints. We are trying to achieve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Create weights that maximize the margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subject to the constraints, no (or as few as possible) data points should lie
    within that margin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, unlike linear regression, where each observation is multiplied by a weight,
    in SVM, the weights are applied to the inner products of just the support vector
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'What does this mean? Well, an inner product for two vectors is just the sum
    of the paired observations'' product. For example, if vector one is *3*, *4*,
    and *2* and vector two is *1*, *2*, and *3*, then you end up with *(3x1) + (4x2)
    + (2x3)* or *17*. With SVMs, if we take a possibility that an inner product of
    each observation has an inner product of every other observation, this amounts
    to the formula that there would be *n(n-1)/2* combinations, where *n* is the number
    of observations. With just *10* observations, we end up with *45* inner products.
    However, SVM only concerns itself with the support vectors'' observations and
    their corresponding weights. For a linear SVM classifier, the formula is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_05-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, `(x, xi)` are the inner products of the support vectors, as `α` is non-zero
    only when an observation is a support vector.
  prefs: []
  type: TYPE_NORMAL
- en: This leads to far fewer terms in the classification algorithm and allows the
    use of the `kernel` function, commonly referred to as the kernel trick.
  prefs: []
  type: TYPE_NORMAL
- en: The trick in this is that the `kernel` function mathematically summarizes the
    transformation of the features in higher dimensions instead of creating them explicitly.
    In a simplistic sense, a kernel function computes a dot product between two vectors.
    This has the benefit of creating the higher dimensional, nonlinear space, and
    decision boundary while keeping the optimization problem computationally efficient.
    The `kernel` functions compute the inner product in a higher dimensional space
    without transforming them into the higher dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notation for popular kernels is expressed as the inner (dot) product of
    the features, with `x[i]` and `x[j]` representing vectors, gamma, and `c` parameters,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_06-1.png)'
  prefs: []
  type: TYPE_IMG
- en: As for the selection of the nonlinear techniques, they require some trial and
    error, but we will walk through the various selection techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Business case
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the upcoming case study, we will apply KNN and SVM to the same dataset. This
    will allow us to compare the R code and learning methods on the same problem,
    starting with KNN. We will also spend some time drilling down into the confusion
    matrix, comparing a number of statistics to evaluate model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data that we will examine was originally collected by the **National Institute
    of Diabetes and Digestive and Kidney Diseases** (**NIDDK**). It consists of `532`
    observations and eight input features along with a binary outcome (`Yes`/`No`).
    The patients in this study were of Pima Indian descent from South Central Arizona.
    The NIDDK data shows that for the last 30 years, research has helped scientists
    to prove that obesity is a major risk factor in the development of diabetes. The
    Pima Indians were selected for the study as one-half of the adult Pima Indians
    have diabetes and 95 per cent of those with diabetes are overweight. The analysis
    will focus on adult women only. Diabetes was diagnosed according to the WHO criteria
    and was of the type of diabetes that is known as **type 2**. In this type of diabetes,
    the pancreas is still able to function and produce insulin and it used to be referred
    to as non-insulin-dependent diabetes.
  prefs: []
  type: TYPE_NORMAL
- en: Our task is to examine and predict those individuals that have diabetes or the
    risk factors that could lead to diabetes in this population. Diabetes has become
    an epidemic in the USA, given the relatively sedentary lifestyle and high-caloric
    diet. According to the **American Diabetes Association** (**ADA**), the disease
    was the seventh leading cause of death in the USA in 2010, despite being underdiagnosed.
    Diabetes is also associated with a dramatic increase in comorbidities, such as
    hypertension, dyslipidemia, stroke, eye diseases, and kidney diseases. The costs
    of diabetes and its complications are enormous. The ADA estimates that the total
    cost of the disease in 2012 was approximately $490 billion. For further background
    information on the problem, refer to ADA's website at [http://www.diabetes.org/diabetes-basics/statistics/](http://www.diabetes.org/diabetes-basics/statistics/).
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding and preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset for the `532` women is in two separate data frames. The variables
    of interest are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`npreg`: This is the number of pregnancies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glu`: This is the plasma glucose concentration in an oral glucose tolerance
    test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bp`: This is the diastolic blood pressure (mm Hg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skin`: This is triceps skin-fold thickness measured in mm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bmi`: This is the body mass index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ped`: This is the diabetes pedigree function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`age`: This is the age in years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type`: This is diabetic, `Yes` or `No`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The datasets are contained in the R package, `MASS`. One data frame is named
    `Pima.tr` and the other is named `Pima.te`. Instead of using these as separate
    `train` and `test` sets, we will combine them and create our own in order to discover
    how to do such a task in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, let''s load the following packages that we will need for the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now load the datasets and check their structure, ensuring that they
    are the same, starting with `Pima.tr`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the structures, we can be confident that we can combine the data
    frames into one. This is very easy to do using the `rbind()` function, which stands
    for row binding and appends the data. If you had the same observations in each
    frame and wanted to append the features, you would bind them by columns using
    the `cbind()` function. You will simply name your new data frame and use this
    syntax: `new data = rbind(data frame1, data frame2)`. Our code thus becomes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, double-check the structure. We can see that there are no issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s do some exploratory analysis by putting this in boxplots. For this,
    we want to use the outcome variable, `"type"`, as our ID variable. As we did with
    logistic regression, the `melt()` function will do this and prepare a data frame
    that we can use for the boxplots. We will call the new data frame `pima.melt`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The boxplot layout using the `ggplot2` package is quite effective, so we will
    use it. In the `ggplot()` function, we will specify the data to use, the `x` and
    `y` variables, and what type of plot, and create a series of plots with two columns.
    In the following code, we will put the response variable as `x` and its value
    as `y` in `aes()`. Then, `geom_boxplot()` creates the boxplots. Finally, we will
    build the boxplots in two columns with `facet_wrap()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B06473_05_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is an interesting plot because it is difficult to discern any dramatic
    differences in the plots, probably with the exception of **glucose** (**glu**).
    As you may have suspected, the fasting glucose appears to be significantly higher
    in the patients currently diagnosed with diabetes. The main problem here is that
    the plots are all on the same y-axis scale. We can fix this and produce a more
    meaningful plot by standardizing the values and then re-plotting. R has a built-in
    function, `scale()`, which will convert the values to a mean of zero and a standard
    deviation of one. Let''s put this in a new data frame called `pima.scale`, converting
    all of the features and leaving out the `type` response. Additionally, while doing
    KNN, it is important to have the features on the same scale with a mean of zero
    and a standard deviation of one. If not, then the distance calculations in the
    nearest neighbor calculation are flawed. If something is measured on a scale of
    1 to 100, it will have a larger effect than another feature that is measured on
    a scale of 1 to 10\. Note that when you scale a data frame, it automatically becomes
    a matrix. Using the `data.frame()` function, convert it back to a data frame,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will need to include the response in the data frame, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s just repeat the boxplotting process again with `melt()` and `ggplot()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_05.png)'
  prefs: []
  type: TYPE_IMG
- en: With the features scaled, the plot is easier to read. In addition to glucose,
    it appears that the other features may differ by `type`, in particular, `age`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before splitting this into `train` and `test` sets, let''s have a look at the
    correlation with the R function, `cor()`. This will produce a matrix instead of
    a plot of the Pearson correlations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are a couple of correlations to point out: `npreg`/`age` and `skin`/`bmi`.
    Multicollinearity is generally not a problem with these methods, assuming that
    they are properly trained and the hyperparameters are tuned.
  prefs: []
  type: TYPE_NORMAL
- en: 'I think we are now ready to create the `train` and `test` sets, but before
    we do so, I recommend that you always check the ratio of `Yes` and `No` in our
    response. It is important to make sure that you will have a balanced split in
    the data, which may be a problem if one of the outcomes is sparse. This can cause
    a bias in a classifier between the majority and minority classes. There is no
    hard and fast rule on what is an improper balance. A good rule of thumb is that
    you strive for at least a 2:1 ratio in the possible outcomes (He and Wa, 2013):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The ratio is 2:1 so we can create the `train` and `test` sets with our usual
    syntax using a 70/30 split in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: All seems to be in order, so we can move on to building our predictive models
    and evaluating them, starting with KNN.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will discuss various aspects pertaining to modeling and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: KNN modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As previously mentioned, it is critical to select the most appropriate parameter
    (`k` or `K`) when using this technique. Let''s put the `caret` package to good
    use again in order to identify `k`. We will create a grid of inputs for the experiment,
    with `k` ranging from `2` to `20` by an increment of `1`. This is easily done
    with the `expand.grid()` and `seq()` functions. The `caret` package parameter
    that works with the KNN function is simply `.k`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also incorporate cross-validation in the selection of the parameter,
    creating an object called `control` and utilizing the `trainControl()` function
    from the `caret` package, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create the object that will show us how to compute the optimal
    `k` value with the `train()` function, which is also part of the `caret` package.
    Remember that while conducting any sort of random sampling, you will need to set
    the `seed` value as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The object created by the `train()` function requires the model formula, `train`
    data name, and an appropriate method. The model formula is the same as we''ve
    used before-`y~x`. The method designation is simply `knn`. With this in mind,
    this code will create the object that will show us the optimal `k` value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the object provides us with the `k` parameter that we are seeking,
    which is `k=17`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In addition to the results that yield `k=17`, we get the information in the
    form of a table on the `Accuracy` and `Kappa` statistics and their standard deviations
    from the cross-validation. `Accuracy` tells us the percentage of observations
    that the model classified correctly. `Kappa` refers to what is known as **Cohen's
    Kappa statistic**. The `Kappa` statistic is commonly used to provide a measure
    of how well two evaluators can classify an observation correctly. It provides
    an insight into this problem by adjusting the accuracy scores, which is done by
    accounting for the evaluators being totally correct by mere chance. The formula
    for the statistic is *Kappa = (per cent of agreement - per cent of chance agreement)
    / (1 - per cent of chance agreement)*.
  prefs: []
  type: TYPE_NORMAL
- en: The *per cent of agreement* is the rate that the evaluators agreed on for the
    class (accuracy), and *percent of chance agreement* is the rate that the evaluators
    randomly agreed on. The higher the statistic, the better they performed with the
    maximum agreement being one. We will work through an example when we will apply
    our model on the `test` data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will utilize the `knn()` function from the `class` package.
    With this function, we will need to specify at least four items. These would be
    the `train` inputs, the `test` inputs, correct labels from the `train` set, and
    `k`. We will do this by creating the `knn.test` object and see how it performs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With the object created, let''s examine the confusion matrix and calculate
    the accuracy and `kappa`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy is done by simply dividing the correctly classified observations
    by the total observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy of 71 per cent is less than that we achieved on the `train` data,
    which was almost eighty per cent. We can now produce the `kappa` statistic as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The `kappa` statistic of 0.49 is what we achieved with the `train` set. Altman(1991)
    provides a heuristic to assist us in the interpretation of the statistic, which
    is shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Value of *K*** | **Strength of Agreement** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| <0.20 | Poor |'
  prefs: []
  type: TYPE_TB
- en: '| 0.21-0.40 | Fair |'
  prefs: []
  type: TYPE_TB
- en: '| 0.41-0.60 | Moderate |'
  prefs: []
  type: TYPE_TB
- en: '| 0.61-0.80 | Good |'
  prefs: []
  type: TYPE_TB
- en: '| 0.81-1.00 | Very good |'
  prefs: []
  type: TYPE_TB
- en: With our `kappa` only moderate and with an accuracy just over 70 per cent on
    the `test` set, we should see whether we can perform better by utilizing weighted
    neighbors. A weighting schema increases the influence of neighbors that are closest
    to an observation versus those that are farther away. The farther away the observation
    is from a point in space, the more penalized its influence is. For this technique,
    we will use the `kknn` package and its `train.kknn()` function to select the optimal
    weighting scheme.
  prefs: []
  type: TYPE_NORMAL
- en: The `train.kknn()` function uses LOOCV that we examined in the prior chapters
    in order to select the best parameters for the optimal `k` neighbors, one of the
    two distance measures, and a `kernel` function.
  prefs: []
  type: TYPE_NORMAL
- en: The unweighted `k` neighbors algorithm that we created uses the Euclidian distance,
    as we discussed previously. With the `kknn` package, there are options available
    to compare the sum of the absolute differences versus the Euclidian distance.
    The package refers to the distance calculation used as the `Minkowski` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: As for the weighting of the distances, many different methods are available.
    For our purpose, the package that we will use has ten different weighting schemas,
    which includes the unweighted ones. They are rectangular (unweighted), triangular,
    epanechnikov, biweight, triweight, cosine, inversion, gaussian, rank, and optimal.
    A full discussion of these weighting techniques is available in *Hechenbichler
    K.* and *Schliep K.P.* (2004).
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, let''s focus on just two: `triangular` and `epanechnikov`.
    Prior to having the weights assigned, the algorithm standardizes all the distances
    so that they are between zero and one. The triangular weighting method multiplies
    the observation distance by one minus the distance. With Epanechnikov, the distance
    is multiplied by ¾ times (one minus the distance two). For our problem, we will
    incorporate these weighting methods along with the standard unweighted version
    for comparison purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After specifying a random seed, we will create the `train` set object with
    `kknn()`. This function asks for the maximum number of *k* values (`kmax`), `distance`
    (one is equal to Euclidian and two is equal to absolute), and `kernel`. For this
    model, `kmax` will be set to `25` and `distance` will be `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'A nice feature of the package is the ability to plot and compare the results,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image_05_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This plot shows **k** on the x-axis and the percentage of misclassified observations
    by `kernel`. To my pleasant surprise, the unweighted (**rectangular**) version
    at `k: 19` performs the best. You can also call the object to see what the classification
    error and the best parameter are in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'So, with this data, weighting the distance does not improve the model accuracy
    in training and, as we can see here, didn''t even do as well on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: There are other weights that we could try, but as I tried these other weights,
    the results that I achieved were not more accurate than these. We don't need to
    pursue KNN any further. I would encourage you to experiment with various parameters
    on your own to see how they perform.
  prefs: []
  type: TYPE_NORMAL
- en: SVM modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the `e1071` package to build our SVM models. We will start with
    a linear support vector classifier and then move on to the nonlinear versions.
    The `e1071` package has a nice function for SVM called `tune.svm()`, which assists
    in the selection of the tuning parameters/kernel functions. The `tune.svm()` function
    from the package uses cross-validation to optimize the tuning parameters. Let''s
    create an object called `linear.tune` and call it using the `summary()` function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The optimal `cost` function is one for this data and leads to a misclassification
    error of roughly 21 per cent. We can make predictions on the `test` data and examine
    that as well using the `predict()` function and applying `newdata = test`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The linear support vector classifier has slightly outperformed KNN on both the
    `train` and `test` sets. The `e1071` package has a nice function for SVM called
    `tune.svm()` that assists in the selection of the `tuning parameters/kernel` functions.
    We will now see if nonlinear methods will improve our performance and also use
    cross-validation to select tuning parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first `kernel` function that we will try is `polynomial`, and we will be
    tuning two parameters: a degree of polynomial (`degree`) and kernel coefficient
    (`coef0`). The `polynomial` order will be `3`, `4`, and `5` and the coefficient
    will be in increments from `0.1` to `4`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The model has selected `degree` of `3` for the polynomial and coefficient of
    `0.1`. Just as the linear SVM, we can create predictions on the `test` set with
    these parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This did not perform quite as well as the linear model. We will now run the
    radial basis function. In this instance, the one parameter that we will solve
    for is `gamma`, which we will examine in increments of `0.1` to `4`. If `gamma`
    is too small, the model will not capture the complexity of the decision boundary;
    if it is too large, the model will severely overfit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The best `gamma` value is 0.5, and the performance at this setting does not
    seem to improve much over the other SVM models. We will check for the `test` set
    as well in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance is downright abysmal. One last shot to improve here would be
    with `kernel = "sigmoid"`. We will be solving for two parameters-- `gamma` and
    the kernel coefficient (`coef0`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This error rate is in line with the linear model. It is now just a matter of
    whether it performs better on the `test` set or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Lo and behold! We finally have a test performance that is in line with the performance
    on the `train` data. It appears that we can choose the sigmoid kernel as the best
    predictor.
  prefs: []
  type: TYPE_NORMAL
- en: So far we've played around with different models. Now, let's evaluate their
    performance along with the linear model using metrics other than just the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've looked at two different types of modeling techniques here, and for all
    intents and purposes, KNN has fallen short. The best accuracy on the `test` set
    for KNN was only around 71 per cent. Conversely, with SVM, we could obtain an
    accuracy close to 80 per cent. Before just simply selecting the most accurate
    mode, in this case, the SVM with the sigmoid kernel, let's look at how we can
    compare them with a deep examination of the confusion matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exercise, we can turn to our old friend, the `caret` package and utilize
    the `confusionMatrix()` function.  Keep in mind that we previously used the same
    function from the `InformationValue` package.  The `caret` package version provides
    much more detail and it will produce all of the statistics that we need in order
    to evaluate and select the best model. Let''s start with the last model that we
    built first, using the same syntax that we used in the base `table()` function
    with the exception of specifying the `positive` class, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The function produces some items that we already covered such as `Accuracy`
    and `Kappa`. Here are the other statistics that it produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '`No Information Rate` is the proportion of the largest class; 63 per cent did
    not have diabetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P-Value` is used to test the hypothesis that the accuracy is actually better
    than `No Information Rate`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will not concern ourselves with `Mcnemar's Test`, which is used for the analysis
    of the matched pairs, primarily in epidemiology studies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sensitivity` is the true positive rate; in this case, the rate of those not
    having diabetes has been correctly identified as such.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Specificity` is the true negative rate or, for our purposes, the rate of a
    diabetic that has been correctly identified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The positive predictive value (`Pos Pred Value`) is the probability of someone
    in the population classified as being diabetic and truly has the disease. The
    following formula is used:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The negative predictive value (`Neg Pred Value`) is the probability of someone
    in the population classified as not being diabetic and truly does not have the
    disease. The formula for this is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/image_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`Prevalence` is the estimated population prevalence of the disease, calculated
    here as the total of the second column (the `Yes` column) divided by the total
    observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Detection Rate` is the rate of the true positives that have been identified,
    in our case, 35, divided by the total observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Detection Prevalence` is the predicted prevalence rate, or in our case, the
    bottom row divided by the total observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Balanced Accuracy` is the average accuracy obtained from either class. This
    measure accounts for a potential bias in the classifier algorithm, thus potentially
    overpredicting the most frequent class. This is simply *Sensitivity + Specificity
    divided by 2*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The sensitivity of our model is not as powerful as we would like and tells
    us that we are missing some features from our dataset that would improve the rate
    of finding the true diabetic patients. We will now compare these results with
    the linear SVM, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As we can see by comparing the two models, the linear SVM is inferior across
    the board. Our clear winner is the sigmoid kernel SVM. However, there is one thing
    that we are missing here and that is any sort of feature selection. What we have
    done is just thrown all the variables together as the feature input space and
    let the blackbox SVM calculations give us a predicted classification. One of the
    issues with SVMs is that the findings are very difficult to interpret. There are
    a number of ways to go about this process that I feel are beyond the scope of
    this chapter; this is something that you should begin to explore and learn on
    your own as you become comfortable with the basics that have been outlined previously.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection for SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, all is not lost on feature selection and I want to take some space
    to show you a quick way of how to begin exploring this matter. It will require
    some trial and error on your part. Again, the `caret` package helps out in this
    matter as it will run a cross-validation on a linear SVM based on the `kernlab`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will need to set the random seed, specify the cross-validation
    method in the caret''s `rfeControl()` function, perform a recursive feature selection
    with the `rfe()` function, and then test how the model performs on the `test`
    set. In `rfeControl()`, you will need to specify the function based on the model
    being used. There are several different functions that you can use. Here we will
    need `lrFuncs`. To see a list of the available functions, your best bet is to
    explore the documentation with `?rfeControl` and `?caretFuncs`. The code for this
    example is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To create the `svm.features` object, it was important to specify the inputs
    and response factor, number of input features via `sizes`, and linear method from
    `kernlab`, which is the `svmLinear` syntax. Other options are available using
    this method, such as `svmPoly`. No method for a sigmoid kernel is available. Calling
    the object allows us to see how the various feature sizes perform, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Counter-intuitive as it is, the five variables perform quite well by themselves
    as well as when `skin` and `bp` are included. Let''s try this out on the `test`
    set, remembering that the accuracy of the full model was 76.2 per cent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This did not perform as well and we can stick with the full model. You can see
    through trial and error how this technique can play out in order to determine
    some simple identification of feature importance. If you want to explore the other
    techniques and methods that you can apply here, and for blackbox techniques in
    particular, I recommend that you start by reading the work by Guyon and Elisseeff
    (2003) on this subject.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we reviewed two new classification techniques: KNN and SVM.
    The goal was to discover how these techniques work, and the differences between
    them, by building and comparing models on a common dataset in order to predict
    if an individual had diabetes. KNN involved both the unweighted and weighted nearest
    neighbor algorithms. These did not perform as well as the SVMs in predicting whether
    an individual had diabetes or not.'
  prefs: []
  type: TYPE_NORMAL
- en: We examined how to build and tune both the linear and nonlinear support vector
    machines using the `e1071` package. We used the extremely versatile `caret` package
    to compare the predictive ability of a linear and nonlinear support vector machine
    and saw that the nonlinear support vector machine with a sigmoid kernel performed
    the best.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we touched on how you can use the `caret` package to perform a crude
    feature selection, as this is a difficult challenge with a blackbox technique
    such as SVM. This can be a major challenge when using these techniques and you
    will need to consider how viable they are in order to address the business question.
  prefs: []
  type: TYPE_NORMAL
