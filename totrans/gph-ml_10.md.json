["```py\nnetworkx==2.4 \nscikit-learn==0.24.0\nstellargraph==1.2.1\nspacy==3.0.3\npandas==1.1.3\nnumpy==1.19.2\nnode2vec==0.3.3\nKeras==2.0.2\ntensorflow==2.4.1\ncommunities==2.2.0\ngensim==3.8.3\nmatplotlib==3.3.4\nnltk==3.5\nfasttext==0.9.2\n```", "```py\nfrom nltk.corpus import reuters\ncorpus = pd.DataFrame([\n    {\"id\": _id,\n     \"text\": reuters.raw(_id).replace(\"\\n\", \"\"), \n     \"label\": reuters.categories(_id)}\n    for _id in reuters.fileids()\n])\n```", "```py\nfrom collections import Counter\nCounter([label for document_labels in corpus[\"label\"] for label in document_labels]).most_common()\n```", "```py\ncorpus[\"clean_text\"] = corpus[\"text\"].apply(\n    lambda x: x.replace(\"\\n\", \"\")\n)\n```", "```py\nfrom langdetect import detect\nimport numpy as np\ndef getLanguage(text: str):\n    try:\n        return langdetect.detect(text)\n    except:\n        return np.nan\ncorpus[\"language\"] = corpus[\"text\"].apply(langdetect.detect)\n```", "```py\npython -m spacy download en_core_web_sm\n```", "```py\nnlp = spacy.load('en_core_web_md')\nparsed = nlp(text)\n```", "```py\n    for sent in parsed.sents:\n        for token in sent:\n            print(token)\n    ```", "```py\n    displacy.render(parsed, style='ent', jupyter=True)\n    ```", "```py\n    nlp = spacy.load('en_core_web_md')\n    sample_corpus[\"parsed\"] = sample_corpus[\"clean_text\"]\\\n        .apply(nlp)\n    ```", "```py\nfrom subject_object_extraction import findSVOs\ncorpus[\"triplets\"] = corpus[\"parsed\"].apply(\n    lambda x: findSVOs(x, output=\"obj\")\n)\nedge_list = pd.DataFrame([\n    {\n        \"id\": _id,\n        \"source\": source.lemma_.lower(),\n        \"target\": target.lemma_.lower(),\n        \"edge\": edge.lemma_.lower()\n    }\n    for _id, triplets in corpus[\"triplets\"].iteritems()\n    for (source, (edge, neg), target) in triplets\n])\n```", "```py\nedges[\"edge\"].value_counts().head(10)\n```", "```py\nG = nx.from_pandas_edgelist(\n    edges, \"source\", \"target\", \n    edge_attr=True, create_using=nx.MultiDiGraph()\n)\n```", "```py\nG=nx.from_pandas_edgelist(\n    edges[edges[\"edge\"]==\"lend\"], \"source\", \"target\",\n    edge_attr=True, create_using=nx.MultiDiGraph()\n)\n```", "```py\nfrom gensim.summarization import keywords\ntext = corpus[\"clean_text\"][0]\n keywords(text, words=10, split=True, scores=True,\n         pos_filter=('NN', 'JJ'), lemmatize=True)\n```", "```py\n[('trading', 0.4615130639538529),\n ('said', 0.3159855693494515),\n ('export', 0.2691553824958079),\n ('import', 0.17462010006456888),\n ('japanese electronics', 0.1360932626379031),\n ('industry', 0.1286043740379779),\n ('minister', 0.12229815662000462),\n ('japan', 0.11434500812642447),\n ('year', 0.10483992409352465)]\n```", "```py\ncorpus[\"keywords\"] = corpus[\"clean_text\"].apply(\n    lambda text: keywords(\n       text, words=10, split=True, scores=True,\n       pos_filter=('NN', 'JJ'), lemmatize=True)\n)\n```", "```py\ndef extractEntities(ents, minValue=1, \n                    typeFilters=[\"GPE\", \"ORG\", \"PERSON\"]):\n    entities = pd.DataFrame([\n       {\n          \"lemma\": e.lemma_, \n          \"lower\": e.lemma_.lower(),\n          \"type\": e.label_\n       } for e in ents if hasattr(e, \"label_\")\n    ])\n    if len(entities)==0:\n        return pd.DataFrame()\n    g = entities.groupby([\"type\", \"lower\"])\n    summary = pd.concat({\n        \"alias\": g.apply(lambda x: x[\"lemma\"].unique()),\n        \"count\": g[\"lower\"].count()\n    }, axis=1)\n    return summary[summary[\"count\"]>1]\\\n             .loc[pd.IndexSlice[typeFilters, :, :]]\n\ndef getOrEmpty(parsed, _type):\n    try:  \n        return list(parsed.loc[_type][\"count\"]\\\n           .sort_values(ascending=False).to_dict().items())\n    except:\n        return []\ndef toField(ents):\n    typeFilters=[\"GPE\", \"ORG\", \"PERSON\"]\n    parsed = extractEntities(ents, 1, typeFilters)\n    return pd.Series({_type: getOrEmpty(parsed, _type)\n                      for _type in typeFilters})\n```", "```py\nentities = corpus[\"parsed\"].apply(lambda x: toField(x.ents))\n```", "```py\nmerged = pd.concat([corpus, entities], axis=1)\n```", "```py\nedges = pd.DataFrame([\n    {\"source\": _id, \"target\": keyword, \"weight\": score, \"type\": _type}\n    for _id, row in merged.iterrows()\n    for _type in [\"keywords\", \"GPE\", \"ORG\", \"PERSON\"] \n    for (keyword, score) in row[_type]\n])\n```", "```py\nG = nx.Graph()\nG.add_nodes_from(edges[\"source\"].unique(), bipartite=0)\n G.add_nodes_from(edges[\"target\"].unique(), bipartite=1)\n G.add_edges_from([\n    (row[\"source\"], row[\"target\"])\n    for _, row in edges.iterrows()\n])\n```", "```py\nType: Graph\nNumber of nodes: 25752\nNumber of edges: 100311\nAverage degree:   7.7905\n```", "```py\ndocument_nodes = {n \n                  for n, d in G.nodes(data=True)\n                  if d[\"bipartite\"] == 0}\nentity_nodes = {n \n                for n, d in G.nodes(data=True)\n                if d[\"bipartite\"] == 1}\n```", "```py\nnodes_with_low_degree = {n \n    for n, d in nx.degree(G, nbunch=entity_nodes) if d<5}\nsubGraph = G.subgraph(set(G.nodes) - nodes_with_low_degree)\n```", "```py\nentityGraph = overlap_weighted_projected_graph(\n    subGraph,\n    {n for n in subGraph.nodes() if n in entity_nodes}\n)\n```", "```py\nNumber of nodes: 2386\nNumber of edges: 120198\nAverage degree: 100.7527\n```", "```py\nfilteredEntityGraph = entityGraph.edge_subgraph(\n    [edge \n     for edge in entityGraph.edges\n     if entityGraph.edges[edge][\"weight\"]>0.05])\n```", "```py\nNumber of nodes: 2265\nNumber of edges: 8082\nAverage degree:   7.1364   \n```", "```py\ncomponents = nx.connected_components(filteredEntityGraph)\n pd.Series([len(c) for c in components])\n```", "```py\ncomp = components[0] \nglobal_metrics = pd.Series({\n    \"shortest_path\": nx.average_shortest_path_length(comp),\n    \"clustering_coefficient\": nx.average_clustering(comp),\n    \"global_efficiency\": nx.global_efficiency(comp)\n })\n```", "```py\n{\n    'shortest_path': 4.715073779178782,\n    'clustering_coefficient': 0.21156314975836915,\n    'global_efficiency': 0.22735551077454275\n}\n```", "```py\nimport community\ncommunities = community.best_partition(filteredEntityGraph)\n```", "```py\nfrom node2vec import Node2Vec\nnode2vec = Node2Vec(filteredEntityGraph, dimensions=5) \nmodel = node2vec.fit(window=10) \nembeddings = model.wv\n```", "```py\n[('turkish', 0.9975333213806152),\n ('lira', 0.9903393983840942),\n ('rubber', 0.9884852170944214),\n ('statoil', 0.9871745109558105),\n ('greek', 0.9846569299697876),\n ('xuto', 0.9830175042152405),\n ('stanley', 0.9809650182723999),\n ('conference', 0.9799597263336182),\n ('released', 0.9793018102645874),\n ('inra', 0.9775203466415405)]\n```", "```py\ndocumentGraph = overlap_weighted_projected_graph(\n    G,\n    document_nodes\n)\n```", "```py\nallEdgesWeights = pd.Series({\n    (d[0], d[1]): d[2][\"weight\"] \n    for d in documentGraph.edges(data=True)\n})\n```", "```py\nfilteredDocumentGraph = documentGraph.edge_subgraph(\n    allEdgesWeights[(allEdgesWeights>0.6)].index.tolist()\n)\n```", "```py\ncomponents = pd.Series({\n    ith: component \n    for ith, component in enumerate(\n        nx.connected_components(filteredDocumentGraph)\n    )\n})\n```", "```py\ncoreDocumentGraph = nx.subgraph(\n    filteredDocumentGraph,\n    [node \n     for nodes in components[components.apply(len)>8].values\n     for node in nodes]\n)\n```", "```py\nType: Graph\nNumber of nodes: 1050\nNumber of edges: 7112\nAverage degree:  13.5467\n```", "```py\nimport community\ncommunities = pd.Series(\n    community.best_partition(filteredDocumentGraph)\n)\n```", "```py\nfrom collections import Counter\ndef getTopicRatio(df):\n    return Counter([label \n                    for labels in df[\"label\"] \n                    for label in labels])\n\ncommunityTopics = pd.DataFrame.from_dict({\n    cid: getTopicRatio(corpus.loc[comm.index])\n    for cid, comm in communities.groupby(communities)\n }, orient=\"index\")\nnormalizedCommunityTopics = (\n    communityTopics.T / communityTopics.sum(axis=1)\n).T\n```", "```py\nnormalizedCommunityTopics.apply(\n    lambda x: np.sum(-np.log(x)), axis=1)\n```", "```py\ntopicsCorrelation = normalizedCommunityTopics.corr().fillna(0)\n```", "```py\ntopicsCorrelation[topicsCorrelation<0.8]=0\ntopicsGraph = nx.from_pandas_adjacency(topicsCorrelation)\n```", "```py\nfrom collections import Counter\ntopics = Counter(\n    [label \n     for document_labels in corpus[\"label\"] \n     for label in document_labels]\n).most_common(10)\n```", "```py\n[('earn', 3964), ('acq', 2369), ('money-fx', 717), \n('grain', 582), ('crude', 578), ('trade', 485), \n('interest', 478), ('ship', 286), ('wheat', 283), \n('corn', 237)]\n```", "```py\ntopicsList = [topic[0] for topic in topics]\n topicsSet = set(topicsList)\ndataset = corpus[corpus[\"label\"].apply(\n    lambda x: len(topicsSet.intersection(x))>0\n)]\n```", "```py\n    from node2vec import Node2Vec\n    node2vec = Node2Vec(G, dimensions=10) \n    model = node2vec.fit(window=20) \n    embeddings = model.wv \n    ```", "```py\n    pd.DataFrame(embeddings.vectors,\n                 index=embeddings.index2word\n    ).to_pickle(f\"graphEmbeddings_{dimension}_{window}.p\")\n    ```", "```py\n    from sklearn.base import BaseEstimator\n    class EmbeddingsTransformer(BaseEstimator):\n        def __init__(self, embeddings_file):\n            self.embeddings_file = embeddings_file        \n        def fit(self, *args, **kwargs):\n            self.embeddings = pd.read_pickle(\n                self.embeddings_file)\n            return self        \n        def transform(self, X):\n            return self.embeddings.loc[X.index]    \n        def fit_transform(self, X, y):\n            return self.fit().transform(X)\n    ```", "```py\n    def train_test_split(corpus):\n        indices = [index for index in corpus.index]\n        train_idx = [idx \n                     for idx in indices \n                     if \"training/\" in idx]\n        test_idx = [idx \n                    for idx in indices \n                    if \"test/\" in idx]\n        return corpus.loc[train_idx], corpus.loc[test_idx]\n    train, test = train_test_split(dataset)\n    ```", "```py\n    def get_features(corpus):\n        return corpus[\"parsed\"]\n    def get_labels(corpus, topicsList=topicsList):\n        return corpus[\"label\"].apply(\n            lambda labels: pd.Series(\n               {label: 1 for label in labels}\n            ).reindex(topicsList).fillna(0)\n        )[topicsList]\n    def get_features_and_labels(corpus):\n        return get_features(corpus), get_labels(corpus)\n    features, labels = get_features_and_labels(train)\n    ```", "```py\n    from sklearn.pipeline import Pipeline\n    from sklearn.ensemble import RandomForestClassifier \n    from sklearn.multioutput import MultiOutputClassifier\n    pipeline = Pipeline([\n        (\"embeddings\", EmbeddingsTransformer(\n            \"my-place-holder\")\n        ),\n        (\"model\", MultiOutputClassifier(\n            RandomForestClassifier())\n        )\n    ])\n    ```", "```py\n    from glob import glob\n    param_grid = {\n        \"embeddings__embeddings_file\": glob(\"graphEmbeddings_*\"),\n        \"model__estimator__n_estimators\": [50, 100],\n        \"model__estimator__max_features\": [0.2,0.3, \"auto\"], \n    }\n    grid_search = GridSearchCV(\n        pipeline, param_grid=param_grid, cv=5, n_jobs=-1)\n    ```", "```py\n    model = grid_search.fit(features, labels)\n    ```", "```py\ndef get_predictions(model, features):\n    return pd.DataFrame(\n        model.predict(features),\n        columns=topicsList, index=features.index)\npreds = get_predictions(model, get_features(test))\n labels = get_labels(test)\n```", "```py\nfrom sklearn.metrics import classification_report\nprint(classification_report(labels, preds))\n```", "```py\n              precision    recall  f1-score   support\n           0       0.97      0.94      0.95      1087\n           1       0.93      0.74      0.83       719\n           2       0.79      0.45      0.57       179\n           3       0.96      0.64      0.77       149\n           4       0.95      0.59      0.73       189\n           5       0.95      0.45      0.61       117\n           6       0.87      0.41      0.56       131\n           7       0.83      0.21      0.34        89\n           8       0.69      0.34      0.45        71\n           9       0.61      0.25      0.35        56\n   micro avg       0.94      0.72      0.81      2787\n   macro avg       0.85      0.50      0.62      2787\nweighted avg       0.92      0.72      0.79      2787\n samples avg       0.76      0.75      0.75      2787\n```", "```py\n    def my_spacy_tokenizer(pos_filter=[\"NOUN\", \"VERB\", \"PROPN\"]):\n        def tokenizer(doc):\n            return [token.lemma_ \n                    for token in doc \n                    if (pos_filter is None) or \n                       (token.pos_ in pos_filter)] \n        return tokenizer \n    ```", "```py\n    cntVectorizer = TfidfVectorizer(\n        analyzer=my_spacy_tokenizer(),\n        max_df = 0.25, min_df = 2, max_features = 10000\n    )\n    ```", "```py\n    trainFeatures, trainLabels = get_features_and_labels(train)\n    testFeatures, testLabels = get_features_and_labels(test)\n    trainedIDF = cntVectorizer.fit_transform(trainFeatures)\n    testIDF = cntVectorizer.transform(testFeatures)\n    ```", "```py\n    documentFeatures = pd.concat([trainedIDF, testIDF])\n    ```", "```py\n    entityTypes = {\n        entity: ith \n        for ith, entity in enumerate(edges[\"type\"].unique())\n    }\n    entities = edges\\\n        .groupby([\"target\", \"type\"])[\"source\"]\\\n        .count()\\\n        .groupby(level=0).apply(\n            lambda s: s.droplevel(0)\\\n                       .reindex(entityTypes.keys())\\\n                       .fillna(0))\\\n        .unstack(level=1)\n    entityFeatures = (entities.T / entities.sum(axis=1))\n    ```", "```py\n    from stellargraph import StellarGraph\n    _edges = edges[edges[\"source\"].isin(documentFeatures.index)]\n    nodes = {«entity»: entityFeatures, \n             «document»: documentFeatures}\n    stellarGraph = StellarGraph(\n        nodes, _edges,\n        target_column=»target», edge_type_column=»type»\n    )\n    ```", "```py\n    print(stellarGraph.info())\n    ```", "```py\n    StellarGraph: Undirected multigraph\n     Nodes: 23998, Edges: 86849\n    Node types:\n      entity: [14964]\n        Features: float32 vector, length 6\n        Edge types: entity-GPE->document, entity-ORG->document, entity-PERSON->document, entity-keywords->document\n      document: [9034]\n        Features: float32 vector, length 10000\n        Edge types: document-GPE->entity, document-ORG->entity,\n     document-PERSON->entity, document-keywords->entity\n    Edge types:\n        document-keywords->entity: [78838]\n            Weights: range=[0.0827011, 1], mean=0.258464,\n     std=0.0898612\n            Features: none\n        document-ORG->entity: [4129]\n            Weights: range=[2, 22], mean=3.24122, std=2.30508\n            Features: none\n        document-GPE->entity: [2943]\n            Weights: range=[2, 25], mean=3.25926, std=2.07008\n            Features: none\n        document-PERSON->entity: [939]\n            Weights: range=[2, 14], mean=2.97444, std=1.65956\n            Features: none\n    ```", "```py\n    targets = labels.reindex(documentFeatures.index).fillna(0)\n     sampled, hold_out = train_test_split(targets)\n    allNeighbors = np.unique([n \n        for node in sampled.index \n        for n in stellarGraph.neighbors(node)\n    ])\n    subgraph = stellarGraph.subgraph(\n        set(sampled.index).union(allNeighbors)\n    )\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    train, leftOut = train_test_split(\n        sampled,\n        train_size=0.1,\n        test_size=None,\n        random_state=42\n    )\n    validation, test = train_test_split(\n        leftOut, train_size=0.2, test_size=None, random_state=100,\n    ) \n    ```", "```py\n    from stellargraph.mapper import HinSAGENodeGenerator\n    batch_size = 50\n    num_samples = [10, 5]\n    generator = HinSAGENodeGenerator(\n        subgraph, batch_size, num_samples,\n        head_node_type=\"document\"\n    )\n    ```", "```py\n    train_gen = generator.flow(train.index, train, shuffle=True)\n     val_gen = generator.flow(validation.index, validation)\n    ```", "```py\n    from stellargraph.layer import HinSAGE\n    from tensorflow.keras import layers\n    graphsage_model = HinSAGE(\n        layer_sizes=[32, 32], generator=generator,\n        bias=True, dropout=0.5\n    )\n    x_inp, x_out = graphsage_model.in_out_tensors()\n    prediction = layers.Dense(\n        units=train.shape[1], activation=\"sigmoid\"\n    )(x_out)\n    ```", "```py\n    from tensorflow.keras import optimizers, losses, Model\n    model = Model(inputs=x_inp, outputs=prediction)\n    model.compile(\n        optimizer=optimizers.Adam(lr=0.005),\n        loss=losses.binary_crossentropy,\n        metrics=[\"acc\"]\n    )\n    ```", "```py\n    history = model.fit(\n        train_gen, epochs=50, validation_data=val_gen,\n        verbose=1, shuffle=False\n    )\n    ```", "```py\n    test_gen = generator.flow(test.index, test)\n     test_metrics = model.evaluate(test_gen)\n    ```", "```py\n    loss: 0.0933\n    accuracy: 0.8795\n    ```", "```py\n    test_predictions = pd.DataFrame(\n        model.predict(test_gen), index=test.index,\n        columns=test.columns)\n    test_results = pd.concat({\n        \"target\": test,\n        \"preds\": test_predictions\n    }, axis=1)\n    ```", "```py\n    thresholds = [0.01,0.05,0.1,0.2,0.3,0.4,0.5] \n    f1s = {}\n    for th in thresholds:\n        y_true = test_results[\"target\"]\n        y_pred = 1.0*(test_results[\"preds\"]>th)\n        f1s[th] = f1_score(y_true, y_pred, average=\"macro\")    \n    pd.Series(f1s).plot()\n    ```", "```py\n    print(classification_report(\n        test_results[\"target\"], 1.0*(test_results[\"preds\"]>0.2))\n    )\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.92      0.97      0.94      2075\n               1       0.85      0.96      0.90      1200\n               2       0.65      0.90      0.75       364\n               3       0.83      0.95      0.89       305\n               4       0.86      0.68      0.76       296\n               5       0.74      0.56      0.63       269\n               6       0.60      0.80      0.69       245\n               7       0.62      0.10      0.17       150\n               8       0.49      0.95      0.65       149\n               9       0.44      0.88      0.58       129\n       micro avg       0.80      0.89      0.84      5182\n       macro avg       0.70      0.78      0.70      5182\n    weighted avg       0.82      0.89      0.84      5182\n     samples avg       0.83      0.90      0.85      5182\n    ```", "```py\n    generator = HinSAGENodeGenerator(\n        stellarGraph, batch_size, num_samples,\n        head_node_type=\"document\")\n    ```", "```py\n    hold_out = hold_out[hold_out.sum(axis=1) > 0]\n    hold_out_gen = generator.flow(hold_out.index, hold_out)\n    ```", "```py\n    hold_out_predictions = model.predict(hold_out_gen)\n    preds = pd.DataFrame(1.0*(hold_out_predictions > 0.2),\n                         index = hold_out.index,\n                         columns = hold_out.columns)\n    results = pd.concat(\n        {\"target\": hold_out,\"preds\": preds}, axis=1\n    )\n    ```", "```py\n    print(classification_report(\n        results[\"target\"], results[\"preds\"])\n    )\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.93      0.99      0.96      1087\n               1       0.90      0.97      0.93       719\n               2       0.64      0.92      0.76       179\n               3       0.82      0.95      0.88       149\n               4       0.85      0.62      0.72       189\n               5       0.74      0.50      0.59       117\n               6       0.60      0.79      0.68       131\n               7       0.43      0.03      0.06        89\n               8       0.50      0.96      0.66        71\n               9       0.39      0.86      0.54        56\n       micro avg       0.82      0.89      0.85      2787\n       macro avg       0.68      0.76      0.68      2787\n    weighted avg       0.83      0.89      0.84      2787\n    samples avg       0.84      0.90      0.86      2787\n    ```"]