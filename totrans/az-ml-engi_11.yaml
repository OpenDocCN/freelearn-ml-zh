- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Distributed Training in AMLS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting topic is how we can process large-scale datasets to train machine
    learning and deep learning models. For example, large-scale text-based mining,
    entity extraction, sentiments, and image or video-based, including image classification,
    image multiclassification, and object detection, are all very memory intensive
    and need large compute resources to process, which may take hours or sometimes
    days and weeks to complete.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, if you have big data that contains business information and want
    to build machine learning models, then distributed learning can help. This chapter
    will cover how we can run large-scale models with large datasets. You will see
    different ways of computing large, distributed models.
  prefs: []
  type: TYPE_NORMAL
- en: There are different ways to distribute compute and data and achieve faster and
    better performance for large-scale training. Here, we are going to learn about
    a few techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism is widely used when there is a large volume of data that can
    be partitioned. We can run parallel computing to achieve better performance. CPU-based
    computing also performs well when scaled horizontally and vertically. The goal
    would be to process each partition and compute in groups, such as one partition,
    and then apply compute, and do that in parallel across all partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism is another area where you can scale the model training in
    deep learning modeling. Model parallelism is heavily compute based and, in most
    cases, GPU-based computing is needed to get better performance and time. In this
    chapter, we will look at some distributed training libraries available for us
    to use in the **Azure Machine** **Learning** service.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of distributed training: data and model parallelism.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed training with PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed training with TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can review all the code for this chapter at [https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering](https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering).
  prefs: []
  type: TYPE_NORMAL
- en: 'To access your workspace, recall the steps from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your workspace name from what has been created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the workspace user interface, on the left-hand side, click **Compute**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the **Compute** screen, select your last used compute instance and select
    **Start**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your compute instance will change from **Stopped** to **Starting**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous chapter, we cloned this book’s GitHub repository. If you have
    not already done so, continue to follow the steps provided. If you have already
    cloned the repository, skip to *step 9*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open the terminal on your compute instance. Note that the path will include
    your user in the directory. Type the following into your terminal to clone the
    sample notebooks into your working directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Clicking on the refresh icon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, create a compute cluster called `gpu-cluster` with two nodes and select
    one of the GPU’s available VMs, such as the NC6 or NC24 series.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data parallelism is widely used when there is a large volume of data that can
    be partitioned. We can run parallel computing to achieve better performance. CPU-based
    computing also performs well when scaled horizontally and vertically. The goal
    would be to process each partition and compute in groups, such as one partition,
    and then apply compute, and do that parallel across all partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model parallelism is another way to scale the model training in deep learning
    modeling. Model parallelism is heavily compute-based and, in most cases, GPU-based
    computing is needed to get better performance and time. Let’s look at some distributed
    training libraries available for us to use in the **Azure Machine** **Learning**
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Azure Machine Learning, we can perform distributed learning in various ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training with PyTorch**: PyTorch is one of the most well-known
    and widely used machine learning libraries for large-scale vision, text, and other
    unstructured data machine learning. It uses deep learning, such as convolutional
    neural network or recurrent neural network-based development. PyTorch is a deep
    learning framework developed by Meta (Facebook).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch implementations are very simple and easy to use and tend to eliminate
    the complications of other libraries in the marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed training with TensorFlow**: TensorFlow is a deep learning library
    created by Google. Given that the science is difficult, it was designed to make
    deep learning development simple and easy to implement. In the beginning stages,
    TensorFlow’s implementation was very difficult and required excessive lines of
    code. Another project called Keras was created to simplify this process; then,
    they were joined together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most current version is much more simple and easier to use compared to older
    versions. We have just covered the most popular frameworks used in the industry
    for distributed learning in the deep learning world.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Both of the aforementioned SDKs are being continuously developed and improved,
    and new functionality is always being added since the artificial intelligence
    field and the number of algorithms used are growing.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use PyTorch while performing deep learning
    model training before distributing that training within multiple cores and running
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we can write some simple PyTorch code that can be run in Azure
    Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to write code to perform distributed training
    using the PyTorch framework for vision-based deep learning algorithms. We will
    be using Python code to create the model and then train it with a compute cluster.
    All the code is available in this book’s GitHub repository for learning and execution
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training job Python file to process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a dataset while leveraging the user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com) and select your workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to **Compute** and click **Start** to start the compute instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for the compute instance to start; then, click **Jupyter** to start coding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t have a compute cluster, please follow the instructions in the previous
    chapters to create a new one. A compute instance with a CPU is good for development;
    we will use GPU-based content for model training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t have enough quotas for your GPU, please create a Service Ticket
    in the Azure portal to increase your quotas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, create a new folder for this chapter. I am calling mine `Chapter 11`. Also,
    create a subfolder called `PyTorchDistributed`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside, I am also creating a new directory for the `src` folder, where all the
    Python code training files will be stored. The `PyTorchDistributed` folder (`root`
    folder) will be used for submitting Python files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the terminal to run our Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to write our training code. So, navigate into the `src` folder
    and create a new text file called `train.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the sample code in this chapter, we will be using an open source dataset;
    it has no **Personally Identifiable Information** (**PII**) or privacy or legal
    issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s import all the libraries needed for the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Library imports](img/B18003_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Library imports
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must create the neural network architecture. A neural network architecture
    is what is used for training to create the brain. Depending on the accuracy required,
    you can build your network based on how many layers are needed. The neural network
    architecture is not the focus of this book, but there are a lot of resources available
    for designing one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Neural network architecture](img/B18003_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Neural network architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s write the training code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Training code](img/B18003_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Training code
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will evaluate the model metrics. Model evaluation is an important
    step in the training process as it validates model performance in terms of accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 – Evaluation code](img/B18003_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Evaluation code
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create a `main` function that will gather the data for the
    model, then invoke the `main` function and start to process the training code.
    Then, it will evaluate the model. Please refer to the following sample code for
    the details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – Sample main code](img/B18003_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Sample main code
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code that specifies the distributed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – Distributed dataset code](img/B18003_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Distributed dataset code
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the model is distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.7 – Model distribution code](img/B18003_11_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Model distribution code
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will create a `job.py` file that downloads the data needed for the
    experiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s create a dataset for further training processes. This dataset will
    invoke the compute cluster needed for the distributed training. The following
    sample code invokes the workspace and gets the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Job file dataset code](img/B18003_11_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Job file dataset code
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code parallelizes the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.9 – Job file invoking distributed training](img/B18003_11_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Job file invoking distributed training
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding screenshot, the code is distributed during model training,
    and this process is very simple. `PyTorchConfiguration`, along with `process_count`
    and `node_count`, are the configurations we must provide to distribute the model
    training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`PyTorchConfiguration` takes three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`communication_backend`: This can be set to `Nccl` or `Gloo`. `Nccl` is selected
    by default.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`process_count`: This parameter configures how many processes run inside the
    nodes for parallelization purposes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`node_count`: This is where we specify how many nodes to use for the job. `node`
    is based on how many cores are available. The higher the number of nodes, the
    faster the processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the job and wait for it to finish. Once the job has been submitted, navigate
    to your workspace’s user interface, click on **jobs**, and go to **details** to
    see how this works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.10 – Job output](img/B18003_11_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Job output
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to run distributed training using the PyTorch
    framework for a large dataset for custom vision-based deep learning modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are going to look at the TensorFlow framework and see how we can achieve
    distributed learning with a large dataset for custom vision-based deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to learn how to take large image files and build
    custom deep learning models such as object detection or image classification using
    TensorFlow. By doing so, we’ll learn how to distribute across multiple virtual
    machines to achieve faster performance for training.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training job Python file to process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to create a dataset that leverages the user interface:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://ml.azure.com](https://ml.azure.com) and select your workspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to **Compute** and click **Start** to start the compute instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wait for the compute instance to start; then, click **Jupyter** to start coding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t have a compute cluster, please follow the instructions in the previous
    chapters to create a new one. A compute instance with a CPU is good for development;
    we will use GPU-based content for model training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you don’t have enough quotas for your GPU, please create a Service Ticket
    in the Azure portal to increase your quotas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, create a new folder for this chapter. I am creating a folder called `Chapter
    11`. Then, create a subfolder called `TensorflowDistributed`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Inside, I am also creating a new directory for the `src` folder where all the
    Python code training files will be stored. `TensorflowDistributed` (root folder)
    will be used for submitting Python files. If the `TensorflowDistributed` folder
    doesn’t exist, please create one. Create the preceding folder under the `Chapter
    11` folder from *step 6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can use the terminal to run our Python code.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to write our training code. So, navigate to the `src` folder and
    create a new text file called `train.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the sample code in this chapter, we are using an open source dataset; it
    contains no PII and doesn’t have any privacy or legal issues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s import all the libraries needed for the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Library imports](img/B18003_11_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Library imports
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must perform dataset processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Dataset processing](img/B18003_11_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.12 – Dataset processing
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create a model. We will use the Keras library to simplify the neural
    network architecture. The layers depend on your use case and accuracy. I have
    seen large network architectures with low accuracy and too few layers produce
    poor results. So, we have to find the right balance in terms of layers through
    experimentation and build the neural network architecture from there:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Model neural network](img/B18003_11_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Model neural network
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create the `main` function, which will run the model training process
    in a distributed manner. The `main` function is where all the logic flow is tied
    together to get the training process working. As you can see, `tf.distribute`
    specifies the distribution strategies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – TensorFlow distribution code](img/B18003_11_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – TensorFlow distribution code
  prefs: []
  type: TYPE_NORMAL
- en: '`Tf.distribute.experimental.MultiWorkerMirroredStrategy` synchronously replicates
    all the variables and computation across the worker nodes to process. It mainly
    uses GPU (given the large-scale processing). The preceding implementation allows
    multiple workers to work together to achieve better performance to complete the
    training run faster.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will have to create some Python code called `job` in the root folder,
    which we will execute in a terminal window to execute the TensorFlow code in the
    command line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we must create an `environment.yaml` file. This will create the environment
    to run the model training. Here is the sample code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.15 – environment.yaml code](img/B18003_11_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – environment.yaml code
  prefs: []
  type: TYPE_NORMAL
- en: Next, we must create the `jobtensorflow.py` file, which uses an Azure Machine
    Learning SDK to configure the training process and then execute it once the job
    has been submitted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `code` section, specify a workspace environment to use and a training
    Python file to use for the experiment. There are a few changes you need to make
    to execute the code. The workspace, environment, and training Python files’ names
    can change depending on how you are implementing them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Sample environment and experiment](img/B18003_11_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Sample environment and experiment
  prefs: []
  type: TYPE_NORMAL
- en: Next, we must create some code that will set the distribution strategy and then
    invoke the training experiment. We can use the `TensorflowConfiguration` class
    to configure how to parallelize the training job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TensorflowConfiguration` takes two parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`worker_count`: The number of worker nodes used to parallelize. The default
    value is `1`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`parameter_server_count`: This parameter is set for a number of tasks to run
    the previous `worker_count`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Distribution strategy and job submission](img/B18003_11_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Distribution strategy and job submission
  prefs: []
  type: TYPE_NORMAL
- en: 'Wait for the experiment to run. This will take a few minutes to a few hours,
    depending on the dataset’s size. Once the experiment has finished running, navigate
    to your workspace’s user interface, go to the **job** section, and select the
    job to view its output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Job output](img/B18003_11_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Job output
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to create code that will run large-scale TensorFlow
    distributed training for a large custom vision-based deep learning model. The
    code is structured to run for a long time and report backlogs for us to check
    and validate. These jobs can be submitted as batch jobs so that we don’t have
    to keep watching what happens. Instead, we can submit the job and come back after
    a few hours to see how the model run performed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a lot of topics in this chapter. We learned how to create code
    to distribute PyTorch and TensorFlow deep learning models using the Azure Machine
    Learning service’s Python SDK. We also saw how easy and seamless it is to build
    code that performs in a timely fashion by distributing the model training with
    large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this chapter was to show you how to build seamless code that can
    execute large-scale models via batch processing without you having to watch them
    run. The Azure Machine Learning SDK allows us to submit the job and then come
    back later and check the output.
  prefs: []
  type: TYPE_NORMAL
- en: This is the last chapter of this book; I hope you had an amazing time reading
    and learning about Azure Machine Learning and how to build machine learning models.
    We would like to hear about your experience in applying machine learning or deep
    learning in your organization. Azure Machine Learning will make your journey simple
    and easy with open source in mind.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thank you so much for reading this book. This book will help you study for
    certifications such as AI 102 (AI Engineer – Training | Microsoft Learn: [https://learn.microsoft.com/en-us/certifications/roles/ai-engineer](https://learn.microsoft.com/en-us/certifications/roles/ai-engineer))
    and DP 100 (Exam DP-100: Designing and Implementing a Data Science Solution on
    Azure – Certifications | Microsoft Learn: [https://learn.microsoft.com/en-us/certifications/exams/dp-100](https://learn.microsoft.com/en-us/certifications/exams/dp-100)).'
  prefs: []
  type: TYPE_NORMAL
