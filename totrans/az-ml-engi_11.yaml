- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Using Distributed Training in AMLS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AMLS中使用分布式训练
- en: An interesting topic is how we can process large-scale datasets to train machine
    learning and deep learning models. For example, large-scale text-based mining,
    entity extraction, sentiments, and image or video-based, including image classification,
    image multiclassification, and object detection, are all very memory intensive
    and need large compute resources to process, which may take hours or sometimes
    days and weeks to complete.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的话题是我们如何处理大规模数据集以训练机器学习和深度学习模型。例如，大规模基于文本的挖掘、实体提取、情感分析，以及基于图像或视频的，包括图像分类、图像多分类和目标检测，都是非常内存密集型的，需要大量的计算资源来处理，可能需要数小时、有时甚至数周才能完成。
- en: In addition, if you have big data that contains business information and want
    to build machine learning models, then distributed learning can help. This chapter
    will cover how we can run large-scale models with large datasets. You will see
    different ways of computing large, distributed models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果您有包含业务信息的大数据，并想构建机器学习模型，那么分布式学习可以帮助您。本章将介绍我们如何使用大型数据集运行大规模模型。您将看到计算大型、分布式模型的不同方法。
- en: There are different ways to distribute compute and data and achieve faster and
    better performance for large-scale training. Here, we are going to learn about
    a few techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来分配计算和数据，以实现大规模训练的更快和更好性能。在这里，我们将了解一些技术。
- en: Data parallelism is widely used when there is a large volume of data that can
    be partitioned. We can run parallel computing to achieve better performance. CPU-based
    computing also performs well when scaled horizontally and vertically. The goal
    would be to process each partition and compute in groups, such as one partition,
    and then apply compute, and do that in parallel across all partitions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当有大量可以划分的数据时，数据并行被广泛使用。我们可以运行并行计算以获得更好的性能。基于CPU的计算在水平扩展和垂直扩展时也表现良好。目标是对每个分区和组进行计算，例如一个分区，然后应用计算，并在所有分区上并行执行。
- en: Model parallelism is another area where you can scale the model training in
    deep learning modeling. Model parallelism is heavily compute based and, in most
    cases, GPU-based computing is needed to get better performance and time. In this
    chapter, we will look at some distributed training libraries available for us
    to use in the **Azure Machine** **Learning** service.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行是您可以扩展深度学习建模中模型训练的另一个领域。模型并行高度依赖于计算，并且在大多数情况下，需要基于GPU的计算以获得更好的性能和时间。在本章中，我们将探讨可用于在**Azure
    Machine** **Learning**服务中使用的分布式训练库。
- en: 'There are two main types of distributed training: data and model parallelism.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练主要有两种类型：数据并行和模型并行。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Data parallelism
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行
- en: Model parallelism
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型并行
- en: Distributed training with PyTorch
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch进行分布式训练
- en: Distributed training with TensorFlow
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow进行分布式训练
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can review all the code for this chapter at [https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering](https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处查看本章所有代码的[https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering](https://github.com/PacktPublishing/Azure-Machine-Learning-Engineering)。
- en: 'To access your workspace, recall the steps from the previous chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问您的 workspace，请回想上一章中的步骤：
- en: Go to [https://ml.azure.com](https://ml.azure.com).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往[https://ml.azure.com](https://ml.azure.com)。
- en: Select your workspace name from what has been created.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从已创建的内容中选择您的工作空间名称。
- en: From the workspace user interface, on the left-hand side, click **Compute**.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从工作空间用户界面，在左侧，点击**Compute**。
- en: On the **Compute** screen, select your last used compute instance and select
    **Start**.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Compute**屏幕上，选择您最后使用的计算实例，并选择**Start**。
- en: Your compute instance will change from **Stopped** to **Starting**.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您的计算实例将从**Stopped**变为**Starting**。
- en: In the previous chapter, we cloned this book’s GitHub repository. If you have
    not already done so, continue to follow the steps provided. If you have already
    cloned the repository, skip to *step 9*.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一章中，我们已克隆了本书的GitHub仓库。如果您尚未这样做，请继续按照提供的步骤操作。如果您已经克隆了仓库，请跳转到*步骤9*。
- en: 'Open the terminal on your compute instance. Note that the path will include
    your user in the directory. Type the following into your terminal to clone the
    sample notebooks into your working directory:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的计算实例上打开终端。请注意，路径将包括您的用户目录。在您的终端中输入以下内容以将示例笔记本克隆到工作目录中：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Clicking on the refresh icon.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击刷新图标。
- en: Now, create a compute cluster called `gpu-cluster` with two nodes and select
    one of the GPU’s available VMs, such as the NC6 or NC24 series.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个名为`gpu-cluster`的计算集群，包含两个节点，并选择一个可用的GPU虚拟机，例如NC6或NC24系列。
- en: Review the notebooks in your `Azure-Machine-Learning-Engineering` directory.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查阅您`Azure-Machine-Learning-Engineering`目录中的笔记本。
- en: Data parallelism
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据并行
- en: Data parallelism is widely used when there is a large volume of data that can
    be partitioned. We can run parallel computing to achieve better performance. CPU-based
    computing also performs well when scaled horizontally and vertically. The goal
    would be to process each partition and compute in groups, such as one partition,
    and then apply compute, and do that parallel across all partitions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当有大量可以划分的数据时，数据并行被广泛使用。我们可以运行并行计算以获得更好的性能。当水平或垂直扩展时，基于CPU的计算也表现良好。目标是对每个分区和组进行计算，例如一个分区，然后应用计算，并在所有分区上并行执行。
- en: Model parallelism
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型并行
- en: Model parallelism is another way to scale the model training in deep learning
    modeling. Model parallelism is heavily compute-based and, in most cases, GPU-based
    computing is needed to get better performance and time. Let’s look at some distributed
    training libraries available for us to use in the **Azure Machine** **Learning**
    service.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行是另一种在深度学习建模中扩展模型训练的方法。模型并行主要基于计算，在大多数情况下，需要基于GPU的计算以获得更好的性能和时间。让我们看看在Azure
    Machine Learning服务中可用的分布式训练库。
- en: 'In Azure Machine Learning, we can perform distributed learning in various ways:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure Machine Learning中，我们可以以多种方式执行分布式学习：
- en: '**Distributed training with PyTorch**: PyTorch is one of the most well-known
    and widely used machine learning libraries for large-scale vision, text, and other
    unstructured data machine learning. It uses deep learning, such as convolutional
    neural network or recurrent neural network-based development. PyTorch is a deep
    learning framework developed by Meta (Facebook).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用PyTorch进行分布式训练**：PyTorch是用于大规模视觉、文本和其他非结构化数据机器学习最知名和最广泛使用的机器学习库之一。它使用深度学习，如卷积神经网络或循环神经网络进行开发。PyTorch是由Meta（Facebook）开发的深度学习框架。'
- en: PyTorch implementations are very simple and easy to use and tend to eliminate
    the complications of other libraries in the marketplace.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的实现非常简单且易于使用，并且倾向于消除市场上其他库的复杂性。
- en: '**Distributed training with TensorFlow**: TensorFlow is a deep learning library
    created by Google. Given that the science is difficult, it was designed to make
    deep learning development simple and easy to implement. In the beginning stages,
    TensorFlow’s implementation was very difficult and required excessive lines of
    code. Another project called Keras was created to simplify this process; then,
    they were joined together.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用TensorFlow进行分布式训练**：TensorFlow是由谷歌创建的深度学习库。鉴于科学研究的难度，它被设计成使深度学习开发变得简单且易于实现。在初期阶段，TensorFlow的实现非常困难，需要大量的代码行。随后，为了简化这一过程，创建了一个名为Keras的项目；然后，它们被合并在一起。'
- en: The most current version is much more simple and easier to use compared to older
    versions. We have just covered the most popular frameworks used in the industry
    for distributed learning in the deep learning world.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与旧版本相比，最新版本要简单得多，使用起来也更方便。我们刚刚介绍了在深度学习领域用于分布式学习的最流行的框架。
- en: Note
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Both of the aforementioned SDKs are being continuously developed and improved,
    and new functionality is always being added since the artificial intelligence
    field and the number of algorithms used are growing.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两种SDK都在持续开发和改进中，并且由于人工智能领域和算法数量的增长，总是会有新的功能被添加。
- en: Distributed training with PyTorch
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行分布式训练
- en: In this chapter, we will learn how to use PyTorch while performing deep learning
    model training before distributing that training within multiple cores and running
    it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在执行深度学习模型训练之前使用PyTorch，然后再在多个核心中分布训练并在其上运行。
- en: Let’s look at how we can write some simple PyTorch code that can be run in Azure
    Machine Learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何编写一些简单的PyTorch代码，这些代码可以在Azure Machine Learning中运行。
- en: Distributed training code
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式训练代码
- en: In this section, we will learn how to write code to perform distributed training
    using the PyTorch framework for vision-based deep learning algorithms. We will
    be using Python code to create the model and then train it with a compute cluster.
    All the code is available in this book’s GitHub repository for learning and execution
    purposes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何编写代码以使用 PyTorch 框架进行基于视觉的深度学习算法的分布式训练。我们将使用 Python 代码来创建模型，然后使用计算集群对其进行训练。所有代码都可在本书的
    GitHub 仓库中找到，用于学习和执行。
- en: Creating a training job Python file to process
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于处理的训练作业 Python 文件
- en: 'Follow these steps to create a dataset while leveraging the user interface:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建数据集，同时利用用户界面：
- en: Go to [https://ml.azure.com](https://ml.azure.com) and select your workspace.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 [https://ml.azure.com](https://ml.azure.com) 并选择你的工作区。
- en: Go to **Compute** and click **Start** to start the compute instance.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 **Compute** 并点击 **Start** 以启动计算实例。
- en: Wait for the compute instance to start; then, click **Jupyter** to start coding.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待计算实例启动；然后，点击 **Jupyter** 开始编码。
- en: If you don’t have a compute cluster, please follow the instructions in the previous
    chapters to create a new one. A compute instance with a CPU is good for development;
    we will use GPU-based content for model training.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有计算集群，请遵循前几章中的说明来创建一个新的集群。具有 CPU 的计算实例适合开发；我们将使用基于 GPU 的内容进行模型训练。
- en: If you don’t have enough quotas for your GPU, please create a Service Ticket
    in the Azure portal to increase your quotas.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你的 GPU 配额不足，请在 Azure 门户中创建一个服务票证来增加你的配额。
- en: Now, create a new folder for this chapter. I am calling mine `Chapter 11`. Also,
    create a subfolder called `PyTorchDistributed`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为这一章创建一个新文件夹。我将其命名为 `Chapter 11`。同时，创建一个名为 `PyTorchDistributed` 的子文件夹。
- en: Inside, I am also creating a new directory for the `src` folder, where all the
    Python code training files will be stored. The `PyTorchDistributed` folder (`root`
    folder) will be used for submitting Python files.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内部，我还在 `src` 文件夹中创建了一个新目录，其中将存储所有 Python 训练文件。`PyTorchDistributed` 文件夹（`root`
    文件夹）将用于提交 Python 文件。
- en: We can use the terminal to run our Python code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用终端来运行我们的 Python 代码。
- en: Now, we need to write our training code. So, navigate into the `src` folder
    and create a new text file called `train.py`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要编写我们的训练代码。因此，导航到 `src` 文件夹并创建一个名为 `train.py` 的新文本文件。
- en: For the sample code in this chapter, we will be using an open source dataset;
    it has no **Personally Identifiable Information** (**PII**) or privacy or legal
    issues.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本章的示例代码，我们将使用一个开源数据集；它没有 **个人身份信息**（**PII**）或隐私或法律问题。
- en: 'Let’s import all the libraries needed for the code:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入代码所需的全部库：
- en: '![Figure 11.1 – Library imports](img/B18003_11_001.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 库导入](img/B18003_11_001.jpg)'
- en: Figure 11.1 – Library imports
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 库导入
- en: 'Next, we must create the neural network architecture. A neural network architecture
    is what is used for training to create the brain. Depending on the accuracy required,
    you can build your network based on how many layers are needed. The neural network
    architecture is not the focus of this book, but there are a lot of resources available
    for designing one:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建神经网络架构。神经网络架构是用于训练以创建大脑的。根据所需的准确性，你可以根据所需的层数来构建你的网络。神经网络架构不是本书的重点，但有很多资源可用于设计一个：
- en: '![Figure 11.2 – Neural network architecture](img/B18003_11_002.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 神经网络架构](img/B18003_11_002.jpg)'
- en: Figure 11.2 – Neural network architecture
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 神经网络架构
- en: 'Now, let’s write the training code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们编写训练代码：
- en: '![Figure 11.3 – Training code](img/B18003_11_003.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 训练代码](img/B18003_11_003.jpg)'
- en: Figure 11.3 – Training code
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 训练代码
- en: 'Next, we will evaluate the model metrics. Model evaluation is an important
    step in the training process as it validates model performance in terms of accuracy:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将评估模型指标。模型评估是训练过程中的一个重要步骤，因为它验证了模型在准确性方面的性能：
- en: '![Figure 11.4 – Evaluation code](img/B18003_11_004.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 评估代码](img/B18003_11_004.jpg)'
- en: Figure 11.4 – Evaluation code
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 评估代码
- en: 'Next, we need to create a `main` function that will gather the data for the
    model, then invoke the `main` function and start to process the training code.
    Then, it will evaluate the model. Please refer to the following sample code for
    the details:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个 `main` 函数，该函数将收集模型所需的数据，然后调用 `main` 函数并开始处理训练代码。然后，它将评估模型。请参阅以下示例代码以获取详细信息：
- en: '![Figure 11.5 – Sample main code](img/B18003_11_005.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 样本主代码](img/B18003_11_005.jpg)'
- en: Figure 11.5 – Sample main code
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 示例主代码
- en: 'Here is the code that specifies the distributed dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是指定分布式数据集的代码：
- en: '![Figure 11.6 – Distributed dataset code](img/B18003_11_006.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 分布式数据集代码](img/B18003_11_006.jpg)'
- en: Figure 11.6 – Distributed dataset code
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 分布式数据集代码
- en: 'This is where the model is distributed:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型分布的地方：
- en: '![Figure 11.7 – Model distribution code](img/B18003_11_007.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – 模型分布代码](img/B18003_11_007.jpg)'
- en: Figure 11.7 – Model distribution code
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – 模型分布代码
- en: Next, we will create a `job.py` file that downloads the data needed for the
    experiment.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 `job.py` 文件，下载实验所需的数据。
- en: 'Now, let’s create a dataset for further training processes. This dataset will
    invoke the compute cluster needed for the distributed training. The following
    sample code invokes the workspace and gets the dataset:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个数据集，用于进一步的训练过程。此数据集将调用分布式训练所需的计算集群。以下示例代码调用工作区并获取数据集：
- en: '![Figure 11.8 – Job file dataset code](img/B18003_11_008.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 作业文件数据集代码](img/B18003_11_008.jpg)'
- en: Figure 11.8 – Job file dataset code
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 作业文件数据集代码
- en: 'The following code parallelizes the training process:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码并行化训练过程：
- en: '![Figure 11.9 – Job file invoking distributed training](img/B18003_11_009.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – 作业文件调用分布式训练](img/B18003_11_009.jpg)'
- en: Figure 11.9 – Job file invoking distributed training
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 作业文件调用分布式训练
- en: As shown in the preceding screenshot, the code is distributed during model training,
    and this process is very simple. `PyTorchConfiguration`, along with `process_count`
    and `node_count`, are the configurations we must provide to distribute the model
    training process.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前述截图所示，代码在模型训练期间进行分布，此过程非常简单。`PyTorchConfiguration`，连同 `process_count` 和 `node_count`，是我们必须提供的配置，以分布模型训练过程。
- en: '`PyTorchConfiguration` takes three parameters:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorchConfiguration` 拥有三个参数：'
- en: '`communication_backend`: This can be set to `Nccl` or `Gloo`. `Nccl` is selected
    by default.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`communication_backend`：这可以设置为 `Nccl` 或 `Gloo`。默认选择 `Nccl`。'
- en: '`process_count`: This parameter configures how many processes run inside the
    nodes for parallelization purposes.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`process_count`：此参数配置节点内并行化目的的进程数量。'
- en: '`node_count`: This is where we specify how many nodes to use for the job. `node`
    is based on how many cores are available. The higher the number of nodes, the
    faster the processing.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`node_count`：这是指定作业使用多少节点的地方。`node` 是基于可用的核心数量。节点数量越多，处理速度越快。'
- en: Run the job and wait for it to finish. Once the job has been submitted, navigate
    to your workspace’s user interface, click on **jobs**, and go to **details** to
    see how this works.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行作业并等待其完成。一旦提交作业，导航到你的工作区用户界面，点击 **作业**，然后转到 **详情** 以查看其工作原理。
- en: '![Figure 11.10 – Job output](img/B18003_11_010.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.10 – 作业输出](img/B18003_11_010.jpg)'
- en: Figure 11.10 – Job output
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 – 作业输出
- en: In this section, we learned how to run distributed training using the PyTorch
    framework for a large dataset for custom vision-based deep learning modeling.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用 PyTorch 框架运行分布式训练，以对大型数据集进行基于视觉的自定义深度学习建模。
- en: Now, we are going to look at the TensorFlow framework and see how we can achieve
    distributed learning with a large dataset for custom vision-based deep learning
    models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看 TensorFlow 框架，并了解如何使用大数据集实现自定义视觉深度学习模型的分布式学习。
- en: Distributed training with TensorFlow
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 的分布式训练
- en: In this section, we are going to learn how to take large image files and build
    custom deep learning models such as object detection or image classification using
    TensorFlow. By doing so, we’ll learn how to distribute across multiple virtual
    machines to achieve faster performance for training.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用 TensorFlow 构建自定义深度学习模型，如目标检测或图像分类，并处理大图像文件。通过这样做，我们将学习如何跨多个虚拟机分布，以实现训练性能的更快提升。
- en: Creating a training job Python file to process
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建用于处理的训练作业 Python 文件
- en: 'Follow these steps to create a dataset that leverages the user interface:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建利用用户界面的数据集：
- en: Go to [https://ml.azure.com](https://ml.azure.com) and select your workspace.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://ml.azure.com](https://ml.azure.com) 并选择你的工作区。
- en: Go to **Compute** and click **Start** to start the compute instance.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 **计算** 并点击 **启动** 以启动计算实例。
- en: Wait for the compute instance to start; then, click **Jupyter** to start coding.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待计算实例启动；然后，点击 **Jupyter** 开始编码。
- en: If you don’t have a compute cluster, please follow the instructions in the previous
    chapters to create a new one. A compute instance with a CPU is good for development;
    we will use GPU-based content for model training.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有计算集群，请遵循前几章中的说明来创建一个新的集群。具有 CPU 的计算实例适合开发；我们将使用基于 GPU 的内容进行模型训练。
- en: If you don’t have enough quotas for your GPU, please create a Service Ticket
    in the Azure portal to increase your quotas.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你没有足够的 GPU 配额，请在 Azure 门户中创建一个服务票证来增加你的配额。
- en: Now, create a new folder for this chapter. I am creating a folder called `Chapter
    11`. Then, create a subfolder called `TensorflowDistributed`.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为这一章创建一个新的文件夹。我正在创建一个名为 `Chapter 11` 的文件夹。然后，创建一个名为 `TensorflowDistributed`
    的子文件夹。
- en: Inside, I am also creating a new directory for the `src` folder where all the
    Python code training files will be stored. `TensorflowDistributed` (root folder)
    will be used for submitting Python files. If the `TensorflowDistributed` folder
    doesn’t exist, please create one. Create the preceding folder under the `Chapter
    11` folder from *step 6*.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内部，我还在 `src` 文件夹中创建了一个新的目录，所有训练文件都将存储在这里。`TensorflowDistributed`（根文件夹）将用于提交
    Python 文件。如果 `TensorflowDistributed` 文件夹不存在，请创建一个。从 *步骤 6* 开始，在 `Chapter 11` 文件夹下创建前面的文件夹。
- en: We can use the terminal to run our Python code.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用终端来运行我们的 Python 代码。
- en: Now, we need to write our training code. So, navigate to the `src` folder and
    create a new text file called `train.py`.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要编写我们的训练代码。因此，导航到 `src` 文件夹并创建一个名为 `train.py` 的新文本文件。
- en: For the sample code in this chapter, we are using an open source dataset; it
    contains no PII and doesn’t have any privacy or legal issues.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本章的示例代码，我们使用了一个开源数据集；它不包含任何个人身份信息（PII），也没有任何隐私或法律问题。
- en: 'Let’s import all the libraries needed for the code:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入代码所需的全部库：
- en: '![Figure 11.11 – Library imports](img/B18003_11_011.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.11 – 库导入](img/B18003_11_011.jpg)'
- en: Figure 11.11 – Library imports
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 – 库导入
- en: 'Next, we must perform dataset processing:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须执行数据集处理：
- en: '![Figure 11.12 – Dataset processing](img/B18003_11_012.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – 数据集处理](img/B18003_11_012.jpg)'
- en: Figure 11.12 – Dataset processing
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – 数据集处理
- en: 'Now, let’s create a model. We will use the Keras library to simplify the neural
    network architecture. The layers depend on your use case and accuracy. I have
    seen large network architectures with low accuracy and too few layers produce
    poor results. So, we have to find the right balance in terms of layers through
    experimentation and build the neural network architecture from there:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个模型。我们将使用 Keras 库来简化神经网络架构。层取决于你的用例和精度。我见过大型网络架构精度低且层数过少导致结果不佳。因此，我们必须通过实验找到层的正确平衡，并据此构建神经网络架构：
- en: '![Figure 11.13 – Model neural network](img/B18003_11_013.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.13 – 模型神经网络](img/B18003_11_013.jpg)'
- en: Figure 11.13 – Model neural network
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – 模型神经网络
- en: 'Now, let’s create the `main` function, which will run the model training process
    in a distributed manner. The `main` function is where all the logic flow is tied
    together to get the training process working. As you can see, `tf.distribute`
    specifies the distribution strategies:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 `main` 函数，它将以分布式方式运行模型训练过程。`main` 函数是所有逻辑流程汇聚在一起以使训练过程工作的地方。正如你所见，`tf.distribute`
    指定了分布策略：
- en: '![Figure 11.14 – TensorFlow distribution code](img/B18003_11_014.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.14 – TensorFlow 分布式代码](img/B18003_11_014.jpg)'
- en: Figure 11.14 – TensorFlow distribution code
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 – TensorFlow 分布式代码
- en: '`Tf.distribute.experimental.MultiWorkerMirroredStrategy` synchronously replicates
    all the variables and computation across the worker nodes to process. It mainly
    uses GPU (given the large-scale processing). The preceding implementation allows
    multiple workers to work together to achieve better performance to complete the
    training run faster.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tf.distribute.experimental.MultiWorkerMirroredStrategy` 同步地在工作节点之间复制所有变量和计算以进行处理。它主要使用
    GPU（考虑到大规模处理）。前面的实现允许多个工作节点协同工作以实现更好的性能，从而更快地完成训练运行。'
- en: Next, we will have to create some Python code called `job` in the root folder,
    which we will execute in a terminal window to execute the TensorFlow code in the
    command line.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在根文件夹中创建一些名为 `job` 的 Python 代码，我们将在终端窗口中执行这些代码以在命令行中执行 TensorFlow 代码。
- en: 'Now, we must create an `environment.yaml` file. This will create the environment
    to run the model training. Here is the sample code:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须创建一个 `environment.yaml` 文件。这将创建运行模型训练的环境。以下是示例代码：
- en: '![Figure 11.15 – environment.yaml code](img/B18003_11_015.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图11.15 – environment.yaml代码](img/B18003_11_015.jpg)'
- en: Figure 11.15 – environment.yaml code
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15 – environment.yaml代码
- en: Next, we must create the `jobtensorflow.py` file, which uses an Azure Machine
    Learning SDK to configure the training process and then execute it once the job
    has been submitted.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须创建`jobtensorflow.py`文件，该文件使用Azure Machine Learning SDK来配置训练过程，并在作业提交后执行它。
- en: 'In the `code` section, specify a workspace environment to use and a training
    Python file to use for the experiment. There are a few changes you need to make
    to execute the code. The workspace, environment, and training Python files’ names
    can change depending on how you are implementing them:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`code`部分，指定要使用的作业空间环境和用于实验的训练Python文件。您需要做出一些更改才能执行代码。作业空间、环境和训练Python文件的名字可能会根据您的实现方式而改变：
- en: '![Figure 11.16 – Sample environment and experiment](img/B18003_11_016.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16 – 示例环境和实验](img/B18003_11_016.jpg)'
- en: Figure 11.16 – Sample environment and experiment
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 – 示例环境和实验
- en: Next, we must create some code that will set the distribution strategy and then
    invoke the training experiment. We can use the `TensorflowConfiguration` class
    to configure how to parallelize the training job.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须编写一些代码来设置分布策略，然后调用训练实验。我们可以使用`TensorflowConfiguration`类来配置如何并行化训练任务。
- en: '`TensorflowConfiguration` takes two parameters, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorflowConfiguration`接受两个参数，如下所示：'
- en: '`worker_count`: The number of worker nodes used to parallelize. The default
    value is `1`.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`worker_count`：用于并行化的工作节点数量。默认值为`1`。'
- en: '`parameter_server_count`: This parameter is set for a number of tasks to run
    the previous `worker_count`:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`parameter_server_count`：此参数用于设置运行先前`worker_count`所需的任务数量：'
- en: '![Figure 11.17 – Distribution strategy and job submission](img/B18003_11_017.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图11.17 – 分布策略和作业提交](img/B18003_11_017.jpg)'
- en: Figure 11.17 – Distribution strategy and job submission
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 – 分布策略和作业提交
- en: 'Wait for the experiment to run. This will take a few minutes to a few hours,
    depending on the dataset’s size. Once the experiment has finished running, navigate
    to your workspace’s user interface, go to the **job** section, and select the
    job to view its output:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待实验运行。这需要几分钟到几小时，具体取决于数据集的大小。一旦实验完成运行，导航到您的工作空间用户界面，转到**作业**部分，选择作业以查看其输出：
- en: '![Figure 11.18 – Job output](img/B18003_11_018.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18 – 作业输出](img/B18003_11_018.jpg)'
- en: Figure 11.18 – Job output
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 – 作业输出
- en: In this section, you learned how to create code that will run large-scale TensorFlow
    distributed training for a large custom vision-based deep learning model. The
    code is structured to run for a long time and report backlogs for us to check
    and validate. These jobs can be submitted as batch jobs so that we don’t have
    to keep watching what happens. Instead, we can submit the job and come back after
    a few hours to see how the model run performed.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您学习了如何创建代码，该代码将运行大规模TensorFlow分布式训练，用于大型基于自定义视觉的深度学习模型。代码结构旨在长时间运行，并报告日志以便我们检查和验证。这些作业可以作为批量作业提交，这样我们就不必持续监视发生了什么。相反，我们可以提交作业，几小时后再回来查看模型运行的表现。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have covered a lot of topics in this chapter. We learned how to create code
    to distribute PyTorch and TensorFlow deep learning models using the Azure Machine
    Learning service’s Python SDK. We also saw how easy and seamless it is to build
    code that performs in a timely fashion by distributing the model training with
    large volumes of data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了众多主题。我们学习了如何使用Azure Machine Learning服务的Python SDK创建代码来分发PyTorch和TensorFlow深度学习模型。我们还看到了通过大量数据分布模型训练来构建代码的简便性和无缝性。
- en: The goal of this chapter was to show you how to build seamless code that can
    execute large-scale models via batch processing without you having to watch them
    run. The Azure Machine Learning SDK allows us to submit the job and then come
    back later and check the output.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是向您展示如何构建无缝的代码，通过批量处理执行大规模模型，而无需您监视其运行。Azure Machine Learning SDK允许我们提交作业，然后稍后回来检查输出。
- en: This is the last chapter of this book; I hope you had an amazing time reading
    and learning about Azure Machine Learning and how to build machine learning models.
    We would like to hear about your experience in applying machine learning or deep
    learning in your organization. Azure Machine Learning will make your journey simple
    and easy with open source in mind.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的最后一章；希望您在阅读和学习Azure机器学习以及如何构建机器学习模型的过程中度过了一段美好的时光。我们很乐意了解您在组织中应用机器学习或深度学习时的经验。考虑到开源，Azure机器学习将使您的旅程变得简单而轻松。
- en: 'Thank you so much for reading this book. This book will help you study for
    certifications such as AI 102 (AI Engineer – Training | Microsoft Learn: [https://learn.microsoft.com/en-us/certifications/roles/ai-engineer](https://learn.microsoft.com/en-us/certifications/roles/ai-engineer))
    and DP 100 (Exam DP-100: Designing and Implementing a Data Science Solution on
    Azure – Certifications | Microsoft Learn: [https://learn.microsoft.com/en-us/certifications/exams/dp-100](https://learn.microsoft.com/en-us/certifications/exams/dp-100)).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢您阅读这本书。这本书将帮助您为以下认证考试做准备，例如AI 102（AI工程师 – 训练 | 微软学习：[https://learn.microsoft.com/en-us/certifications/roles/ai-engineer](https://learn.microsoft.com/en-us/certifications/roles/ai-engineer)）和DP
    100（考试DP-100：在Azure上设计和实现数据科学解决方案 – 认证 | 微软学习：[https://learn.microsoft.com/en-us/certifications/exams/dp-100](https://learn.microsoft.com/en-us/certifications/exams/dp-100)）。
