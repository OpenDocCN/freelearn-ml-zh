- en: Chapter 4. Image Transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers the methods to change an image into an alternate representation
    of data in order to cover important problems of computer vision and image processing.
    Some examples of these methods are artifacts that are used to find image edges
    as well as transforms that help us find lines and circles in an image. In this
    chapter, we have covered stretch, shrink, warp, and rotate operations. A very
    useful and famous transform is Fourier, which transforms signals between the time
    domain and frequency domain. In OpenCV, you can find the **Discrete Fourier Transform**
    (**DFT**) and **Discrete Cosine Transform** (**DCT**). Another transform that
    we've covered in this chapter is related to integral images that allow rapid summing
    of sub regions, which is a very useful step in tracking faces algorithm. Besides
    this, you will also get to see distance transform and histogram equalization in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradients and sobel derivatives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Laplace and canny transforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The line and circle Hough transforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric transforms: stretch, shrink, warp, and rotate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integral images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distance transforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram equalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned a handful of transforms that
    will enable you to find edges, lines, and circles in images. Besides, you will
    be able to stretch, shrink, warp, and rotate images as well as you will be able
    to change the domain from the spatial domain to the frequency domain. Other important
    transforms used for face tracking will be covered in this chapter as well. Finally,
    distance transforms and histogram equalization will also be explored in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The Gradient and Sobel derivatives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A key building block in computer vision is finding edges and this is closely
    related to finding an approximation to derivatives in an image. From basic calculus,
    it is known that a derivative shows the variation of a given function or an input
    signal with some dimension. When we find the local maximum of the derivative,
    this will yield regions where the signal varies the most, which for an image might
    mean an edge. Hopefully, there's an easy way to approximate a derivative for discrete
    signals through a kernel convolution. A convolution basically means applying some
    transforms to every part of the image. The most used transform for differentiation
    is the Sobel filter [1], which works for horizontal, vertical, and even mixed
    partial derivatives of any order.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to approximate the value for the horizontal derivative, the following
    sobel kernel matrix is convoluted with an input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Gradient and Sobel derivatives](img/3972OS_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This means that, for each input pixel, the calculated value of its upper-right
    neighbor plus twice its right neighbor, plus its bottom-right neighbor, minus
    its upper-left neighbor, minus its left neighbor, minus its left-bottom neighbor
    will be calculated, yielding a resulting image. In order to use this operator
    in OpenCV, you can call Imgproc''s `Sobel` function according to the following
    signature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `src` parameter is the input image and `dst` is the output. `Ddepth` is
    the output image''s depth and when this is assigned as `-1`, this has the same
    depth as the source. The `dx` and `dy` parameters will inform us about the order
    in each of these directions. When setting `dy` to `0` and `dx` to `1`, the kernel
    that we''ve used is the one mentioned in the preceding matrix. The example project
    `kernels` from this chapter shows a customizable look of these operators, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Gradient and Sobel derivatives](img/3972OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The Laplace and Canny transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another quite useful operator to find edges is the Laplacian transformation.
    Instead of relying on the first order derivatives, OpenCV''s Laplacian transformation
    implements the discrete operator for the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Laplace and Canny transforms](img/3972OS_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The matrix can be approximated to the convolution with the following kernel
    when using finite difference methods and a 3x3 aperture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Laplace and Canny transforms](img/3972OS_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The signature for the preceding function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: While source and destination matrices are simple parameters, `ddepth` is the
    depth of the destination matrix. When you set this parameter to `-1`, it will
    have the same depth as the source image, although you might want more depth when
    you apply this operator. Besides this, there are overloaded versions of this method
    that receive an aperture size, a scale factor, and an adding scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides using the Laplacian method, you can also use the Canny algorithm, which
    is an excellent approach that was proposed by computer scientist John F. Canny,
    who optimized edge detection for low error rate, single identification, and correct
    localization. In order to fulfill it, the Canny algorithm applies a Gaussian to
    filter the noise, calculates intensity gradients through sobel, suppresses spurious
    responses, and applies double thresholds followed by a hysteresis that suppresses
    the weak and unconnected edges. For more information, check this paper [2]. The
    method''s signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `image` parameter is the input matrix, `edges` is the output image, `threshold1`
    is the first threshold for the hysteresis procedure (values smaller than this
    will be ignored), and `threshold2` is the high threshold for hysteresis (values
    higher than this will be considered as strong edges, while the smaller values
    and the ones higher than the low threshold will be checked for connection with
    strong edges). The aperture size is used for the Sobel operator when calculating
    the gradient and the `boolean` informs us which norm to use for the gradient.
    You can also check out the source code to use this operator in the kernel's project
    sample in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The line and circle Hough transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In case you need to find straight lines or circles in an image, you can use
    Hough transforms, as they are very useful. In this section, we will cover OpenCV
    methods to extract them from your image.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind the original Hough line transform is that any point in a binary
    image could be part of a set of lines. Suppose each straight line could be parameterized
    by the *y =* *mx + b* line equation, where *m* is the line slope and *b* is the
    *y* axis intercept of this line. Now, we could iterate the whole binary image,
    storing each of the *m* and *b* parameters and checking their accumulation. The
    local maximum points of the *m* and *b* parameters would yield equations of straight
    lines that mostly appeared in the image. Actually, instead of using the slope
    and *y* axis interception point, we use the polar straight line representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since OpenCV not only supports the standard Hough transform, but also the progressive
    probabilistic Hough transform for which the two functions are `Imgproc.HoughLines`
    and `Imgproc.HoughLinesP`, respectively. For detailed information, refer to [3].
    These functions'' signatures are explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hough` project from this chapter shows an example of the usage of them.
    The following is the code to retrieve lines from `Imgproc.HoughLines`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to apply the Hough transform over an edge image; therefore,
    the first two lines of the preceding code will take care of this. Then, the original
    image is cloned for display and a `Mat` object is created in the fourth line in
    order to keep the lines. In the last line, we can see the application of `HoughLines`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third parameter in `Imgproc.HoughLines` refers to the distance resolution
    of the accumulator in pixels, while the fourth parameter is the angle resolution
    of the accumulator in radians. The fifth parameter is the accumulator threshold,
    which means that only the lines with more than the specified amount of votes will
    be returned. The `lowThreshold` variable is tied to the scale slider in the example
    application for the user to experiment with it. It is important to observe that
    the lines are returned in the `lines` matrix, which has two columns in which each
    line returns the `rho` and `theta` parameters of the polar coordinates. These
    coordinates refer to the distance between the top-left corner of the image and
    the line rotation in radians, respectively. Following this example, you will find
    out how to draw the lines from the returned matrix. You can see the working of
    the Hough transform in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The line and circle Hough transforms](img/3972OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Besides having the standard Hough transform, OpenCV also offers a probabilistic
    Hough line transform as well as a circular version. Both the implementations are
    explored in the same `Hough` sample project, and the following screenshot shows
    the working of the circular version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The line and circle Hough transforms](img/3972OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Geometric transforms – stretch, shrink, warp, and rotate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While working with images and computer vision, it is very common that you will
    require the ability to preprocess an image using known geometric transforms, such
    as stretching, shrinking, rotation, and warping. The latter is the same as nonuniform
    resizing. These transforms can be realized through the multiplication of source
    points with a 2 x 3 matrix and they get the name of **affine transformations**
    while turning rectangles in parallelograms. Hence, they have the limitation of
    requiring the destination to have parallel sides. On the other hand, a 3 x 3 matrix
    multiplication represents perspective transforms. They offer more flexibility
    since they can map a 2D quadrilateral to another. The following screenshot shows
    a very useful application of this concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will find out which is the perspective transform that maps the side
    of a building in a perspective view to its frontal view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Geometric transforms – stretch, shrink, warp, and rotate](img/3972OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the input to this problem is the perspective photograph of the building,
    which is seen on the left-hand side of the preceding image, as well as the four
    corner points of the highlighted quadrilateral shape. The output is to the right
    and shows what a viewer would see if he/she looks at the side of the building.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since affine transforms are a subset of perspective transformations, we will
    focus on the latter ones here. The code available for this example is in the `warps`
    project of this chapter. The main method used here is `warpPerspective` from `Imgproc`.
    It applies a perspective transformation to an input image. Here is the method
    signature for the `warpPerspective` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Mat src` parameter is, naturally, the input image, which is the left-hand
    side image in the preceding screenshot, while `dst Mat` is the image on the right-hand
    side; make sure you initialize this parameter before using the method. The not-so-straightforward
    parameter here is `Mat M`, which is the warping matrix. In order to calculate
    it, you can use the `getPerspectiveTransform` method from `Imgproc` as well. This
    method will calculate the perspective matrix from two sets of the four correlated
    2D points, the source and destination points. In our example, the source points
    are the ones that are highlighted on the left-hand side of the screenshot, while
    the destination points are the four corner points of the image to the right. These
    points can be stored through the `MatOfPoint2f` class, which stores the `Point`
    objects. The `getPerspectiveTransform` method''s signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`Mat src` and `Mat dst` are the same as the `MatOfPoint2f` class mentioned
    previously, which is a subclass of `Mat`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we added a mouse listener to retrieve points clicked by the
    user. A detail to be kept in mind is that these points are stored in the order:
    top-left, top-right, bottom-left, and bottom-right. In the example application,
    the currently modified point can be chosen through four radio buttons above the
    images. The act of clicking and dragging listeners has been added to the code,
    so both approaches work.'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete Fourier Transform and Discrete Cosine Transform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with image analysis, it would be very useful if you could change
    an image from the spatial domain, which is the image in terms of its *x* and *y*
    coordinates, to the frequency domain—the image decomposed in its high and low
    frequency components—so that you would be able to see and manipulate frequency
    parameters. This could come in handy in image compression because it is known
    that human vision is not much sensitive to high frequency signals as it is to
    low frequency signals. In this way, you could transform an image from the spatial
    domain to the frequency domain and remove high frequency components, reducing
    the required memory to represent the image and hence compressing it. An image
    frequency can be pictured in a better way by the next image.
  prefs: []
  type: TYPE_NORMAL
- en: In order to change an image from the spatial domain to the frequency domain,
    the Discrete Fourier Transform can be used. As we might need to bring it back
    from the frequency domain to the spatial domain, another transform, which is the
    Inverse Discrete Fourier Transform, can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formal definition of DFT is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete Fourier Transform and Discrete Cosine Transform](img/3972OS_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `f(i,j)` value is the image in the spatial domain and `F(k,l)` is the image
    in the frequency domain. Note that `F(k,l)` is a complex function, which means
    that it has a real and an imaginary part. This way, it will be represented by
    two OpenCV `Mat` objects or by `Mat` with two channels. The easiest way to analyze
    a DFT is by plotting its magnitude and taking its logarithm, since values for
    the DFT can be in different orders of magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, this is a pulse pattern, which is a signal that can come from
    zero, represented as black, to the top, represented as white, on its left, and
    its Fourier transform magnitude with the applied logarithm to its right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete Fourier Transform and Discrete Cosine Transform](img/3972OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking back at the preceding DFT transform, we can think of `F(k,l)` as the
    value that would be yielded by multiplying each point of the spatial image with
    a base function, which is related to the frequency domain, and by summing the
    products. Remember that base functions are sinusoidal and they have increasing
    frequencies. This way, if some of the base functions oscillate at the same rate
    as the signal, it will be able to sum up to a big number, which will be seen as
    a white dot on the Fourier Transform image. On the other hand, if the given frequency
    is not present in the image, the oscillation and multiplication with the image
    will result in a small number, which won't be noticed in the Fourier Transform
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another thing to observe from the equation is that `F(0,0)` will yield a base
    function that is always `1`. This way, `F(0,0)` will simply refer to the sum of
    all the pixels of the spatial image. We can also check whether `F(N-1, N-1)` corresponds
    to the base function related to the highest frequency in the image. Note that
    the previous image basically has a DC component, which would be the image mean
    and it could be checked from the white dot in the middle of the Discrete Fourier
    transform image. Besides, the image to the left could be seen as a series of pulses
    and hence it would have a frequency in the *x* axis, which can be noticed by the
    two dots near the central point in the Fourier Transform image to the right. Nonetheless,
    we will need to use multiple frequencies to approximate the pulse shape. In this
    way, more dots can be seen in the *x*-axis of the image to the right. The following
    screenshot gives more insight and helps you understand the Fourier analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Discrete Fourier Transform and Discrete Cosine Transform](img/3972OS_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will again check the DC level at the center of the DFT image, to the
    right, as a bright central dot. Besides, we can also check multiple frequencies
    in a diagonal pattern. An important piece of information that can be retrieved
    is the direction of spatial variation, which is clearly seen as bright dots in
    the DFT image.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is time to work on some code now. The following code shows you how to make
    room to apply the DFT. Remember, from the preceding screenshot, that the result
    of a DFT is complex. Besides, we need them stored as floating point values. This
    way, we first convert our 3-channel image to gray and then to a float. After this,
    we put the converted image and an empty `Mat` object into a list of mats, combining
    them into a single `Mat` object through the use of the `Core.merge` function,
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s easy to apply an in-place Discrete Fourier Transform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In order to get some meaningful information, we will print the image, but first,
    we have to obtain its magnitude. In order to get it, we will use the standard
    way that we learned in school, which is getting the square root of the sum of
    the squares of the real and complex parts of numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, OpenCV has a function for this, which is `Core.magnitude`, whose signature
    is `magnitude(Mat x, Mat y, Mat magnitude)`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Before using `Core.magnitude`, just pay attention to the process of unpacking
    a DFT in the splitted mats using `Core.split`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the values can be in different orders of magnitude, it is important to
    get the values in a logarithmic scale. Before doing this, it is important to add
    `1` to all the values in the matrix just to make sure we won''t get negative values
    when applying the `log` function. Besides this, there''s already an OpenCV function
    to deal with logarithms, which is `Core.log`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to shift the image to the center, so that it''s easier to analyze
    its spectrum. The code to do this is simple and goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As a last step, it''s important to normalize the image, so that it can be seen
    in a better way. Before we normalize it, it should be converted to CV_8UC1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When using the DFT, it's often enough to calculate only half of the DFT when
    you deal with real-valued data, as is the case with images. This way, an analog
    concept called the Discrete Cosine Transform can be used. In case you want it,
    it can be invoked through `Core.dct`.
  prefs: []
  type: TYPE_NORMAL
- en: Integral images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Some face recognition algorithms, such as OpenCV''s face detection algorithm
    make heavy use of features like the ones shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These are the so-called Haar-like features and they are calculated as the sum
    of pixels in the white area minus the sum of pixels in the black area. You might
    find this type of a feature kind of odd, but when training it for face detection,
    it can be built to be an extremely powerful classifier using only two of these
    features, as depicted in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In fact, a classifier that uses only the two preceding features can be adjusted
    to detect 100 percent of a given face training database with only 40 percent of
    false positives. Taking out the sum of all pixels in an image as well as calculating
    the sum of each area can be a long process. However, this process must be tested
    for each frame in a given input image, hence calculating these features fast is
    a requirement that we need to fulfill.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define an integral image sum as the following expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance, if the following matrix represents our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'An integral image would be like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The trick here follows from the following property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Integral images](img/3972OS_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This means that in order to find the sum of a given rectangle bounded by the
    points `(x1,y1)`, `(x2,y1)`, `(x2,y2)`, and `(x1,y2)`, you just need to use the
    integral image at the point `(x2,y2)`, but you also need to subtract the points
    `(x1-1,y2)` from `(x2,y1-1)`. Also, since the integral image at `(x1-1, y1-1)`
    has been subtracted twice, we just need to add it once.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will generate the preceding matrix and make use of `Imgproc.integral`
    to create the integral images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output of this program is like the one shown in the preceding matrices for
    A and Sum A.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to verify that the output is a 4 x 4 matrix because of the initial
    row and column of zeroes, which are used to make the computation efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Distance transforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Simply put, a distance transform applied to an image will generate an output
    image whose pixel values will be the closest distance to a zero-valued pixel in
    the input image. Basically, they will have the closest distance to the background,
    given a specified distance measure. The following screenshot gives you an idea
    of what happens to the silhouette of a human body:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance transforms](img/3972OS_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Human silhouette by J E Theriot
  prefs: []
  type: TYPE_NORMAL
- en: This transform can be very useful in the process of getting the topological
    skeleton of a given segmented image as well as to produce blurring effects. Another
    interesting application of this transform is in the segmentation of overlapping
    objects, along with a watershed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, the distance transform is applied to an edge image, which results
    from a Canny filter. We are going to make use of Imgproc''s `distanceTransform`
    method, which can be seen in action in the `distance` project, which you can find
    in this chapter''s source code. Here are the most important lines of this example
    program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Firstly, a Canny edge detector filter is applied to the input image. Then, a
    threshold with `THRESH_BINARY_INV` converts the edges to black and beans to white.
    Only then, the distance transform is applied. The first argument is the input
    image, the second one is the output matrix, and the third argument specifies how
    distances are calculated. In our example, `CVDIST_L2` means Euclidean, while other
    distances, such as `CVDIST_L1` or `CVDIST_L12`, among others exist. Since the
    output of `distanceTtransform` is a single channel 32 bit Float image, a conversion
    is required. Finally, we apply `Core.multiply` to increase the contrast.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot gives you a good idea of the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Distance transforms](img/3972OS_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Histogram equalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The human visual system is very sensitive to contrast in images, which is the
    difference in the color and brightness of different objects. Besides, the human
    eye is a miraculous system that can feel intensities at the 10[16] light levels
    [4]. No wonder some sensors could mess up the image data.
  prefs: []
  type: TYPE_NORMAL
- en: When analyzing images, it is very useful to draw their histograms. They simply
    show you the lightness distribution of a digital image. In order to do that, you
    need to count the number of pixels with the exact lightness and plot that as a
    distribution graph. This gives us a great insight into the dynamic range of an
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a camera picture has been captured with a very narrow light range, it
    gets difficult to see the details in the shadowed areas or other areas with poor
    local contrast. Fortunately, there''s a technique to spread frequencies for uniform
    intensity distribution, which is called **histogram equalization**. The following
    image shows the same picture with their respective histograms before and after
    the histogram equalization technique is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram equalization](img/3972OS_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the light values, located at the rightmost part of the upper histogram,
    are rarely used, while the middle range values are too tied. Spreading the values
    along the full range yields better contrast and details can be more easily perceived
    by this. The histogram equalized image makes better use of intensities that generate
    better contrast. In order to accomplish this task, a cumulative distribution can
    be used to remap the histogram to something that resembles a uniform distribution.
    Then, it's just a matter of checking where the points from the original histogram
    would be mapped to the uniform distribution through the use of a cumulative Gaussian
    distribution, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the good part is that all these details have been wrapped in a simple
    call to OpenCV''s `equalizeHist` function. Here is the sample from the `histogram`
    project in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This piece of code simply converts the image to a single channel image; however,
    you can use `equalizeHist` on a color image as long as you treat each channel
    separately. The `Imgproc.equalizeHist` method outputs the corrected image following
    the previously mentioned concept.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*A 3x3 Isotropic Gradient Operator for Image Processing* presented at a talk
    at the Stanford Artificial Project in 1968, by I. Sobel and G. Feldman.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*A Computational Approach To Edge Detection*, IEEE Trans. Pattern Analysis
    and Machine Intelligence, by Canny, J.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Robust Detection of Lines Using the Progressive Probabilistic Hough Transform*,
    CVIU 78 1, by Matas, J. and Galambos, C., and Kittler, J.V. pp 119-137 (2000).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Advanced High Dynamic Range Imaging: Theory and Practice*, CRC Press, by Banterle,
    Francesco; Artusi, Alessandro; Debattista, Kurt; Chalmers, Alan.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered the key aspects of computer vision's daily use. We started
    with the important edge detectors, where you gained the experience of how to find
    them through the Sobel, Laplacian, and Canny edge detectors. Then, we saw how
    to use the Hough transforms to find straight lines and circles. After that, the
    geometric transforms stretch, shrink, warp, and rotate were explored with an interactive
    sample. We then explored how to transform images from the spatial domain to the
    frequency domain using the Discrete Fourier analysis. After that, we showed you
    a trick to calculate Haar-like features fast in an image through the use of integral
    images. We then explored the important distance transforms and finished the chapter
    by explaining histogram equalization to you.
  prefs: []
  type: TYPE_NORMAL
- en: Now, be ready to dive into machine learning algorithms, as we will cover how
    to detect faces in the next chapter. Also, you will learn how to create your own
    object detector and understand how supervised learning works in order to better
    train your classification trees.
  prefs: []
  type: TYPE_NORMAL
