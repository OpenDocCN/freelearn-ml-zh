<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Working with Image Alignment and Stitching"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Working with Image Alignment and Stitching</h1></div></div></div><p>One limitation of cameras is the limited field of view, often shortened to FOV. Field-of-view is the parameter that defines how much information can be captured in one frame obtained by the camera. So, to capture an image that requires a larger field-of-view, we use image stitching. Image stitching is a method of joining multiple images to form a bigger image that represents the information that is consistent with the original images.</p><p>In this chapter, we will take a look at the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Image stitching</li><li class="listitem" style="list-style-type: disc">Image alignment</li><li class="listitem" style="list-style-type: disc">Video stabilization</li><li class="listitem" style="list-style-type: disc">Stereo vision</li></ul></div><div class="section" title="Image stitching"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/>Image stitching</h1></div></div></div><p>There has been <a id="id313" class="indexterm"/>a lot of work in image stitching over the years, but we will take a look at the algorithm OpenCV implements internally. Most of it was proposed by Michael Brown and David Lowe. Image stitching<a id="id314" class="indexterm"/> is done in the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Find suitable features and match them reliably across the set of images to obtain the relative positioning.</li><li class="listitem">Develop the geometry to choose reliable features that are invariant to rotation, scale, and illumination.</li><li class="listitem">Match images using the RANSAC algorithm and a probabilistic model for verification.</li><li class="listitem">Align the matched images.</li><li class="listitem">Render the results to obtain a panoramic image. We use automatic straightening, gain compensation, and multi-band blending to achieve a seamlessly stitched panoramic image, as shown here:<div class="mediaobject"><img src="graphics/B02052_06_01.jpg" alt="Image stitching"/></div></li></ol></div><div class="section" title="Feature detection and matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec49"/>Feature detection and matching</h2></div></div></div><p>First, we find <a id="id315" class="indexterm"/>and match SIFT features between all the images. By doing this, we get the scale and orientation associated with each feature point. With these details, we can form a similarity invariant matrix, where we can make appropriate measurements for calculations. We accumulate local gradients in the orientation histograms to obtain such a frame. By implementing such an algorithm, edges can shift slightly without modifying the descriptor values, thereby providing small levels of affine and shift invariances. The algorithm also proposes to achieve the illumination invariance using gradients to eliminate bias and normalizes the descriptor vector to eliminate the gain.</p><p>The algorithm also makes the assumption that a camera only rotates about its optical center. Due to this assumption, we can define rotations along the three primary axes, <span class="emphasis"><em>x</em></span>, <span class="emphasis"><em>y</em></span>, and <span class="emphasis"><em>z</em></span>, as <span class="inlinemediaobject"><img src="graphics/B02052_06_11.jpg" alt="Feature detection and matching"/></span>, and <span class="inlinemediaobject"><img src="graphics/B02052_06_12.jpg" alt="Feature detection and matching"/></span>, respectively. We define a vector <span class="emphasis"><em>θ</em></span>, as <span class="inlinemediaobject"><img src="graphics/B02052_06_13.jpg" alt="Feature detection and matching"/></span>. We also use the focal length, <span class="emphasis"><em>f</em></span> as a parameter. Thus, we get the pairwise homographies as <span class="inlinemediaobject"><img src="graphics/B02052_06_14.jpg" alt="Feature detection and matching"/></span>.</p><div class="mediaobject"><img src="graphics/B02052_06_15.jpg" alt="Feature detection and matching"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B02052_06_16.jpg" alt="Feature detection and matching"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_06_17.jpg" alt="Feature detection and matching"/></span> are the <a id="id316" class="indexterm"/>homographic image positions. <span class="inlinemediaobject"><img src="graphics/B02052_06_18.jpg" alt="Feature detection and matching"/></span> is the image position in a 2-dimensional space:</p><div class="mediaobject"><img src="graphics/B02052_06_19.jpg" alt="Feature detection and matching"/></div><p>The values of <span class="inlinemediaobject"><img src="graphics/B02052_06_20.jpg" alt="Feature detection and matching"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_06_21.jpg" alt="Feature detection and matching"/></span> are defined as follows:</p><div class="mediaobject"><img src="graphics/B02052_06_22.jpg" alt="Feature detection and matching"/></div><div class="mediaobject"><img src="graphics/B02052_06_23.jpg" alt="Feature detection and matching"/></div><p>As you can see, this representation of R is consistent with the exponential form of depicting rotations. We have included provisions to allow small changes in positions. Hence, we have the following result:</p><div class="mediaobject"><img src="graphics/B02052_06_24.jpg" alt="Feature detection and matching"/></div><div class="mediaobject"><img src="graphics/B02052_06_25.jpg" alt="Feature detection and matching"/></div><p><span class="inlinemediaobject"><img src="graphics/B02052_06_26.jpg" alt="Feature detection and matching"/></span> represents the affine transformation of an image obtained by calculating linear homography of <span class="inlinemediaobject"><img src="graphics/B02052_06_27.jpg" alt="Feature detection and matching"/></span>.</p><p>After detecting <a id="id317" class="indexterm"/>features in all the images, we need to match them to find their relative arrangements. For this, we match the overlapping features using the k-nearest neighbors (with <span class="emphasis"><em>k = 4</em></span>) in the feature space to obtain overlapping features. This method is employed to take into consideration the fact that each feature may overlap in more than one image.</p></div><div class="section" title="Image matching"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec50"/>Image matching</h2></div></div></div><p>By now, we <a id="id318" class="indexterm"/>have obtained the features and have the matches between features. Now we <a id="id319" class="indexterm"/>need to obtain the matching images to form the panorama. To form a panorama, we need a small number of images to match any image, so as to find adjacent images. The algorithm suggests the use of six matching images to the current image. This section is performed in two parts. First, we estimate the homography with which the two frames are compatible and we find a set of inliers for the same. For this, we use the RANSAC algorithm. Then we use a probabilistic model to verify the match between the images.</p><div class="section" title="Homography estimation using RANSAC"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec15"/>Homography estimation using RANSAC</h3></div></div></div><p>The <a id="id320" class="indexterm"/>RANSAC algorithm, short for Random Sample Consensus, is an algorithm that uses a small set of randomly chosen matches in images to estimate the image transformation parameters. For image stitching, we use four feature matches to compute the homography between them. For this, the algorithm proposes the use of the direct linear transformation method described by R. Hartley and A. Zisserman. This is performed for 500 iterations and ultimately, the solution with the maximum number of <span class="emphasis"><em>inliers</em></span> is chosen. Inliers are those features whose linear projections are consistent with the homography, H, up to a specified tolerance value for pixels. By performing probability calculations, it was found that the probability of finding a match is very high. For example, if inliers between images match with a probability of 0.5, the probability of not finding the homography is <span class="inlinemediaobject"><img src="graphics/B02052_06_28.jpg" alt="Homography estimation using RANSAC"/></span>. Hence, RANSAC is quite successful at estimating H. This method is called the maximum likelihood estimation.</p></div><div class="section" title="Verification of image matches using a probabilistic model"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec16"/>Verification of image matches using a probabilistic model</h3></div></div></div><p>By the<a id="id321" class="indexterm"/> model obtained till now, we have a set of feature matches within the overlap region (inliers), and some features within the area of overlap that do not match (outliers). Using a probabilistic model, we will verify that the obtained set of inliers and outliers produces a valid image match. The algorithm makes the assumption that the probability of the <span class="inlinemediaobject"><img src="graphics/B02052_06_29.jpg" alt="Verification of image matches using a probabilistic model"/></span> feature matching is an independent Bernoulli trial. The two equations that are obtained from this are shown as follows:</p><div class="mediaobject"><img src="graphics/B02052_06_30.jpg" alt="Verification of image matches using a probabilistic model"/></div><div class="mediaobject"><img src="graphics/B02052_06_31.jpg" alt="Verification of image matches using a probabilistic model"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B02052_06_32.jpg" alt="Verification of image matches using a probabilistic model"/></span> represents the total number of features present in the overlap area. <span class="inlinemediaobject"><img src="graphics/B02052_06_33.jpg" alt="Verification of image matches using a probabilistic model"/></span> represents the total number of inliers. <span class="emphasis"><em>m</em></span> specifies whether the two images have been matched <a id="id322" class="indexterm"/>correctly or not. <span class="inlinemediaobject"><img src="graphics/B02052_06_35.jpg" alt="Verification of image matches using a probabilistic model"/></span> is the probability of the feature being an inlier, given a correct image match. <span class="inlinemediaobject"><img src="graphics/B02052_06_36.jpg" alt="Verification of image matches using a probabilistic model"/></span> is the probability that the feature is not an inlier, given a correct image match. <span class="inlinemediaobject"><img src="graphics/B02052_06_37.jpg" alt="Verification of image matches using a probabilistic model"/></span> represents the set of feature matches <span class="inlinemediaobject"><img src="graphics/B02052_06_38.jpg" alt="Verification of image matches using a probabilistic model"/></span>. <span class="emphasis"><em>B</em></span> represents the binomial distribution, as shown here:</p><div class="mediaobject"><img src="graphics/B02052_06_39.jpg" alt="Verification of image matches using a probabilistic model"/></div><p>For the purpose of this algorithm, the values of <span class="inlinemediaobject"><img src="graphics/B02052_06_40.jpg" alt="Verification of image matches using a probabilistic model"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_06_41.jpg" alt="Verification of image matches using a probabilistic model"/></span> are set to 0.6 and 0.1, respectively. Using Bayes rule, we can calculate the probability of an image match being valid as:</p><div class="mediaobject"><img src="graphics/B02052_06_42.jpg" alt="Verification of image matches using a probabilistic model"/></div><p>An image match is considered to be valid if the value of the preceding expression is greater than a pre-chosen minimum probability. The algorithm suggests the use of <span class="inlinemediaobject"><img src="graphics/B02052_06_43.jpg" alt="Verification of image matches using a probabilistic model"/></span> and <span class="inlinemediaobject"><img src="graphics/B02052_06_44.jpg" alt="Verification of image matches using a probabilistic model"/></span>. The match is accepted if the following equation is satisfied, and rejected otherwise:</p><div class="mediaobject"><img src="graphics/B02052_06_45.jpg" alt="Verification of image matches using a probabilistic model"/></div><p>A condition <a id="id323" class="indexterm"/>that arises from the assumption made earlier is that for a valid image match the following equation must be satisfied:</p><div class="mediaobject"><img src="graphics/B02052_06_46.jpg" alt="Verification of image matches using a probabilistic model"/></div><p>In the original paper, the authors also proposed a method by which the parameters can be learnt from the images rather than assigning fixed values to them.</p></div></div><div class="section" title="Bundle adjustment"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec51"/>Bundle adjustment</h2></div></div></div><p>Brown <a id="id324" class="indexterm"/>and Lowe's algorithm proposes the use of bundle adjustment to obtain all the camera parameters, jointly for a given set of matches between the<a id="id325" class="indexterm"/> images. For this, images are added to a bundle adjuster in decreasing order of the number of feature matches. Each time, the new image is initialized with the rotation and focal length of the image to which it matched. Then we use the Levenberg-Marquadt algorithm to update the camera parameters. The Levenberg-Marquadt algorithm is generally used to solve non-linear least squares problems in curve fitting problems.</p><p>This algorithm tries to minimize the sum of the squared projection errors. For this, each feature is projected on to every other image to which the original image matches, and then the sum of the squared distances is minimized with respect to the camera parameters. If the <span class="inlinemediaobject"><img src="graphics/B02052_06_47.jpg" alt="Bundle adjustment"/></span> feature in one image matches the <span class="inlinemediaobject"><img src="graphics/B02052_06_48.jpg" alt="Bundle adjustment"/></span> feature in another, we obtain the residual for the projection as follows:</p><div class="mediaobject"><img src="graphics/B02052_06_49.jpg" alt="Bundle adjustment"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B02052_06_50.jpg" alt="Bundle adjustment"/></span> represents the <span class="inlinemediaobject"><img src="graphics/B02052_06_51.jpg" alt="Bundle adjustment"/></span> feature in the <span class="inlinemediaobject"><img src="graphics/B02052_06_52.jpg" alt="Bundle adjustment"/></span> image, <span class="inlinemediaobject"><img src="graphics/B02052_06_53.jpg" alt="Bundle adjustment"/></span> is the residual after the projection of the <span class="inlinemediaobject"><img src="graphics/B02052_06_54.jpg" alt="Bundle adjustment"/></span> feature from <span class="inlinemediaobject"><img src="graphics/B02052_06_55.jpg" alt="Bundle adjustment"/></span> image on the <span class="inlinemediaobject"><img src="graphics/B02052_06_56.jpg" alt="Bundle adjustment"/></span> image, and <span class="inlinemediaobject"><img src="graphics/B02052_06_57.jpg" alt="Bundle adjustment"/></span> is the projection of <span class="inlinemediaobject"><img src="graphics/B02052_06_58.jpg" alt="Bundle adjustment"/></span> from the <span class="inlinemediaobject"><img src="graphics/B02052_06_59.jpg" alt="Bundle adjustment"/></span> image on the <span class="inlinemediaobject"><img src="graphics/B02052_06_60.jpg" alt="Bundle adjustment"/></span> image.</p><p>Then, the <a id="id326" class="indexterm"/>error function is calculated by summing up all <a id="id327" class="indexterm"/>the robustified residual field errors, over all the features, spanning all the images. For this robustification, the Huber robust error function is used:</p><div class="mediaobject"><img src="graphics/B02052_06_61.jpg" alt="Bundle adjustment"/></div><p>On solving this, we get a non-linear equation, which is solved using the Levenberg-Marquadt algorithm, to estimate the values of the camera parameters.</p></div><div class="section" title="Automatic panoramic straightening"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec52"/>Automatic panoramic straightening</h2></div></div></div><p>So far, the<a id="id328" class="indexterm"/> algorithm has been able to successfully find matches between images and able to stitch them together. However, there <a id="id329" class="indexterm"/>still exists an unknown 3D rotation component, which causes the panorama to be formed in a wave-like output, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B02052_06_02.jpg" alt="Automatic panoramic straightening"/></div><p>This arises mainly due to the fact that the camera would not have been perfectly level while clicking the multiple images.</p><p>This is solved by taking a heuristic into consideration regarding the way people click panoramic images. It is assumed that it is highly unlikely for a user to rotate the camera while clicking the image, so the camera vectors generally lie on the same plane. So, we try to find the null vector of the covariance matrix of the camera vectors and the vector normal to the plane of the center and horizon. This way, we can then apply the rotation on the images to effectively remove the wavy effect.</p><div class="mediaobject"><img src="graphics/B02052_06_03.jpg" alt="Automatic panoramic straightening"/></div></div><div class="section" title="Gain compensation"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec53"/>Gain compensation</h2></div></div></div><p>Gain<a id="id330" class="indexterm"/> is the <a id="id331" class="indexterm"/>camera parameter that describes the sensitivity of the image to light. Different images could have been clicked at different levels of gain. To overcome this situation, we make use of gain compensation, as shown here:</p><div class="mediaobject"><img src="graphics/B02052_06_04.jpg" alt="Gain compensation"/></div><p>Gain compensation refers to the normalization of the gain in images to facilitate a seamlessly stitched image. The method used is similar to the one used to compute the camera parameters. The error function used here is the sum of the errors in gain-normalized intensities for all the overlapping pixels:</p><div class="mediaobject"><img src="graphics/B02052_06_05.jpg" alt="Gain compensation"/></div></div><div class="section" title="Multi-band blending"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec54"/>Multi-band blending</h2></div></div></div><p>Even after <a id="id332" class="indexterm"/>gain compensation, the stitching doesn't appear to be seamless. We need to apply a good blending algorithm to join the images without<a id="id333" class="indexterm"/> it being noticeable that the image has been stitched from multiple images.</p><p>For this, we apply a good blending strategy. We choose a blending algorithm in which we assign a weight function to each image. This weight function varies linearly with weight = 1 at the center and weight = 0 at the edges. This weight function is also extended to a spherical coordinate system. A simple weighted sum of the intensities along each ray can be calculated using these weight functions, but this would cause high frequency areas to be blurred out.</p><p>Due to this, we need to implement multi-band blending. The multi-band blending blends low frequency regions over a large area, where it blends high frequency regions over a relatively smaller area. We assign weights to each image, using <span class="inlinemediaobject"><img src="graphics/B02052_06_62.jpg" alt="Multi-band blending"/></span>, such that the value of <span class="inlinemediaobject"><img src="graphics/B02052_06_63.jpg" alt="Multi-band blending"/></span> is 1 where there is maximum weight in the image and 0 where the maximum weight for the region is from some other image. We then successively blur out these weight graphs to ultimately get the blending weights for each band.</p><p>Then we linearly combine the overlapping images for each band with respect to the blend weights. The amount of blurring depends on the frequency of the band. This results in the high frequency bands being blended over short regions, while the low frequency bands get blended over large regions:</p><div class="mediaobject"><img src="graphics/B02052_06_06.jpg" alt="Multi-band blending"/></div></div><div class="section" title="Image stitching using OpenCV"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec55"/>Image stitching using OpenCV</h2></div></div></div><p>The following <a id="id334" class="indexterm"/>is the image stitching pipeline:</p><div class="mediaobject"><img src="graphics/B02052_06_07.jpg" alt="Image stitching using OpenCV"/></div><p>We will now see <a id="id335" class="indexterm"/>how to implement image stitching.</p><p>First, we will set up our project in the same way as we did for all the previous chapters. We will use the package name <code class="literal">com.packtpub.masteringopencvandroid.chapter6</code> for this project. First, we will edit our manifest file.</p><p>We will add all the required permissions to this project. We require the permissions to access the camera, and also to read and write to the external storage. So, add the following code to your manifest:</p><div class="informalexample"><pre class="programlisting">&lt;uses-permission android:name="android.permission.CAMERA" /&gt;
&lt;uses-permission
    android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;
&lt;uses-permission
    android:name="android.permission.READ_PHONE_STATE" /&gt;
&lt;uses-permission
    android:name="android.permission.READ_EXTERNAL_STORAGE" /&gt;</pre></div><p>Then we will declare our activities. We only require one activity for this project. We will call it the <code class="literal">StitchingActivity</code>.</p><div class="section" title="Setting up Android NDK"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec17"/>Setting up Android NDK</h3></div></div></div><p>We<a id="id336" class="indexterm"/> require NDK for this project as the stitching module is unavailable in OpenCV's Java SDK. So, we will write the C++ code and compile it using Android NDK in order for it to be used as a part of our project. To do this, first download <a id="id337" class="indexterm"/>NDK from <a class="ulink" href="http://developer.android.com/tools/sdk/ndk">http://developer.android.com/tools/sdk/ndk</a> and extract it to a location on your computer. Then go to your <code class="literal">local.properties</code> file and add the following line:</p><div class="informalexample"><pre class="programlisting">ndk.dir=&lt;location of the ndk directory&gt;</pre></div><p>Next, go to your <code class="literal">build.gradle</code> file that is located in the main module of your project. In this file, inside the <code class="literal">defaultConfig</code> tag, add the following code:</p><div class="informalexample"><pre class="programlisting">ndk {
    moduleName "stitcher"
}</pre></div><p>This is the name of the module, which will contain our functions, where our computations will be performed. Now, under the <code class="literal">android</code> tag, after <code class="literal">defaultConfig</code> ends, add the following lines:</p><div class="informalexample"><pre class="programlisting">sourceSets.main {
    jniLibs.srcDir 'src/main/libs'
    jni.srcDirs = [] //disable automatic ndk-build call
}</pre></div><p>This defines where our compiled libraries will be located. After this, we need to set up the NDK part of our project. In the <code class="literal">src</code> folder, add a folder called <code class="literal">jni</code>. In this folder, we need to create two files. The first one is <code class="literal">Android.mk</code>. This contains information about the files in the project. Copy the following lines to this file. Remember to replace <code class="literal">OpenCV4AndroidSDK</code> with the location on your computer:</p><div class="informalexample"><pre class="programlisting">LOCAL_PATH := $(call my-dir)

include $(CLEAR_VARS)

OPENCV_CAMERA_MODULES:=on
OPENCV_INSTALL_MODULES:=on

include &lt;OpenCV4AndroidSDK&gt;/sdk/native/jni/OpenCV.mk

LOCAL_MODULE    := stitcher
LOCAL_SRC_FILES := stitcher.cpp
LOCAL_LDLIBS +=  -llog -ldl

include $(BUILD_SHARED_LIBRARY)</pre></div><p>Now, create<a id="id338" class="indexterm"/> another file named <code class="literal">Application.mk</code>. This defines the architectures for which the code has to be compiled. Copy the following lines to this file:</p><div class="informalexample"><pre class="programlisting">APP_STL := gnustl_static
APP_CPPFLAGS := -frtti -fexceptions
APP_ABI := armeabi-v7a
APP_PLATFORM := android-8</pre></div><p>Now we are all set to use NDK code in our project.</p></div><div class="section" title="The layout and Java code"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec18"/>The layout and Java code</h3></div></div></div><p>Next we<a id="id339" class="indexterm"/> will draw our layout. For this project, we only need one layout with one <code class="literal">ImageView</code> tag to display the stitched image and two <code class="literal">Buttons</code>. One of the buttons is used to click more images and one is used to signify that there are no more images to be clicked. We will also put all the items in a <code class="literal">ScrollView</code> tag to be able to see the full image if its size exceeds the screen size. Our <code class="literal">activity_stitching.xml</code> file is as follows:</p><div class="informalexample"><pre class="programlisting">&lt;?xml version="1.0" encoding="utf-8"?&gt;
&lt;ScrollView 
    android:layout_width="match_parent"
    android:layout_height="match_parent" &gt;
    &lt;LinearLayout android:orientation="vertical" android:layout_width="match_parent"
        android:layout_height="wrap_content"&gt;

        &lt;ImageView
            android:layout_width="match_parent"
            android:layout_height="0dp"
            android:layout_weight="0.5"
            android:id="@+id/ivImage" /&gt;
        &lt;LinearLayout
            android:layout_width="match_parent"
            android:layout_height="wrap_content"
            android:orientation="horizontal"&gt;
            &lt;Button
                android:layout_width="match_parent"
                android:layout_height="wrap_content"
                android:layout_weight="0.5"
                android:id="@+id/bClickImage"
                android:text="Click more images"/&gt;
            &lt;Button
                android:layout_width="match_parent"
                android:layout_height="wrap_content"
                android:layout_weight="0.5"
                android:id="@+id/bDone"
                android:text="Done"/&gt;
        &lt;/LinearLayout&gt;

    &lt;/LinearLayout&gt;
&lt;/ScrollView&gt;</pre></div><p>Now we have<a id="id340" class="indexterm"/> to write our Java code. In the <code class="literal">StitchingActivity.java</code> file, in your OpenCV <code class="literal">BaseLoaderCallback</code> object, edit the <code class="literal">onManagerConnected</code> function by adding the following line in <code class="literal">case LoaderCallbackInterface.SUCCESS</code>:</p><div class="informalexample"><pre class="programlisting">System.loadLibrary("stitcher");</pre></div><p>Notice that this is the same name that we gave our module in our <code class="literal">Android.mk</code> file. In our Java code, we will first declare and initialize all the variables that we will need. We have a button called <code class="literal">bClickImage</code>, which, on clicking, calls Android's camera intent and requests the system's camera app to click a picture and sends it to the app. We will convert this <code class="literal">Bitmap</code> image into an OpenCV <code class="literal">Mat</code> and store it in an <code class="literal">ArrayList</code>. We will stitch all the images together in the end, when the user clicks on the <code class="literal">bDone</code> button. The <code class="literal">onClickListener</code> for both the buttons is as follows:</p><div class="informalexample"><pre class="programlisting">bClickImage.setOnClickListener(new View.OnClickListener() {
    @Override
    public void onClick(View v) {
        Intent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
        File imagesFolder = new File(FILE_LOCATION);
        imagesFolder.mkdirs();
        File image = new File(imagesFolder, "panorama_"+ (clickedImages.size()+1) + ".jpg");
        fileUri = Uri.fromFile(image);
        Log.d("StitchingActivity", "File URI = " + fileUri.toString());
        intent.putExtra(MediaStore.EXTRA_OUTPUT, fileUri); // set the image file name

        // start the image capture Intent
        startActivityForResult(intent, CLICK_PHOTO);
    }
});

bDone.setOnClickListener(new View.OnClickListener() {
    @Override
    public void onClick(View v) {
        if(clickedImages.size()==0){
            Toast.makeText(getApplicationContext(), 
              "No images clicked", Toast.LENGTH_SHORT).show();
        } else if(clickedImages.size()==1){
            Toast.makeText(getApplicationContext(), "Only one image clicked", Toast.LENGTH_SHORT).show();
            Bitmap image = Bitmap.createBitmap(src.cols(), src.rows(), Bitmap.Config.ARGB_8888);
            Utils.matToBitmap(src, image);
            ivImage.setImageBitmap(image);
        } else {
            createPanorama();
        }
    }
});</pre></div><p>The <code class="literal">onActivityResult</code> function<a id="id341" class="indexterm"/> is called when the camera intent returns from the camera app. We need to check whether an image has been clicked and add it to the <code class="literal">ArrayList</code>, if required. We will use OpenCV's <code class="literal">BitmapToMat</code> function to convert the image from an Android Bitmap to an OpenCV Mat. The code is as follows:</p><div class="informalexample"><pre class="programlisting">switch(requestCode) {
    case CLICK_PHOTO:
        if(resultCode == RESULT_OK){
            try {
                final InputStream imageStream = getContentResolver().openInputStream(fileUri);
                final Bitmap selectedImage = BitmapFactory.decodeStream(imageStream);
                src = new Mat(selectedImage.getHeight(), selectedImage.getWidth(), CvType.CV_8UC4);
                Imgproc.resize(src, src, new Size(src.rows()/4, src.cols()/4));
                Utils.bitmapToMat(selectedImage, src);
                Imgproc.cvtColor(src, src, Imgproc.COLOR_BGR2RGB);
                clickedImages.add(src);
            } catch (FileNotFoundException e) {
                e.printStackTrace();
            }
        }
        break;
}</pre></div><p>In <code class="literal">onClickListener</code> for <code class="literal">bDone</code>, we called a <code class="literal">createPanorama</code> function. In this function, we will <a id="id342" class="indexterm"/>execute an <code class="literal">AsyncTask</code>, as this is a computationally intensive task. In <code class="literal">AsyncTask</code>, we will call upon our NDK to perform the actual computation. This is what our <code class="literal">doInBackground</code> looks like:</p><div class="informalexample"><pre class="programlisting">Mat srcRes = new Mat();
int success = StitchPanorama(clickedImages.toArray(), clickedImages.size(), srcRes.getNativeObjAddr());
if(success==0){
    return null;
}
Imgproc.cvtColor(srcRes, srcRes, Imgproc.Color_BGR2RGBA);
Bitmap bitmap = Bitmap.createBitmap(srcRes.cols(), srcRes.rows(), Bitmap.Config.ARGB_8888);
Utils.matToBitmap(srcRes, bitmap);
return bitmap;</pre></div><p>We also need to declare the <code class="literal">StitchPanorama</code> function as a native function so that Android knows where to look for it when executing:</p><div class="informalexample"><pre class="programlisting">public native int StitchPanorama(Object images[], int size, long addrSrcRes);</pre></div><p>After this, in <code class="literal">onPostExecute</code>, we just need to set the returned <code class="literal">Bitmap</code> as the source for <code class="literal">ImageView</code>. This completes our Java code for this project, and all the major stitching is done using the C++ code.</p></div><div class="section" title="The C++ code"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec19"/>The C++ code</h3></div></div></div><p>In <a id="id343" class="indexterm"/>your <code class="literal">jni</code> folder, create the <code class="literal">stitcher.cpp</code> file. Notice that this is the same name as set in the <code class="literal">Android.mk</code> file. First, we need to include some libraries that we will require. We will also declare some namespaces that we will be using and some global variables as follows:</p><div class="informalexample"><pre class="programlisting">#include &lt;jni.h&gt;
#include &lt;vector&gt;

#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/highgui/highgui.hpp"
#include &lt;opencv2/stitching/stitcher.hpp&gt;
using namespace cv;
using namespace std;

char FILEPATH[100] = "/storage/emulated/0/Download/PacktBook/Chapter6/panorama_stitched.jpg";</pre></div><p>Then we need to declare our function and write our code in it. To declare the function, write the following code:</p><div class="informalexample"><pre class="programlisting">extern "C" {
    JNIEXPORT jint JNICALL Java_com_packtpub_masteringopencvandroid_chapter6_StitchingActivity_StitchPanorama(JNIEnv*, jobject, jobjectArray, jint, jlong);
    JNIEXPORT jint JNICALL Java_com_packtpub_masteringopencvandroid_chapter6_StitchingActivity_StitchPanorama(JNIEnv* env, jobject, jobjectArray images, jint size, jlong resultMatAddr)
    {
        …
    }
}</pre></div><p>The ellipses are placeholders for where our code will go. Notice the variables and their orders compared to the variables declared in our Java code. First, we will initialize some variables and also convert the Mat object that we sent from Java to a C++ Mat:</p><div class="informalexample"><pre class="programlisting">jint resultReturn = 0;
vector&lt;Mat&gt; clickedImages = vector&lt;Mat&gt;();
Mat output_stitched = Mat();
Mat&amp; srcRes = *(Mat*)resultMatAddr, img;</pre></div><p>Here, we have used the address of the Mat object and  type-casted it to a C++ Mat pointer. Next we need to convert the Mat array sent from Java to a C++ vector. We will use the following code:</p><div class="informalexample"><pre class="programlisting">jclass clazz = (env)-&gt;FindClass("org/opencv/core/Mat");
jmethodID getNativeObjAddr = (env)-&gt;GetMethodID(clazz, "getNativeObjAddr", "()J");

for(int i=0; i &lt; size; i++){
    jobject obj = (env-&gt;GetObjectArrayElement(images, i));
    jlong result = (env)-&gt;CallLongMethod(obj, getNativeObjAddr, NULL);
    img = *(Mat*)result;
    resize(img, img, Size(img.rows/10, img.cols/10));
    clickedImages.push_back(img);
    env-&gt;DeleteLocalRef(obj);
}
env-&gt;DeleteLocalRef(images);</pre></div><p>We need to<a id="id344" class="indexterm"/> manually delete the local objects as the C++ code doesn't automatically call the garbage collector, and being on mobile, it is highly important to optimize the memory use.</p><p>Now we will use OpenCV's stitcher module to stitch our images:</p><div class="informalexample"><pre class="programlisting">Stitcher stitcher = Stitcher::createDefault();
Stitcher::Status status = stitcher.stitch(clickedImages, output_stitched);

output_stitched.copyTo(srcRes);

imwrite(FILEPATH, srcRes);

if (status == Stitcher::OK)
    resultReturn = 1;
else
    resultReturn = 0;

return resultReturn;</pre></div><p>We have used the default setup for stitching the images; however, the stitcher module allows the modification of the pipeline by giving more control to the developer. Check out the available options at <a class="ulink" href="http://docs.opencv.org/modules/stitching/doc/introduction.html">http://docs.opencv.org/modules/stitching/doc/introduction.html</a>.</p><p>Now we just need to build our C++ code file to generate the object files that our Java code will use to make function calls to C++ functions. For this, you will need to open the terminal/command prompt, and then use the <code class="literal">cd</code> command to change the active directory to <code class="literal">&lt;project_dir&gt;/app/src/main/jni</code>. Now we need to build our files. For this, you need to use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&lt;ndk_dir&gt;/ndk-build</strong></span>
</pre></div><p>This will generate our object files and place them in the <code class="literal">obj</code> and <code class="literal">libs</code> folders.</p><p>This completes our <a id="id345" class="indexterm"/>project on image stitching using OpenCV on Android. You can see the stitched results in the following images.</p><p>The following is the first sample image:</p><div class="mediaobject"><img src="graphics/B02052_06_08.jpg" alt="The C++ code"/></div><p>The following is the second sample image:</p><div class="mediaobject"><img src="graphics/B02052_06_09.jpg" alt="The C++ code"/></div><p>The following is<a id="id346" class="indexterm"/> the result of applying image stitching over these two sample images:</p><div class="mediaobject"><img src="graphics/B02052_06_10.jpg" alt="The C++ code"/></div><p>There are chances that your code might crash due to the high memory requirements of the stitcher module. This<a id="id347" class="indexterm"/> is a limitation of the mobile ecosystem and can be overcome by including a server in the middle to perform the computations instead. You can modify the source of the app to send the images to the server, which in turn performs the stitching and returns the stitched result, which can be displayed in the app.</p></div></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Summary</h1></div></div></div><p>In this chapter, we saw how panoramic images are stitched. We took a look at image alignment by finding homography, using RANSAC, and image stitching as a whole. We also saw how it can be implemented in Android using OpenCV. These image alignment techniques can also be used for video stabilization. In the next chapter, we will take a look at how we can make use of machine learning algorithms to automate some of the complex tasks that generally require a human to be present.</p></div></body></html>