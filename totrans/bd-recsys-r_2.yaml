- en: Chapter 2. Data Mining Techniques Used in Recommender Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though the primary objective of this book is to build recommender systems, a
    walkthrough of the commonly used data-mining techniques is a necessary step before
    jumping into building recommender systems. In this chapter, you will learn about
    popular data preprocessing techniques, data-mining techniques, and data-evaluation
    techniques commonly used in recommender systems. The first section of the chapter
    tells you how a data analysis problem is solved, followed by data preprocessing
    steps such as similarity measures and dimensionality reduction. The next section
    of the chapter deals with data mining techniques and their evaluation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarity measures include:'
  prefs: []
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson correlation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensionality reduction techniques include:'
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data-mining techniques include:'
  prefs: []
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods, such as bagging, boosting, and random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving a data analysis problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Any data analysis problem involves a series of steps such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying a business problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the problem domain with the help of a domain expert.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying data sources and data variables suitable for the analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing or a cleansing step, such as identifying missing values,
    quantitative and qualitative variables and transformations, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing exploratory analysis to understand the data, mostly through visual
    graphs such as box plots or histograms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing basic statistics such as mean, median, modes, variances, standard
    deviations, correlation among the variables, and covariance to understand the
    nature of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing the data into training and testing datasets and running a model using
    machine-learning algorithms with training datasets, using cross-validation techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validating the model using the test data to evaluate the model on the new data.
    If needed, improve the model based on the results of the validation step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize the results and deploy the model for real-time predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image displays the resolution to a data analysis problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solving a data analysis problem](img/B03888_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data analysis steps
  prefs: []
  type: TYPE_NORMAL
- en: Data preprocessing techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preprocessing is a crucial step for any data analysis problem. The model's
    accuracy depends mostly on the quality of the data. In general, any data preprocessing
    step involves data cleansing, transformations, identifying missing values, and
    how they should be treated. Only the preprocessed data can be fed into a machine-learning
    algorithm. In this section, we will focus mainly on data preprocessing techniques.
    These techniques include similarity measurements (such as Euclidean distance,
    Cosine distance, and Pearson coefficient) and dimensionality-reduction techniques,
    such as **Principal component analysis** (**PCA**), which are widely used in recommender
    systems. Apart from PCA, we have **singular value decomposition** (**SVD**), subset
    feature selection methods to reduce the dimensions of the dataset, but we limit
    our study to PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, every recommender system works on the
    concept of similarity between items or users. In this section, let's explore some
    similarity measures such as Euclidian distance, Cosine distance, and Pearson correlation.
  prefs: []
  type: TYPE_NORMAL
- en: Euclidian distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The simplest technique for calculating the similarity between two items is
    by calculating its Euclidian distance. The Euclidean distance between two points/objects
    (point *x* and point *y*) in a dataset is defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Euclidian distance](img/B03888_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, (*x*, *y*) are two consecutive data points, and *n* is the
    number of attributes for the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'R script to calculate the Euclidean distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Cosine distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cosine similarity is a measure of similarity between two vectors of an inner
    product space that measures the cosine of the angle between them. Cosine similarity
    is given by this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cosine distance](img/B03888_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'R script to calculate the cosine distance is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this equation, *x* is the matrix containing all variables in a dataset. The
    `cosine` function is available in the `lsa` package.
  prefs: []
  type: TYPE_NORMAL
- en: Pearson correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarity between two products can also be given by the correlation existing
    between their variables. Pearson''s correlation coefficient is a popular correlation
    coefficient calculated between two variables as the covariance of the two variables
    divided by the product of their standard deviations. This is given by *ƿ* *(rho)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Pearson correlation](img/B03888_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'R script is given by these lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Empirical studies showed that Pearson coefficient outperformed other similarity
    measures for user-based collaborative filtering recommender systems. The studies
    also show that Cosine similarity consistently performs well in item-based collaborative
    filtering.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most commonly faced problems while building recommender systems is
    high-dimensional and sparse data. At many times, we face a situation where we
    have a large set of features and fewer data points. In such situations, when we
    fit a model to the dataset, the predictive power of the model will be lower. This
    scenario is often termed as the curse of dimensionality. In general, adding more
    data points or decreasing the feature space, also known as dimensionality reduction,
    often reduces the effects of the curse of dimensionality. In this chapter, we
    will discuss PCA, a popular dimensionality reduction technique to reduce the effects
    of the curse of dimensionality.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Principal component analysis is a classical statistical technique for dimensionality
    reduction. The PCA algorithm transforms the data with high-dimensional space to
    a space with fewer dimensions. The algorithm linearly transforms m-dimensional
    input space to n-dimensional (*n<m*) output space, with the objective to minimize
    the amount of information/variance lost by discarding (*m-n*) dimensions. PCA
    allows us to discard the variables/features that have less variance.
  prefs: []
  type: TYPE_NORMAL
- en: Technically speaking, PCA uses orthogonal projection of highly correlated variables
    to a set of values of linearly uncorrelated variables called principal components.
    The number of principal components is less than or equal to the number of original
    variables. This linear transformation is defined in such a way that the first
    principal component has the largest possible variance. It accounts for as much
    of the variability in the data as possible by considering highly correlated features.
    Each succeeding component in turn has the highest variance using the features
    that are less correlated with the first principal component and that are orthogonal
    to the preceding component.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this in simple terms. Assume we have three dimensional data
    space with two features more correlated with each other than with the third. We
    now want to reduce the data to two-dimensional space using PCA. The first principal
    component is created in such a way that it explains maximum variance using the
    two correlated variables along the data. In the following graph, the first principal
    component (bigger line) is along the data explaining most variance. To choose
    the second principal component, we need to choose another line that has the highest
    variance, is uncorrelated, and is orthogonal to the first principal component.
    The implementation and technical details of PCA are beyond the scope of this book,
    so we will discuss how it is used in R.
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B03888_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will illustrate PCA using the USArrests dataset. The USArrests dataset contains
    crime-related statistics, such as Assault, Murder, Rape, and UrbanPop per 100,000
    residents in 50 states in the US:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Standard deviations**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Rotation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B03888_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding image, known as a biplot, we can see the two principal components
    (**PC1** and **PC2**) of the USArrests dataset. The red arrows represent the loading
    vectors, which represent how the feature space varies along the principal component
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plot, we can see that the first principal component vector, **PC1**,
    more or less places equal weight on three features: **Rape**, **Assault**, and
    **Murder**. This means that these three features are more correlated with each
    other than the **UrbanPop** feature. In the second principal component, **PC2**
    places more weight on **UrbanPop** than the remaining 3 features are less correlated
    with them.'
  prefs: []
  type: TYPE_NORMAL
- en: Data mining techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at commonly used data-mining algorithms, such
    as k-means clustering, support vector machines, decision trees, bagging, boosting,
    and random forests. Evaluation techniques such as cross validation, regularization,
    confusion matrix, and model comparison are explained in brief.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster analysis is the process of grouping objects together in a way that objects
    in one group are more similar than objects in other groups.
  prefs: []
  type: TYPE_NORMAL
- en: An example would be identifying and grouping clients with similar booking activities
    on a travel portal, as shown in the following figure.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, each group is called a cluster, and each member (data
    point) of the cluster behaves in a manner similar to its group members.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cluster analysis](img/B03888_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cluster analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'Cluster analysis is an unsupervised learning method. In supervised methods,
    such as regression analysis, we have input variables and response variables. We
    fit a statistical model to the input variables to predict the response variable.
    Whereas in unsupervised learning methods, however, we do not have any response
    variable to predict; we only have input variables. Instead of fitting a model
    to the input variables to predict the response variable, we just try to find patterns
    within the dataset. There are three popular clustering algorithms: hierarchical
    cluster analysis, *k*-means cluster analysis, and two-step cluster analysis. In
    the following section, we will learn about *k*-means clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the k-means cluster algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*k*-means is an unsupervised, iterative algorithm where k is the number of
    clusters to be formed from the data. Clustering is achieved in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: In this step, we randomly choose two cluster points
    (red dot and green dot) and assign each data point to the cluster point that is
    closer to it (top part of the following image).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Move centroid step**: In this step, we take the average of the points of
    all the examples in each group and move the centroid to the new position, that
    is, mean position calculated (bottom part of the following image).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding steps are repeated until all the data points are grouped into
    two groups and the mean of the data points after moving the centroid doesn't change.
  prefs: []
  type: TYPE_NORMAL
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_08_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Steps of cluster analysis
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image shows how a clustering algorithm works on data to form
    clusters. See the R implementation of *k*-means clustering on iris dataset as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Cluster analysis results
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding image shows the formation of clusters on the iris data, and the
    clusters account for 95 percent of the data. In the preceding example, the number
    of clusters of *k* value is selected using the `elbow` method, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image shows the cost reduction for *k* values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_10_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding figure, we can observe that the direction of the cost function
    is changed at cluster number 5\. Hence, we choose 5 as our number of clusters
    k. Since the number of optimal clusters is found at the elbow of the graph, we
    call it the elbow method.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Support vector machine algorithms are a form of supervised learning algorithms
    employed to solve classification problems. SVM is generally treated as one of
    the best algorithms to deal with classification problems. Given a set of training
    examples, where each data point falls into one of two categories, an SVM training
    algorithm builds a model that assigns new data points into one category or the
    other. This model is a representation of the examples as a points in space, mapped
    so that the examples of the separate categories are divided by a margin that is
    as wide as possible, as shown in the following image. New examples are then mapped
    into that same space and predicted to belong to a category based on which side
    of the gap they fall on. In this section, we will go through an overview and implementation
    of SVMs without going into mathematical details.
  prefs: []
  type: TYPE_NORMAL
- en: 'When SVM is applied to a p-dimensional dataset, the data is mapped to a *p-1*
    dimensional hyperplane, and the algorithm finds a clear boundary with a sufficient
    margin between classes. Unlike other classification algorithms that also create
    a separating boundary to classify data points, SVM tries to choose a boundary
    that has the maximum margin to separate the classes, as shown in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machine](img/B03888_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Consider a two-dimensional dataset having two classes, as shown in the preceding
    image. Now, when the SVM algorithm is applied, first it checks whether a one-dimensional
    hyperplane exists to map all the data points. If the hyperplane exists, the linear
    classifier creates a decision boundary with a margin to separate the classes.
    In the preceding image, the thick red line is the decision boundary, and the thinner
    blue and red lines are the margins of each class from the boundary. When new test
    data is used to predict the class, the new data falls into one of the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points to be noted:'
  prefs: []
  type: TYPE_NORMAL
- en: Though an infinite number of hyperplanes can be created, SVM chooses only one
    hyperplane that has the maximum margin, that is, the separating hyperplane that
    is farthest from the training observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This classifier is only dependent on the data points that lie on the margins
    of the hyperplane, that is, on thin margins in the image, but not on other observations
    in the dataset. These points are called support vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision boundary is affected only by the support vectors but not by other
    observations located away from the boundaries. If we change the data points other
    than the support vectors, there would not be any effect on the decision boundary.
    However, if the support vectors are changed, the decision boundary changes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large margin on the training data will also have a large margin on the test
    data to classify the test data correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machines also perform well with non-linear datasets. In this
    case, we use radial kernel functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See the R implementation of SVM on the iris dataset in the following code snippet.
    We used the `e1071` package to run SVM. In R, the `SVM()` function contains the
    implementation of support vector machines present in the `e1071` package.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will see that the `SVM()` method is called with the `tune()` method,
    which does cross validation and runs the model on different values of the cost
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cross-validation method is used to evaluate the accuracy of the predictive
    model before testing on future unseen data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Call**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Parameters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tune$best.model` object tells us that the model works best with the cost
    parameter as `10` and total number of support vectors as `25`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are a simple, fast, tree-based supervised learning algorithm
    to solve classification problems. Though not very accurate when compared to other
    logistic regression methods, this algorithm comes in handy while dealing with
    recommender systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the decision trees with an example. Imagine a situation where you
    have to predict the class of flower based on its features such as petal length,
    petal width, sepal length, and sepal width. We will apply the decision tree methodology
    to solve this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the entire data at the start of the algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, choose a suitable question/variable to divide the data into two parts.
    In our case, we chose to divide the data based on petal length *> 2.45* and *<=
    2.45*. This separates flower class `setosa` from the rest of the classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, further divide the data having petal length *>2.45*, based on the same
    variable with petal length *< 4.5* and *>= 4.5*, as shown in the following image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This splitting of the data will be further divided by narrowing down the data
    space until we reach a point where all the bottom points represent the response
    variables or where further logical split cannot be done on the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following decision tree image, we have one root node, four internal
    nodes where data split occurred, and five terminal nodes where data split cannot
    be done any further. They are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Petal.Length <2.45** as root node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Petal.Length <4.85**, **Sepal.Length <5.15,** and **Petal.Width <1.75** are
    called internal nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final nodes having the class of the flowers are called terminal nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lines connecting the nodes are called the branches of the tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While predicting responses on new data using the previously built model, each
    new data point is taken through each node, a question is asked, and a logical
    path is taken to reach its logical class, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decision trees](img/B03888_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See the decision tree implementation in R on the iris dataset using the tree
    package available from **Comprehensive R Archive Network** (**CRAN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary of the mode is given here. It tells us that the misclassification
    rate is 0.0381, indicating that the model is accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Classification tree**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data mining, we use ensemble methods, which means using multiple learning
    algorithms to obtain better predictive results than applying any single learning
    algorithm on any statistical problem. This section will provide an overview of
    popular ensemble methods such as bagging, boosting, and random forests
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bagging is also known as Bootstrap aggregating. It is designed to improve the
    stability and accuracy of machine-learning algorithms. It helps avoid over fitting
    and reduces variance. This is mostly used with decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bagging involves randomly generating Bootstrap samples from the dataset and
    trains the models individually. Predictions are then made by aggregating or averaging
    all the response variables:'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a dataset (*Xi, Yi*), where *i=1 …n*, contains *n* data
    points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, randomly select B samples with replacements from the original dataset using
    Bootstrap technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, train the B samples with regression/classification models independently.
    Then, predictions are made on the test set by averaging the responses from all
    the B models generated in the case of regression. Alternatively, the most often
    occurring class among B samples is generated in the case of classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random forests are improvised supervised algorithms than bootstrap aggregation
    or bagging methods, though they are built on a similar approach. Unlike selecting
    all the variables in all the B samples generated using the Bootstrap technique
    in bagging, we select only a few predictor variables randomly from the total variables
    for each of the B samples. Then, these samples are trained with the models. Predictions
    are made by averaging the result of each model. The number of predictors in each
    sample is decided using the formula *m = √p*, where *p* is the total variable
    count in the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key notes:'
  prefs: []
  type: TYPE_NORMAL
- en: This approach removes the condition of dependency of strong predictors in the
    dataset as we intentionally select fewer variables than all the variables for
    every iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach also de-correlates variables, resulting in less variability in
    the model and, hence, more reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Refer to the R implementation of random forests on the iris dataset using the
    `randomForest` package available from CRAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Call**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike with bagging, where multiple copies of Bootstrap samples are created,
    a new model is fitted for each copy of the dataset, and all the individual models
    are combined to create a single predictive model, each new model is built using
    information from previously built models. Boosting can be understood as an iterative
    method involving two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: A new model is built on the residuals of previous models instead of the response
    variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, the residuals are calculated from this model and updated to the residuals
    used in the previous step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding two steps are repeated for multiple iterations, allowing each
    new model to learn from its previous mistakes, thereby improving the model accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![Boosting](img/B03888_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting](img/B03888_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following code snippet, the output value for the `predict()` function
    is used in the `apply()` function to pick the response with the highest probability
    among each row in the `pred` matrix. The resultant output from the `apply()` function
    is the prediction for the response variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Evaluating data-mining algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we have seen various data-mining techniques used in
    recommender systems. In this section, you will learn how to evaluate models built
    using data-mining techniques. The ultimate goal for any data analytics model is
    to perform well on future data. This objective could be achieved only if we build
    a model that is efficient and robust during the development stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'While evaluating any model, the most important things we need to consider are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the model is over fitting or under fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How well the model fits the future data or test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under fitting, also known as bias, is a scenario when the model doesn't even
    perform well on training data. This means that we fit a less robust model to the
    data. For example, say the data is distributed non-linearly and we are fitting
    the data with a linear model. From the following image, we see that data is non-linearly
    distributed. Assume that we have fitted a linear model (orange line). In this
    case, during the model building stage itself, the predictive power will be low.
  prefs: []
  type: TYPE_NORMAL
- en: Over fitting is a scenario when the model performs well on training data, but
    does really bad on test data. This scenario arises when the model memorizes the
    data pattern rather than learning from data. For example, say the data is distributed
    in a non-linear pattern, and we have fitted a complex model, shown using the green
    line. In this case, we observe that the model is fitted very close to the data
    distribution, taking care of all the ups and downs. In this case, the model is
    most likely to fail on previously unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating data-mining algorithms](img/B03888_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding image shows simple, complex, and appropriate fitted models' training
    data. The green fit represents overfitting, the orange line represents underfitting,
    the black and blue lines represent the appropriate model, which is a trade-off
    between underfit and overfit.
  prefs: []
  type: TYPE_NORMAL
- en: Any fitted model is evaluated to avoid previously mentioned scenarios using
    cross validation, regularization, pruning, model comparisons, ROC curves, confusion
    matrices, and so on .
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross validation**: This is a very popular technique for model evaluation
    for almost all models. In this technique, we divide the data into two datasets:
    a training dataset and a test dataset. The model is built using the training dataset
    and evaluated using the test dataset. This process is repeated many times. The
    test errors are calculated for every iteration. The averaged test error is calculated
    to generalize the model accuracy at the end of all the iterations.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularization**: In this technique, the data variables are penalized to
    reduce the complexity of the model with the objective to minimize the cost function.
    There are two most popular regularization techniques: ridge regression and lasso
    regression. In both techniques, we try to reduce the variable co-efficient to
    zero. Thus, a smaller number of variables will fit the data optimally.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: This technique is popularly used in evaluating a classification
    model. We build a confusion matrix using the results of the model. We calculate
    precision and recall/sensitivity/specificity to evaluate the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: This is the probability whether the truly classified records
    are relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall/Sensitivity**: This is the probability whether the relevant records
    are truly classified.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Specificity**: Also known as true negative rate, this is the proportion of
    truly classified wrong records.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix shown in the following image is constructed using the results
    of classification models discussed in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating data-mining algorithms](img/B03888_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TRUE POSITVE (TP)**: This is a count of all the responses where the actual
    response is negative and the model predicted is positive'
  prefs: []
  type: TYPE_NORMAL
- en: '**FALSE POSITIVE (FP)**: This is a count of all the responses where the actual
    response is negative, but the model predicted is positive. It is, in general,
    a **FALSE ALARM**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**FALSE NEGATIVE (FN)**: This is a count of all the responses where the actual
    response is positive, but the model predicted is negative. It is, in general,
    **A MISS**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**TRUE NEGATIVE (TN)**: This is a count of all the responses where the actual
    response is negative, and the model predicted is negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, precision and recall/specificity is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Evaluating data-mining algorithms](img/B03888_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Model comparison**: A classification problem can be solved using one or more
    statistical models. For example, a classification problem can be solved using
    logistic regression, a decision tree, ensemble methods, and SVM. How do you choose
    which model fits the data well? A number of approaches are available for a suitable
    model selection, such as **Akaike information criteria** (**AIC**), **Bayesian**
    **information criteria** (**BIC**), and Adjusted R^2, Cᵨ. For each model, AIC
    / BIC / Adjusted R^2 is calculated. The model with least of these values is selected
    as the best model.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Downloading the example code**'
  prefs: []
  type: TYPE_NORMAL
- en: You can download the example code fies from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the fies e-mailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about popular data preprocessing techniques, data-mining
    techniques, and evaluation techniques commonly used in recommender systems. In
    the next chapter, you will learn about the recommender systems introduced in [Chapter
    1](ch01.html "Chapter 1. Getting Started with Recommender Systems"), *Getting
    Started with Recommender Systems*, in more detail.
  prefs: []
  type: TYPE_NORMAL
