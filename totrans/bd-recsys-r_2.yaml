- en: Chapter 2. Data Mining Techniques Used in Recommender Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：推荐系统中使用的数据挖掘技术
- en: Though the primary objective of this book is to build recommender systems, a
    walkthrough of the commonly used data-mining techniques is a necessary step before
    jumping into building recommender systems. In this chapter, you will learn about
    popular data preprocessing techniques, data-mining techniques, and data-evaluation
    techniques commonly used in recommender systems. The first section of the chapter
    tells you how a data analysis problem is solved, followed by data preprocessing
    steps such as similarity measures and dimensionality reduction. The next section
    of the chapter deals with data mining techniques and their evaluation techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的主要目标是构建推荐系统，但在构建推荐系统之前，了解常用的数据挖掘技术是必要的步骤。在本章中，你将了解推荐系统中常用的数据预处理技术、数据挖掘技术和数据评估技术。本章的第一部分告诉你如何解决数据分析问题，随后是数据预处理步骤，如相似度度量降维。本章的下一部分处理数据挖掘技术及其评估技术。
- en: 'Similarity measures include:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度度量包括：
- en: Euclidean distance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: Cosine distance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦距离
- en: Pearson correlation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: 'Dimensionality reduction techniques include:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术包括：
- en: Principal component analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'Data-mining techniques include:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘技术包括：
- en: k-means clustering
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means聚类
- en: Support vector machine
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Ensemble methods, such as bagging, boosting, and random forests
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成方法，如bagging、boosting和随机森林
- en: Solving a data analysis problem
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决数据分析问题
- en: 'Any data analysis problem involves a series of steps such as:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据分析问题都涉及一系列步骤，例如：
- en: Identifying a business problem.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别业务问题。
- en: Understanding the problem domain with the help of a domain expert.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在领域专家的帮助下理解问题域。
- en: Identifying data sources and data variables suitable for the analysis.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别适合分析的数据来源和数据变量。
- en: Data preprocessing or a cleansing step, such as identifying missing values,
    quantitative and qualitative variables and transformations, and so on.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理或清洗步骤，例如识别缺失值、定量和定性变量以及转换等。
- en: Performing exploratory analysis to understand the data, mostly through visual
    graphs such as box plots or histograms.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行探索性分析以理解数据，主要通过箱线图或直方图等视觉图形。
- en: Performing basic statistics such as mean, median, modes, variances, standard
    deviations, correlation among the variables, and covariance to understand the
    nature of the data.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行基本统计，如均值、中位数、众数、方差、标准差、变量之间的相关性以及协方差，以了解数据的性质。
- en: Dividing the data into training and testing datasets and running a model using
    machine-learning algorithms with training datasets, using cross-validation techniques.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集，并使用机器学习算法和训练集运行模型，使用交叉验证技术。
- en: Validating the model using the test data to evaluate the model on the new data.
    If needed, improve the model based on the results of the validation step.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试数据验证模型，以评估模型在新数据上的表现。如果需要，根据验证步骤的结果改进模型。
- en: Visualize the results and deploy the model for real-time predictions.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化结果并将模型部署用于实时预测。
- en: 'The following image displays the resolution to a data analysis problem:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了数据分析问题的解决方案：
- en: '![Solving a data analysis problem](img/B03888_02_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![解决数据分析问题](img/B03888_02_01.jpg)'
- en: Data analysis steps
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析步骤
- en: Data preprocessing techniques
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理技术
- en: Data preprocessing is a crucial step for any data analysis problem. The model's
    accuracy depends mostly on the quality of the data. In general, any data preprocessing
    step involves data cleansing, transformations, identifying missing values, and
    how they should be treated. Only the preprocessed data can be fed into a machine-learning
    algorithm. In this section, we will focus mainly on data preprocessing techniques.
    These techniques include similarity measurements (such as Euclidean distance,
    Cosine distance, and Pearson coefficient) and dimensionality-reduction techniques,
    such as **Principal component analysis** (**PCA**), which are widely used in recommender
    systems. Apart from PCA, we have **singular value decomposition** (**SVD**), subset
    feature selection methods to reduce the dimensions of the dataset, but we limit
    our study to PCA.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是任何数据分析问题的关键步骤。模型的准确性主要取决于数据的质量。通常，任何数据预处理步骤都涉及数据清洗、转换、识别缺失值以及如何处理它们。只有预处理后的数据才能输入到机器学习算法中。在本节中，我们将主要关注数据预处理技术。这些技术包括相似度度量（如欧几里得距离、余弦距离和皮尔逊系数）以及降维技术，例如广泛用于推荐系统的**主成分分析**（**PCA**）。除了PCA之外，我们还有**奇异值分解**（**SVD**）、子集特征选择方法来降低数据集的维度，但我们的研究仅限于PCA。
- en: Similarity measures
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相似度度量
- en: As discussed in the previous chapter, every recommender system works on the
    concept of similarity between items or users. In this section, let's explore some
    similarity measures such as Euclidian distance, Cosine distance, and Pearson correlation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，每个推荐系统都是基于物品或用户之间的相似性概念工作的。在本节中，让我们探讨一些相似度度量，例如欧几里得距离、余弦距离和皮尔逊相关系数。
- en: Euclidian distance
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The simplest technique for calculating the similarity between two items is
    by calculating its Euclidian distance. The Euclidean distance between two points/objects
    (point *x* and point *y*) in a dataset is defined by the following equation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两个物品之间相似度的最简单技术是计算其欧几里得距离。在数据集中，两个点/对象（点 *x* 和点 *y*）之间的欧几里得距离由以下方程定义：
- en: '![Euclidian distance](img/B03888_02_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![欧几里得距离](img/B03888_02_02.jpg)'
- en: In this equation, (*x*, *y*) are two consecutive data points, and *n* is the
    number of attributes for the dataset.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，(*x*, *y*) 是两个连续的数据点，*n* 是数据集的属性数量。
- en: 'R script to calculate the Euclidean distance is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算欧几里得距离的R脚本如下所示：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Cosine distance
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 余弦距离
- en: 'Cosine similarity is a measure of similarity between two vectors of an inner
    product space that measures the cosine of the angle between them. Cosine similarity
    is given by this equation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度是内积空间中两个向量之间相似度的度量，它衡量了它们之间角度的余弦值。余弦相似度由以下方程给出：
- en: '![Cosine distance](img/B03888_02_03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![余弦距离](img/B03888_02_03.jpg)'
- en: 'R script to calculate the cosine distance is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 计算余弦距离的R脚本如下所示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this equation, *x* is the matrix containing all variables in a dataset. The
    `cosine` function is available in the `lsa` package.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*x* 是包含数据集中所有变量的矩阵。`cosine` 函数在 `lsa` 包中可用。
- en: Pearson correlation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数
- en: 'Similarity between two products can also be given by the correlation existing
    between their variables. Pearson''s correlation coefficient is a popular correlation
    coefficient calculated between two variables as the covariance of the two variables
    divided by the product of their standard deviations. This is given by *ƿ* *(rho)*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 两个产品之间的相似度也可以通过它们变量之间的相关系数来表示。皮尔逊相关系数是两个变量之间的一种流行相关系数，它是两个变量的协方差除以它们标准差的乘积。这由
    *ƿ* *(rho)* 给出：
- en: '![Pearson correlation](img/B03888_02_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![皮尔逊相关系数](img/B03888_02_04.jpg)'
- en: 'R script is given by these lines of code:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: R 脚本如下所示：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Empirical studies showed that Pearson coefficient outperformed other similarity
    measures for user-based collaborative filtering recommender systems. The studies
    also show that Cosine similarity consistently performs well in item-based collaborative
    filtering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实证研究表明，皮尔逊系数在基于用户的协同过滤推荐系统中优于其他相似度度量。研究还表明，余弦相似度在基于物品的协同过滤中表现始终良好。
- en: Dimensionality reduction
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度约简
- en: One of the most commonly faced problems while building recommender systems is
    high-dimensional and sparse data. At many times, we face a situation where we
    have a large set of features and fewer data points. In such situations, when we
    fit a model to the dataset, the predictive power of the model will be lower. This
    scenario is often termed as the curse of dimensionality. In general, adding more
    data points or decreasing the feature space, also known as dimensionality reduction,
    often reduces the effects of the curse of dimensionality. In this chapter, we
    will discuss PCA, a popular dimensionality reduction technique to reduce the effects
    of the curse of dimensionality.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建推荐系统时，最常见的问题之一是高维和稀疏数据。很多时候，我们会遇到一个特征集很大而数据点很少的情况。在这种情况下，当我们将模型拟合到数据集时，模型的预测能力会降低。这种情况通常被称为维度诅咒。一般来说，增加数据点或减少特征空间（也称为降维）通常可以减少维度诅咒的影响。在本章中，我们将讨论PCA，这是一种流行的降维技术，用于减少维度诅咒的影响。
- en: Principal component analysis
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Principal component analysis is a classical statistical technique for dimensionality
    reduction. The PCA algorithm transforms the data with high-dimensional space to
    a space with fewer dimensions. The algorithm linearly transforms m-dimensional
    input space to n-dimensional (*n<m*) output space, with the objective to minimize
    the amount of information/variance lost by discarding (*m-n*) dimensions. PCA
    allows us to discard the variables/features that have less variance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析是一种经典的降维统计技术。PCA算法将高维空间中的数据转换到低维空间。该算法将m维输入空间线性变换为n维（*n<m*）输出空间，目标是通过丢弃（*m-n*）维来最小化丢失的信息/方差。PCA允许我们丢弃具有较小方差的变量/特征。
- en: Technically speaking, PCA uses orthogonal projection of highly correlated variables
    to a set of values of linearly uncorrelated variables called principal components.
    The number of principal components is less than or equal to the number of original
    variables. This linear transformation is defined in such a way that the first
    principal component has the largest possible variance. It accounts for as much
    of the variability in the data as possible by considering highly correlated features.
    Each succeeding component in turn has the highest variance using the features
    that are less correlated with the first principal component and that are orthogonal
    to the preceding component.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，主成分分析（PCA）通过将高度相关的变量正交投影到一组线性不相关的变量（称为主成分）上来实现。主成分的数量小于或等于原始变量的数量。这种线性变换被定义为第一个主成分具有最大的可能方差。它通过考虑高度相关的特征来解释数据中的尽可能多的变异性。每个后续成分依次使用与第一个主成分相关性较低且与前一个成分正交的特征，以获得最高的方差。
- en: Let's understand this in simple terms. Assume we have three dimensional data
    space with two features more correlated with each other than with the third. We
    now want to reduce the data to two-dimensional space using PCA. The first principal
    component is created in such a way that it explains maximum variance using the
    two correlated variables along the data. In the following graph, the first principal
    component (bigger line) is along the data explaining most variance. To choose
    the second principal component, we need to choose another line that has the highest
    variance, is uncorrelated, and is orthogonal to the first principal component.
    The implementation and technical details of PCA are beyond the scope of this book,
    so we will discuss how it is used in R.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用简单的话来理解这个概念。假设我们有一个三维数据空间，其中有两个特征比与第三个特征更相关。现在我们想使用PCA将数据减少到二维空间。第一个主成分是以一种方式创建的，它使用数据中的两个相关变量来解释最大的方差。在下面的图中，第一个主成分（较粗的线）沿着数据解释了大部分的方差。为了选择第二个主成分，我们需要选择另一条具有最高方差、不相关且与第一个主成分正交的线。PCA的实现和技术细节超出了本书的范围，因此我们将讨论如何在R中使用它。
- en: '![Principal component analysis](img/B03888_02_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B03888_02_05.jpg)'
- en: 'We will illustrate PCA using the USArrests dataset. The USArrests dataset contains
    crime-related statistics, such as Assault, Murder, Rape, and UrbanPop per 100,000
    residents in 50 states in the US:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用USArrests数据集来说明PCA。USArrests数据集包含与犯罪相关的统计数据，例如袭击、谋杀、强奸和每10万人中的城市人口数，这些数据来自美国50个州：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Standard deviations**:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**标准差**：'
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Rotation**:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**旋转**：'
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![Principal component analysis](img/B03888_02_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B03888_02_06.jpg)'
- en: In the preceding image, known as a biplot, we can see the two principal components
    (**PC1** and **PC2**) of the USArrests dataset. The red arrows represent the loading
    vectors, which represent how the feature space varies along the principal component
    vectors.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，被称为双图，我们可以看到USArrests数据集的两个主成分（**PC1** 和 **PC2**）。红色箭头代表载荷向量，它们表示特征空间沿着主成分向量的变化。
- en: 'From the plot, we can see that the first principal component vector, **PC1**,
    more or less places equal weight on three features: **Rape**, **Assault**, and
    **Murder**. This means that these three features are more correlated with each
    other than the **UrbanPop** feature. In the second principal component, **PC2**
    places more weight on **UrbanPop** than the remaining 3 features are less correlated
    with them.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，第一个主成分向量，**PC1**，在三个特征：**强奸**、**攻击**和**谋杀**上或多或少给予相同的权重。这意味着这三个特征之间的相关性比**城市人口**特征更高。在第二个主成分，**PC2**比其他三个特征更重视**城市人口**，而其他三个特征与它们的相关性较低。
- en: Data mining techniques
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挖掘技术
- en: In this section, we will look at commonly used data-mining algorithms, such
    as k-means clustering, support vector machines, decision trees, bagging, boosting,
    and random forests. Evaluation techniques such as cross validation, regularization,
    confusion matrix, and model comparison are explained in brief.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨常用的数据挖掘算法，如k-均值聚类、支持向量机、决策树、bagging、boosting和随机森林。交叉验证、正则化、混淆矩阵和模型比较等评估技术将简要介绍。
- en: Cluster analysis
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析
- en: Cluster analysis is the process of grouping objects together in a way that objects
    in one group are more similar than objects in other groups.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是将对象分组的过程，使得同一组中的对象比其他组中的对象更相似。
- en: An example would be identifying and grouping clients with similar booking activities
    on a travel portal, as shown in the following figure.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是识别和分组在旅游门户上有相似预订活动的客户，如图所示。
- en: In the preceding example, each group is called a cluster, and each member (data
    point) of the cluster behaves in a manner similar to its group members.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，每个组被称为一个聚类，聚类中的每个成员（数据点）的行为与其组内成员相似。
- en: '![Cluster analysis](img/B03888_02_07.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![聚类分析](img/B03888_02_07.jpg)'
- en: Cluster analysis
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析
- en: 'Cluster analysis is an unsupervised learning method. In supervised methods,
    such as regression analysis, we have input variables and response variables. We
    fit a statistical model to the input variables to predict the response variable.
    Whereas in unsupervised learning methods, however, we do not have any response
    variable to predict; we only have input variables. Instead of fitting a model
    to the input variables to predict the response variable, we just try to find patterns
    within the dataset. There are three popular clustering algorithms: hierarchical
    cluster analysis, *k*-means cluster analysis, and two-step cluster analysis. In
    the following section, we will learn about *k*-means clustering.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是一种无监督学习方法。在监督方法中，如回归分析，我们有输入变量和响应变量。我们将统计模型拟合到输入变量以预测响应变量。然而，在无监督学习方法中，我们没有任何响应变量来预测；我们只有输入变量。我们不是将模型拟合到输入变量以预测响应变量，而是试图在数据集中找到模式。有三种流行的聚类算法：层次聚类分析、*k*-均值聚类分析和两步聚类分析。在下一节中，我们将学习*k*-均值聚类。
- en: Explaining the k-means cluster algorithm
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释k-均值聚类算法
- en: '*k*-means is an unsupervised, iterative algorithm where k is the number of
    clusters to be formed from the data. Clustering is achieved in two steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-均值是一种无监督的迭代算法，其中k是要从数据中形成的聚类数量。聚类分为两个步骤：'
- en: '**Cluster assignment step**: In this step, we randomly choose two cluster points
    (red dot and green dot) and assign each data point to the cluster point that is
    closer to it (top part of the following image).'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**聚类分配步骤**：在这个步骤中，我们随机选择两个聚类点（红色点和绿色点）并将每个数据点分配给离它更近的聚类点（以下图像的上半部分）。'
- en: '**Move centroid step**: In this step, we take the average of the points of
    all the examples in each group and move the centroid to the new position, that
    is, mean position calculated (bottom part of the following image).'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**移动质心步骤**：在这个步骤中，我们取每个组中所有示例点的平均值并将质心移动到新的位置，即计算出的平均位置（以下图像的下半部分）。'
- en: The preceding steps are repeated until all the data points are grouped into
    two groups and the mean of the data points after moving the centroid doesn't change.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的步骤会重复进行，直到所有数据点都被分成两组，并且移动后的数据点均值不发生变化。
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_08_new.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![解释 k-means 聚类算法](img/B03888_02_08_new.jpg)'
- en: Steps of cluster analysis
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析的步骤
- en: 'The preceding image shows how a clustering algorithm works on data to form
    clusters. See the R implementation of *k*-means clustering on iris dataset as
    follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像展示了聚类算法如何在数据上工作以形成簇。以下是如何在鸢尾花数据集上实现 *k*-均值聚类的 R 语言版本：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_09.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![解释 k-means 聚类算法](img/B03888_02_09.jpg)'
- en: Cluster analysis results
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析结果
- en: 'The preceding image shows the formation of clusters on the iris data, and the
    clusters account for 95 percent of the data. In the preceding example, the number
    of clusters of *k* value is selected using the `elbow` method, as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了在鸢尾花数据上形成簇的情况，这些簇占数据量的 95%。在先前的例子中，使用 `elbow` 方法选择了 *k* 值的簇数，如下所示：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following image shows the cost reduction for *k* values:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像显示了 *k* 值的成本降低：
- en: '![Explaining the k-means cluster algorithm](img/B03888_02_10_new.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![解释 k-means 聚类算法](img/B03888_02_10_new.jpg)'
- en: From the preceding figure, we can observe that the direction of the cost function
    is changed at cluster number 5\. Hence, we choose 5 as our number of clusters
    k. Since the number of optimal clusters is found at the elbow of the graph, we
    call it the elbow method.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以观察到成本函数的方向在簇数为 5 时发生变化。因此，我们选择 5 作为我们的簇数 k。由于最佳簇数出现在图表的肘部，我们称之为肘部方法。
- en: Support vector machine
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Support vector machine algorithms are a form of supervised learning algorithms
    employed to solve classification problems. SVM is generally treated as one of
    the best algorithms to deal with classification problems. Given a set of training
    examples, where each data point falls into one of two categories, an SVM training
    algorithm builds a model that assigns new data points into one category or the
    other. This model is a representation of the examples as a points in space, mapped
    so that the examples of the separate categories are divided by a margin that is
    as wide as possible, as shown in the following image. New examples are then mapped
    into that same space and predicted to belong to a category based on which side
    of the gap they fall on. In this section, we will go through an overview and implementation
    of SVMs without going into mathematical details.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机算法是一种用于解决分类问题的监督学习算法。SVM 通常被视为处理分类问题中最好的算法之一。给定一组训练示例，其中每个数据点属于两个类别之一，SVM
    训练算法构建一个模型，将新数据点分配到其中一个类别。这个模型是示例在空间中的表示，映射方式使得不同类别的示例通过尽可能宽的边界分开，如下面的图像所示。然后，将新示例映射到相同的空间，并根据它们落在间隙的哪一侧预测它们属于哪个类别。在本节中，我们将概述
    SVM 的实现，而不涉及数学细节。
- en: 'When SVM is applied to a p-dimensional dataset, the data is mapped to a *p-1*
    dimensional hyperplane, and the algorithm finds a clear boundary with a sufficient
    margin between classes. Unlike other classification algorithms that also create
    a separating boundary to classify data points, SVM tries to choose a boundary
    that has the maximum margin to separate the classes, as shown in the following
    image:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当 SVM 应用于 p 维数据集时，数据被映射到 *p-1* 维超平面，算法找到具有足够边界的清晰边界来区分类别。与其他也创建分离边界来分类数据点的分类算法不同，SVM
    尝试选择具有最大边界的边界来区分类别，如下面的图像所示：
- en: '![Support vector machine](img/B03888_02_11.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![支持向量机](img/B03888_02_11.jpg)'
- en: Consider a two-dimensional dataset having two classes, as shown in the preceding
    image. Now, when the SVM algorithm is applied, first it checks whether a one-dimensional
    hyperplane exists to map all the data points. If the hyperplane exists, the linear
    classifier creates a decision boundary with a margin to separate the classes.
    In the preceding image, the thick red line is the decision boundary, and the thinner
    blue and red lines are the margins of each class from the boundary. When new test
    data is used to predict the class, the new data falls into one of the two classes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有两个类别的二维数据集，如图所示。现在，当应用SVM算法时，首先它会检查是否存在一个一维超平面来映射所有数据点。如果存在超平面，线性分类器会创建一个带有边界的决策边界来分离类别。在先前的图像中，粗红色的线是决策边界，而较细的蓝色和红色线是每个类别边界的外部边界。当使用新的测试数据来预测类别时，新的数据将落入两个类别之一。
- en: 'Here are some key points to be noted:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些需要注意的关键点：
- en: Though an infinite number of hyperplanes can be created, SVM chooses only one
    hyperplane that has the maximum margin, that is, the separating hyperplane that
    is farthest from the training observations.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然可以创建无限多个超平面，但SVM只选择具有最大边界的超平面，即离训练观察结果最远的分离超平面。
- en: This classifier is only dependent on the data points that lie on the margins
    of the hyperplane, that is, on thin margins in the image, but not on other observations
    in the dataset. These points are called support vectors.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个分类器只依赖于位于超平面边缘的数据点，即在图像中的细边界，而不是数据集中的其他观察结果。这些点被称为支持向量。
- en: The decision boundary is affected only by the support vectors but not by other
    observations located away from the boundaries. If we change the data points other
    than the support vectors, there would not be any effect on the decision boundary.
    However, if the support vectors are changed, the decision boundary changes.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策边界只受支持向量的影响，而不受远离边界的其他观察结果的影响。如果我们改变除了支持向量之外的数据点，决策边界不会有任何影响。然而，如果支持向量改变，决策边界会改变。
- en: A large margin on the training data will also have a large margin on the test
    data to classify the test data correctly.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据上的大边界也会在测试数据上有大边界，以便正确分类测试数据。
- en: Support vector machines also perform well with non-linear datasets. In this
    case, we use radial kernel functions.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机在非线性数据集上也表现良好。在这种情况下，我们使用径向核函数。
- en: See the R implementation of SVM on the iris dataset in the following code snippet.
    We used the `e1071` package to run SVM. In R, the `SVM()` function contains the
    implementation of support vector machines present in the `e1071` package.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，您可以查看在鸢尾花数据集上SVM的R实现。我们使用了`e1071`包来运行SVM。在R中，`SVM()`函数包含了`e1071`包中存在的支持向量机的实现。
- en: Now, we will see that the `SVM()` method is called with the `tune()` method,
    which does cross validation and runs the model on different values of the cost
    parameters.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看到`SVM()`方法与`tune()`方法一起被调用，后者进行交叉验证并在成本参数的不同值上运行模型。
- en: 'The cross-validation method is used to evaluate the accuracy of the predictive
    model before testing on future unseen data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证方法用于在测试未来未见数据之前评估预测模型的准确性：
- en: '[PRE8]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Call**:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**调用**:'
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Parameters**:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数**:'
- en: '[PRE10]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `tune$best.model` object tells us that the model works best with the cost
    parameter as `10` and total number of support vectors as `25`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`tune$best.model`对象告诉我们，该模型在成本参数为`10`和总支持向量数为`25`时效果最佳：'
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Decision trees
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are a simple, fast, tree-based supervised learning algorithm
    to solve classification problems. Though not very accurate when compared to other
    logistic regression methods, this algorithm comes in handy while dealing with
    recommender systems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种简单、快速的基于树的监督学习算法，用于解决分类问题。虽然与其他逻辑回归方法相比，其准确性不高，但在处理推荐系统时，这个算法非常有用。
- en: 'We define the decision trees with an example. Imagine a situation where you
    have to predict the class of flower based on its features such as petal length,
    petal width, sepal length, and sepal width. We will apply the decision tree methodology
    to solve this problem:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个例子定义决策树。想象一下，你必须根据花瓣长度、花瓣宽度、萼片长度和萼片宽度等特征来预测花的类别。我们将应用决策树方法来解决这个问题：
- en: Consider the entire data at the start of the algorithm.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在算法开始时考虑整个数据。
- en: Now, choose a suitable question/variable to divide the data into two parts.
    In our case, we chose to divide the data based on petal length *> 2.45* and *<=
    2.45*. This separates flower class `setosa` from the rest of the classes.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，选择一个合适的问题/变量将数据分成两部分。在我们的案例中，我们选择根据花瓣长度 *> 2.45* 和 *<= 2.45* 来分割数据。这将从其他类别中分离出花类
    `setosa`。
- en: Now, further divide the data having petal length *>2.45*, based on the same
    variable with petal length *< 4.5* and *>= 4.5*, as shown in the following image.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，根据与花瓣长度 *< 4.5* 和 *>= 4.5* 相同的变量，进一步将花瓣长度 *> 2.45* 的数据进行分割，如下面的图像所示。
- en: This splitting of the data will be further divided by narrowing down the data
    space until we reach a point where all the bottom points represent the response
    variables or where further logical split cannot be done on the data.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种数据分割将进一步通过缩小数据空间进行细分，直到所有底部点都代表响应变量或数据不能再进行进一步逻辑分割。
- en: 'In the following decision tree image, we have one root node, four internal
    nodes where data split occurred, and five terminal nodes where data split cannot
    be done any further. They are defined as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的决策树图像中，我们有一个根节点，四个发生数据分割的内部节点，以及五个不能再进行数据分割的终端节点。它们定义如下：
- en: '**Petal.Length <2.45** as root node'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣长度 <2.45** 作为根节点'
- en: '**Petal.Length <4.85**, **Sepal.Length <5.15,** and **Petal.Width <1.75** are
    called internal nodes'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**花瓣长度 <4.85**、**花萼长度 <5.15** 和 **花瓣宽度 <1.75** 被称为内部节点'
- en: Final nodes having the class of the flowers are called terminal nodes
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有花朵类别的最终节点被称为终端节点
- en: The lines connecting the nodes are called the branches of the tree
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接节点的线条被称为树的分支
- en: 'While predicting responses on new data using the previously built model, each
    new data point is taken through each node, a question is asked, and a logical
    path is taken to reach its logical class, as shown in the following figure:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用先前构建的模型在新的数据上预测响应时，每个新的数据点都会通过每个节点，提出一个问题，并采取逻辑路径到达其逻辑类别，如下面的图所示：
- en: '![Decision trees](img/B03888_02_12.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B03888_02_12.jpg)'
- en: See the decision tree implementation in R on the iris dataset using the tree
    package available from **Comprehensive R Archive Network** (**CRAN**).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅使用**综合R档案网络**（**CRAN**）提供的tree包在iris数据集上实现的决策树。
- en: 'The summary of the mode is given here. It tells us that the misclassification
    rate is 0.0381, indicating that the model is accurate:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结信息在此给出。它告诉我们误分类率为0.0381，表明模型是准确的：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Classification tree**:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类树**：'
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Ensemble methods
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法
- en: In data mining, we use ensemble methods, which means using multiple learning
    algorithms to obtain better predictive results than applying any single learning
    algorithm on any statistical problem. This section will provide an overview of
    popular ensemble methods such as bagging, boosting, and random forests
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据挖掘中，我们使用集成方法，这意味着使用多个学习算法来获得比在任何一个统计问题中应用单个学习算法更好的预测结果。本节将概述流行的集成方法，如bagging、boosting和随机森林。
- en: Bagging
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: Bagging is also known as Bootstrap aggregating. It is designed to improve the
    stability and accuracy of machine-learning algorithms. It helps avoid over fitting
    and reduces variance. This is mostly used with decision trees.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging也称为Bootstrap aggregating。它旨在提高机器学习算法的稳定性和准确性。它有助于避免过拟合并减少方差。这主要用于决策树。
- en: 'Bagging involves randomly generating Bootstrap samples from the dataset and
    trains the models individually. Predictions are then made by aggregating or averaging
    all the response variables:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging涉及从数据集中随机生成Bootstrap样本并单独训练模型。然后通过聚合或平均所有响应变量进行预测：
- en: For example, consider a dataset (*Xi, Yi*), where *i=1 …n*, contains *n* data
    points.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集（*Xi, Yi*），其中*i=1 …n*，包含*n*个数据点。
- en: Now, randomly select B samples with replacements from the original dataset using
    Bootstrap technique.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，使用Bootstrap技术从原始数据集中随机选择带有替换的B样本。
- en: Next, train the B samples with regression/classification models independently.
    Then, predictions are made on the test set by averaging the responses from all
    the B models generated in the case of regression. Alternatively, the most often
    occurring class among B samples is generated in the case of classification.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，独立地对B样本使用回归/分类模型进行训练。然后，通过平均回归情况下所有生成的B模型的响应，在测试集上进行预测。或者，在分类情况下，生成B样本中最常出现的类别。
- en: Random forests
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Random forests
- en: Random forests are improvised supervised algorithms than bootstrap aggregation
    or bagging methods, though they are built on a similar approach. Unlike selecting
    all the variables in all the B samples generated using the Bootstrap technique
    in bagging, we select only a few predictor variables randomly from the total variables
    for each of the B samples. Then, these samples are trained with the models. Predictions
    are made by averaging the result of each model. The number of predictors in each
    sample is decided using the formula *m = √p*, where *p* is the total variable
    count in the original dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是比自助聚合或袋装方法更复杂的监督算法，尽管它们基于类似的方法。与在袋装中使用Bootstrap技术生成的所有B样本中选择所有变量不同，我们从总变量中随机选择几个预测变量作为每个B样本。然后，这些样本使用模型进行训练。预测是通过平均每个模型的预测结果来进行的。每个样本中的预测变量数量是通过公式
    *m = √p* 来决定的，其中 *p* 是原始数据集中变量的总数。
- en: 'Here are some key notes:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关键点：
- en: This approach removes the condition of dependency of strong predictors in the
    dataset as we intentionally select fewer variables than all the variables for
    every iteration
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法消除了数据集中强预测变量的依赖性条件，因为我们故意选择比每次迭代的全部变量更少的变量
- en: This approach also de-correlates variables, resulting in less variability in
    the model and, hence, more reliability
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法还会去相关变量，导致模型中的变异性降低，因此更可靠
- en: 'Refer to the R implementation of random forests on the iris dataset using the
    `randomForest` package available from CRAN:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考CRAN上可用的`randomForest`包在鸢尾花数据集上的随机森林R实现：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Call**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**调用**：'
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Boosting
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Boosting
- en: 'Unlike with bagging, where multiple copies of Bootstrap samples are created,
    a new model is fitted for each copy of the dataset, and all the individual models
    are combined to create a single predictive model, each new model is built using
    information from previously built models. Boosting can be understood as an iterative
    method involving two steps:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装不同，袋装会创建多个Bootstrap样本副本，每个数据集副本都会拟合一个新的模型，并将所有单个模型组合成一个预测模型，每个新模型都是使用先前构建的模型的信息构建的。Boosting可以理解为涉及两个步骤的迭代方法：
- en: A new model is built on the residuals of previous models instead of the response
    variable
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新模型是基于先前模型的残差而不是响应变量构建的
- en: Now, the residuals are calculated from this model and updated to the residuals
    used in the previous step
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，从该模型计算残差并更新为上一步中使用的残差
- en: 'The preceding two steps are repeated for multiple iterations, allowing each
    new model to learn from its previous mistakes, thereby improving the model accuracy:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个步骤会重复多次迭代，允许每个新模型从其先前的错误中学习，从而提高模型精度：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Boosting](img/B03888_02_13.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting](img/B03888_02_13.jpg)'
- en: 'The output of the preceding code is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码的输出如下：
- en: '![Boosting](img/B03888_02_14.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![Boosting](img/B03888_02_14.jpg)'
- en: 'In the following code snippet, the output value for the `predict()` function
    is used in the `apply()` function to pick the response with the highest probability
    among each row in the `pred` matrix. The resultant output from the `apply()` function
    is the prediction for the response variable:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，`predict()`函数的输出值用于`apply()`函数，以从`pred`矩阵的每一行中选择概率最高的响应。`apply()`函数的结果输出是响应变量的预测：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Evaluating data-mining algorithms
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估数据挖掘算法
- en: In the previous sections, we have seen various data-mining techniques used in
    recommender systems. In this section, you will learn how to evaluate models built
    using data-mining techniques. The ultimate goal for any data analytics model is
    to perform well on future data. This objective could be achieved only if we build
    a model that is efficient and robust during the development stage.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了在推荐系统中使用的各种数据挖掘技术。在本节中，你将学习如何评估使用数据挖掘技术构建的模型。任何数据分析模型的最终目标是在未来数据上表现良好。只有当我们构建一个在开发阶段既高效又健壮的模型时，才能实现这一目标。
- en: 'While evaluating any model, the most important things we need to consider are
    as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估任何模型时，我们需要考虑的最重要的事情如下：
- en: Whether the model is over fitting or under fitting
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否过拟合或欠拟合
- en: How well the model fits the future data or test data
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型与未来数据或测试数据拟合得有多好
- en: Under fitting, also known as bias, is a scenario when the model doesn't even
    perform well on training data. This means that we fit a less robust model to the
    data. For example, say the data is distributed non-linearly and we are fitting
    the data with a linear model. From the following image, we see that data is non-linearly
    distributed. Assume that we have fitted a linear model (orange line). In this
    case, during the model building stage itself, the predictive power will be low.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合，也称为偏差，是指模型甚至在训练数据上表现不佳的情况。这意味着我们为数据拟合了一个不太稳健的模型。例如，假设数据是非线性分布的，而我们用线性模型拟合数据。从以下图像中，我们看到数据是非线性分布的。假设我们拟合了一个线性模型（橙色线）。在这种情况下，在模型构建阶段本身，预测能力就会很低。
- en: Over fitting is a scenario when the model performs well on training data, but
    does really bad on test data. This scenario arises when the model memorizes the
    data pattern rather than learning from data. For example, say the data is distributed
    in a non-linear pattern, and we have fitted a complex model, shown using the green
    line. In this case, we observe that the model is fitted very close to the data
    distribution, taking care of all the ups and downs. In this case, the model is
    most likely to fail on previously unseen data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是指模型在训练数据上表现良好，但在测试数据上表现真的很差的情况。这种情况发生在模型记住数据模式而不是从数据中学习的时候。例如，假设数据以非线性模式分布，而我们拟合了一个复杂模型，用绿色线表示。在这种情况下，我们观察到模型非常接近数据分布进行拟合，注意到了所有的起伏。在这种情况下，模型最有可能在之前未见过的数据上失败。
- en: '![Evaluating data-mining algorithms](img/B03888_02_15.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![评估数据挖掘算法](img/B03888_02_15.jpg)'
- en: The preceding image shows simple, complex, and appropriate fitted models' training
    data. The green fit represents overfitting, the orange line represents underfitting,
    the black and blue lines represent the appropriate model, which is a trade-off
    between underfit and overfit.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像展示了简单、复杂和适当拟合模型的训练数据。绿色拟合代表过拟合，橙色线代表欠拟合，黑色和蓝色线代表适当的模型，这是欠拟合和过拟合之间的权衡。
- en: Any fitted model is evaluated to avoid previously mentioned scenarios using
    cross validation, regularization, pruning, model comparisons, ROC curves, confusion
    matrices, and so on .
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 任何拟合模型都会通过交叉验证、正则化、剪枝、模型比较、ROC曲线、混淆矩阵等来评估，以避免之前提到的场景。
- en: '**Cross validation**: This is a very popular technique for model evaluation
    for almost all models. In this technique, we divide the data into two datasets:
    a training dataset and a test dataset. The model is built using the training dataset
    and evaluated using the test dataset. This process is repeated many times. The
    test errors are calculated for every iteration. The averaged test error is calculated
    to generalize the model accuracy at the end of all the iterations.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**交叉验证**：这是几乎所有模型模型评估的一个非常流行的技术。在这种技术中，我们将数据分为两个数据集：一个训练数据集和一个测试数据集。模型使用训练数据集构建并使用测试数据集评估。这个过程重复多次。每次迭代都计算测试误差。计算所有迭代的平均测试误差，以概括所有迭代结束时模型的准确性。'
- en: '**Regularization**: In this technique, the data variables are penalized to
    reduce the complexity of the model with the objective to minimize the cost function.
    There are two most popular regularization techniques: ridge regression and lasso
    regression. In both techniques, we try to reduce the variable co-efficient to
    zero. Thus, a smaller number of variables will fit the data optimally.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**：在这种技术中，数据变量受到惩罚以降低模型的复杂性，目的是最小化成本函数。有两种最流行的正则化技术：岭回归和Lasso回归。在这两种技术中，我们都试图将变量系数降低到零。因此，更少的变量将能够最优地拟合数据。'
- en: '**Confusion matrix**: This technique is popularly used in evaluating a classification
    model. We build a confusion matrix using the results of the model. We calculate
    precision and recall/sensitivity/specificity to evaluate the model.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：这种技术在评估分类模型时非常流行。我们使用模型的输出结果构建一个混淆矩阵。我们计算精确度和召回率/灵敏度/特异性来评估模型。'
- en: '**Precision**: This is the probability whether the truly classified records
    are relevant.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**：这是真正分类记录是否相关的概率。'
- en: '**Recall/Sensitivity**: This is the probability whether the relevant records
    are truly classified.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率/灵敏度**：这是相关记录是否真正被分类的概率。'
- en: '**Specificity**: Also known as true negative rate, this is the proportion of
    truly classified wrong records.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**特异性**：也称为真正负率，这是真正分类错误记录的比例。'
- en: 'A confusion matrix shown in the following image is constructed using the results
    of classification models discussed in the previous section:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 下图所示的混淆矩阵是使用前一小节中讨论的分类模型的结果构建的：
- en: '![Evaluating data-mining algorithms](img/B03888_02_16.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![评估数据挖掘算法](img/B03888_02_16.jpg)'
- en: 'Let''s understand the confusion matrix:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解混淆矩阵：
- en: '**TRUE POSITVE (TP)**: This is a count of all the responses where the actual
    response is negative and the model predicted is positive'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**真正 (TP)**: 这是实际响应为负且模型预测为正的所有响应的计数。'
- en: '**FALSE POSITIVE (FP)**: This is a count of all the responses where the actual
    response is negative, but the model predicted is positive. It is, in general,
    a **FALSE ALARM**.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**误报 (FP)**: 这是实际响应为负但模型预测为正的所有响应的计数。通常，这是一个**误报**。'
- en: '**FALSE NEGATIVE (FN)**: This is a count of all the responses where the actual
    response is positive, but the model predicted is negative. It is, in general,
    **A MISS**.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**漏报 (FN)**: 这是实际响应为正但模型预测为负的所有响应的计数。通常，这是一个**漏报**。'
- en: '**TRUE NEGATIVE (TN)**: This is a count of all the responses where the actual
    response is negative, and the model predicted is negative.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**真负 (TN)**: 这是实际响应为负且模型预测也为负的所有响应的计数。'
- en: 'Mathematically, precision and recall/specificity is calculated as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，精确度和召回率/特异性计算如下：
- en: '![Evaluating data-mining algorithms](img/B03888_02_17.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![评估数据挖掘算法](img/B03888_02_17.jpg)'
- en: '**Model comparison**: A classification problem can be solved using one or more
    statistical models. For example, a classification problem can be solved using
    logistic regression, a decision tree, ensemble methods, and SVM. How do you choose
    which model fits the data well? A number of approaches are available for a suitable
    model selection, such as **Akaike information criteria** (**AIC**), **Bayesian**
    **information criteria** (**BIC**), and Adjusted R^2, Cᵨ. For each model, AIC
    / BIC / Adjusted R^2 is calculated. The model with least of these values is selected
    as the best model.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型比较**：可以使用一个或多个统计模型来解决分类问题。例如，可以使用逻辑回归、决策树、集成方法和SVM来解决分类问题。您如何选择哪个模型与数据拟合得更好？有几种方法可用于合适的模型选择，例如**赤池信息量准则
    (AIC**)、**贝叶斯信息准则 (BIC**)和调整后的R^2、Cᵨ。对于每个模型，计算AIC / BIC / 调整后的R^2。具有这些值中最小值的模型被选为最佳模型。'
- en: Tip
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Downloading the example code**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code fies from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all the Packt Publishing books you have purchased. If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the fies e-mailed directly to you.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[http://www.packtpub.com](http://www.packtpub.com)下载您购买的所有Packt Publishing书籍的示例代码文件。如果您在其他地方购买了这本书，您可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you learned about popular data preprocessing techniques, data-mining
    techniques, and evaluation techniques commonly used in recommender systems. In
    the next chapter, you will learn about the recommender systems introduced in [Chapter
    1](ch01.html "Chapter 1. Getting Started with Recommender Systems"), *Getting
    Started with Recommender Systems*, in more detail.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了在推荐系统中常用的一些流行数据预处理技术、数据挖掘技术和评估技术。在下一章中，您将更详细地学习[第1章](ch01.html "第1章.
    推荐系统入门")中介绍的推荐系统，即“推荐系统入门”。
