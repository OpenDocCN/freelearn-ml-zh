<html><head></head><body>
		<div id="_idContainer127">
			<h1 id="_idParaDest-96"><a id="_idTextAnchor096"/>Chapter 6: Model to Production and Beyond</h1>
			<p>In the last chapter, we discussed model training and prediction for online and batch models with <strong class="bold">Feast</strong>. For the exercise, we used the Feast infrastructure that was deployed to the AWS cloud during the exercises in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Stores to ML Models</em>. During these exercises, we looked at how Feast decouples feature engineering from model training and model prediction. We also learned how to use offline and online stores during batch and online prediction.</p>
			<p>In this chapter, we will reuse the feature engineering pipeline and the model built in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Stores to ML Models</em>, and <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>, to productionize the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) pipeline. The goal of this chapter is to reuse everything that we have built in the previous chapters, such as Feast infrastructure on AWS, feature engineering, model training, and model-scoring notebooks, to productionize the ML model. As we go through the exercises, it will give us an opportunity to look at how early adoption of Feast not only decoupled the ML pipeline stages but also accelerated the production readiness of the ML model. Once we productionize the batch and online ML pipelines, we will look at how the adoption of Feast opens up opportunities for other aspects of the ML life cycle, such as feature monitoring, automated model retraining, and also how it can accelerate the development of a future ML model. This chapter will help you understand how to productionize the batch and online models that use Feast, and how to use Feast for feature drift monitoring and model retraining.</p>
			<p>We will discuss the following topics in order:</p>
			<ul>
				<li>Setting up Airflow for orchestration</li>
				<li>Productionizing a batch model pipeline </li>
				<li>Productionizing an online model pipeline</li>
				<li>Beyond model production</li>
			</ul>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor097"/>Technical requirements</h1>
			<p>To follow the code examples in the chapter, the resources created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, and <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>, are required. You will need familiarity with Docker and any notebook environment, which could be a local setup, such as Jupyter, or an online notebook environment, such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account with full access to some of the resources, such as Redshift, S3, Glue, DynamoDB, and the IAM console. You can create a new account and use all the services for free during the trial period. You can find the code examples of the book and feature repository in the following GitHub links:</p>
			<ul>
				<li><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06</a></li>
				<li><a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation</a></li>
			</ul>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor098"/>Setting up Airflow for orchestration</h1>
			<p>To <a id="_idIndexMarker365"/>productionize the online and batch model, we need a workflow orchestration tool that can run the ML pipelines for us on schedule. There <a id="_idIndexMarker366"/>are a bunch of tools available, such as Apache Airflow, AWS Step Functions, and SageMaker Pipelines. You can also run it as GitHub workflows if you prefer. Depending on the tools you are familiar with or offered at your organization, orchestration may differ. For this <a id="_idIndexMarker367"/>exercise, we will use Amazon <strong class="bold">Managed Workflows for Apache Airflow</strong> (<strong class="bold">MWAA</strong>). As the name suggests, it is an Apache Airflow-managed service by AWS. Let's create an Amazon MWAA environment in AWS.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Amazon MWAA doesn't have a free trial. You can view the pricing for the usage at this URL: <a href="https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/">https://aws.amazon.com/managed-workflows-for-apache-airflow/pricing/</a>. Alternatively, you can choose to run Airflow locally or on EC2 instances (EC2 has free tier resources). You can find the setup instructions to run Airflow locally or on EC2 here:</p>
			<p class="callout">Airflow local setup: <a href="https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4">https://towardsdatascience.com/getting-started-with-airflow-locally-and-remotely-d068df7fcb4</a></p>
			<p class="callout">Airflow on EC2: <a href="https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441">https://christo-lagali.medium.com/getting-airflow-up-and-running-on-an-ec2-instance-ae4f3a69441</a></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor099"/>S3 bucket for Airflow metadata</h2>
			<p>Before <a id="_idIndexMarker368"/>we create an environment, we need an S3 bucket to store the Airflow dependencies, <strong class="bold">Directed Acyclic Graphs</strong> (<strong class="bold">DAGs</strong>), and<a id="_idIndexMarker369"/> so on. To create an S3 bucket first, please follow the instructions in the <em class="italic">Amazon S3 for storing data</em> subsection of <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. Alternatively, you can also choose to use an existing bucket. We will be creating a new bucket with the name <strong class="source-inline">airflow-for-ml-mar-2022</strong>. In the S3 bucket, create a folder named <strong class="source-inline">dags</strong>. We will be using this folder to store all the Airflow DAGs. </p>
			<p>The Amazon MWAA provides multiple different ways to configure additional plugins and Python dependencies to be installed in the Airflow environment. Since we need to install a few Python dependencies to run our project, we need to tell Airflow to install these required dependencies. One way of doing it is by using the <strong class="source-inline">requirements.txt</strong> file. The following code block shows the contents of the file:</p>
			<pre class="source-code">papermill==2.3.4</pre>
			<pre class="source-code">boto3==1.21.41</pre>
			<pre class="source-code">ipython==8.2.0</pre>
			<pre class="source-code">ipykernel==6.13.0</pre>
			<pre class="source-code">apache-airflow-providers-papermill==2.2.3</pre>
			<p>Save the contents of the preceding code block in a <strong class="source-inline">requirements.txt</strong> file. We will be<a id="_idIndexMarker370"/> using <strong class="source-inline">papermill</strong> (<a href="https://papermill.readthedocs.io/en/latest/">https://papermill.readthedocs.io/en/latest/</a>) to run the Python notebooks. You can also extract code and run the Python script using the <strong class="source-inline">bash</strong> or <strong class="source-inline">python</strong> operator available in Airflow.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you are running Airflow locally, make sure that the library versions are compatible with the Airflow version. The Amazon MWAA Airflow version at the time of writing is 2.2.2.</p>
			<p>Once you <a id="_idIndexMarker371"/>have the <strong class="source-inline">requirement.txt</strong> file created, upload it into the S3 bucket we have created. We will be using it in the next section during the environment creation.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor100"/>Amazon MWAA environment for orchestration</h2>
			<p>Now that <a id="_idIndexMarker372"/>we have the required <a id="_idIndexMarker373"/>resources for creating the Amazon MWAA environment, let's follow the following steps to create the environment: </p>
			<ol>
				<li>To create a new environment, log in to your AWS account and navigate to the Amazon MWAA console using the search bar in the AWS console. Alternatively, visit <a href="https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments">https://us-east-1.console.aws.amazon.com/mwaa/home?region=us-east-1#environments</a>. The following web page will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B18024_06_001.jpg" alt="Figure 6.1 – The Amazon MWAA environments console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – The Amazon MWAA environments console</p>
			<ol>
				<li value="2">On the page displayed in <em class="italic">Figure 6.1</em>, click on the <strong class="bold">Create environment</strong> button, and the<a id="_idIndexMarker374"/> following<a id="_idIndexMarker375"/> page will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B18024_06_002.jpg" alt="Figure 6.2 – Amazon MWAA environment details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Amazon MWAA environment details</p>
			<ol>
				<li value="3">Provide a name for the Amazon MWAA environment on the page displayed in <em class="italic">Figure 6.2</em>. Scroll down to the <strong class="bold">DAG code in Amazon S3</strong> section; you should see the following parameters on the screen:</li>
			</ol>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B18024_06_003.jpg" alt="Figure 6.3 – Amazon MWAA – the DAG code in S3 section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – Amazon MWAA – the DAG code in S3 section</p>
			<ol>
				<li value="4">On<a id="_idIndexMarker376"/> the screen displayed in <em class="italic">Figure 6.3</em>, enter the S3 bucket in the textbox or use the <strong class="bold">Browse S3</strong> button. Here, we will use the S3 bucket that we created earlier in the section. Once <a id="_idIndexMarker377"/>you select the S3 bucket, the other fields will appear. For <strong class="bold">DAGs folder</strong>, select the folder that we created earlier in the S3 bucket. Also, for the <strong class="bold">Requirements file - optional</strong> field, browse for the <strong class="source-inline">requirements.txt</strong> file that we uploaded or enter the path to the file. As we don't need any plugins to run the project, we can leave the optional <strong class="bold">Plugins file</strong> field blank. Click on the <strong class="bold">Next</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B18024_06_004.jpg" alt="Figure 6.4 – Amazon MWAA advanced settings&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Amazon MWAA advanced settings</p>
			<ol>
				<li value="5">The next<a id="_idIndexMarker378"/> page displayed is shown in <em class="italic">Figure 6.4</em>. For <strong class="bold">Virtual private cloud (VPC)</strong>, select the available default VPC from the dropdown. One caveat here is that the selected VPC should <a id="_idIndexMarker379"/>have at least two private subnets. If it doesn't have the private subnets, when you try to select <strong class="bold">Subnet 1</strong> and <strong class="bold">Subnet 2</strong>, you will notice that all the options are grayed out. If you run into this scenario, click on <strong class="bold">Create MWAA VPC</strong>. It will take you to the CloudFormation console; once you have filled in the form with all the parameters, follow through and click on <strong class="bold">Create stack</strong>. It will create a VPC that can be used by Amazon MWAA. Once the VPC is created, come back to this window and select the new VPC and subnets, and continue. </li>
				<li>After selecting the VPC, for <strong class="bold">Web server access</strong>, select <strong class="bold">Public network</strong>; leave everything else to default, and scroll all the way down. In the <strong class="bold">Permissions</strong> section, you will notice that it says it will create a new role for Amazon MWAA. Make a note of the role name. We will have to add permissions to this role later. After that, click on <strong class="bold">Next</strong>.</li>
				<li>On the next page, review all the input provided, scroll all the way down, and click on <strong class="bold">Create environment</strong>. It will take a few minutes to create the environment.</li>
				<li>Once the <a id="_idIndexMarker380"/>environment is created, you should be able to see the environment in the <strong class="bold">Available</strong> state <a id="_idIndexMarker381"/>on the Amazon MWAA environments page. Pick the environment that we just created and click on the <strong class="bold">Open Airflow UI</strong> link. An Airflow home page will be displayed, similar to the one in the following figure:<div id="_idContainer099" class="IMG---Figure"><img src="image/B18024_06_005.jpg" alt="Figure 6.5 – The Airflow UI&#13;&#10;"/></div></li>
			</ol>
			<p class="figure">  </p>
			<p class="figure-caption">Figure 6.5 – The Airflow UI</p>
			<ol>
				<li value="9">To test whether everything is working fine, let's quickly create a simple DAG and look at how it works. The following code block creates a simple DAG with a dummy operator and a Python operator: <p class="source-code">from datetime import datetime</p><p class="source-code">from airflow import DAG</p><p class="source-code">from airflow.operators.dummy_operator import DummyOperator</p><p class="source-code">from airflow.operators.python_operator import PythonOperator</p><p class="source-code">def print_hello():</p><p class="source-code">    return 'Hello world from first Airflow DAG!'</p><p class="source-code">dag = DAG('hello_world', </p><p class="source-code">          description='Hello World DAG',</p><p class="source-code">          schedule_interval='@daily',</p><p class="source-code">          start_date=datetime(2017, 3, 20), </p><p class="source-code">          catchup=False)</p><p class="source-code">start = DummyOperator(task_id="start", dag=dag)</p><p class="source-code">hello_operator = PythonOperator(</p><p class="source-code">    task_id='hello_task', </p><p class="source-code">    python_callable=print_hello, </p><p class="source-code">    dag=dag)</p><p class="source-code">start &gt;&gt; hello_operator</p></li>
				<li>The DAG defined<a id="_idIndexMarker382"/> in the<a id="_idIndexMarker383"/> preceding code is pretty simple; it has two tasks – <strong class="source-inline">start</strong> and <strong class="source-inline">hello_operator</strong>. The <strong class="source-inline">start</strong> task is a <strong class="source-inline">DummyOperator</strong>, does nothing, and is used for making the DAG look pretty on the UI. The <strong class="source-inline">hello_operator</strong> task just invokes a function that returns a message. In the last line, we define a dependency between the operators. </li>
				<li>Copy the<a id="_idIndexMarker384"/> preceding <a id="_idIndexMarker385"/>code block, save the file as <strong class="source-inline">example_dag.py</strong>, and upload it to the <strong class="source-inline">dags</strong> folder in S3 that we created earlier. (My S3 location is <strong class="source-inline">s3://airflow-for-ml-mar-2022/dags</strong>.) Once you upload it, it should appear in the Airflow UI within seconds. The following figure displays the Airflow UI with the DAG:</li>
			</ol>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B18024_06_006.jpg" alt="Figure 6.6 – The Airflow UI with the example DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – The Airflow UI with the example DAG</p>
			<ol>
				<li value="12">By default, the DAGs are disabled; hence, when you visit the page, you may not see the exact page such as the one displayed in <em class="italic">Figure 6.6</em>. Enable the DAG by clicking on the toggle button in the left-most column. Once enabled, DAG will run for the first time and update the run results. You can also trigger the DAG using the icon in the <strong class="bold">Links</strong> column. Click on the <strong class="bold">hello_world</strong> hyperlink in the DAG column in the UI. You will see the details page of the DAG with different tabs. Feel free to play around and look at the different options available on the details page.  </li>
				<li>The following figure displays the graph view of the DAG:</li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B18024_06_007.jpg" alt="Figure 6.7 – The graph view of the DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – The graph view of the DAG</p>
			<ol>
				<li value="14">Now that we have verified that Airflow is set up correctly, let's add the required permissions for Airflow to run the ML pipeline. </li>
				<li>If <a id="_idIndexMarker386"/>you recall, during the<a id="_idIndexMarker387"/> last step of environment creation (the paragraph following <em class="italic">Figure 6.4</em>), we made note of the role name the Airflow environment is using to run the DAGs. Now, we need to add permissions to the role. To do so, navigate to the AWS IAM roles console page using the search function or visit https://us-east-1.console.aws.amazon.com/iamv2/home?region=us-east-1#/roles. In the console, you should see the IAM role that is associated with the Airflow environment. Select the IAM role; you should see the following page:</li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B18024_06_008.jpg" alt="Figure 6.8 – The Amazon MWAA IAM role&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – The Amazon MWAA IAM role</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">If you didn't make a note, you can find the role name in the environment details page on the AWS console.</p>
			<ol>
				<li value="16">In <em class="italic">Figure 6.8</em>, click <a id="_idIndexMarker388"/>on <strong class="bold">Add permissions</strong>; from<a id="_idIndexMarker389"/> the dropdown, select <strong class="bold">Attach policies</strong>, and you will be taken to the following page:</li>
			</ol>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18024_06_009.jpg" alt="Figure 6.9 – IAM – Attach policies&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – IAM – Attach policies</p>
			<ol>
				<li value="17">On the <a id="_idIndexMarker390"/>web page, search <a id="_idIndexMarker391"/>and select the following policies – <strong class="bold">AmazonS3FullAccess</strong>, <strong class="bold">AWSGlueConsoleFullAccess</strong>, <strong class="bold">AmazonRedshiftFullAccess</strong>, and <strong class="bold">AmazonDynamoDBFullAccess</strong>. Once the policies are selected, scroll down and click on <strong class="bold">Attach policies</strong> to save the role with the new policies. <p class="callout-heading">Important Note</p><p class="callout">It is never a good idea to assign full access to any of the resources without restrictions. When you run an enterprise application, it is recommended to restrict access based on the resources, such as read-only access to a specific S3 bucket and DynamoDB tables.</p><p class="callout">If you are running Airflow locally, you can use the IAM user credential in the notebook.</p></li>
			</ol>
			<p>Now that our orchestration system is ready, let's look at how to use it to productionize the ML pipeline.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor101"/>Productionizing the batch model pipeline</h1>
			<p>In <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, for model training, we used the features ingested by the feature engineering notebook. We also created a model-scoring <a id="_idIndexMarker392"/>notebook that fetches features for a set of customers from Feast and runs predictions for it using the trained model. For the sake of the experiment, let's assume that the raw data freshness latency is a day. That means the features need to be regenerated once a day, and the model needs to score customers against those features once a day and store the results in an S3 bucket for consumption. To achieve this, thanks to our early organization and decoupling of stages, all we need to do is run the feature engineering and model scoring notebook/Python script once a day consecutively. Now that we also have a tool to perform this, let's go ahead and schedule this workflow in the Airflow environment.</p>
			<p>The following figure displays how we will <a id="_idIndexMarker393"/>be operationalizing the batch model:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B18024_06_010.jpg" alt="Figure 6.10 – Operationalization of the batch model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Operationalization of the batch model</p>
			<p>As you can see in the figure, to operationalize the workflow, we will use Airflow to orchestrate the feature-engineering and model-scoring notebooks. The raw data source for feature engineering, in our case, is the S3 bucket where <strong class="source-inline">online-retail.csv</strong> is stored. As we have already designed our scoring notebook to load the production model from the model repo (in our case, an S3 bucket) and store the prediction results in a S3 bucket, we will reuse the same notebook. One thing you might notice here is that we are not using the model-training notebook for every run; the reason is obvious – we want to run predictions against a version of the model that has been validated, tested, and also met our performance criteria on the test data.</p>
			<p>Before <a id="_idIndexMarker394"/>scheduling this workflow, I have done minor changes to the feature-engineering notebook and model-prediction notebooks. The final notebooks can be found at the following GitHub URL: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb)">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/(ch6_feature_engineering.ipynb,ch6_model_prediction.ipynb)</a>. To schedule the workflow, download the final notebooks from GitHub and upload them to an S3 bucket that we created earlier, as an Airflow environment will need to access these notebooks during runs. I will upload it to the following location: <strong class="source-inline">s3://airflow-for-ml-mar-2022/notebooks/</strong>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">AWS secret access key and an S3 path – I have commented out AWS credentials in both the notebooks, as we are adding permissions to an Amazon MWAA IAM role. If you are running it in local Airflow, please uncomment and add secrets. Also, update the S3 URLs wherever necessary, as the S3 URLs point to the private buckets that I have created during the exercises.  </p>
			<p class="callout">Feature repo – As we have seen before, we must clone the feature repo so that the Feast library can read the metadata. You can follow the same <strong class="source-inline">git clone</strong> (provided that <strong class="source-inline">git</strong> is installed) approach or set up a GitHub workflow to push the repo to S3 and download the same in the notebook. I have left both code blocks in the notebook with comments. You can use whichever is convenient.</p>
			<p class="callout">S3 approach – To use an S3 download approach, clone the repo in your local system and run the following commands in the Linux terminal to upload it to a specific S3 location:</p>
			<p class="callout"><strong class="bold">export AWS_ACCESS_KEY_ID=&lt;aws_key&gt;</strong></p>
			<p class="callout"><strong class="bold">export AWS_SECRET_ACCESS_KEY=&lt;aws_secret&gt;</strong></p>
			<p class="callout"><strong class="bold">AWS_DEFAULT_REGION=us-east-1</strong></p>
			<p class="callout"><strong class="bold">aws s3 cp customer_segmentation s3://&lt;s3_bucket&gt;/customer_segmentation --recursive</strong></p>
			<p class="callout">On successful upload, you should be able to see the folder contents in the S3 bucket.</p>
			<p>Now that <a id="_idIndexMarker395"/>the notebooks are ready, let's write the Airflow DAG for the batch model pipeline. The DAG will have the following tasks in order – <strong class="source-inline">start</strong> (<strong class="source-inline">dummy operator</strong>), <strong class="source-inline">feature_engineering</strong> (<strong class="source-inline">Papermill operator</strong>), <strong class="source-inline">model_prediction</strong> (<strong class="source-inline">Papermill operator</strong>), and <strong class="source-inline">end</strong> (<strong class="source-inline">dummy operator</strong>).</p>
			<p>The following code block contains the first part of the Airflow DAG:</p>
			<pre class="source-code">from datetime import datetime</pre>
			<pre class="source-code">from airflow import DAG</pre>
			<pre class="source-code">from airflow.operators.dummy_operator import DummyOperator</pre>
			<pre class="source-code">from airflow.providers.papermill.operators.papermill import PapermillOperator</pre>
			<pre class="source-code">import uuid</pre>
			<pre class="source-code">dag = DAG('customer_segmentation_batch_model', </pre>
			<pre class="source-code">          description='Batch model pipeline', </pre>
			<pre class="source-code">          <strong class="bold">schedule_interval='@daily'</strong>, </pre>
			<pre class="source-code">          start_date=datetime(2017, 3, 20), catchup=False)</pre>
			<p>In the preceding code block, we have defined the imports and DAG parameters such as <strong class="source-inline">name</strong>, <strong class="source-inline">schedule_interval</strong>, and <strong class="source-inline">start_date</strong>. The <strong class="source-inline">schedule_interval='@daily'</strong> schedule says that the DAG should run daily.</p>
			<p>The following code block defines the rest of the DAG (the second part), which contains all the tasks and the dependencies among them:</p>
			<pre class="source-code">start = DummyOperator(task_id="<strong class="bold">start</strong>", dag=dag)</pre>
			<pre class="source-code">run_id = str(uuid.uuid1())</pre>
			<pre class="source-code">feature_eng = PapermillOperator(</pre>
			<pre class="source-code">    task_id="<strong class="bold">feature_engineering</strong>",</pre>
			<pre class="source-code">    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_feature_engineering.ipynb",</pre>
			<pre class="source-code">    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_feature_engineering_{ run_id }.ipynb",</pre>
			<pre class="source-code">    dag=dag,</pre>
			<pre class="source-code">    trigger_rule="all_success"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">model_prediction = PapermillOperator(</pre>
			<pre class="source-code">    task_id="<strong class="bold">model_prediction</strong>",</pre>
			<pre class="source-code">    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_model_prediction.ipynb",</pre>
			<pre class="source-code">    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_model_prediction_{run_id}.ipynb",</pre>
			<pre class="source-code">    dag=dag,</pre>
			<pre class="source-code">    trigger_rule="all_success"</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">end = DummyOperator(task_id="<strong class="bold">end</strong>", dag=dag, </pre>
			<pre class="source-code">                    trigger_rule="all_success")</pre>
			<pre class="source-code">start &gt;&gt; feature_eng &gt;&gt; model_prediction &gt;&gt; end</pre>
			<p>As you can <a id="_idIndexMarker396"/>see in the code block, there are four steps that will execute one after the other. The <strong class="source-inline">feature_engineering</strong> and <strong class="source-inline">model_prediction</strong> steps are run using <strong class="source-inline">PapermillOperator</strong>. This takes the path to the S3 notebook as input. I have also set an output path to another S3 location so that we can check the output notebook of each run. The last line defines the dependency between the tasks. Save the preceding two code blocks (the first and second parts) as a Python file and call it <strong class="source-inline">batch-model-pipeline-dag.py</strong>. After saving the file, navigate to the S3 console to upload the file into the <strong class="source-inline">dags</strong> folder that we pointed <a id="_idIndexMarker397"/>our Airflow environment to in <em class="italic">Figure 6.3</em>. The uploaded file is processed by the Airflow scheduler. When you navigate to the Airflow UI, you should see the new DAG called <strong class="bold">customer_segmentation_batch_model</strong> on the screen.</p>
			<p>The following figure displays the Airflow UI with the DAG:</p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B18024_06_011.jpg" alt="Figure 6.11 – The batch model DAG on Airflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – The batch model DAG on Airflow</p>
			<p>As we have not enabled the DAG by default option during the Airflow environment creation (which can be set in Airflow configuration variables in Amazon MWAA), when the DAG appears on the UI for the first time, it will be disabled. Click on the toggle button on the left-most column to enable it. Once enabled, the DAG will run for the first time. Click on the <strong class="bold">customer_segmentation_batch_model</strong> hyperlink to navigate to the details page, and feel free to look around to see the different visualization and properties of the DAG. If you navigate to the <strong class="bold">Graph</strong> tab, the DAG will be displayed, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B18024_06_012.jpg" alt="Figure 6.12 – The batch model DAG graph view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – The batch model DAG graph view</p>
			<p>In <em class="italic">Figure 6.12</em>, you can see the graph view of the DAG. If there were any failures in the last run, they will <a id="_idIndexMarker398"/>appear in red outline. You can also view the logs of successful execution or failures for each of the tasks. As all the tasks are green, this means everything went well. You can also see the results of the last few runs in <em class="italic">Figure 6.11</em>. Airflow also provides you with the history of all the runs. </p>
			<p>Now that the task run is complete, we can go and check the output notebook, the S3 bucket for the new set of features, or the S3 bucket for the new set of predictions. All three should be available after successful runs. Here, we will be verifying just the prediction results folder, but feel free to verify the others as well.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">In case of any failures, verify the logs for the failed tasks (click on the failed task in graph view to see available information). Check the permissions for Amazon MWAA, the S3 paths for input/output, and also whether all the requirements are installed in the Amazon MWAA environment.</p>
			<p>The following screenshot shows the new prediction results in an S3 bucket:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B18024_06_013.jpg" alt="Figure 6.13 – The prediction results in an S3 bucket&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – The prediction results in an S3 bucket</p>
			<p>In addition, you<a id="_idIndexMarker399"/> can also do all kinds of fancy things with Airflow, such as sending email notifications for failure, Slack notifications for daily runs, and integration with PagerDuty. Feel free to explore the options. Here is a<a id="_idIndexMarker400"/> list of supported providers in Airflow: <a href="https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html">https://airflow.apache.org/docs/apache-airflow-providers/packages-ref.html</a>. </p>
			<p>Now that our batch model is running in production, let's look at how to productionize the online model with Feast.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor102"/>Productionizing an online model pipeline</h1>
			<p>In the <a id="_idIndexMarker401"/>previous chapter, for the online model, we built REST endpoints to serve on-demand predictions for customer segmentation. Though the online model is hosted as a REST endpoint, it needs a supporting infrastructure for the following functions:</p>
			<ul>
				<li>To serve features in real time (we have Feast for that)</li>
				<li>To keep features up to date (we will use the feature-engineering notebook with Airflow orchestration for this)</li>
			</ul>
			<p>In this chapter, we will continue from where we left off and use the feature-engineering notebook built in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models,</em> in combination with a notebook to synchronize offline data to an online store in Feast. </p>
			<p>The following<a id="_idIndexMarker402"/> figure shows the operationalization<a id="_idIndexMarker403"/> of the online model pipeline:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B18024_06_014.jpg" alt="Figure 6.14 – The operationalization of the online model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – The operationalization of the online model</p>
			<p>As you can see in <em class="italic">Figure 6.14</em>, we will use Airflow for the orchestration of feature engineering; data freshness is still one day here, and scheduling can be done for a shorter duration. Feast can also support streaming data if there is a need. The following URL has an example you can use: <a href="https://docs.Feast.dev/reference/data-sources/push">https://docs.Feast.dev/reference/data-sources/push</a>. The REST endpoints developed in <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>, will be Dockerized and deployed as a SageMaker endpoint. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Once Dockerized, the Docker image can be used to deploy into any containerized environment, such as Elastic Container Service, Elastic BeanStalk, and Kubernetes. We are using SageMaker, as it takes less time to set up and also has advantages such as data capture and IAM authentication that come out of the box. </p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Orchestration of a feature engineering job</h2>
			<p>As we<a id="_idIndexMarker404"/> already have two notebooks (feature engineering and sync offline to online store) and we are familiar with Airflow, let's schedule the feature engineering workflow first. Again, in the notebook, I have done some minor changes. Please verify the changes before using it. You can find the notebooks (<strong class="source-inline">ch6_feature_engineering.ipynb</strong> a<a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter06/notebooks">nd <strong class="source-inline">ch6_sync_offline_online.ipynb</strong>) here: https://github.com/PacktPublishing/Feature-Store-for-Machin</a>e-Learning/tree/main/Chapter06/notebooks. Just the way we did it for the batch model, download the<a id="_idIndexMarker405"/> notebooks and upload them to a specific S3 location. I will be uploading them to the same location as before: <strong class="source-inline">s3://airflow-for-ml-mar-2022/notebooks/</strong>. Now that the notebooks are ready, let's write the Airflow DAG for the online model pipeline. The DAG will have the following steps in order – <strong class="source-inline">start</strong> (<strong class="source-inline">dummy operator</strong>), <strong class="source-inline">feature_engineering</strong> (<strong class="source-inline">Papermill operator</strong>), <strong class="source-inline">sync_offline_to_online</strong> (<strong class="source-inline">Papermill operator</strong>), and <strong class="source-inline">end</strong> (<strong class="source-inline">dummy operator</strong>).</p>
			<p>The following code block contains the first part of the Airflow DAG:</p>
			<pre class="source-code">from datetime import datetime</pre>
			<pre class="source-code">from airflow import DAG</pre>
			<pre class="source-code">from airflow.operators.dummy_operator import DummyOperator</pre>
			<pre class="source-code">from airflow.providers.papermill.operators.papermill import PapermillOperator</pre>
			<pre class="source-code">dag = DAG('customer_segmentation_online_model', </pre>
			<pre class="source-code">          description='Online model pipeline', </pre>
			<pre class="source-code">          <strong class="bold">schedule_interval='@daily'</strong>, </pre>
			<pre class="source-code">          start_date=datetime(2017, 3, 20), catchup=False)</pre>
			<p>Just like in the case of the batch model pipeline DAG, this contains the DAG parameters.</p>
			<p>The following code block defines the rest of the DAG (the second part), which contains all the tasks and the dependencies among them:</p>
			<pre class="source-code">start = DummyOperator(task_id="<strong class="bold">start</strong>")</pre>
			<pre class="source-code">run_time = datetime.now()</pre>
			<pre class="source-code">feature_eng = PapermillOperator(</pre>
			<pre class="source-code">    task_id="<strong class="bold">feature_engineering</strong>",</pre>
			<pre class="source-code">    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_feature_engineering.ipynb",</pre>
			<pre class="source-code">    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_feature_engineering_{run_time}.ipynb",</pre>
			<pre class="source-code">    trigger_rule="all_success",</pre>
			<pre class="source-code">    dag=dag</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">sync_offline_to_online = PapermillOperator(</pre>
			<pre class="source-code">    task_id="<strong class="bold">sync_offline_to_online</strong>",</pre>
			<pre class="source-code">    input_nb="s3://airflow-for-ml-mar-2022/notebooks/ch6_sync_offline_online.ipynb",</pre>
			<pre class="source-code">    output_nb=f"s3://airflow-for-ml-mar-2022/notebooks/runs/ch6_sync_offline_online_{run_time}.ipynb",</pre>
			<pre class="source-code">    trigger_rule="all_success",</pre>
			<pre class="source-code">    dag=dag</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">end = DummyOperator(task_id="<strong class="bold">end</strong>", trigger_rule="all_success")</pre>
			<pre class="source-code">start &gt;&gt; feature_eng &gt;&gt; sync_offline_to_online &gt;&gt; end</pre>
			<p>The structure <a id="_idIndexMarker406"/>of the Airflow DAG is similar to the batch model DAG we looked at earlier; the only difference is the third task, <strong class="source-inline">sync_offline_to_online</strong>. This notebook syncs the latest features from offline data to online data. Save the preceding two code blocks (the first and second parts) as a Python file and call it <strong class="source-inline">online-model-pipeline-dag.py</strong>. After saving the file, navigate to the S3 console to upload the file into the <strong class="source-inline">dags</strong> folder that we pointed our Airflow environment to in <em class="italic">Figure 6.3</em>. As with the batch model, the uploaded file is processed by the Airflow scheduler, and when you navigate to the Airflow UI, you should see the new DAG called <strong class="bold">customer_segmentation_online_model</strong> on the screen.</p>
			<p>The following <a id="_idIndexMarker407"/>screenshot displays the Airflow UI with the DAG:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B18024_06_015.jpg" alt="Figure 6.15 – The Airflow UI with both the online and batch models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – The Airflow UI with both the online and batch models</p>
			<p>To enable the DAG, click on the toggle button on the left-most column. Once enabled, the DAG will run for the first time. Click on the <strong class="bold">customer_segmentation_online_model</strong> hyperlink to navigate to the details page, and feel free to look around to see the different visualization and properties of the DAG. If you navigate to the <strong class="bold">Graph</strong> tab, the DAG will be displayed, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B18024_06_016.jpg" alt="Figure 6.16 – The online model pipeline graph view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – The online model pipeline graph view</p>
			<p>As you can<a id="_idIndexMarker408"/> see in <em class="italic">Figure 6.16</em>, on successful runs, the graph will be green. As discussed during the batch model pipeline execution, you can verify the output notebook, DynamoDB tables, or S3 bucket to make sure that everything is working fine and can also check logs in case of failures. </p>
			<p>Now that the first part of the online model pipeline is ready, let's Dockerize the REST endpoints we developed in the previous chapter and deploy them as a SageMaker endpoint. </p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Deploying the model as a SageMaker endpoint</h2>
			<p>To <a id="_idIndexMarker409"/>deploy the model to SageMaker, we need to first dockerize the REST API we built in <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>. Before we do that, let's create an <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), where <a id="_idIndexMarker410"/>we can save the Docker image of the model and use it in SageMaker endpoint configurations. </p>
			<h3>An ECR for the Docker image</h3>
			<p>To <a id="_idIndexMarker411"/>create the ECR resource, navigate to the ECR console from the search bar or use the following URL: <a href="https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1">https://us-east-1.console.aws.amazon.com/ecr/repositories?region=us-east-1</a>. The following page will be displayed:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B18024_06_017.jpg" alt="Figure 6.17 – The ECR home page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – The ECR home page</p>
			<p>On the page <a id="_idIndexMarker412"/>displayed in <em class="italic">Figure 6.17</em>, you can choose either the <strong class="bold">Private</strong> or <strong class="bold">Public</strong> repository tab. Then, click on the <strong class="bold">Create repository</strong> button:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B18024_06_018.jpg" alt="Figure 6.18 – ECR – Create repository&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – ECR – Create repository</p>
			<p>I have selected <strong class="bold">Private</strong> here; depending on whether you choose <strong class="bold">Private</strong> or <strong class="bold">Public</strong>, the options will change, but either way, it's straightforward. Fill in the required fields, scroll all <a id="_idIndexMarker413"/>the way down, and click on <strong class="bold">Create repository</strong>. Once the repository is created, go into the repository details page, and you should see a page similar to the one shown in <em class="italic">Figure 6.19</em>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Private repositories are secured with IAM, whereas public repositories can be accessed by anybody on the internet. Public repositories are mainly used for sharing/open sourcing your work with others outside an organization:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B18024_06_019.jpg" alt=" Figure 6.19 – ECR repository details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 6.19 – ECR repository details</p>
			<p>On the preceding page, click on <strong class="bold">View push commands</strong>, and you should see a popup, similar to the one shown in <em class="italic">Figure 6.20</em>:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B18024_06_020.jpg" alt="Figure 6.20 – ECR push commands&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20 – ECR push commands</p>
			<p>Depending on<a id="_idIndexMarker414"/> the operating system you are using for building the Docker image, save the necessary commands. We will use these commands to build the Docker image. </p>
			<h3>Building the Docker image</h3>
			<p>As mentioned earlier, we <a id="_idIndexMarker415"/>will be using the REST endpoints built in the previous chapter in this section. If you recall correctly, we had added two REST endpoints, <strong class="source-inline">ping</strong> and <strong class="source-inline">invocations</strong>. These endpoints are not random, though the same can be hosted in any container environment. To host a Docker image in the SageMaker endpoints, the requirement is that it should have the <strong class="source-inline">ping</strong> (which is the <strong class="source-inline">GET</strong> method) and <strong class="source-inline">invocations</strong> (which is the <strong class="source-inline">POST</strong> method) routes. I have added a couple of files to the same folder structure, which will be useful for building the Docker image. The REST code and folder structure are available at the following URL: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/online-model-rest-api</a>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The additional files are <strong class="source-inline">Dockerfile</strong>, <strong class="source-inline">requirements.txt</strong>, and <strong class="source-inline">serve</strong>.</p>
			<p>Consecutively, clone the REST code to the local system, copy the feature repository into the <strong class="source-inline">root</strong> directory <a id="_idIndexMarker416"/>of the project, export the credentials, and then run the commands in <em class="italic">Figure 6.20</em>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can use the same user credential that was created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. However, we had missed adding ECR permissions to the user. Please navigate to the IAM console and add <strong class="bold">AmazonEC2ContainerRegistryFullAccess</strong> to the user. Otherwise, you will get an access error. </p>
			<p>The following are the example commands:</p>
			<p class="source-code">cd online-model-rest-api/</p>
			<p class="source-code">export AWS_ACCESS_KEY_ID=&lt;AWS_KEY&gt;</p>
			<p class="source-code">export AWS_SECRET_ACCESS_KEY=&lt;AWS_SECRET&gt;</p>
			<p class="source-code">export AWS_DEFAULT_REGION=us-east-1</p>
			<p class="source-code">aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin &lt;account_number&gt;.dkr.ecr.us-east-1.amazonaws.com</p>
			<p class="source-code">docker build -t customer-segmentation .</p>
			<p class="source-code">docker tag customer-segmentation:latest &lt;account_number&gt;.dkr.ecr.us-east-1.amazonaws.com/customer-segmentation:latest</p>
			<p class="source-code">docker push &lt;account_number&gt;.dkr.ecr.us-east-1.amazonaws.com/customer-segmentation:latest</p>
			<p>The commands logs in to ECR using the credentials set in the environment, builds the Docker image, and tags and pushes the Docker image to the registry. Once the image is pushed, if you navigate back to the screen in <em class="italic">Figure 6.19</em>, you should see the new image, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B18024_06_021.jpg" alt="Figure 6.21 – ECR with the pushed image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.21 – ECR with the pushed image</p>
			<p>Now that <a id="_idIndexMarker417"/>the image is ready, copy the image <strong class="bold">Uniform Resource Identifier (URI)</strong> by clicking on the icon next to <strong class="bold">Copy URI</strong>, as shown in <em class="italic">Figure 6.21</em>. Let's deploy the Docker image as a SageMaker endpoint next.</p>
			<h3>Creating a SageMaker endpoint</h3>
			<p>Amazon SageMaker <a id="_idIndexMarker418"/>aims at providing managed infrastructure for ML. In this section, we will only be using the SageMaker inference components. SageMaker endpoints are used for deploying a model as REST endpoints for real-time prediction. It supports Docker image models and also supports a few flavors out of the box. We will be using the Docker image that we pushed into the ECR in the previous section. SageMaker endpoints are built using three building blocks – models, endpoint configs, and endpoints. Let's use these building blocks and create an endpoint next.</p>
			<h4>A SageMaker model</h4>
			<p>The <a id="_idIndexMarker419"/>model is used to define the model parameters such as the name, the location of the model, and the IAM role. To define a model, navigate to the SageMaker console using the search bar and look for <strong class="source-inline">Models</strong> in the <strong class="bold">Inference</strong> section. Alternatively, visit <a href="https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models">https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/models</a>. The following screen will be displayed:</p>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B18024_06_022.jpg" alt="Figure 6.22 – The SageMaker Models console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.22 – The SageMaker Models console</p>
			<p>On the <a id="_idIndexMarker420"/>displayed page, click on <strong class="bold">Create model</strong> to navigate to the next screen. The following page will be displayed:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B18024_06_023.jpg" alt="Figure 6.23 – SageMaker – Create model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.23 – SageMaker – Create model</p>
			<p>As shown in <em class="italic">Figure 6.23</em>, input a model name, and for the IAM role, select <strong class="bold">Create a new role</strong> from the dropdown. A new popup appears, as displayed in the following screenshot:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B18024_06_024.jpg" alt="Figure 6.24 – The SageMaker model – Create an IAM role&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.24 – The SageMaker model – Create an IAM role</p>
			<p>In the popup, leave<a id="_idIndexMarker421"/> everything as default for the purpose of this exercise and click on <strong class="bold">Create role</strong>. AWS will create an IAM role, and on the same screen, you should see a message in the dialog with a link to the IAM role. The following figure shows the displayed message:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B18024_06_025.jpg" alt="Figure 6.25 – The SageMaker model – the new execution role is created&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.25 – The SageMaker model – the new execution role is created</p>
			<p>Now, if you<a id="_idIndexMarker422"/> recall correctly, we are using DynamoDB as the online store; as we are reading data on demand from DynamoDB tables, the IAM role needs access to them. Therefore, navigate to the IAM role we just created using the link displayed on the page in a new tab, add <strong class="bold">AmazonDynamoDBFullAccess</strong>, and come back to this tab. Scroll down to the <strong class="bold">Container definition 1</strong> section, where you should see the following parameters:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B18024_06_026.jpg" alt="Figure 6.26 – The SageMaker model – the Container definition 1 section &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.26 – The SageMaker model – the Container definition 1 section </p>
			<p>For the <strong class="bold">Location of inference code image</strong> parameter, paste the image URI that we copied <a id="_idIndexMarker423"/>from the screen, as displayed in <em class="italic">Figure 6.21</em>. Leave the others as their defaults and scroll again to the <strong class="bold">Network</strong> section:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B18024_06_027.jpg" alt="Figure 6.27 – The Sagemaker Model – the Network section&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.27 – The Sagemaker Model – the Network section</p>
			<p>Here, select the <strong class="bold">VPC</strong> to <strong class="bold">Default vpc</strong>, select one or two subnets from the list, and choose the<a id="_idIndexMarker424"/> default security group. Scroll down to the bottom and click on <strong class="bold">Create model</strong>.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">It is never a good idea to select the default security group for production deployment, as inbound rules are not restrictive. </p>
			<p>Now that the model is ready, let's create the endpoint configuration next.</p>
			<h4>Endpoint configuration</h4>
			<p>To set up<a id="_idIndexMarker425"/> the endpoint configuration, navigate to the SageMaker console using the search bar and look for <strong class="source-inline">Endpoint Configurations</strong> in the <strong class="bold">Inference</strong> section. Alternatively, visit <a href="https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig">https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpointConfig</a>. The following page will be displayed:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B18024_06_028.jpg" alt="Figure 6.28 – The Sagemaker Endpoint configuration console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.28 – The Sagemaker Endpoint configuration console</p>
			<p>On the displayed web page, click on <strong class="bold">Create endpoint configuration</strong>. You will be navigated to the following page:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B18024_06_029.jpg" alt="Figure 6.29 – SageMaker – Create endpoint configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.29 – SageMaker – Create endpoint configuration</p>
			<p>On this<a id="_idIndexMarker426"/> screen, fill in the <strong class="bold">Endpoint configuration name</strong> field; I have given the name <strong class="source-inline">customer-segmentation-config</strong>. Scroll down to the <strong class="bold">Data capture</strong> section. This is used to define what percent of real-time inference data needs to be captured, where (the S3 location), and how it needs to be stored (JSON or CSV). You can choose to enable this or leave it disabled. I have left it disabled for this exercise. If you enable it, it will ask you for additional information. The section following <strong class="bold">Data capture</strong> is <strong class="bold">Production variants</strong>. This is used for setting up multiple model variants, and A/B testing of the models. For now, since we only have one variant, let's add that here. To add a variant, click on the <strong class="bold">Add model</strong> link in the section; the following popup will appear:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B18024_06_030.jpg" alt="Figure 6.30 – SageMaker – adding a model to the endpoint config&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.30 – SageMaker – adding a model to the endpoint config</p>
			<p>In the popup, select <a id="_idIndexMarker427"/>the model that we created earlier, scroll all the way down, and click on <strong class="bold">Create endpoint configuration</strong>.</p>
			<h4>SageMaker endpoint creation</h4>
			<p>The<a id="_idIndexMarker428"/> last step is to use the endpoint configuration to create an endpoint. To create a SageMaker endpoint, navigate to the SageMaker console using the search bar and look for <strong class="source-inline">Endpoints</strong> in the <strong class="bold">Inference</strong> section. Alternatively, visit https://us-east-1.console.aws.amazon.com/sagemaker/home?region=us-east-1#/endpoints. The following page will be displayed:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B18024_06_031.jpg" alt="Figure 6.31 – The SageMaker Endpoints console&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.31 – The SageMaker Endpoints console</p>
			<p>On the <a id="_idIndexMarker429"/>page shown in <em class="italic">Figure 6.31</em>, click on <strong class="bold">Create endpoint</strong> to navigate to the following page:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B18024_06_032.jpg" alt="Figure 6.31 – SageMaker – creating an endpoint&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.31 – SageMaker – creating an endpoint</p>
			<p>On the web page displayed in <em class="italic">Figure 6.31</em>, provide an endp<a id="_idTextAnchor105"/>oint name. I have given the name <strong class="source-inline">customer-segmentation-endpoint</strong>. Scroll down to the <strong class="bold">Endpoint configuration</strong> section, select the endpoint <a id="_idIndexMarker430"/>configuration we created earlier, and click on the <strong class="bold">Select endpoint configuration</strong> button. Once it is selected, click on <strong class="bold">Create endpoint</strong>. It will take a few minutes to create an endpoint. When the endpoint status changes to <strong class="bold">Available</strong>, your model is live for serving real-time traffic.</p>
			<h4>Testing the SageMaker endpoint</h4>
			<p>The next thing <a id="_idIndexMarker431"/>we need to know is how to consume the model. There are different ways – you can use the SageMaker library, Amazon SDK client (Python, TypeScript, or any other available), or a SageMaker endpoint URL. All these methods default to AWS IAM authentication. If you have special requirements and want to expose the model without authentication or with custom authentication, it can be achieved using the API gateway and Lambda authorizer. For the purpose of this exercise, we will be using the <strong class="source-inline">boto3</strong> client to invoke the API. Irrespective of how we invoke the endpoint, the results should be the same.</p>
			<p>The following code block invokes the endpoint using the <strong class="source-inline">boto3</strong> client:  </p>
			<pre class="source-code">import json</pre>
			<pre class="source-code">import boto3</pre>
			<pre class="source-code">import os</pre>
			<pre class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key&gt;"</pre>
			<pre class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</pre>
			<pre class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</pre>
			<pre class="source-code">payload = json.dumps({"customer_list":["12747.0", "12841.0"]})</pre>
			<pre class="source-code">runtime = boto3.client("runtime.sagemaker")</pre>
			<pre class="source-code">response = runtime.invoke_endpoint(</pre>
			<pre class="source-code">    EndpointName= "customer-segmentation-endpoint", </pre>
			<pre class="source-code">    ContentType="application/json", Body=payload</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">response = response["Body"].read()</pre>
			<pre class="source-code">result = json.loads(response.decode("utf-8"))</pre>
			<pre class="source-code">print(results)</pre>
			<p>In the<a id="_idIndexMarker432"/> preceding code block, we are invoking the endpoint that we created to run predictions for two customers with the <strong class="source-inline">12747.0</strong> and <strong class="source-inline">12841.0</strong> IDs. The endpoint will respond within milliseconds with the predictions for the given customer IDs. Now, the endpoint can be shared with the model consumers.</p>
			<p>Now that the model is in production, let's look at a few aspects that come after a model moves to production.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor106"/>Beyond model production</h1>
			<p>In this section, we will discuss the<a id="_idIndexMarker433"/> postproduction aspects of ML and how we benefit from the adoption of a feature store.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/>Feature drift monitoring and model retraining</h2>
			<p>Once the <a id="_idIndexMarker434"/>model is in production, the next question that will come up frequently is how the model is performing in production. There may be different metrics used to measure the performance of a model – for instance, for a recommendation model, performance may be measured by <a id="_idIndexMarker435"/>a conversion rate, which is how often the recommended product was purchased. Similarly, predicting the next action of a customer may be measured by error rate, and so on. There is no universal way of doing it. But if a model's performance is bad, it needs to be retrained or replaced with a new one. </p>
			<p>One other aspect that defines when a model should be retrained is when the feature starts to drift away from the values with which it was trained. For example, let's say the mean frequency value of the customer during the initial model training was 10, but now, the mean frequency value is 25. Similarly, the lowest monetary value was initially $100.00 and now it is $500.00. This is<a id="_idIndexMarker436"/> called <strong class="bold">data drift</strong>. </p>
			<p>Data drift monitoring measures the change in the statistical distribution of the data; in the case of feature monitoring, it is comparing the change in the statistical distribution of a feature from <em class="italic">t1</em> time to <em class="italic">t2</em> time. The article at the following URL discusses different metrics for data drift monitoring: <a href="https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f">https://towardsdatascience.com/automating-data-drift-thresholding-in-machine-learning-systems-524e6259f59f</a>.</p>
			<p>With a<a id="_idIndexMarker437"/> feature store, it is easy to retrieve a training dataset from two different points in time, namely the dataset used for <a id="_idIndexMarker438"/>model training and the latest feature values for all the features used in model training. Now, all we need to do is run data drift monitoring on schedule to generate a drift report. The standardization that Feast brought to the table is, since the data is stored and retrieved using standard APIs, a generic feature drift monitoring can be run on schedule for all the datasets in the feature store. The feature drift report can be used as one of the indicators for model retraining. If feature drift is affecting the model's performance, it can be retrained with the latest dataset, and deployed and AB-tested with the current production model. </p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/>Model reproducibility and prediction issues</h2>
			<p>If you<a id="_idIndexMarker439"/> recall from <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, model reproducibility is one of the common problems of ML. We need a way to consistently reproduce the model (or training data used for model). Without a <a id="_idIndexMarker440"/>feature store, if the underlying raw data that is used to generate features changes, it is not possible to reproduce the same training dataset. However, with a feature store, as we discussed earlier, the features are versioned with a timestamp (one of the columns in the features DataFrame is an event timestamp). Hence, we can query the historical data to generate the same feature set used for model training. If the algorithm used for training the model is not stochastic, the model can also be reproduced. Let's try this out. </p>
			<p>Since we have already done something similar to this in the <em class="italic">Model training with a feature store</em> section of <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>, we will reuse the same code to run this experiment. Copy and run all the code till you create the entity DataFrame and then replace the <strong class="source-inline">event_timestamp</strong> column with an older timestamp (the timestamp of when the model was trained), as shown here. In this case, the model was trained at <strong class="source-inline">2022-03-26 16:24:21</strong>, as shown in <em class="italic">Figure 5.1</em> of <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>:</p>
			<pre class="source-code">## replace timestamp to older time stamp.</pre>
			<pre class="source-code">entity_df["event_timestamp"] = pd.to_datetime("2022-03-26 16:24:21")</pre>
			<p>Once you are done replacing the timestamp, continue running the code from the <em class="italic">Dee's model training experiments</em> section of <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>. You should be able to generate the exact same dataset that was used in Dee's model training (in this case, the dataset in <em class="italic">Figure 5.2 </em>of <a href="B18024_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Training and Inference</em>). Hence, if the model uses a nonrandom algorithm, then the model can also be reproduced using the feature set. </p>
			<p>One other <a id="_idIndexMarker441"/>advantage of a feature store is the <a id="_idIndexMarker442"/>debugging prediction issue. Let's consider a scenario where you have a website-facing model that is classifying a transaction as fraudulent or not. During the peak hour, it flagged a few transactions as fraudulent, but the transactions were legitimate. The customer called in and complained to the customer service department, and now it's the data scientist Subbu's turn to figure out what went wrong. If there was no feature store in the project, to reproduce the issue, Subbu would have to go into the raw data, try to generate the features, and see whether the behavior still remains the same. If not, Subbu would have to go into the application log, process it, look for user behavior before the event, try to reproduce it from the user interaction perspective, and also capture the features for all these trials, hoping that the issue can be reproduced at least once.</p>
			<p>On the other hand, with the feature store used in the project, Subbu will figure out the approximate time when the event happened, what the entities and features used in the model are, and what the version of the model that was running in production was at the time of the event. With this information, Subbu will connect to the feature store and fetch all the features used in the model for all the entities for the approximate time range when the issue happened. Let's say that the event occurred between 12:00pm to 12:15pm today, features were streaming, and the freshness interval was around 30 seconds. This means that, on average, for a given entity, there is a chance that features will change in the next 30 seconds from any given time. </p>
			<p>To reproduce the issue, Subbu will form an entity DataFrame with the same entity ID repeated 30 times in one column and, for the event time column, a timestamp from 12:00pm to 12:15pm with 30-second intervals. With this entity DataFrame, Subbu will query the historical store using the Feast API and run the prediction for the generated features. If the issue is reproduced, Subbu has the feature set that caused the issue. If not, using the entity DataFrame, the interval will be reduced to less than 30 seconds, maybe to 10 seconds, to figure out if features changed at a faster pace than 30 seconds. Subbu can continue doing this till she finds the feature set that reproduces the issue. </p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/>A headstart for the next model</h2>
			<p>Now that the<a id="_idIndexMarker443"/> model has productionized, the data scientist Subbu picks up the next problem statement. Let's assume that the next ML model has to predict <a id="_idIndexMarker444"/>the <strong class="bold">Next Purchase Day</strong> (<strong class="bold">NPD</strong>) of a customer. The use case here could be that based on the NPD, we want to run a campaign for a customer. If a customers' purchase day is farther in the future, we want to offer a special deal so that we can encourage purchasing sooner. Now, before going to a raw dataset, Subbu can look for available features based on how the search and discoverability aspect is integrated into the feature store. Since Feast moved from service-oriented to SDK/CLI-oriented, there is a need for catalog tools, a GitHub repository of all the feature repositories, a data mesh portal, and so on. However, in the case of feature stores such as SageMaker or Databricks, users can connect to feature store endpoints (with SageMaker runtime using a boto3 or Databricks workspace) and browse through the available feature definitions using the API or from the UI. I have not used the Tecton feature store before, but Tecton also offers a UI for its feature store that can be used to browse through the available features. As you can see, this is one of the drawbacks of the different versions of Feast between 0.9.X and 0.20.X (0.20 is the version at the time of writing).</p>
			<p>Let's assume, for now, that Subbu has a way to locate all the feature repositories. Now, she can connect and browse through them to figure out what the projects and feature definitions are that could be useful in the NPD model. So far, we have just one feature repository that has the customer RFM features that we have been using so far, and these features can be useful in the model. To use these features, all Subbu has to do is get read access to the AWS resource, and the latest RFM features will be available every day for experimentation and can also be used if the model moves to production.</p>
			<p>To see how beneficial the feature store would be during the development of the subsequent model, we should try building the NPD. I will go through the initial few steps to get you started on the model. As we followed a blog during the development of the first model, we will be following another part in the same blog series, which can be found at <a href="https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027">https://towardsdatascience.com/predicting-next-purchase-day-15fae5548027</a>. Please read through the blog, as it discusses the approach and why the author thinks specific features will be useful. Here, we will be skipping ahead to the feature engineering section. </p>
			<p>We will be using the feature set that the blog author uses, which includes the following:</p>
			<ul>
				<li>RFM features and clusters</li>
				<li>The number of days between the last three purchases</li>
				<li>The mean and standard deviation of the differences between the purchases </li>
			</ul>
			<p>The<a id="_idIndexMarker445"/> first feature set already exists in the feature store; we don't need to do anything extra for it. But for the other two, we need to do feature engineering from the raw data. The notebook at <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/blob/main/Chapter06/notebooks/ch6_next_purchase_day_feature_engineering.ipynb</a> has the required feature engineering to generate the features in the preceding second and third bullet points. I will leave the ingestion of these features into a feature store and using the features (RFM) from the previous model in combination with these to train a new model as an exercise. As you develop and productionize this model, you will see the benefit of the feature store and how it can accelerate model building. </p>
			<p>Next, let's discuss how to change a feature definition when the model is in production.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/>Changes to feature definition after production </h2>
			<p>So far, we<a id="_idIndexMarker446"/> have discussed feature ingestion, query, and changes to a feature set during the development phases. However, we haven't talked about changes to feature definitions when the model is in production. Often, it is argued that changing feature definition once the model moves to production is difficult. The reason for this is that there is a chance that multiple models are using feature definitions and any changes to them will have a cascading effect on the models. This is one of the reasons why some feature stores don't yet support updates on feature definitions. We need a way to handle the change effectively here. </p>
			<p>This is still a gray area; there is no right or wrong way of doing it. We can adopt any mechanism that we use in other software engineering processes. A simple one could be the versioning of the feature views, similar to the way we do our REST APIs or Python libraries. Whenever a change is needed for a production feature set, assuming that it is being used by others, a new version of the feature view (let's call it <strong class="source-inline">customer-segemantion-v2</strong>) will be created and used. However, the previous version will still need to be managed until all the models migrate. If, for any reason, there are models that need the older version and cannot be migrated to the newer version of the feature table/views, it may have to be managed or handed over to the team that needs it. There needs to be some discussion on ownership of the features and feature engineering jobs. </p>
			<p>This is where the<a id="_idIndexMarker447"/> concept of data as a product is very meaningful. The missing piece here is a framework for producers and consumers to define contracts and notify changes. The data producers need a way of publishing their data products; here, the data product is feature views. The consumers of the product can subscribe to the data product and use it. During the feature set changes, the producers can define a new version of the data product and depreciate the older version so that consumers will be notified of what the changes are. This is just my opinion on a solution, but I'm sure there are better minds out there who may already be implementing another solution.  </p>
			<p>With that, let's summarize what we have learned in this chapter and move on to the next one.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Summary </h1>
			<p>In this chapter, we aimed at using everything we built in the previous chapters and productionizing the ML models for batch and online use cases. To do that, we created an Amazon MWAA environment and used it for the orchestration of the batch model pipeline. For the online model, we used Airflow for the orchestration of the feature engineering pipeline and the SageMaker inference components to deploy a Docker online model as a SageMaker endpoint. We looked at how a feature store facilitates the postproduction aspects of ML, such as feature drift monitoring, model reproducibility, debugging prediction issues, and how to change a feature set when the model is in production. We also looked at how data scientists get a headstart on the new model with the use of a feature store. So far, we have used Feast in all our exercises; in the next chapter, we will look at a few of the feature stores that are available on the market and how they differ from Feast, alongside some examples.</p>
		</div>
	</body></html>