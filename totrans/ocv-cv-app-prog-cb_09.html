<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09" class="calibre1"/>Chapter 9. Describing and Matching Interest Points</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Matching local templates</li><li class="listitem">Describing local intensity patterns</li><li class="listitem">Describing keypoints with binary features</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch09lvl1sec57" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">In the previous chapter, we learned how to detect special points in an image with the objective of subsequently performing local image analysis. These keypoints are chosen to be distinctive enough such that if a keypoint is detected on the image of an object, then the same point is expected to be detected in other images depicting the same object. We also described some more sophisticated interest point detectors that can assign a representative scale factor and/or an orientation to a keypoint. As we will see in this recipe, this additional information can be useful to normalize scene representations with respect to viewpoint variations.</p><p class="calibre8">In order to perform image analysis based on interest points, we now need to build rich representations that uniquely describe each of these keypoints. This chapter looks at the different approaches that have been proposed to extract <strong class="calibre2">descriptors</strong><a id="id726" class="calibre1"/> from interest points. These descriptors are generally 1D or 2D vectors of binary, integer, or floating-point numbers that describe a keypoint and its neighborhood. A good descriptor should be distinctive enough to uniquely represent each keypoint of an image; it should be robust enough to have the same points represented similarly in spite of possible illumination changes or viewpoint variations. Ideally, it should also be compact to facilitate processing operations.</p><p class="calibre8">One of the most common operations accomplished with keypoints is image matching. The objective of performing this task could be, for example, to relate two images of the same scene or to detect the occurrence of a target object in an image. Here, we will study some basic matching strategies, a subject that will be further discussed in the next chapter.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec58" class="calibre1"/>Matching local templates</h1></div></div></div><p class="calibre8">Feature point <strong class="calibre2">matching</strong><a id="id727" class="calibre1"/> is<a id="id728" class="calibre1"/> the operation by which one can put in correspondence points from one image to points from another image (or points from an image set). Image points should match when they correspond to the image of the same scene element (or the object point) in the real world.</p><p class="calibre8">A single pixel is certainly not sufficient to make a decision on the similarity of two keypoints. This is why an image <strong class="calibre2">patch</strong><a id="id729" class="calibre1"/> around each keypoint must be considered during the matching process. If two patches correspond to the same scene element, then one might expect their pixels to exhibit similar values. A direct pixel-by-pixel comparison of pixel patches is the solution presented in this recipe. This is probably the simplest approach to feature point matching, but as we will see, it is not the most reliable one. Nevertheless, in several situations, it can give good results.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec167" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Most often, patches are defined as squares of odd sizes centered at the keypoint position. The similarity between two square patches can then be measured by comparing the corresponding pixel intensity values inside the patches. A simple <strong class="calibre2">Sum of Squared Differences</strong> (<strong class="calibre2">SSD</strong>)<a id="id730" class="calibre1"/> is a popular solution. The feature matching strategy then works as follows. First, the keypoints are detected in each image. Here, let's use the FAST detector:</p><div><pre class="programlisting">  // Define keypoints vector
  std::vector&lt;cv::KeyPoint&gt; keypoints1;
  std::vector&lt;cv::KeyPoint&gt; keypoints2;
  // Define feature detector
  cv::FastFeatureDetector fastDet(80);
  // Keypoint detection
  fastDet.detect(image1,keypoints1);
  fastDet.detect(image2,keypoints2);</pre></div><p class="calibre8">We then define a rectangle of the size <code class="email">11x11</code> that will be used to define patches around each keypoint:</p><div><pre class="programlisting">  // Define a square neighborhood
  const int nsize(11); // size of the neighborhood
  cv::Rect neighborhood(0, 0, nsize, nsize); // 11x11
  cv::Mat patch1;
  cv::Mat patch2;</pre></div><p class="calibre8">The keypoints in one image are compared with all the keypoints in the other image. For each keypoint of <a id="id731" class="calibre1"/>the first image, the most similar patch in the second image is identified. This process is implemented using two nested loops, as shown in the following code:</p><div><pre class="programlisting">// For all keypoints in first image
// find best match in second image
cv::Mat result;
std::vector&lt;cv::DMatch&gt; matches;

//for all keypoints in image 1
for (int i=0; i&lt;keypoints1.size(); i++) {
  
  // define image patch
  neighborhood.x = keypoints1[i].pt.x-nsize/2;
  neighborhood.y = keypoints1[i].pt.y-nsize/2;

  // if neighborhood of points outside image, 
  // then continue with next point
  if (neighborhood.x&lt;0 || neighborhood.y&lt;0 ||
      neighborhood.x+nsize &gt;= image1.cols || 
          neighborhood.y+nsize &gt;= image1.rows)
      continue;

  //patch in image 1
  patch1 = image1(neighborhood);

  // reset best correlation value;
  cv::DMatch bestMatch;

  //for all keypoints in image 2
  for (int j=0; j&lt;keypoints2.size(); j++) {

      // define image patch
      neighborhood.x = keypoints2[j].pt.x-nsize/2;
      neighborhood.y = keypoints2[j].pt.y-nsize/2;

      // if neighborhood of points outside image, 
      // then continue with next point
      if (neighborhood.x&lt;0 || neighborhood.y&lt;0 ||
        neighborhood.x + nsize &gt;= image2.cols ||
             neighborhood.y + nsize &gt;= image2.rows)
          continue;

      // patch in image 2
      patch2 = image2(neighborhood);

      // match the two patches                  
         cv::matchTemplate(patch1,patch2,result,
                           CV_TM_SQDIFF_NORMED);

      // check if it is a best match
      if (result.at&lt;float&gt;(0,0) &lt; bestMatch.distance) {

        bestMatch.distance= result.at&lt;float&gt;(0,0);
        bestMatch.queryIdx= i;
        bestMatch.trainIdx= j;
      }
    }

    // add the best match
    matches.push_back(bestMatch);
}</pre></div><p class="calibre8">Note the use <a id="id732" class="calibre1"/>of the <code class="email">cv::matchTemplate</code> function<a id="id733" class="calibre1"/>, which we will describe in the next section, that computes the patch similarity score. When a potential match is identified, this match is represented through the use of a <a id="id734" class="calibre1"/>
<code class="email">cv::DMatch</code> object. This object stores the index of the two matching keypoints as well as the similarity score.</p><p class="calibre8">The more similar the two image patches are, the higher the probability that these patches correspond to the same scene point. This is why it is a good idea to sort the resulting match points by their similarity scores:</p><div><pre class="programlisting">  // extract the 25 best matches
  std::nth_element(matches.begin(),
                    matches.begin()+25,matches.end());
  matches.erase(matches.begin()+25,matches.end());</pre></div><p class="calibre8">You can then simply retain the matches that pass a given similarity threshold. Here, we chose to keep only the <code class="email">N</code> best matching points (we use <code class="email">N=25</code> to facilitate the visualization of the matching results).</p><p class="calibre8">Interestingly, there is an OpenCV function that can display the matching results by concatenating the two images and joining each corresponding point by a line. The function is used as follows:</p><div><pre class="programlisting">  // Draw the matching results
  cv::Mat matchImage;
  cv::drawMatches(image1,keypoints1,          // first image
                   image2,keypoints2,         // second image
                   matches,                   // vector of matches
                   cv::Scalar(255,255,255),   // color of lines
                   cv::Scalar(255,255,255));  // color of points</pre></div><p class="calibre8">Here is the resulting match result:</p><div><img src="img/00145.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec168" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The results obtained<a id="id735" class="calibre1"/> are certainly not perfect, but a visual inspection of the matched image points shows a number of successful matches. It can also be observed that the repetitive structures of the building cause some confusion. Also, since we tried to match all the points in the left image with the ones in the right image, we obtained cases where a point in the right image was matched with multiple points in the left image. This is an asymmetrical matching situation that can be corrected by, for example, keeping only the match with the best score for each point in the right image.</p><p class="calibre8">To compare <a id="id736" class="calibre1"/>the image patches from each image, here we used a simple criterion, that is, a pixel-per-pixel sum of the squared difference specified using the <code class="email">CV_TM_SQDIFF</code> flag. If we compare the point <code class="email">(x,y)</code> of image <code class="email">I
<sub class="calibre20">1</sub></code> with a putative match at <code class="email">(x',y')</code> in image <code class="email">I</code>
<code class="email"><sub class="calibre20">2</sub></code>, then the similarity measure is given as follows:</p><div><img src="img/00146.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Here, the sum of the <code class="email">(i,j)</code> point provides the offset to cover the square template centered at each point. Since the difference between adjacent pixels in similar patches should be small, the best-matching patches should be the ones with the smallest sum. This is what is done in the main loop of the matching function; that is, for each keypoint in one image, we identify the keypoint in the other image that gives the lowest sum of the squared difference. We can also reject matches for which this sum is over a certain threshold value. In our case, we simply sort them from the most similar to the least similar ones.</p><p class="calibre8">In our example, the matching was done with square patches of size <code class="email">11x11</code>. A larger neighborhood creates more distinctive patches, but it also makes them more sensitive to local scene variations.</p><p class="calibre8">Comparing two image windows from a simple sum of square differences will work relatively well as long as the two images show the scene from similar points of views and similar viewing conditions. Indeed, a simple lighting change will increase or decrease all the pixel intensities of a patch, resulting in a large square difference. To make matching more invariant to lighting changes, other formulae that could be used to measure the similarity between two image windows exist. OpenCV offers a number of these. A very useful formula is the normalized sum of square differences (the <code class="email">CV_TM_SQDIFF_NORMED</code> flag):</p><div><img src="img/00147.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Other similarity measures are based on the concept of correlation, defined in the signal processing theory as follows (with the <code class="email">CV_TM_CCORR</code> flag):</p><div><img src="img/00148.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This value will be maximal when two patches are similar.</p><p class="calibre8">The identified matches <a id="id737" class="calibre1"/>are stored in a vector of the <code class="email">cv::DMatch</code> instances. Essentially, the <code class="email">cv::DMatch</code><a id="id738" class="calibre1"/> data structure contains the first index that refers to an element in the first vector of keypoints and the second index that refers to the matching feature in the second vector of keypoints. It also contains a real value that represents the distance between the two matched descriptors. This distance value is used in the definition of <code class="email">operator&lt;</code> when comparing two <code class="email">cv::DMatch</code> instances.</p><p class="calibre8">When we drew the matches in the previous section, we wanted to limit the number of lines to make the results more readable. Therefore, we only displayed the <code class="email">25</code> matches that had the lowest distance. To do this, we used the <code class="email">std::nth_element</code> function that positions the Nth element in a sorted order at the Nth position, with all the smaller elements placed before this element. Once this is done, the vector is simply purged of its remaining elements.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec169" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The <code class="email">cv::matchTemplate</code> function<a id="id739" class="calibre1"/> is at the heart of our feature matching method. We used it here in a very specific way, which is to compare two image patches. However, this function has been designed to be used in a more generic way.</p><div><div><div><div><h3 class="title2"><a id="ch09lvl3sec42" class="calibre1"/>Template matching</h3></div></div></div><p class="calibre8">A <a id="id740" class="calibre1"/>common task in image analysis is to detect the occurrence of a specific pattern or object in an image. This can be done by defining a small image of the object, a template, and searching for a similar occurrence in a given image. In general, the search is limited to a region of interest inside which we think the object can be found. The template is then slid over this region, and a similarity measure is computed at each pixel location. This is the operation performed by the <code class="email">cv::matchTemplate</code> function. The input is a template image of a small size and an image over which the search is performed. The result is a <code class="email">cv::Mat</code> function of floating-point values that correspond to the similarity score at each pixel location. If the template is of the size <code class="email">MxN</code> and the image is of the size <code class="email">WxH</code>, then the resulting matrix will have a size of <code class="email">W-N+1xH-N+1</code>. In general, you will be interested in the location of the highest similarity; so, the typical template matching code will look as follows (assuming that the target variable is our template):</p><div><pre class="programlisting">// define search region
cv::Mat roi(image2, 
  // here top half of the image
  cv::Rect(0,0,image2.cols,image2.rows/2)); 
      
// perform template matching
cv::matchTemplate(
  roi,    // search region
  target, // template
  result, // result
  CV_TM_SQDIFF); // similarity measure

// find most similar location
double minVal, maxVal;
cv::Point minPt, maxPt;
cv::minMaxLoc(result, &amp;minVal, &amp;maxVal, &amp;minPt, &amp;maxPt);

// draw rectangle at most similar location
// at minPt in this case
cv::rectangle(roi, 
   cv::Rect(minPt.x, minPt.y, target.cols , target.rows), 
   255);</pre></div><p class="calibre8">Remember <a id="id741" class="calibre1"/>that this is a costly operation, so you should limit the search area and use a template having a size of only a few pixels.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec170" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The next recipe, <em class="calibre9">Describing local intensity patterns</em>, describes the <code class="email">cv::BFMatcher</code> class that implements the matching strategy that was used in this recipe</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec59" class="calibre1"/>Describing local intensity patterns</h1></div></div></div><p class="calibre8">The SURF and SIFT keypoint<a id="id742" class="calibre1"/> detection algorithms, discussed in <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points,</em> define a location, an orientation, and a scale for each of the detected features. The scale factor information is useful to define the size of a window of analysis around each feature point. Thus, the defined neighborhood would include the same visual information no matter what the scale of the object to which the feature belongs has been pictured. This recipe will show you how to describe an interest point's neighborhood using <a id="id743" class="calibre1"/>
<strong class="calibre2">feature descriptors</strong>. In image analysis, the visual information included in this neighborhood can be used to characterize each feature point in order to make each point distinguishable from the others. Feature descriptors are usually N-dimensional vectors that describe a feature point in a way that is invariant to change in lighting and to small perspective deformations. Generally, descriptors can be compared using simple distance metrics, for example, the Euclidean distance. Therefore, they constitute a powerful tool that can be used in feature matching applications.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec171" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">OpenCV 2 proposes a general interface to compute the descriptors of a list of keypoints. It is called <a id="id744" class="calibre1"/>
<code class="email">cv::DescriptorExtractor</code>, and we will use it in a way similar to the way we used the <code class="email">cv::FeatureDetector</code> interface in the previous chapter. In fact, most feature-based <a id="id745" class="calibre1"/>methods include both a detector and a descriptor component; that's why classes such as <code class="email">cv::SURF</code> and <code class="email">cv::SIFT</code> implement both these interfaces. This means that you have to create only one object to detect and describe keypoints. Here is how you can proceed if you want to match two images:</p><div><pre class="programlisting">  // Define feature detector
  // Construct the SURF feature detector object
  cv::Ptr&lt;cv::FeatureDetector&gt; detector = new cv::SURF(1500.);

  // Keypoint detection
  // Detect the SURF features
  detector-&gt;detect(image1,keypoints1);
  detector-&gt;detect(image2,keypoints2);

  // SURF includes both the detector and descriptor extractor
  cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor = detector;

  // Extract the descriptor
   cv::Mat descriptors1;
   cv::Mat descriptors2;
   descriptor-&gt;compute(image1,keypoints1,descriptors1);
   descriptor-&gt;compute(image2,keypoints2,descriptors2);</pre></div><p class="calibre8">For SIFT, you will simply create a <code class="email">cv::SIFT()</code> object instead. The result is a matrix (that is, a <code class="email">cv::Mat</code> instance) that will contain as many rows as the number of elements in the keypoint vector. Each of these rows is an N-dimensional descriptor vector. In the case of the SURF descriptor, it has a default size of <code class="email">64</code>, and for SIFT, the default dimension is <code class="email">128</code>. This vector characterizes the intensity pattern surrounding a feature point. The more similar the two feature points, the closer their descriptor vectors should be.</p><p class="calibre8">These descriptors will now be used to match our keypoints. Exactly as we did in the previous recipe, each feature descriptor vector in the first image is compared to all the feature descriptors in the second image. The pair that obtains the best score (that is, the pair with the lowest distance between the two descriptor vectors) is then kept as the best match for that feature. This process is repeated for all the features in the first image. Very conveniently, this process is implemented in OpenCV in the <code class="email">cv::BFMatcher</code> class, so we do not need to re-implement the double loops that we previously built. This class is used as follows:</p><div><pre class="programlisting">   // Construction of the matcher 
   cv::BFMatcher matcher(cv::NORM_L2);
   // Match the two image descriptors
   std::vector&lt;cv::DMatch&gt; matches;
   matcher.match(descriptors1,descriptors2, matches);</pre></div><p class="calibre8">This class is <a id="id746" class="calibre1"/>a subclass of the <code class="email">cv::DescriptorMatcher</code> class<a id="id747" class="calibre1"/> that defines the common interface for different matching strategies. The result is a vector of the <code class="email">cv::DMatch</code> instances.</p><p class="calibre8">With the current Hessian threshold for SURF, we obtained <code class="email">90</code> keypoints for the first image and <code class="email">80</code> for the second. The brute-force approach will then produce <code class="email">90</code> matches. Using the <code class="email">cv::drawMatches</code> class<a id="id748" class="calibre1"/> as in the previous recipe produces the following image:</p><div><img src="img/00149.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As can be seen, several of these matches correctly link a point on the left-hand side with its corresponding point on the right-hand side. You might notice some errors; some of these are due to the fact that the observed building has a symmetrical facade, which makes some of the local matches ambiguous. For SIFT, with the same number of keypoints, we obtained the following match result:</p><div><img src="img/00150.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec172" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">Good feature descriptors <a id="id749" class="calibre1"/>must be invariant to small changes in illumination and viewpoint and to the presence of image noise. Therefore, they are often based on local intensity differences. This is the case for the SURF descriptors, which locally apply the following simple kernels around a keypoint:</p><div><img src="img/00151.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first kernel simply measures the local intensity difference in the horizontal direction (designated as <code class="email">dx</code>), and<a id="id750" class="calibre1"/> the second measures this difference in the vertical direction (designated as <code class="email">dy</code>). The size of the neighborhood used to extract the descriptor vector is generally defined as 20 times the scale factor of the feature (that is, <code class="email">20σ</code>). This square region is then split into <code class="email">4x4</code> smaller square subregions. For each subregion, the kernel responses (<code class="email">dx</code> and <code class="email">dy</code>) are computed at <code class="email">5x5</code> regularly-spaced locations (with the kernel size being <code class="email">2σ</code>). All of these responses are summed up as follows in order to extract four descriptor values for each subregion:</p><div><img src="img/00152.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Since there are <code class="email">4x4=16</code> subregions, we have a total of <code class="email">64</code> descriptor values. Note that in order to give more importance to the neighboring pixels, that is, values closer to the keypoint, the kernel responses are weighted by a Gaussian centered at the keypoint location (with <code class="email">σ=3.3</code>).</p><p class="calibre8">The <code class="email">dx</code> and <code class="email">dy</code> responses are also used to estimate the orientation of the feature. These values are computed (with a kernel size of <code class="email">4σ</code>) within a circular neighborhood of radius <code class="email">6σ</code> at locations regularly spaced by intervals of <code class="email">σ</code>. For a given orientation, the responses inside a certain angular interval (<code class="email">π/3</code>) are summed, and the orientation giving the longest vector is defined as the dominant orientation.</p><p class="calibre8">SIFT is a richer descriptor that uses an image gradient instead of simple intensity differences. It also splits the square neighborhood around each keypoint into <code class="email">4x4</code> subregions (it is also possible to use <code class="email">8x8</code> or <code class="email">2x2</code> subregions). Inside each of these regions, a histogram of gradient orientations is built. The orientations are discretized into 8 bins, and each gradient orientation entry is incremented by a value proportional to the gradient magnitude. This is illustrated by the following figure, inside which each star-shaped arrow set represents a local histogram of a gradient orientation:</p><div><img src="img/00153.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">These <code class="email">16</code> histograms of 8 bins each concatenated together then produce a descriptor of <code class="email">128</code> dimensions. Note<a id="id751" class="calibre1"/> that as for SURF, the gradient values are weighted by a Gaussian filter centered at the keypoint location in order to make the descriptor less sensitive to sudden changes in gradient orientations at the perimeter of the defined neighborhood. The final descriptor is then normalized to make the distance measurement more consistent.</p><p class="calibre8">With SURF and SIFT features and descriptors, scale-invariant matching can be achieved. Here is an example that shows the SURF match result for two images at different scales (here, the 50 best matches have been displayed):</p><div><img src="img/00154.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec173" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The match result produced by any matching algorithm always contains a significant number of incorrect matches. In order to improve the quality of the match set, there exist a number of strategies. Two of them are discussed here.</p><div><div><div><div><h3 class="title2"><a id="ch09lvl3sec43" class="calibre1"/>Cross-checking matches</h3></div></div></div><p class="calibre8">A simple<a id="id752" class="calibre1"/> approach to validate the matches obtained is to repeat the same procedure a second time, but this time, each keypoint of the second image is compared with all the keypoints of the first image. A match is considered valid only if we obtain the same pair of keypoints in both directions (that is, each keypoint is the best match of the other). The <code class="email">cv::BFMatcher</code> function<a id="id753" class="calibre1"/> gives the option to use this strategy. It is indeed included as a flag; when set to <code class="email">true</code>, it forces the function to perform the reciprocal match cross-check:</p><div><pre class="programlisting">   // Construction of the matcher with cross-check 
   cv::BFMatcher matcher2(cv::NORM_L2, // distance measure
                         true);       // cross-check flag</pre></div><p class="calibre8">The improved <a id="id754" class="calibre1"/>match results are as shown in the following screenshot (in the case of SURF):</p><div><img src="img/00155.jpeg" alt="Cross-checking matches" class="calibre10"/></div><p class="calibre11"> </p></div><div><div><div><div><h3 class="title2"><a id="ch09lvl3sec44" class="calibre1"/>The ratio test</h3></div></div></div><p class="calibre8">We have already<a id="id755" class="calibre1"/> noted that repetitive elements in scene objects create unreliable results because of the ambiguity in matching visually similar structures. What happens in such cases is that a keypoint will match well with more than one other keypoint. Since the probability of selecting the wrong correspondence is high, it might be preferable to reject a match in this case.</p><p class="calibre8">To use this strategy, we then need to find the best two matching points of each keypoint. This can be done by using the <code class="email">knnMatch</code> method of the <code class="email">cv::DescriptorMatcher</code> class. Since we want only two best matches, we specify <code class="email">k=2</code>.</p><div><pre class="programlisting">  // find the best two matches of each keypoint
    std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt; matches2;
    matcher.knnMatch(descriptors1,descriptors2, matches2, 
                 2); // find the k best matches</pre></div><p class="calibre8">The next step <a id="id756" class="calibre1"/>is to reject all the best matches with a matching distance similar to that of their second best match. Since <code class="email">knnMatch</code> produces a <code class="email">std::vector</code> class of <code class="email">std::vector</code> (this second vector is of size <code class="email">k</code>), we do this by looping over each keypoint match and perform a ratio test (this ratio will be one if the two best distances are equal). Here is how we can do it:</p><div><pre class="programlisting">// perform ratio test
double ratio= 0.85;
std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt;::iterator it;
for (it= matches2.begin(); it!= matches2.end(); ++it) {

  //   first best match/second best match
  if ((*it)[0].distance/(*it)[1].distance &lt; ratio) {
    // it is an acceptable match
    matches.push_back((*it)[0]);
  }
}
// matches is the new match set</pre></div><p class="calibre8">The initial match set made up of <code class="email">90</code> pairs is now reduced to <code class="email">23</code> pairs; a good proportion of these are now correct matches:</p><div><img src="img/00156.jpeg" alt="The ratio test" class="calibre10"/></div><p class="calibre11"> </p></div><div><div><div><div><h3 class="title2"><a id="ch09lvl3sec45" class="calibre1"/>Distance thresholding</h3></div></div></div><p class="calibre8">An even simpler<a id="id757" class="calibre1"/> strategy consists of rejecting matches for which the distance between their descriptors is too high. This is done using the <code class="email">radiusMatch</code> method<a id="id758" class="calibre1"/> of the <code class="email">cv::DescriptorMatcher</code> class:</p><div><pre class="programlisting">// radius match
float maxDist= 0.4;
std::vector&lt;std::vector&lt;cv::DMatch&gt;&gt; matches2;
matcher.radiusMatch(descriptors1, descriptors2, matches2, 
                  maxDist); // maximum acceptable distance
                             // between the 2 descriptors</pre></div><p class="calibre8">The result is again a <code class="email">std::vector</code> class of <code class="email">std::vector</code> because the method will retain all the matches with a distance smaller than the specified threshold. This means that a given keypoint might have more than one matching point in the other image. Conversely, other keypoints will<a id="id759" class="calibre1"/> not have any matches associated with them (the corresponding inner <code class="email">std::vector</code> class will then have a size of <code class="email">0</code>). This time, the initial match set of <code class="email">90</code> pairs is reduced to <code class="email">37</code> pairs as shown in the following screenshot:</p><div><img src="img/00157.jpeg" alt="Distance thresholding" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Obviously, you can combine all these strategies in order to improve your matching results.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec174" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Detecting scale-invariant features</em> recipe in <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points,</em> presents the associated SURF and SIFT feature detectors and provides more references on the subject</li><li class="listitem">The <em class="calibre9">Matching images using random sample consensus</em> recipe in <a class="calibre1" title="Chapter 10. Estimating Projective Relations in Images" href="part0067_split_000.html#page">Chapter 10</a>, <em class="calibre9">Estimating Projective Relations in Images</em>, explains how to use the image and the scene geometry in order to obtain a match set of even better quality</li><li class="listitem">The <em class="calibre9">Matching feature points in stereo pairs: A comparative study of some matching strategies </em>article by E. Vincent and R. Laganière in <em class="calibre9">Machine, Graphics and Vision, pp. 237-260, 2001</em>, describes other simple matching strategies that could be used to improve the quality of the match set</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch09lvl1sec60" class="calibre1"/>Describing keypoints with binary features</h1></div></div></div><p class="calibre8">In the <a id="id760" class="calibre1"/>previous recipe, we learned how to describe a keypoint using rich descriptors extracted from the image intensity gradient. These <a id="id761" class="calibre1"/>descriptors are floating-point vectors that have a dimension of <code class="email">64</code>, <code class="email">128</code>, or sometimes even longer. This makes them costly to manipulate. In order to reduce the memory and computational load associated with these descriptors, the idea of using binary descriptors has been recently introduced. The challenge here is to make them easy to compute and yet keep them robust to scene and viewpoint changes. This recipe describes some of these binary descriptors. In particular, we will look at the ORB and BRISK descriptors for which we presented their associated feature point detectors in <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points</em>.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch09lvl2sec175" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Owing to the nice generic interface on top of which the OpenCV detectors and the descriptors module are built, using a binary descriptor such as ORB is no different from using descriptors such as SURF and SIFT. The complete feature-based image matching sequence is as follows:</p><div><pre class="programlisting">// Define keypoints vector
std::vector&lt;cv::KeyPoint&gt; keypoints1, keypoints2;
// Construct the ORB feature detector object
cv::Ptr&lt;cv::FeatureDetector&gt; detector = 
  new cv::ORB(100); // detect approx 100 ORB points  
// Detect the ORB features
detector-&gt;detect(image1,keypoints1);
detector-&gt;detect(image2,keypoints2);
// ORB includes both the detector and descriptor extractor
cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor = detector;
// Extract the descriptor
cv::Mat descriptors1, descriptors2;
descriptor-&gt;compute(image1,keypoints1,descriptors1);
descriptor-&gt;compute(image2,keypoints2,descriptors2);
// Construction of the matcher 
cv::BFMatcher matcher(
     cv::NORM_HAMMING); // always use hamming norm
                        // for binary descriptors
// Match the two image descriptors
std::vector&lt;cv::DMatch&gt; matches;
matcher.match(descriptors1,descriptors2, matches);</pre></div><p class="calibre8">The only difference resides in the use of the <strong class="calibre2">Hamming</strong> norm<a id="id762" class="calibre1"/> (the <code class="email">cv::NORM_HAMMING</code> flag) that measures<a id="id763" class="calibre1"/> the<a id="id764" class="calibre1"/> distance between two binary descriptors by counting the number of bits that are dissimilar. On many processors, this operation is efficiently implemented by using an exclusive OR operation, followed by a simple bit count.</p><p class="calibre8">The following screenshot shows the result of the matching:</p><div><img src="img/00158.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Similar results will be obtained with another popular binary feature detector/descriptor: BRISK. In this case, the <code class="email">cv::DescriptorExtractor</code> instance is created by the new <code class="email">cv::BRISK(40)</code> call. As we learned in the previous chapter, its first parameter is a threshold that controls the number of detected points.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch09lvl2sec176" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The ORB algorithm<a id="id765" class="calibre1"/> detects oriented feature points at multiple scales. Based on this result, the <a id="id766" class="calibre1"/>ORB descriptor <a id="id767" class="calibre1"/>extracts a representation of each keypoint by using simple intensity comparisons. In fact, ORB builds on a previously proposed descriptor called BRIEF. This later creates a binary descriptor by simply selecting a random pair of points inside a defined neighborhood around the keypoint. The intensity values of the two pixel points are then compared, and if the first point has a higher intensity, then the value <code class="email">1</code> is assigned to the corresponding descriptor bit value. Otherwise, the value <code class="email">0</code> is assigned. Repeating this test on a number of random pairs generates a descriptor that is made up of several bits; typically, <code class="email">128</code> to <code class="email">512</code> bits (pairwise tests) are used.</p><p class="calibre8">This is the scheme used by ORB. Then, the decision to be made is which set of point pairs should be used to build the descriptor. Indeed, even if the point pairs are randomly chosen, once they have been selected, the same set of binary tests must be performed to build the descriptor of all the keypoints in order to ensure consistency of the results. To make the descriptor more distinctive, intuition tells us that some choices must be better than others. Also, the fact that the orientation of each keypoint is known introduces some bias in the intensity pattern distribution when this one is normalized with respect to this orientation (that is, when the point coordinates are given relative to this keypoint orientation). From these considerations and the experimental validation, ORB has identified a set of <code class="email">256</code> point pairs with high variance and minimal pairwise correlation. In other words, the selected binary tests are the ones that have an equal chance of being <code class="email">0</code> or <code class="email">1</code> over a variety of keypoints and also those that are as independent from each other as possible.</p><p class="calibre8">In addition to the parameters that control the feature detection process, the <code class="email">cv::ORB</code> constructor includes two parameters related to its descriptor. One parameter is used to specify the patch size inside which the point pairs are selected (the default is <code class="email">31x31</code>). The second parameter allows you to perform tests with a triplet or quadruplet of points instead of the default point pairs. Note that it is highly recommended that you use the default settings.</p><p class="calibre8">The descriptor of BRISK is very similar. It is also based on pairwise intensity comparisons with two differences. First, instead of randomly selecting the points from the <code class="email">31x31</code> points of the neighborhood, the chosen points are selected from a sampling pattern of a set of concentric circles (made up of <code class="email">60</code> points) with locations that are equally spaced. Second, the intensity at each of these sample points is a Gaussian-smoothed value with a σ value proportional to the distance from the central keypoint. From these points, BRISK selects <code class="email">512</code> point pairs.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch09lvl2sec177" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Several other binary <a id="id768" class="calibre1"/>descriptors exist, and interested <a id="id769" class="calibre1"/>readers should take a look at the scientific literature to learn more on this subject. Since it is also available in OpenCV, we will describe one additional descriptor here.</p><div><div><div><div><h3 class="title2"><a id="ch09lvl3sec46" class="calibre1"/>FREAK</h3></div></div></div><p class="calibre8">FREAK <a id="id770" class="calibre1"/>stands for <strong class="calibre2">Fast Retina Keypoint</strong>. This is also a binary descriptor, but it does not have an associated detector. It can be applied on any set of keypoints detected, for example, SIFT, SURF, or ORB.</p><p class="calibre8">Like BRISK, the FREAK descriptor is also based on a sampling pattern defined on concentric circles. However, to design their descriptor, the authors used an analogy of the human eye. They observed that on the retina, the density of the ganglion cells decreases with the increase in the distance to the fovea. Consequently, they built a sampling pattern made of <code class="email">43</code> points in which the density of a point is much greater near the central point. To obtain its intensity, each point is filtered with a Gaussian kernel that has a size that also increases with the distance to the center.</p><p class="calibre8">In order to identify the pairwise comparisons that should be performed, an experimental validation has been performed by following a strategy similar to the one used for ORB. By analyzing several thousands of keypoints, the binary tests with the highest variance and lowest correlation are retained, resulting in <code class="email">512</code> pairs.</p><p class="calibre8">FREAK also introduces the idea of performing the descriptor comparisons in cascade. That is, the first <code class="email">128</code> bits representing coarser information (corresponding to the tests performed at the periphery on larger Gaussian kernels) are performed first. Only if the compared descriptors pass this initial step will the remaining tests be performed.</p><p class="calibre8">Using the keypoints detected with ORB, we extract the FREAK descriptors by simply creating the <code class="email">cv::DescriptorExtractor</code> instance as follows:</p><div><pre class="programlisting">cv::Ptr&lt;cv::DescriptorExtractor&gt; descriptor =
   new cv::FREAK(); // to describe with FREAK  </pre></div><p class="calibre8">The match result is as follows:</p><div><img src="img/00159.jpeg" alt="FREAK" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The following figure illustrates the sampling pattern used for the three descriptors presented in this recipe:</p><div><img src="img/00160.jpeg" alt="FREAK" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The first square is the ORB descriptor in which point pairs are randomly selected on a square grid. Each pair of points linked by a line represent a possible test to compare the two pixel intensities. Here, we <a id="id771" class="calibre1"/>show only 8 such pairs; the default ORB uses 256 pairs. The middle square corresponds to the BRISK sampling pattern. Points are uniformly sampled on the shown circles (for clarity, we only identify the points on the first circle here). Finally, the third square shows the log-polar sampling grid of FREAK. While BRISK has a uniform distribution of points, FREAK has a higher density of points closer to the center. For example, in BRISK, you find 20 points on the outer circle, while in the case of FREAK, its outer circle includes only 6 points.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch09lvl2sec178" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre9">Detecting FAST features at multiple scales</em> recipe in <a class="calibre1" title="Chapter 8. Detecting Interest Points" href="part0058_split_000.html#page">Chapter 8</a>, <em class="calibre9">Detecting Interest Points,</em> presents the associated BRISK and ORB feature detectors and provides more references on the subject</li><li class="listitem">The <em class="calibre9">BRIEF: Computing a Local Binary Descriptor Very Fast </em>article by E. M. Calonder, V. Lepetit, M. Ozuysal, T. Trzcinski, C. Strecha, and P. Fua in <em class="calibre9">IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012</em>, describes the BRIEF feature descriptor that inspires the presented binary descriptors</li><li class="listitem">The <em class="calibre9">FREAK: Fast Retina Keypoint </em>article by A.Alahi, R. Ortiz, and P. Vandergheynst in <em class="calibre9">IEEE Conference on Computer Vision and Pattern Recognition, 2012</em>, describes the FREAK feature descriptor</li></ul></div></div></div></body></html>