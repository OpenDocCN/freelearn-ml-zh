<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05" class="calibre6"/>Chapter 5. Transforming Images with Morphological Operations</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Eroding and dilating images using morphological filters</li><li class="listitem">Opening and closing images using morphological filters</li><li class="listitem">Applying morphological operators on gray-level images</li><li class="listitem">Segmenting images using watersheds</li><li class="listitem">Extracting distinctive regions using MSER</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec34" class="calibre6"/>Introduction</h1></div></div></div><p class="calibre8">
<strong class="calibre15">Mathematical morphology</strong> is a theory that was developed in the 1960s for the analysis and processing of discrete images. It defines a series of operators that transform an image by probing it with a predefined shape element. The way this shape element intersects the neighborhood of a pixel determines the result of the operation. This chapter presents the most important morphological operators. It also explores the problems of image segmentation and feature detection using algorithms based on morphological operators.</p></div></div>
<div><div><div><div><h1 class="title1"><a id="ch05lvl1sec35" class="calibre6"/>Eroding and dilating images using morphological filters</h1></div></div></div><p class="calibre8">Erosion and dilation are the most fundamental morphological operators. Therefore, we will present them in this first recipe. The fundamental component in mathematical morphology is the <strong class="calibre15">structuring element</strong>. A structuring element can be simply defined as a configuration of pixels (the square shape in the following figure) on which an origin is defined (also called an <strong class="calibre15">anchor point</strong>). Applying a morphological filter consists of probing each pixel of the image using this structuring element. When the origin of the structuring element is aligned with a given pixel, its intersection with the image defines a set of pixels on which a particular morphological operation is applied (the nine shaded pixels in the following figure). In principle, the structuring element can be of any shape, but most often, a simple shape such as a square, circle, or diamond with the origin at the center is used. Custom structuring elements can be useful to emphasize or eliminate regions of particular shapes.</p><p class="calibre8">
</p><div><img alt="Eroding and dilating images using morphological filters" src="img/image_05_001.jpg" class="calibre17"/></div><p class="calibre8">
</p><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec102" class="calibre6"/>Getting ready</h2></div></div></div><p class="calibre8">As morphological filters often work on binary images, we will use the binary image that was created through thresholding in the first recipe of the previous chapter. However, since the convention is to have the foreground objects represented by high (white) pixel values and the background objects by low (black) pixel values in morphology, we have negated the image.</p><p class="calibre8">In morphological terms, the following image is said to be the <strong class="calibre15">complement</strong> of the image that was created in the previous chapter:</p><p class="calibre8">
</p><div><img alt="Getting ready" src="img/image_05_002.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec103" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">Erosion and dilation are implemented in OpenCV as simple functions, which are <code class="literal">cv::erode</code> and <code class="literal">cv::dilate</code>. Their usage is straightforward:</p><pre class="programlisting">    // Read input image 
    cv::Mat image= cv::imread("binary.bmp"); 
 
    // Erode the image 
    // with the default 3x3 structuring element (SE) 
    cv::Mat eroded;  // the destination image 
    cv::erode(image,eroded,cv::Mat()); 
 
    // Dilate the image 
    cv::Mat dilated;  // the destination image 
    cv::dilate(image,dilated,cv::Mat()); 
</pre><p class="calibre8">The two images produced by these function calls are seen in the following images. The first one shows erosion:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_003.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The second image shows the dilation result:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_006.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec104" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">As for all morphological filters, the two filters of this recipe operate on sets of pixels defined by a structuring element. Recall that when applied to a given pixel, the anchor point of the structuring element is aligned with this pixel location, and all the pixels that intersect the structuring element are included in the current set. <strong class="calibre15">Erosion</strong> replaces the current pixel with the minimum pixel value found in the defined pixel set. <strong class="calibre15">Dilation</strong> is the complementary operator, and it replaces the current pixel with the maximum pixel value found in the defined pixel set. Since the input binary image contains only black (value <code class="literal">0</code>) and white (value <code class="literal">255</code>) pixels, each pixel is replaced by either a white or black pixel.</p><p class="calibre8">A good way to picturize the effect of these two operators is to think in terms of background (black) and foreground (white) objects. With erosion, if the structuring element when placed at a given pixel location touches the background (that is, one of the pixels in the intersecting set is black), then this pixel will be sent to the background. In the case of dilation, if the structuring element on a background pixel touches a foreground object, then this pixel will be assigned a white value. This explains why the size of the objects has been reduced (the shape has been eroded) in the eroded image while it has been expanded in the dilated image. Note how some of the small objects (which can be considered as "noisy" background pixels) have also been completely eliminated in the eroded image. Similarly, the dilated objects are now larger, and some of the "holes" inside them have been filled. By default, OpenCV uses a <code class="literal">3x3</code> square structuring element. This default structuring element is obtained when an empty matrix (that is, <code class="literal">cv::Mat()</code>) is specified as the third argument in the function call, as it was done in the preceding example. You can also specify a structuring element of the size (and shape) you want by providing a matrix in which the nonzero element defines the structuring element. For example, to apply a <code class="literal">7x7</code> structuring element, you would proceed as follows:</p><pre class="programlisting">    // Erode the image with a larger SE 
    // create a 7x7 mat with containing all 1s 
    cv::Mat element(7,7,CV_8U,cv::Scalar(1)); 
    // erode the image with that SE 
    cv::erode(image,eroded,element); 
</pre><p class="calibre8">The effect is much more destructive in this case, as shown in the following screenshot:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_05_008.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Another way to obtain a similar result is to repetitively apply the same structuring element on an image. The two functions have an optional parameter to specify the number of repetitions:</p><pre class="programlisting">    // Erode the image 3 times 
    cv::erode(image,eroded,cv::Mat(),cv::Point(-1,-1), 3); 
</pre><p class="calibre8">The <code class="literal">cv::Point(-1,-1)</code> argument means that the origin is at the center of the matrix (default); it can be defined anywhere on the structuring element. The image that is obtained will be identical to the image we obtained with the <code class="literal">7x7</code> structuring element. Indeed, eroding an image twice is similar to eroding an image with a structuring element dilated with itself. This also applies to dilation.</p><p class="calibre8">Finally, since the notion of background/foreground is arbitrary, we can make the following observation (which is a fundamental property of the erosion/dilation operators). Eroding the foreground objects with a structuring element can be seen as a dilation of the background part of the image. In other words, we can make the following observations:</p><div><ul class="itemizedlist"><li class="listitem">The erosion of an image is equivalent to the complement of the dilation of the complement image</li><li class="listitem">The dilation of an image is equivalent to the complement of the erosion of the complement image</li></ul></div></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec105" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Note that even though we applied our morphological filters on binary images here, these filters can be applied on gray-level or even color images with the same definitions. The third recipe of this chapter will present few morphological operators and their effect on gray-level images.</p><p class="calibre8">Also, note that the OpenCV morphological functions support in-place processing. This means that you can use the input image as the destination image, as follows:</p><pre class="programlisting">    cv::erode(image,image,cv::Mat()); 
</pre><p class="calibre8">OpenCV will create the required temporary image for you for this to work properly.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec106" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Opening and closing images using morphological filters</em> recipe applies the erosion and dilation filters in cascade to produce new operators</li><li class="listitem">The <em class="calibre16">Applying morphological operators on gray-level images</em> recipe introduces other morphological operators that can usefully be applied to gray-level images</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch05lvl1sec36" class="calibre6"/>Opening and closing images using morphological filters</h1></div></div></div><p class="calibre8">The previous recipe introduced you to the two fundamental morphological operators: dilation and erosion. From these, other operators can be defined. The next two recipes will present some of them. The opening and closing operators are presented in this recipe.</p><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec107" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">In order to apply higher-level morphological filters, you need to use the <code class="literal">cv::morphologyEx</code> function with the appropriate function code. For example, the following call will apply the closing operator:</p><pre class="programlisting">    // Close the image 
    cv::Mat element5(5,5,CV_8U,cv::Scalar(1)); 
    cv::Mat closed; 
    cv::morphologyEx(image,closed,    // input and output images 
                     cv::MORPH_CLOSE, // operator code 
                     element5);       // structuring element 
</pre><p class="calibre8">Note that we used a <code class="literal">5x5</code> structuring element to make the effect of the filter more apparent. If we use the binary image of the preceding recipe as input, we will obtain the following image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_009.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Similarly, applying the morphological opening operator will result in the following image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_011.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The preceding image is obtained from the following code:</p><pre class="programlisting">    cv::Mat opened; 
    cv::morphologyEx(image, opened, cv::MORPH_OPEN, element5); 
</pre></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec108" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">The opening and closing filters are simply defined in terms of the basic erosion and dilation operations. <strong class="calibre15">Closing</strong> is defined as the erosion of the dilation of an image. <strong class="calibre15">Opening</strong> is defined as the dilation of the erosion of an image.</p><p class="calibre8">Consequently, one can compute the closing of an image using the following calls:</p><pre class="programlisting">    // dilate original image 
    cv::dilate(image, result, cv::Mat()); 
    // in-place erosion of the dilated image 
    cv::erode(result, result, cv::Mat()); 
</pre><p class="calibre8">The opening filter can be obtained by interchanging these two function calls.</p><p class="calibre8">While examining the result of the closing filter, it can be seen that the small holes of the white foreground objects have been filled. The filter also connects several adjacent objects together. Basically, any holes or gaps that are too small to completely contain the structuring element will be eliminated by the filter.</p><p class="calibre8">Reciprocally, the opening filter eliminated several small objects from the scene. All the objects that were too small to contain the structuring element have been removed.</p><p class="calibre8">These filters are often used in object detection. The closing filter connects the objects erroneously fragmented into smaller pieces together, while the opening filter removes the small blobs introduced by the image noise. Therefore, it is advantageous to use them in a sequence. You can then apply the opening filter before the closing filter if you wish to prioritize noise filtering, but this could be at the price of eliminating parts of fragmented objects.</p><p class="calibre8">The following image is the result of applying the opening filter before the closing filter:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_05_013.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that applying the same opening (and similarly the closing) operator on an image several times has no effect. Indeed, as the holes have been filled by the first opening filter, an additional application of the same filter will not produce any other changes to the image. In mathematical terms, these operators are said to be <strong class="calibre15">idempotent</strong>.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec109" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The opening and closing operators are often used to clean up an image before extracting its connected components as explained in the <em class="calibre16">Extracting connected components</em> recipe of <a href="ch07.html" title="Chapter 7. Extracting Lines, Contours, and Components">
Chapter 7
</a>, <em class="calibre16">Extracting Lines, Contours, and Components</em></li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch05lvl1sec37" class="calibre6"/>Applying morphological operators on gray-level images</h1></div></div></div><p class="calibre8">More advanced morphological operators can be composited by combining the different basic morphological filters introduced in this chapter. This recipe will present two morphological operators that, when applied to gray-level images, can lead to the detection of interesting image features.</p><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec110" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">One interesting morphological operator is the morphological gradient that allows extracting the edges of an image. This one can be accessed through the <code class="literal">cv::morphologyEx</code> function as follows:</p><pre class="programlisting">    // Get the gradient image using a 3x3 structuring element 
    cv::Mat result; 
    cv::morphologyEx(image, result,
                     cv::MORPH_GRADIENT, cv::Mat()); 
</pre><p class="calibre8">The following result shows the extracted contours of the image's elements (the resulting image has been inverted for better viewing):</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_016.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Another useful morphological operator is the top-hat transform. This operator can be used to extract local small foreground objects in an image. The effect of this operator can be demonstrated by applying it on the book image of the last recipe of the previous chapter. This image shows an unevenly illuminated page of a book. A black top-hat transform will extract the characters of this page (considered here as the foreground objects). This operator is also called by using the <code class="literal">cv::morphologyEx</code> function with the appropriate flag:</p><pre class="programlisting">    // Apply the black top-hat transform using a 7x7 structuring element 
    cv::Mat element7(7, 7, CV_8U, cv::Scalar(1)); 
    cv::morphologyEx(image, result, cv::MORPH_BLACKHAT, element7); 
</pre><p class="calibre8">As it can be seen in the following image, this operator successfully extracted most of the characters of the original image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_017.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec111" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">A good way to understand the effect of morphological operators on a gray-level image is to consider an image as a topological relief in which the gray levels correspond to elevation (or altitude). Under this perspective, the bright regions correspond to mountains, while the dark areas correspond to the valleys of the terrain. Also, since edges correspond to a rapid transition between the dark and bright pixels, these can be pictured as abrupt cliffs. If an erosion operator is applied on such a terrain, the net result will be to replace each pixel by the lowest value in a certain neighborhood, thus reducing its height. As a result, cliffs will be eroded as the valleys expand. Dilation has the exact opposite effect; that is, cliffs will gain terrain over the valleys. However, in both cases, the plateau (that is, the area of constant intensity) will remain relatively unchanged.</p><p class="calibre8">These observations lead to a simple way to detect the edges (or cliffs) of an image. This can be done by computing the difference between the dilated and eroded images. Since these two transformed images differ mostly at the edge locations, the image edges will be emphasized by the subtraction. This is exactly what the <code class="literal">cv::morphologyEx</code> function does when the <code class="literal">cv::MORPH_GRADIENT</code> argument is inputted. Obviously, the larger the structuring element is, the thicker the detected edges will be. This edge detection operator is called the <strong class="calibre15">Beucher </strong>
<strong class="calibre15">gradient</strong> (the next chapter will discuss the concept of an image gradient in more detail). Note that similar results can also be obtained by simply subtracting the original image from the dilated one or the eroded image from the original. The resulting edges would simply be thinner.</p><p class="calibre8">The top-hat operator is also based on image difference. This time, the operator uses opening and closing. When a gray-level image is morphologically opened, its local peaks are eliminated; this is due to the erosion operator that is applied first. The rest of the image is preserved. Consequently, the difference between the original image and the opened one is the set of local peaks. These local peaks are the foreground objects we want to extract. In the book example of this recipe, the objective was to extract the characters of the page. Since the foreground objects are, in this case, black over a white background, we used the complementary operator, called the black top-hat, which consists of subtracting the original image from its closing. We used a <code class="literal">7x7</code> structuring element in order to have the closing operation big enough to remove the characters.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec112" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Applying directional filters to detect edges</em> recipe in <a href="ch06.html" title="Chapter 6. Filtering the Images">Chapter 6</a>, <em class="calibre16">Filtering the Images</em>, describes the other filters that perform edge detection</li><li class="listitem">The article, <em class="calibre16">The Morphological gradients, J.-F. Rivest, P. Soille, and S. Beucher, ISET's symposium on electronic imaging science and technology, SPIE</em>, Feb. 1992, discusses the concept of morphological gradients in more detail</li><li class="listitem">The article <em class="calibre16">Morphological operator for corner detection</em>, <em class="calibre16">R. Laganière</em>, <em class="calibre16">Pattern Recognition</em>, volume 31, issue 11, 1998, presents an operator for the detection of corners using morphological filters</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch05lvl1sec38" class="calibre6"/>Segmenting images using watersheds</h1></div></div></div><p class="calibre8">The watershed transformation is a popular image processing algorithm that is used to quickly segment an image into homogenous regions. It relies on the idea that when the image is seen as a topological relief, the homogeneous regions correspond to relatively flat basins delimited by steep edges. With the watershed algorithm, segmentation is achieved by flooding this relief by gradually increasing the level of water in this one. As a result of its simplicity, the original version of this algorithm tends to over-segment the image, which produces multiple small regions. This is why OpenCV proposes a variant of this algorithm that uses a set of predefined markers to guide the definition of the image segments.</p><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec113" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The watershed segmentation is obtained through the use of the <code class="literal">cv::watershed</code> function. The input for this function is a 32-bit signed integer marker image in which each nonzero pixel represents a label. The idea is to mark some pixels of the image that are known to belong to a given region. From this initial labeling, the watershed algorithm will determine the regions to which the other pixels belong. In this recipe, we will first create the marker image as a gray-level image and then convert it into an image of integers. We have conveniently encapsulated this step into a <code class="literal">WatershedSegmenter</code> class containing a method to specify the marker image and a method to compute the watershed:</p><pre class="programlisting">    class WatershedSegmenter { 
 
      private: 
      cv::Mat markers; 
 
      public: 
      void setMarkers(const cv::Mat&amp; markerImage) { 
 
      // Convert to image of ints 
      markerImage.convertTo(markers,CV_32S); 
    } 
 
    cv::Mat process(const cv::Mat &amp;image) { 
 
      // Apply watershed 
      cv::watershed(image,markers); 
      return markers; 
    } 
</pre><p class="calibre8">The way these markers are obtained depends on the application. For example, some preprocessing steps might have resulted in the identification of some pixels that belong to an object of interest. The watershed would then be used to delimitate the complete object from that initial detection. In this recipe, we will simply use the binary image used throughout this chapter in order to identify the animals of the corresponding original image (this is the image shown at the beginning of 
<a href="ch04.html" title="Chapter 4. Counting the Pixels with Histograms">Chapter 4</a>
, <em class="calibre16">Counting the Pixels with Histograms</em>). Therefore, from our binary image, we need to identify pixels that belong to the foreground (the animals) and pixels that belong to the background (mainly the grass). Here, we will mark the foreground pixels with the label <code class="literal">255</code> and the background pixels with the label <code class="literal">128</code> (this choice is totally arbitrary; any label number other than <code class="literal">255</code> will work). The other pixels, that is, the ones for which the labeling is unknown, are assigned the value <code class="literal">0</code>.</p><p class="calibre8">As of now, the binary image includes white pixels that belong to the various parts of the image. We will then severely erode this image in order to retain only the pixels that certainly belong to the foreground objects:</p><pre class="programlisting">    // Eliminate noise and smaller objects 
    cv::Mat fg; 
    cv::erode(binary,fg,cv::Mat(),cv::Point(-1,-1),4); 
</pre><p class="calibre8">The result is the following image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_020.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note that a few pixels that belong to the background forest are still present. Let's keep them. Therefore, they will be considered to correspond to an object of interest. Similarly, we can select a few pixels of the background by a large dilation of the original binary image:</p><pre class="programlisting">    // Identify image pixels without objects 
    cv::Mat bg; 
    cv::dilate(binary,bg,cv::Mat(),cv::Point(-1,-1),4); 
    cv::threshold(bg,bg,1,128,cv::THRESH_BINARY_INV); 
</pre><p class="calibre8">The resulting black pixels correspond to background pixels. This is why the thresholding operation assigns the value <code class="literal">128</code> to these pixels immediately after the dilation. The following image is obtained:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_022.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">These images are combined to form the marker image as follows:</p><pre class="programlisting">    // Create markers image 
    cv::Mat markers(binary.size(),CV_8U,cv::Scalar(0)); 
    markers= fg+bg; 
</pre><p class="calibre8">Note how we used the overloaded <code class="literal">operator+</code> here in order to combine the images. The following image will be used as the input to the watershed algorithm:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_024.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">In this input image, the white areas belong, for sure, to the foreground objects, the gray areas are a part of the background, and the black areas have an unknown label. The role of the watershed segmentation is therefore to assign a label (background/foreground) to the black marked pixels by establishing the exact border delimitating the foreground objects from the background. This segmentation is then obtained as follows:</p><pre class="programlisting">    // Create watershed segmentation object 
    WatershedSegmenter segmenter; 
 
    // Set markers and process 
    segmenter.setMarkers(markers); 
    segmenter.process(image); 
</pre><p class="calibre8">The marker image is then updated such that each zero pixel is assigned one of the input labels, while the pixels that belong to the found boundaries have a value <code class="literal">-1</code>. The resulting image of the labels is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_026.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">And the boundary image is as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_028.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec114" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">As we did in the preceding recipes, we will use the topological map analogy in the description of the watershed algorithm. In order to create watershed segmentation, the idea is to progressively flood the image starting at level 0. As the level of water progressively increases (to levels 1, 2, 3, and so on), catchment basins are formed. The size of these basins also gradually increases and, consequently, the water of two different basins will eventually merge. When this happens, a watershed is created in order to keep the two basins separated. Once the level of water has reached its maximum level, the sets of these created basins and watersheds form the watershed segmentation.</p><p class="calibre8">As expected, the flooding process initially creates many small individual basins. When all of these are merged, many watershed lines are created, which results in an over-segmented image. To overcome this problem, a modification to this algorithm has been proposed in which the flooding process starts from a predefined set of marked pixels. The basins created from these markers are labeled in accordance with the values assigned to the initial marks. When two basins having the same label merge, no watershed is created, thus preventing over-segmentation. This is what happens when the <code class="literal">cv::watershed</code> function is called. The input marker image is updated to produce the final watershed segmentation. Users can input a marker image with any number of labels and pixels of unknown labeling left to value <code class="literal">0</code>. The marker image is chosen to be an image of a 32-bit signed integer in order to be able to define more than <code class="literal">255</code> labels. It also allows the special value, <code class="literal">-1</code>, to be assigned to the pixels associated with a watershed.</p><p class="calibre8">To facilitate the display of the result, we have introduced two special methods. The first method returns an image of the labels (with watersheds at value <code class="literal">0</code>). This is easily done through thresholding, as follows:</p><pre class="programlisting">    // Return result in the form of an image 
    cv::Mat getSegmentation() { 
 
      cv::Mat tmp; 
      // all segment with label higher than 255 
      // will be assigned value 255 
      markers.convertTo(tmp,CV_8U); 
 
      return tmp; 
    } 
</pre><p class="calibre8">Similarly, the second method returns an image in which the watershed lines are assigned the value <code class="literal">0</code>, and the rest of the image is at <code class="literal">255</code>. This time, the <code class="literal">cv::convertTo</code> method is used to achieve this result, as follows:</p><pre class="programlisting">    // Return watershed in the form of an image 
    cv::Mat getWatersheds() { 
 
      cv::Mat tmp; 
      // Each pixel p is transformed into 
      // 255p+255 before conversion 
      markers.convertTo(tmp,CV_8U,255,255); 
 
      return tmp; 
    } 
</pre><p class="calibre8">The linear transformation that is applied before the conversion allows the <code class="literal">-1</code> pixels to be converted into <code class="literal">0</code> (since <code class="literal">-1*255+255=0</code>).</p><p class="calibre8">Pixels with a value greater than <code class="literal">255</code> are assigned the value <code class="literal">255</code>. This is due to the saturation operation that is applied when signed integers are converted into unsigned characters.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec115" class="calibre6"/>There's more...</h2></div></div></div><p class="calibre8">Obviously, the marker image can be obtained in many different ways. For example, users can be interactively asked to mark the objects of an image by painting some areas on the objects and the background of a scene. Alternatively, in an attempt to identify an object located at the center of an image, one can also simply input an image with the central area marked with a certain label and the border of the image (where the background is assumed to be present) marked with another label. This marker image can be created by drawing thick rectangles on a marker image as follows:</p><pre class="programlisting">    // Identify background pixels 
    cv::Mat imageMask(image.size(),CV_8U,cv::Scalar(0)); 
    cv::rectangle(imageMask, cv::Point(5,5),  
                  cv::Point(image.cols-5, image.rows-5),   
                  cv::Scalar(255), 3); 
    // Identify foreground pixels 
    // (in the middle of the image) 
    cv::rectangle(imageMask,
                  cv::Point(image.cols/2-10,image.rows/2-10),
                  cv::Point(image.cols/2+10,image.rows/2+10),
                  cv::Scalar(1), 10); 
</pre><p class="calibre8">If we superimpose this marker image on a test image, we will obtain the following image:</p><p class="calibre8">
</p><div><img alt="There's more..." src="img/image_05_030.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The following is the resulting watershed image:</p><p class="calibre8">
</p><div><img alt="There's more..." src="img/image_05_032.jpg" class="calibre17"/></div><p class="calibre8">
</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec116" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article, <em class="calibre16">The viscous watershed transform</em>, <em class="calibre16">C. Vachier</em> and <em class="calibre16">F. Meyer</em>, <em class="calibre16">Journal of Mathematical Imaging and Vision</em>, volume 22, issue 2-3, May 2005, gives more information on the watershed transform</li></ul></div></div></div>
<div><div><div><div><h1 class="title1"><a id="ch05lvl1sec39" class="calibre6"/>Extracting distinctive regions using MSER</h1></div></div></div><p class="calibre8">In the previous recipe, you learned how an image can be segmented into regions by gradually flooding it and creating watersheds. The <strong class="calibre15">Maximally Stable External Regions</strong> (<strong class="calibre15">MSER</strong>) algorithm uses the same immersion analogy in order to extract meaningful regions in an image. These regions will also be created by flooding the image level by level, but this time, we will be interested in the basins that remain relatively stable for a period of time during the immersion process. It will be observed that these regions correspond to some distinctive parts of the scene objects pictured in the image.</p><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec117" class="calibre6"/>How to do it...</h2></div></div></div><p class="calibre8">The basic class to compute the MSER of an image is <code class="literal">cv::MSER</code>. This class is an abstract interface that inherits from the <code class="literal">cv::Feature2D</code> class; in fact, all feature detectors in OpenCV inherit from this super-class. An instance of the <code class="literal">cv::MSER</code> class can be created by using the <code class="literal">create</code> method. Here, we initialize it by specifying a minimum and maximum size for the detected regions in order to limit the number of detected features as follows:</p><pre class="programlisting">    // basic MSER detector 
    cv::Ptr&lt;cv::MSER&gt; ptrMSER=  
     cv::MSER::create(5,     // delta value for local detection 
                      200,   // min acceptable area 
                      2000); // max acceptable area 
</pre><p class="calibre8">Now, the MSER can be obtained by a call to the <code class="literal">detectRegions</code> method, specifying the input image and the appropriate output data structures, as follows:</p><pre class="programlisting">    // vector of point sets 
    std::vector&lt;std::vector&lt;cv::Point&gt; &gt; points; 
    // vector of rectangles 
    std::vector&lt;cv::Rect&gt; rects; 
    // detect MSER features 
    ptrMSER-&gt;detectRegions(image, points, rects); 
</pre><p class="calibre8">The detection results are provided in the form of a vector of regions represented by the pixel points that compose each of them and by a vector of bounding boxes enclosing the regions. In order to visualize the results, we create a blank image on which we will display the detected regions in different colors (which are randomly chosen). This is done as follows:</p><pre class="programlisting">    // create white image 
    cv::Mat output(image.size(),CV_8UC3); 
    output= cv::Scalar(255,255,255); 
 
    // OpenCV random number generator 
    cv::RNG rng; 
 
    // Display the MSERs in color areas 
    // for each detected feature 
    // reverse order to display the larger MSER first 
    for (std::vector&lt;std::vector&lt;cv::Point&gt; &gt;::reverse_iterator  
             it= points.rbegin(); 
             it!= points.rend(); ++it) { 
 
        // generate a random color 
        cv::Vec3b c(rng.uniform(0,254),  
                    rng.uniform(0,254), rng.uniform(0,254)); 
 
        // for each point in MSER set 
        for (std::vector&lt;cv::Point&gt;::iterator itPts= it-&gt;begin(); 
                    itPts!= it-&gt;end(); ++itPts) { 
 
          // do not overwrite MSER pixels 
          if (output.at&lt;cv::Vec3b&gt;(*itPts)[0]==255) { 
            output.at&lt;cv::Vec3b&gt;(*itPts)= c; 
          } 
        } 
      } 
</pre><p class="calibre8">Note that the MSER form a hierarchy of regions. Therefore, to make all of these visible, we have chosen not to overwrite the larger regions when they include smaller ones. We can detect MSERs on the following image:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_034.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">The resulting image will be as follows:</p><p class="calibre8">
</p><div><img alt="How to do it..." src="img/image_05_036.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Not all regions are visible in this image. Nevertheless, it can be observed how this operator has been able to extract some meaningful regions (for example, the building's windows) from this image.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec118" class="calibre6"/>How it works...</h2></div></div></div><p class="calibre8">MSER uses the same mechanism as the watershed algorithm; that is, it proceeds by gradually flooding the image from level <code class="literal">0</code> to level <code class="literal">255</code>. Note that in image processing, the set of pixels above a certain threshold is often call a <strong class="calibre15">level set</strong>. As the level of water increases, you can observe that the sharply delimitated darker areas form basins that have a relatively stable shape for a period of time (recall that under the immersion analogy, the water levels correspond to the intensity levels). These stable basins are the MSER. These are detected by considering the connected regions (the basins) at each level and measuring their stability. This is done by comparing the current area of a region with the area it previously had when the level was down by a value of delta. When this relative variation reaches a local minimum, the region is identified as a MSER. The delta value that is used to measure the relative stability is the first parameter in the constructor of the <code class="literal">cv::MSER</code> class; its default value is <code class="literal">5</code>. In addition, to be considered, the size of a region must be within a certain predefined range. The acceptable minimum and maximum region sizes are the next two parameters of the constructor. We must also ensure that the MSER is stable (the fourth parameter), that is, the relative variation of its shape is small enough. Stable regions can be included in the larger regions (called parent regions).</p><p class="calibre8">To be valid, a parent MSER must be sufficiently different from its child; this is the diversity criterion, and it is specified by the fifth parameter of the <code class="literal">cv::MSER</code> constructor. In the example used in the previous section, the default values for these last two parameters were used. (The default values are <code class="literal">0.25</code> for the maximum allowable variation of a MSER and <code class="literal">0.2</code> for the minimum diversity of a parent MSER). As you see, the detection of MSERs requires the specification of several parameters which can make it difficult to work well in various contexts.</p><p class="calibre8">The first output of the MSER detector is a vector of point sets; each of these point sets constitutes a region. Since we are generally more interested in a region as a whole rather than its individual pixel locations, it is common to represent a MSER by a simple geometrical shape that enclosed the detected region. The second output of the detection is therefore a list of bounding boxes. We can therefore show the result of the detection by drawing all these rectangular bounding boxes. However, this may represent a large number of rectangles to be drawn which would make the results difficult to visualize (remember that we also have regions inside regions which makes the representation even more cluttered). In the case of our example, let's assume we are mainly interested in detecting the building's windows. We will therefore extract all regions that have an upright rectangular shape. This could be done by comparing the area of each bounding box with the area of the corresponding detected region. If both have the same value (here, we check if the ratio of these two areas is greater than <code class="literal">0.6</code>), then we accept this MSER. The following code implements this test:</p><pre class="programlisting">    // Extract and display the rectangular MSERs 
    std::vector&lt;cv::Rect&gt;::iterator itr = rects.begin(); 
    std::vector&lt;std::vector&lt;cv::Point&gt; &gt;::iterator itp = points.begin(); 
    for (; itr != rects.end(); ++itr, ++itp) { 
      // ratio test 
      if (static_cast&lt;double&gt;(itp-&gt;size())/itr-&gt;area() &gt; 0.6) 
        cv::rectangle(image, *itr, cv::Scalar(255), 2); 
    } 
</pre><p class="calibre8">The extracted MSERs are then as follows:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_05_037.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Other criteria and representation can also be adopted depending on the application. The following code tests if the detected region is not too elongated (based on the aspect ratio of its rotated bounding rectangle) and then displays them using properly oriented bounding ellipses.</p><pre class="programlisting">    // Extract and display the elliptic MSERs 
    for (std::vector&lt;std::vector&lt;cv::Point&gt; &gt;::iterator  
              it = points.begin(); 
              it != points.end(); ++it) { 
       // for each point in MSER set 
       for (std::vector&lt;cv::Point&gt;::iterator itPts = it-&gt;begin(); 
              itPts != it-&gt;end(); ++itPts) { 
 
           // Extract bouding rectangles 
          cv::RotatedRect rr = cv::minAreaRect(*it); 
          // check ellipse elongation 
          if (rr.size.height / rr.size.height &gt; 0.6 ||  
              rr.size.height / rr.size.height &lt; 1.6) 
              cv::ellipse(image, rr, cv::Scalar(255), 2); 
      } 
    }   
</pre><p class="calibre8">The result is the following image:</p><p class="calibre8">
</p><div><img alt="How it works..." src="img/image_05_038.jpg" class="calibre17"/></div><p class="calibre8">
</p><p class="calibre8">Note how the child and parent MSER are often represented by very similar ellipses. In some cases, it would then be interesting to apply a minimum variation criterion on these ellipses in order to eliminate these repeated representations.</p></div><div><div><div><div><h2 class="title2"><a id="ch05lvl2sec119" class="calibre6"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The <em class="calibre16">Computing components' shape descriptors</em> recipe in <a href="ch07.html" title="Chapter 7. Extracting Lines, Contours, and Components">
Chapter 7
</a>, <em class="calibre16">Extracting Lines, Contours, and Components</em>, will show you how to compute other properties of connected point sets</li><li class="listitem"><a href="ch08.html" title="Chapter 8. Detecting Interest Points">
Chapter 8
</a>, <em class="calibre16">Detecting Interest Points</em>, will explain how to use MSER as an interest point detector</li></ul></div></div></div></body></html>