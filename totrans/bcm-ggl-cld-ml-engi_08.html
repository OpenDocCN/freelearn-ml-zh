<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer078">
<h1 class="chapter-number" id="_idParaDest-158"><a id="_idTextAnchor159"/>8</h1>
<h1 id="_idParaDest-159"><a id="_idTextAnchor160"/>Discovering Google Cloud ML API</h1>
<p><strong class="bold">Application programming interfaces</strong> (<strong class="bold">APIs</strong>) allow one computer program to make its data and <a id="_idIndexMarker471"/>functionality available for other programs to use. In other words, through the APIs of a program or a service, users can send a request to and get a response back from the program/service. ML APIs are cloud-based AI services that help you build ML intelligence into your applications, by leveraging APIs <a id="_idIndexMarker472"/>from the <strong class="bold">Cloud Service Provider</strong> (<strong class="bold">CSP</strong>). They are available as REST APIs, client library SDKs, and user interfaces. </p>
<p>Google Cloud ML APIs provide an interface to leverage Google’s ML services in the cloud. In this chapter, we will discuss the Google Cloud ML API spectrum, which includes the following cloud services:</p>
<ul>
<li>Google Cloud sight APIs, including the Cloud Vision API and Cloud Video API</li>
<li>Google Cloud <a id="_idIndexMarker473"/>language APIs, including <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) and Translation </li>
<li>Google Cloud conversation APIs, including Speech-to-Text, Text-to-Speech, and Dialogflow</li>
</ul>
<p>Let’s start with Google’s Cloud Sight API.</p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor161"/>Google Cloud Sight API</h1>
<p>The <strong class="bold">Google Cloud Sight API</strong> offer <a id="_idIndexMarker474"/>powerful Google pre-trained machine learning models for vision processing and video processing. We will examine the concepts for both the Cloud Vision API and the Cloud Video API.</p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor162"/>The Cloud Vision API</h2>
<p>The <strong class="bold">Cloud Vision API</strong> is a <a id="_idIndexMarker475"/>tool to decipher images through Google’s pre-trained advanced ML models. It can interpret images and classify them into lots of categories. It can extract and detect text, whether the text is within pictures or document photos.</p>
<p>Google Cloud Vision allows developers to easily integrate vision detection features within applications, including image labeling, landmark detection, logo detection, and content detection:</p>
<ul>
<li>Image label <a id="_idIndexMarker476"/>detection can detect, identify, and label objects, locations, activities, animal species, products, and many other things that exist within an image.</li>
<li>Landmark <a id="_idIndexMarker477"/>detection can identify landmarks, such as popular landmarks and natural or man-made structures, within an image. It provides the name of the landmark, the bounding polygon vertices, and the location of the landmark (latitude and longitude).</li>
<li>Logo detection <a id="_idIndexMarker478"/>detects popular product logos, such as popular company logos, product logos, and so on, within an image. </li>
<li>Content detection <a id="_idIndexMarker479"/>detects explicit content, such as adult content, violent content, racy content, and so on, within an image. It provides a likelihood that such content is present within the image.</li>
</ul>
<p>Some of the use cases for the Google Cloud Vision API are listed as follows:</p>
<ul>
<li><strong class="bold">Text digitization</strong>: The Cloud Vision API can detect text in images, especially in scanned <a id="_idIndexMarker480"/>images. The best part of it is that you can customize your own models and train them for a specific scenario on top of the core built-in feature. For example, you can put specific doctors’ prescriptions in a customized model and train it on top of the models that Google already provides, to make it more robust.</li>
<li><strong class="bold">Security and surveillance</strong>: Google <a id="_idIndexMarker481"/>Cloud Vision provides very precise facial recognition, facial comparison, and facial tracking. </li>
<li><strong class="bold">Brand research with the Logo API</strong>: Google Cloud Vision offers a separate logo <a id="_idIndexMarker482"/>detection API where you can look up a logo in banner ads to tell the brand name.</li>
<li><strong class="bold">In-store sentiment analysis</strong>: Real-time marketing and customer support can leap forward if <a id="_idIndexMarker483"/>we detect the opinions and emotions of a customer who’s inside a store in real time. Google Cloud Vision catches customers’ sentiments through the facial expressions of a user in a store.</li>
<li><strong class="bold">Robotics</strong>: Enabling <a id="_idIndexMarker484"/>robots to understand their environment is a huge challenge and requires precise object detection by Google Vision APIs.</li>
</ul>
<p>One of the <a id="_idIndexMarker485"/>major areas in Cloud Vision is Google’s Cloud Vision <strong class="bold">optical character recognition</strong> (<strong class="bold">OCR</strong>), a method of converting handwritten texts or printed texts into machine-encoded text. With OCR, the Google Vision API can identify and extract text from images, with two annotations: <strong class="bold">Text Detection</strong> identifies <a id="_idIndexMarker486"/>and extracts text from images, and <strong class="bold">Document Text Detection</strong> extracts <a id="_idIndexMarker487"/>text from images with a format that is optimized for dense text and documents. The JSON extraction response from Text Detection contains the extracted text together with all the individual words that occurred in that text, and the JSON extraction response from Document Text Detection includes page information, blocks, paragraphs, and words, as well as page break information. Google Cloud Vision OCR has the following benefits:</p>
<ul>
<li><strong class="bold">Multi-language support</strong>: Google <a id="_idIndexMarker488"/>Cloud Vision OCR supports many languages. </li>
<li><strong class="bold">Ease of use</strong>: The <a id="_idIndexMarker489"/>OCR model itself is part of the built-in Google Vision <strong class="source-inline">librarysur</strong>. and can be used very easily.</li>
<li><strong class="bold">Fast speed</strong>: The <a id="_idIndexMarker490"/>Google Cloud Storage platform integrates with the OCR API service. Utilizing GCS, the OCR API can be very fast.</li>
<li><strong class="bold">Scalability</strong>: It can <a id="_idIndexMarker491"/>scale, and Google’s OCR pricing strategy encourages users to scale up the usage of the API, as more usage leads to a cheaper average price.</li>
</ul>
<p>With Google’s pre-trained models, the Cloud Vision API provides many ML features that enable <a id="_idIndexMarker492"/>developers to integrate them into various application developments. OCR has many business use cases: banks use OCR to compare statements, hospitals use OCR to convert handwritten forms to standard-text-filled forms, and governments use OCR for survey feedback collections, among others.</p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor163"/>The Cloud Video API</h2>
<p>The <strong class="bold">Google Cloud Video API</strong> enables powerful content discovery within videos. It supports <a id="_idIndexMarker493"/>common video formats, including <strong class="source-inline">.MOVIE</strong>, <strong class="source-inline">.MPEG4</strong>, <strong class="source-inline">.MP4</strong>, and <strong class="source-inline">.AVI</strong>. The Google Cloud Video API provides the following features:</p>
<ul>
<li><strong class="bold">Label detection</strong>: It <a id="_idIndexMarker494"/>can detect the entities within a video, and provide a list of video segment annotations, a list of frame annotations, or a list of shot annotations upon request.</li>
<li><strong class="bold">Shot change detection</strong>: It can annotate a video by a shot or scene, and label objects <a id="_idIndexMarker495"/>that are relative to the video scene.</li>
<li><strong class="bold">Text detection</strong>: It can <a id="_idIndexMarker496"/>provide the actual text as well as the location of the text within the video.</li>
<li><strong class="bold">Explicit content detection</strong>: It can <a id="_idIndexMarker497"/>annotate explicit content and place a timestamp within the video.</li>
<li><strong class="bold">Object tracking</strong>: It can <a id="_idIndexMarker498"/>track multiple objects within a video and provide the location of each object within the various frames.</li>
<li><strong class="bold">Speech transcription</strong>: It can <a id="_idIndexMarker499"/>capture the spoken words/sentences within a video and transcribe them into text.</li>
</ul>
<p>Through the <a id="_idIndexMarker500"/>Google Cloud Video API, developers can annotate videos stored locally or in Cloud Storage, or live-streamed, with contextual information at the level of the entire video, per segment, per shot, and per frame, and develop related intelligent applications. </p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor164"/>The Google Cloud Language API</h1>
<p>Google’s <strong class="bold">Cloud Language API</strong> includes language translation and NLP. The Cloud Translation API allows you to detect a language and translate it to another one, thus you only need to specify a target language since it detects the source language automatically. NLP allows you to uncover the structure and the meaning of the input text. It provides an interface for developers or computer programs to send requests and get responses. </p>
<p>Google’s NLP API has several methods to perform text analysis and annotations:</p>
<ul>
<li><strong class="bold">Sentiment analysis</strong>: Inspects the given text and attempts to determine the overall <a id="_idIndexMarker501"/>attitude expressed within the text, such as positive, negative, or neutral. Sentiment analysis is performed through the <strong class="source-inline">analyzeSentiment</strong> method. The sentiment analysis response fields consist of the following:<ul><li><strong class="bold">Document sentiment</strong> contains the overall sentiment of the document, which <a id="_idIndexMarker502"/>consists of a score of the sentiment ranging between <strong class="source-inline">-1.0</strong> (negative) and <strong class="source-inline">1.0</strong> (positive), based on the overall emotional leaning of the text, and a magnitude of the sentiment, which indicates the overall strength of emotion (both positive and negative) within the given text, between <strong class="source-inline">0.0</strong> and <strong class="source-inline">+inf</strong>. The score of a document’s sentiment indicates the overall emotion of a document. The magnitude of a document’s sentiment indicates how much emotional content is present within the document.</li>
<li><strong class="bold">Language</strong> contains <a id="_idIndexMarker503"/>the language of the document, either passed in the initial request or automatically detected if absent.</li>
<li><strong class="bold">Sentences</strong> contain a list of the sentences extracted from the original document, and <a id="_idIndexMarker504"/>the sentence-level sentiment values attached to each sentence, which contain score and magnitude values as described previously.</li>
</ul></li>
</ul>
<p>Sentiment <a id="_idIndexMarker505"/>analysis indicates the differences between positive and negative emotions in a document but it does not identify the specific positive and negative emotions. For example, when detecting something that is considered <em class="italic">angry</em>, or text that is considered <em class="italic">sad</em>, the analysis response only indicates that the sentiment is negative, not <em class="italic">sad</em> or <em class="italic">angry</em>. </p>
<ul>
<li><strong class="bold">Entity analysis</strong>: Inspects the given text for known entities, such as proper nouns <a id="_idIndexMarker506"/>that map to unique entities (specific people, places, and so on) or common nouns (also called <strong class="bold">nominals</strong>, such as <a id="_idIndexMarker507"/>restaurants, stadiums, and so on). Entity analysis returns a set of detected entities and parameters associated with those entities, such as the entity’s type, the relevance of the entity to the overall text, and locations in the text that refer to the same entity. Entities are returned in the order (highest to lowest) of their salience scores, which reflect their relevance to the overall text.</li>
<li><strong class="bold">Entity sentiment analysis</strong>: Combines both entity analysis and sentiment analysis <a id="_idIndexMarker508"/>and attempts to determine the sentiment (positive or negative) expressed about entities within the text. Entity sentiment is represented by numerical score and magnitude values for each detected entity. Those scores are then aggregated into an overall sentiment score and magnitude for an entity.</li>
<li><strong class="bold">Syntactic analysis</strong>: Extracts linguistic information, breaks up the given text into a <a id="_idIndexMarker509"/>series of sentences and tokens, and provides further analysis on those tokens. Syntactic analysis is performed with the <strong class="source-inline">analyzeSyntax</strong> method. Syntactic analysis consists of the following <a id="_idIndexMarker510"/>operations:<ul><li>Sentence extraction breaks up the stream of text into a series of sentences.</li>
<li>Tokenization breaks the stream of text up into a series of tokens, with each token usually corresponding to a single word.</li>
<li>The Natural Language API then processes the tokens and, using their locations within sentences, adds syntactic information to the tokens.</li>
</ul></li>
<li><strong class="bold">Content classification</strong>: Analyzes the input text content and returns a content category <a id="_idIndexMarker511"/>for the content. Content classification is performed by using the <strong class="source-inline">classifyText</strong> method. For an input text, Google’s Natural Language API filters the categories returned by the <strong class="source-inline">classifyText</strong> method to include only the most relevant categories. </li>
</ul>
<p>In summary, with sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis, Google’s pre-trained models of the Natural Language API can help developers to apply natural language understanding to their applications and solve many business use cases.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor165"/>The Google Cloud Conversation API</h1>
<p>The <strong class="bold">Google Cloud Conversation API</strong> provides a way to have interactive natural language AI conversations. It has three aspects: Cloud Dialogflow, Cloud Text-to-Speech, and Cloud Speech-to-Text:</p>
<ul>
<li><strong class="bold">Dialogflow</strong> provides <a id="_idIndexMarker512"/>a natural <a id="_idIndexMarker513"/>language understanding platform where you can design and integrate a conversational user interface.</li>
<li><strong class="bold">Text-to-Speech</strong> converts <a id="_idIndexMarker514"/>text input <a id="_idIndexMarker515"/>into audio data of natural human speech.</li>
<li><strong class="bold">Speech-to-Text</strong> converts <a id="_idIndexMarker516"/>audio or <a id="_idIndexMarker517"/>speech inputs to texts.</li>
</ul>
<p>Google Cloud Dialogflow allows you to build virtual agents and chatbots. Dialogflow analyzes text or audio inputs and responds using text or speech.</p>
<p>Intents are at the core of Dialogflow. An intent <a id="_idIndexMarker518"/>categorizes an end user’s intention. When a user adds an inputs, Dialogflow does intent classification by using training phrases – examples of what the end users may actually say – to train the model, and the training results are the mapping of the phrases to intents. Using user intents, a typical workflow for Dialogflow is as follows:</p>
<ol>
<li>An end user <a id="_idIndexMarker519"/>will provide an input phrase, which will be sent to some type of agent. The agent performs intent classification and maps the input phrase into an intent.</li>
<li>From the intent, it will extract the relevant parameters and link actions to intents, such as retrieving a bank account balance.</li>
<li>The parameters and actions will be sent to create a response.</li>
<li>The response is speech or text that is returned to the end user.</li>
</ol>
<p>Google Cloud Speech-to-Text enables the integration of Google speech recognition technologies into application development using Google’s advanced AI technologies. There are three main methods in Speech-to-Text:</p>
<ul>
<li><strong class="bold">Synchronous recognition</strong>: When the Speech-to-Text API receives speech audio <a id="_idIndexMarker520"/>data, it performs data recognition/processing <a id="_idIndexMarker521"/>and returns results after all audio data has been processed before it processes the next request. </li>
<li><strong class="bold">Asynchronous recognition</strong>: When the Speech-to-Text API receives audio data <a id="_idIndexMarker522"/>input, it initiates a long-running <a id="_idIndexMarker523"/>operation, which periodically polls for recognition results. </li>
<li><strong class="bold">Streaming recognition</strong>: Real-time <a id="_idIndexMarker524"/>audio recognition, such <a id="_idIndexMarker525"/>as capturing live audio from a microphone. Streaming recognition provides interim results while audio is being captured, allowing results to appear while a user is still speaking.</li>
</ul>
<p>Speech-to-Text <a id="_idIndexMarker526"/>can use one of several machine learning models to transcribe your audio file. Google has trained these speech recognition models for specific audio types and sources. When you send an audio transcription request to Speech-to-Text, you can improve the results that you receive by specifying the source of the original audio. This allows the Speech-to-Text API to process your audio files using a machine learning model trained to recognize speech audio from that particular type of source.</p>
<p>The Google Cloud <a id="_idIndexMarker527"/>Text-to-Speech API allows you to convert text to human-like speech using WaveNet voices, which synthesize speech with more human-like emphasis and inflection on syllables, phonemes, and words. WaveNet produces speech audio that people prefer over other text-to-speech technologies.</p>
<p>Text-to-Speech uses a synthetic voice to create audio from the text that is presented to it. It creates natural-sounding human speech as playable audio. In addition to normal text, the Cloud <a id="_idIndexMarker528"/>Text-to-Speech API can convert <strong class="bold">Speech Synthesis Markup Language</strong> (<strong class="bold">SSML</strong>) to audio. SSML allows you to control the way in which text is converted to speech. </p>
<h1 id="_idParaDest-165"><a id="_idTextAnchor166"/>Summary</h1>
<p>In this chapter, we have introduced the Google Cloud ML API services: the Cloud Sight API with Vision and Video APIs, the Cloud Language API with the Translation and NPL APIs, and the Cloud conversation API with Dialogflow, Speech-to-Text and Text-to-Speech. These Google pre-trained API services provide the best functions and interfaces for ML application development, and we have shown some sample business use cases leveraging Google ML APIs.</p>
<p>To master the Google ML API services, labs are an important part of the learning process, and we have provided some ML API hands-on demonstrations in <a href="B18333_15.xhtml#_idTextAnchor233"><em class="italic">Appendix 5</em></a>, <em class="italic">Practicing with the Google Cloud ML API</em>. Please review and understand all the practice steps.</p>
<p>In the next chapter, we will discuss the best practices in implementing ML in Google Cloud. </p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor167"/>Further reading </h1>
<p>For further insights into the learnings of this chapter, you can refer to the following links: </p>
<ul>
<li><em class="italic">Vision AI | Derive Image Insights via ML  |  Cloud Vision API</em>:</li>
</ul>
<p><a href="https://cloud.google.com/vision/">https://cloud.google.com/vision/</a></p>
<ul>
<li><em class="italic">Video AI - Video Content Analysis  |  Cloud Video Intelligence API</em>: </li>
</ul>
<p><a href="https://cloud.google.com/video-intelligence/">https://cloud.google.com/video-intelligence/</a></p>
<ul>
<li><em class="italic">Speech-to-Text: Automatic Speech Recognition</em>:<span class="hidden"> </span></li>
</ul>
<p><a href="https://cloud.google.com/speech-to-text">https://cloud.google.com/speech-to-text</a></p>
<ul>
<li><em class="italic">Text-to-Speech: Lifelike Speech Synthesis</em>: </li>
</ul>
<p><a href="https://cloud.google.com/text-to-speech">https://cloud.google.com/text-to-speech</a></p>
<ul>
<li><em class="italic">Dialogflow</em>: </li>
</ul>
<p><a href="https://cloud.google.com/dialogflow/">https://cloud.google.com/dialogflow/</a></p>
<ul>
<li><a href="B18333_15.xhtml#_idTextAnchor233"><em class="italic">Appendix 5</em></a>, <em class="italic">Practicing with the Google Cloud ML API</em></li>
</ul>
</div>
<div>
<div id="_idContainer079">
</div>
</div>
</div>
</body></html>