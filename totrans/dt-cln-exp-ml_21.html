<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer269">
<h1 id="_idParaDest-174"><em class="italic"><a id="_idTextAnchor177"/>Chapter 16</em>: K-Means and DBSCAN Clustering</h1>
<p>Data clustering allows us to organize unlabeled data into groups of observations with more in common with other members of the group than with observations outside of the group. There are a surprisingly large number of applications for clustering, either as the final model of a machine learning pipeline or as input for another model. This includes market research, image processing, and document classification. We sometimes also use clustering to improve exploratory data analysis or to create more meaningful visualizations.</p>
<p>K-means <a id="_idIndexMarker1137"/>and <strong class="bold">density-based spatial clustering of applications with noise</strong> (<strong class="bold">DBSCAN</strong>) clustering, like <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), are <a id="_idIndexMarker1138"/>unsupervised learning algorithms. There are no labels to use as the basis for predictions. The purpose of the algorithm is to identify instances that hang together based on their features. Instances that are in close proximity to each other, and further away from other instances, can be considered to be in a cluster. There are a number of ways to gauge<a id="_idIndexMarker1139"/> proximity. <strong class="bold">Partition-based clustering</strong>, such<a id="_idIndexMarker1140"/> as k-means, and <strong class="bold">density-based clustering</strong>, such as DBSCAN, are two of the more popular approaches. We will explore those approaches in this chapter.</p>
<p>Specifically, we will go over the following topics:</p>
<ul>
<li>The key concepts of k-means and DBSCAN clustering</li>
<li>Implementing k-means clustering</li>
<li>Implementing DBSCAN cluster<a id="_idTextAnchor178"/>ing</li>
</ul>
<h1 id="_idParaDest-175"><a id="_idTextAnchor179"/>Technical requirements</h1>
<p>We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this chapter. </p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor180"/>The key concepts of k-means and DBSCAN clustering</h1>
<p>With k-means <a id="_idIndexMarker1141"/>clustering, we<a id="_idIndexMarker1142"/> identify <em class="italic">k</em> clusters, each with a <a id="_idIndexMarker1143"/>center, or <strong class="bold">centroid</strong>. The centroid is the point that minimizes the total squared distance between it and the other data points in the cluster.</p>
<p>An example with made-up data should help here. The data points in <em class="italic">Figure 16.1</em> seem to be in three clusters. (It is not usually that easy to visualize the number of clusters, <em class="italic">k</em>.)</p>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="Figure 16.1 – Data points with three discernible clusters " height="550" src="image/B17978_16_001.jpg" width="816"/>
</div>
</div>
<p class="figure-caption">Figure 16.1 – Data points with three discernible clusters</p>
<p>We perform the following steps to construct the clusters:</p>
<ol>
<li>Assign a random point as the center of each cluster.</li>
<li>Calculate the distance of each point from the center of each cluster.</li>
<li>Assign data points to a cluster based on their proximity to the center point. These first three steps are illustrated in <em class="italic">Figure 16.2</em>. The points with an <strong class="bold">X</strong> are the randomly chosen cluster centers (with <em class="italic">k</em> set at 3). Data points that are closer to the cluster center point than to other cluster center points get assigned to that cluster.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 16.2 – Random points assigned as the center of the cluster " height="547" src="image/B17978_16_002.jpg" width="814"/>
</div>
</div>
<p class="figure-caption">Figure 16.2 – Random points assigned as the center of the cluster</p>
<ol>
<li value="4">Calculate <a id="_idIndexMarker1144"/>a new <a id="_idIndexMarker1145"/>center point for the new cluster. This is illustrated in <em class="italic">Figure 16.3</em>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="Figure 16.3 – New cluster centers calculated " height="548" src="image/B17978_16_003.jpg" width="817"/>
</div>
</div>
<p class="figure-caption">Figure 16.3 – New cluster centers calculated</p>
<ol>
<li value="5">Repeat steps 2 through 4 until there is not much change in the centers.</li>
</ol>
<p>K-means clustering is a very popular algorithm for clustering for several reasons. It is quite intuitive<a id="_idIndexMarker1146"/> and<a id="_idIndexMarker1147"/> typically quite fast. It does have some disadvantages, however. It processes every data point as part of a cluster, so the clusters can be yanked around by extreme values. It also assumes clusters will have spherical shapes.</p>
<p>The evaluation of unsupervised models is less clear than with supervised models, as we do not have a target with which to compare our predictions. A fairly common metric for clustering models is<a id="_idIndexMarker1148"/> the <strong class="bold">silhouette score</strong>. The silhouette score is the mean silhouette coefficient for all <a id="_idIndexMarker1149"/>instances. The <strong class="bold">silhouette coefficient</strong> is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="" height="124" src="image/B17978_16_0011.jpg" width="371"/>
</div>
</div>
<p>Here, <img alt="" height="49" src="image/B17978_16_002.png" width="38"/> is the mean distance to all instances of the next closest cluster for the ith instance, and <img alt="" height="37" src="image/B17978_16_003.png" width="40"/> is the mean distance to the instances of the assigned cluster. This coefficient ranges from -1 to 1, with scores near 1 meaning that the instance is well within the assigned cluster.</p>
<p>Another metric to evaluate our clusters is <a id="_idIndexMarker1150"/>the <strong class="bold">inertia score</strong>. This is the sum of squared distances between each instance and its centroid. This distance will decrease as we increase the number of clusters but there are eventually diminishing marginal returns from increasing the number of clusters. The change in inertia score with <em class="italic">k</em> is often visualized <a id="_idIndexMarker1151"/>using an <strong class="bold">elbow plot</strong>. This plot is called an elbow plot because the slope gets much closer to 0 as we increase <em class="italic">k</em>, so close that it resembles an elbow. This is shown in <em class="italic">Figure 16.4</em>. In this case, we<a id="_idIndexMarker1152"/> would choose a value of <em class="italic">k</em> near the<a id="_idIndexMarker1153"/> elbow.</p>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="Figure 16.4 – An elbow plot with inertia and k " height="634" src="image/B17978_16_004.jpg" width="801"/>
</div>
</div>
<p class="figure-caption">Figure 16.4 – An elbow plot with inertia and k</p>
<p>Another metric often used when evaluating a clustering <a id="_idIndexMarker1154"/>model is the <strong class="bold">Rand index</strong>. The Rand index tells us how frequently two clusterings have assigned the same cluster to instances. Values for the Rand index will range between 0 and 1. We typically use an adjusted Rand index, which corrects for chance in the similarity calculation. Values for the adjusted Rand index can sometimes be negative.</p>
<p><strong class="bold">DBSCAN</strong> takes a different approach to clustering. For each instance, it counts the number of instances within a specified distance of that instance. All instances within ɛ of an instance are said to be in that instance’s ɛ-neighborhood. When the number of instances in an ɛ-neighborhood equals or exceeds the minimum samples value that we specify, that instance is considered a core instance and the ɛ-neighborhood is considered a cluster. Any instance that is more than ɛ from another instance is considered noise. This is<a id="_idIndexMarker1155"/> illustrated<a id="_idIndexMarker1156"/> in <em class="italic">Figure 16.5</em>.</p>
<div>
<div class="IMG---Figure" id="_idContainer262">
<img alt="Figure 16.5 – DBSCAN clustering with minimum samples = five " height="604" src="image/B17978_16_005.jpg" width="778"/>
</div>
</div>
<p class="figure-caption">Figure 16.5 – DBSCAN clustering with minimum samples = five</p>
<p>There are several advantages of this density-based approach. The clusters do not need to be spherical. They can take any shape. We do not need to guess at the number of clusters, though we do need to provide a value for ɛ. Outliers are just interpreted as noise and so do not impact the clusters. (This last point hints at another useful application of DBSCAN: identifying anomalies.)</p>
<p>We will use DBSCAN<a id="_idIndexMarker1157"/> for<a id="_idIndexMarker1158"/> clustering later in this chapter. First, we will examine how to do clustering with k-means, including how to choose a good value for k.</p>
<h1 id="_idParaDest-177"><a id="_idTextAnchor181"/>Implementing k-means clustering</h1>
<p>We can use k-means <a id="_idIndexMarker1159"/>with some of the same data that we used with the supervised learning models that we developed in earlier chapters. The difference is that there is no longer a target for us to predict. Rather, we are interested in how certain instances hang together. Think of how people arrange themselves in groups during a stereotypical high school lunch break and you kind of get a general idea.</p>
<p>We also need to do much of the same preprocessing work that we did with supervised learning models. We will start with that in this section. We will work with data on income gaps between women and men, labor force participation rates, educational attainment, teenage birth frequency, and female participation in politics at the highest level.</p>
<p class="callout-heading">Note</p>
<p class="callout">The income gap dataset is made available for public use by the <em class="italic">United Nations Development Program</em> at <a href="https://www.kaggle.com/datasets/undp/human-development">https://www.kaggle.com/datasets/undp/human-development</a>. There is one record per country with aggregate employment, income, and education data by gender for 2015.</p>
<p>Let’s build a k-means clustering model:</p>
<ol>
<li value="1">We load the familiar libraries. We also load the <strong class="source-inline">KMeans</strong> and <strong class="source-inline">silhouette_score</strong> modules. Recall that the silhouette score is often used to evaluate how good a job our model has done of clustering. We also load <strong class="source-inline">rand_score</strong>, which will allow us to compute the Rand index of similarity between different clusterings:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.cluster import KMeans</p><p class="source-code">from sklearn.metrics import silhouette_score</p><p class="source-code">from sklearn.metrics.cluster import rand_score</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
<li>Next, we load <a id="_idIndexMarker1160"/>the income gap data:<p class="source-code">un_income_gap = pd.read_csv("data/un_income_gap.csv")</p><p class="source-code">un_income_gap.set_index('country', inplace=True)</p><p class="source-code">un_income_gap['incomeratio'] = \</p><p class="source-code">  un_income_gap.femaleincomepercapita / \</p><p class="source-code">    un_income_gap.maleincomepercapita</p><p class="source-code">un_income_gap['educratio'] = \</p><p class="source-code">  un_income_gap.femaleyearseducation / \</p><p class="source-code">     un_income_gap.maleyearseducation</p><p class="source-code">un_income_gap['laborforcepartratio'] = \</p><p class="source-code">  un_income_gap.femalelaborforceparticipation / \</p><p class="source-code">     un_income_gap.malelaborforceparticipation</p><p class="source-code">un_income_gap['humandevratio'] = \</p><p class="source-code">  un_income_gap.femalehumandevelopment / \</p><p class="source-code">     un_income_gap.malehumandevelopment</p></li>
<li>Let’s look at some descriptive statistics:<p class="source-code">num_cols = ['educratio','laborforcepartratio','humandevratio',</p><p class="source-code">  'genderinequality','maternalmortality','incomeratio',</p><p class="source-code">  'adolescentbirthrate', 'femaleperparliament',</p><p class="source-code">  'incomepercapita']</p><p class="source-code">gap = un_income_gap[num_cols]</p><p class="source-code">gap.agg(['count','min','median','max']).T</p><p class="source-code"><strong class="bold">                     count   min    median  max</strong></p><p class="source-code"><strong class="bold">educratio            170.00  0.24   0.93    1.35</strong></p><p class="source-code"><strong class="bold">laborforcepartratio  177.00  0.19   0.75    1.04</strong></p><p class="source-code"><strong class="bold">humandevratio        161.00  0.60   0.95    1.03</strong></p><p class="source-code"><strong class="bold">genderinequality     155.00  0.02   0.39    0.74</strong></p><p class="source-code"><strong class="bold">maternalmortality    178.00  1.00   64.00   1,100.00</strong></p><p class="source-code"><strong class="bold">incomeratio          177.00  0.16   0.60    0.93</strong></p><p class="source-code"><strong class="bold">adolescentbirthrate  183.00  0.60   40.90   204.80</strong></p><p class="source-code"><strong class="bold">femaleperparliament  185.00  0.00   19.60   57.50</strong></p><p class="source-code"><strong class="bold">incomepercapita      188.00  581.00 10,667.00  23,124.00</strong></p></li>
<li>We should <a id="_idIndexMarker1161"/>also look at some correlations. The education ratio (the ratio of female educational level to male educational level) and the human development ratio are highly correlated, as are gender inequality and the adolescent birth rate, as well as the income ratio and the labor force participation ratio:<p class="source-code">corrmatrix = gap.corr(method="pearson")</p><p class="source-code">sns.heatmap(corrmatrix, </p><p class="source-code">  xticklabels=corrmatrix.columns,</p><p class="source-code">  yticklabels=corrmatrix.columns, cmap="coolwarm")</p><p class="source-code">plt.title('Heat Map of Correlation Matrix')</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer263">
<img alt="Figure 16.6 – A heat map of the correlation matrix " height="463" src="image/B17978_16_006.jpg" width="610"/>
</div>
</div>
<p class="figure-caption">Figure 16.6 – A heat map of the correlation matrix</p>
<ol>
<li value="5">We need to<a id="_idIndexMarker1162"/> scale the data before running our model. We also<a id="_idIndexMarker1163"/> use <strong class="bold">KNN imputation</strong> to handle the missing values:<p class="source-code">pipe1 = make_pipeline(MinMaxScaler(), KNNImputer(n_neighbors=5))</p><p class="source-code">gap_enc = pd.DataFrame(pipe1.fit_transform(gap),</p><p class="source-code">  columns=num_cols, index=gap.index)</p></li>
<li>Now, we are ready to run the k-means clustering. We specify a value for the number of clusters. </li>
</ol>
<p>After fitting the model, we can generate a silhouette score. Our silhouette score is not great. This suggests that our clusters are not very far apart. Later, we will look to see whether we can get a better score with more or fewer clusters:</p>
<p class="source-code">kmeans = KMeans(n_clusters=3, random_state=0)</p>
<p class="source-code">kmeans.fit(gap_enc)</p>
<p class="source-code"><strong class="bold">KMeans(n_clusters=3, random_state=0)</strong></p>
<p class="source-code">silhouette_score(gap_enc, kmeans.labels_)</p>
<p class="source-code"><strong class="bold">0.3311928353317411</strong></p>
<ol>
<li value="7">Let’s take a <a id="_idIndexMarker1164"/>closer look at the clusters. We can use the <strong class="source-inline">labels_</strong> attribute to get the clusters:<p class="source-code">gap_enc['cluster'] = kmeans.labels_</p><p class="source-code">gap_enc.cluster.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0     40</strong></p><p class="source-code"><strong class="bold">1    100</strong></p><p class="source-code"><strong class="bold">2     48</strong></p><p class="source-code"><strong class="bold">Name: cluster, dtype: int64</strong></p></li>
<li>We could have used the <strong class="source-inline">fit_predict</strong> method instead to get the clusters, like so:<p class="source-code">pred = pd.Series(kmeans.fit_predict(gap_enc))</p><p class="source-code">pred.value_counts().sort_index()</p><p class="source-code"><strong class="bold">0     40</strong></p><p class="source-code"><strong class="bold">1    100</strong></p><p class="source-code"><strong class="bold">2     48</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p></li>
<li>It is helpful to examine how the clusters differ in terms of the values of their features. Cluster 0 countries have much higher maternal mortality and adolescent birth rate values than countries in the other clusters. Cluster 1 countries have very low maternal mortality and high income per capita. Cluster 2 countries have very low labor force participation ratios (the ratio of female labor force participation to male labor force participation) and income ratios. Recall that we<a id="_idIndexMarker1165"/> have scaled the data:<p class="source-code">gap_cluster = gap_enc.join(cluster)</p><p class="source-code">gap_cluster[['cluster'] + num_cols].groupby(['cluster']).mean().T</p><p class="source-code"><strong class="bold">cluster              0            1           2</strong></p><p class="source-code"><strong class="bold">educratio            0.36         0.66        </strong><strong class="bold">0.54</strong></p><p class="source-code"><strong class="bold">laborforcepartratio  0.80         0.67        0.32</strong></p><p class="source-code"><strong class="bold">humandevratio        0.62         0.87        0.68</strong></p><p class="source-code"><strong class="bold">genderinequality     0.79         0.32        0.62</strong></p><p class="source-code"><strong class="bold">maternalmortality    </strong><strong class="bold">0.44         0.04        0.11</strong></p><p class="source-code"><strong class="bold">incomeratio          0.71         0.60        0.29</strong></p><p class="source-code"><strong class="bold">adolescentbirthrate  0.51         0.15        0.20</strong></p><p class="source-code"><strong class="bold">femaleperparliament  0.33         </strong><strong class="bold">0.43        0.24</strong></p><p class="source-code"><strong class="bold">incomepercapita      0.02         0.19        0.12</strong></p></li>
<li>We can use the <strong class="source-inline">cluster_centers_</strong> attribute to get the center of each cluster. There are nine values representing the center for each of the three clusters, since we used nine features for the clustering:<p class="source-code">centers = kmeans.cluster_centers_</p><p class="source-code">centers.shape</p><p class="source-code"><strong class="bold">(3, 9)</strong></p><p class="source-code">np.set_printoptions(precision=2)</p><p class="source-code">centers</p><p class="source-code"><strong class="bold">array([[0.36, 0.8 , 0.62, 0.79, 0.44, 0.71, 0.51, 0.33, 0.02],</strong></p><p class="source-code"><strong class="bold">       [0.66, 0.67, 0.87, 0.32, 0.04, 0.6 , 0.15, 0.43, 0.19],</strong></p><p class="source-code"><strong class="bold">       [0.54, 0.32, 0.68, 0.62, 0.11, 0.29, 0.2 , 0.24, 0.12]])</strong></p></li>
<li>We plot the <a id="_idIndexMarker1166"/>clusters by some of their features, as well as the center. We place the number for the cluster at the centroid for that cluster:<p class="source-code">fig = plt.figure()</p><p class="source-code">plt.suptitle("Cluster for each Country")</p><p class="source-code">ax = plt.axes(projection='3d')</p><p class="source-code">ax.set_xlabel("Maternal Mortality")</p><p class="source-code">ax.set_ylabel("Adolescent Birth Rate")</p><p class="source-code">ax.set_zlabel("Income Ratio")</p><p class="source-code">ax.scatter3D(gap_cluster.maternalmortality,</p><p class="source-code">  gap_cluster.adolescentbirthrate,</p><p class="source-code">  gap_cluster.incomeratio, c=gap_cluster.cluster, cmap="brg")</p><p class="source-code">for j in range(3):</p><p class="source-code">  ax.text(centers2[j, num_cols.index('maternalmortality')],</p><p class="source-code">  centers2[j, num_cols.index('adolescentbirthrate')],</p><p class="source-code">  centers2[j, num_cols.index('incomeratio')],</p><p class="source-code">  c='black', s=j, fontsize=20, fontweight=800)</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces<a id="_idIndexMarker1167"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer264">
<img alt="Figure 16.7 – A 3D scatter plot of three clusters  " height="492" src="image/B17978_16_007.jpg" width="510"/>
</div>
</div>
<p class="figure-caption">Figure 16.7 – A 3D scatter plot of three clusters </p>
<p>We can see here that the cluster 0 countries have higher maternal mortality and higher adolescent birth rates. Cluster 0 countries have lower income ratios.</p>
<ol>
<li value="12">So far, we have assumed that the best number of clusters to use for our model is three. Let’s build a five-cluster model and see how those results look.</li>
</ol>
<p>The silhouette score has declined from the three-cluster model. That could be an indicator that <a id="_idIndexMarker1168"/>at least some of the clusters are very close together:</p>
<p class="source-code">gap_enc = gap_enc[num_cols]</p>
<p class="source-code">kmeans2 = KMeans(n_clusters=5, random_state=0)</p>
<p class="source-code">kmeans2.fit(gap_enc)</p>
<p class="source-code">silhouette_score(gap_enc, kmeans2.labels_)</p>
<p class="source-code"><strong class="bold">0.2871811434351394</strong></p>
<p class="source-code">gap_enc['cluster2'] = kmeans2.labels_</p>
<p class="source-code">gap_enc.cluster2.value_counts().sort_index()</p>
<p class="source-code"><strong class="bold">0    21</strong></p>
<p class="source-code"><strong class="bold">1    40</strong></p>
<p class="source-code"><strong class="bold">2    48</strong></p>
<p class="source-code"><strong class="bold">3    16</strong></p>
<p class="source-code"><strong class="bold">4    63</strong></p>
<p class="source-code"><strong class="bold">Name: cluster2, dtype: int64</strong></p>
<ol>
<li value="13">Let’s plot the new clusters to get a better sense of where they are:<p class="source-code">fig = plt.figure()</p><p class="source-code">plt.suptitle("Cluster for each Country")</p><p class="source-code">ax = plt.axes(projection='3d')</p><p class="source-code">ax.set_xlabel("Maternal Mortality")</p><p class="source-code">ax.set_ylabel("Adolescent Birth Rate")</p><p class="source-code">ax.set_zlabel("Income Ratio")</p><p class="source-code">ax.scatter3D(gap_cluster.maternalmortality,</p><p class="source-code">  gap_cluster.adolescentbirthrate,</p><p class="source-code">  gap_cluster.incomeratio, c=gap_cluster.cluster2, </p><p class="source-code">  cmap="brg")</p><p class="source-code">for j in range(5):</p><p class="source-code">  ax.text(centers2[j, num_cols.index('maternalmortality')],</p><p class="source-code">  centers2[j, num_cols.index('adolescentbirthrate')],</p><p class="source-code">  centers2[j, num_cols.index('incomeratio')],</p><p class="source-code">  c='black', s=j, fontsize=20, fontweight=800)</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This<a id="_idIndexMarker1169"/> produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer265">
<img alt="Figure 16.8 – A 3D scatter plot of five clusters " height="444" src="image/B17978_16_008.jpg" width="479"/>
</div>
</div>
<p class="figure-caption">Figure 16.8 – A 3D scatter plot of five clusters</p>
<ol>
<li value="14">We can use a statistic called the Rand index to measure the similarity between the clusters:<p class="source-code">rand_score(kmeans.labels_, kmeans2.labels_)</p><p class="source-code">0.7439412902491751</p></li>
<li>We tried<a id="_idIndexMarker1170"/> three-cluster and five-cluster models, but were either of those a good choice? Let’s look at scores for a range of <em class="italic">k</em> values:<p class="source-code">gap_enc = gap_enc[num_cols]</p><p class="source-code">iner_scores = []</p><p class="source-code">sil_scores = []</p><p class="source-code">for j in range(2,20):</p><p class="source-code">  kmeans=KMeans(n_clusters=j, random_state=0)</p><p class="source-code">  kmeans.fit(gap_enc)</p><p class="source-code">  iner_scores.append(kmeans.inertia_)</p><p class="source-code">  sil_scores.append(silhouette_score(gap_enc,</p><p class="source-code">    kmeans.labels_))</p></li>
<li>Let’s plot the inertia scores with an elbow plot:<p class="source-code">plt.title('Elbow Plot')</p><p class="source-code">plt.xlabel('k')</p><p class="source-code">plt.ylabel('Inertia')</p><p class="source-code">plt.plot(range(2,20),iner_scores)</p></li>
</ol>
<p>This produces <a id="_idIndexMarker1171"/>the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer266">
<img alt="Figure 16.9 – An elbow plot of inertia scores " height="445" src="image/B17978_16_009.jpg" width="559"/>
</div>
</div>
<p class="figure-caption">Figure 16.9 – An elbow plot of inertia scores</p>
<ol>
<li value="17">We also create a plot of the silhouette scores:<p class="source-code">plt.title('Silhouette Score')</p><p class="source-code">plt.xlabel('k')</p><p class="source-code">plt.ylabel('Silhouette Score')</p><p class="source-code">plt.plot(range(2,20),sil_scores)</p></li>
</ol>
<p>This produces<a id="_idIndexMarker1172"/> the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer267">
<img alt="Figure 16.10 – An plot of silhouette scores " height="444" src="image/B17978_16_010.jpg" width="569"/>
</div>
</div>
<p class="figure-caption">Figure 16.10 – An plot of silhouette scores</p>
<p>The elbow plot suggests that a value of <em class="italic">k</em> around 6 or 7 would be best. We start to get diminishing returns in inertia at <em class="italic">k</em> values above that. The silhouette score plot suggests a smaller <em class="italic">k</em>, as there is a sharp decline in silhouette scores after that.</p>
<p>K-means clustering helped us make sense of our data on the gap between women and men in terms of income, education, and employment by country. We can now see how certain features hang together, in a way that the simple correlations we did earlier did not reveal. This largely assumed, however, that our clusters have a spherical shape, and we had to do some work to confirm that our value of <em class="italic">k</em> was the best. We will not have any of the same<a id="_idIndexMarker1173"/> issues with DBSCAN clustering, so we will try that in the next section.</p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor182"/>Implementing DBSCAN clustering</h1>
<p>DBSCAN is a very <a id="_idIndexMarker1174"/>flexible approach to clustering. We just need to specify a value for ɛ, also referred <a id="_idIndexMarker1175"/>to as <strong class="bold">eps</strong>. As we have discussed, the ɛ value determines the size of the ɛ-neighborhood around an instance. The minimum samples hyperparameter indicates how many instances around an instance are needed for it to be considered a core instance.</p>
<p class="callout-heading">Note</p>
<p class="callout">We use DBSCAN to cluster the same income gap data that we worked with in the previous section.</p>
<p>Let’s build a DBSCAN clustering model:</p>
<ol>
<li value="1">We start by loading familiar libraries, plus the <strong class="source-inline">DBSCAN</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.cluster import DBSCAN</p><p class="source-code">from sklearn.impute import KNNImputer</p><p class="source-code">from sklearn.metrics import silhouette_score</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p></li>
<li>We import the code to load and preprocess the wage income data that we worked with in the previous section. Since that code is unchanged, there is no need to repeat it here:<p class="source-code">import incomegap as ig</p><p class="source-code">gap = ig.gap</p><p class="source-code">num_cols = ig.num_cols</p></li>
<li>We are now <a id="_idIndexMarker1176"/>ready to preprocess the data and fit a DBSCAN model . We have chosen an eps value of 0.35 here largely through trial and error. We could have also looped over a range of eps values and compared silhouette score:<p class="source-code">pipe1 = make_pipeline(MinMaxScaler(),</p><p class="source-code">  KNNImputer(n_neighbors=5))</p><p class="source-code">gap_enc = pd.DataFrame(pipe1.fit_transform(gap),</p><p class="source-code">  columns=num_cols, index=gap.index)</p><p class="source-code">dbscan = DBSCAN(eps=0.35, min_samples=5)</p><p class="source-code">dbscan.fit(gap_enc)</p><p class="source-code">silhouette_score(gap_enc, dbscan.labels_)</p><p class="source-code"><strong class="bold">0.31106297603736455</strong></p></li>
<li>We can use the <strong class="source-inline">labels_</strong> attribute to see the clusters. We have 17 noise instances, those with a cluster of -1. The remaining observations are in one of two clusters:<p class="source-code">gap_enc['cluster'] = dbscan.labels_</p><p class="source-code">gap_enc.cluster.value_counts().sort_index()</p><p class="source-code"><strong class="bold">-1     17</strong></p><p class="source-code"><strong class="bold"> 0    139</strong></p><p class="source-code"><strong class="bold"> 1     32</strong></p><p class="source-code"><strong class="bold">Name: cluster, dtype: int64</strong></p><p class="source-code">gap_enc = \</p><p class="source-code"> gap_enc.loc[gap_enc.cluster!=-1]</p></li>
<li>Let’s take a closer look at which features are associated with each cluster. Cluster 1 countries are very different from cluster 0 countries in <strong class="source-inline">maternalmortality</strong>, <strong class="source-inline">adolescentbirthrate</strong>, and <strong class="source-inline">genderinequality</strong>. These were important features with the k-means clustering as well, but there is one fewer cluster with DBSCAN and the overwhelming<a id="_idIndexMarker1177"/> majority of instances fall into one cluster:<p class="source-code">gap_enc[['cluster'] + num_cols].\</p><p class="source-code">  groupby(['cluster']).mean().T</p><p class="source-code"><strong class="bold">cluster                     0            1</strong></p><p class="source-code"><strong class="bold">educratio                   0.63</strong><strong class="bold">         0.35</strong></p><p class="source-code"><strong class="bold">laborforcepartratio         0.57         0.82</strong></p><p class="source-code"><strong class="bold">humandevratio               0.82         0.62</strong></p><p class="source-code"><strong class="bold">genderinequality            0.40         0.79</strong></p><p class="source-code"><strong class="bold">maternalmortality           0.05         0.45</strong></p><p class="source-code"><strong class="bold">incomeratio                 0.51</strong><strong class="bold">         0.71</strong></p><p class="source-code"><strong class="bold">adolescentbirthrate         0.16         0.50</strong></p><p class="source-code"><strong class="bold">femaleperparliament         0.36         0.30</strong></p><p class="source-code"><strong class="bold">incomepercapita             0.16         0.02</strong></p></li>
<li>Let’s visualize the clusters:<p class="source-code">fig = plt.figure()</p><p class="source-code">plt.suptitle("Cluster for each Country")</p><p class="source-code">ax = plt.axes(projection='3d')</p><p class="source-code">ax.set_xlabel("Maternal Mortality")</p><p class="source-code">ax.set_ylabel("Adolescent Birth Rate")</p><p class="source-code">ax.set_zlabel("Gender Inequality")</p><p class="source-code">ax.scatter3D(gap_cluster.maternalmortality,</p><p class="source-code">  gap_cluster.adolescentbirthrate,</p><p class="source-code">  gap_cluster.genderinequality, c=gap_cluster.cluster, </p><p class="source-code">  cmap="brg")</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces <a id="_idIndexMarker1178"/>the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer268">
<img alt="Figure 16.11 – A 3D scatter plot of the cluster for each country " height="438" src="image/B17978_16_011.jpg" width="463"/>
</div>
</div>
<p class="figure-caption">Figure 16.11 – A 3D scatter plot of the cluster for each country</p>
<p>DBSCAN is an excellent tool for clustering, particularly when the characteristics of our data mean that<a id="_idIndexMarker1179"/> k-means clustering is not a good option; for example, when the clusters are not spherical. It also has the advantage of not being influenced by outliers.</p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor183"/>Summary</h1>
<p>We sometimes need to organize our instances into groups with similar characteristics. This can be useful even when there is no target to predict. We can use the clusters created for visualizations, as we did in this chapter. Since the clusters are easy to interpret, we can use them to hypothesize why some features move together. We can also use the clustering results in subsequent analysis.</p>
<p>This chapter explored two popular clustering techniques, k-means and DBSCAN. Both techniques are intuitive, efficient, and handle clustering reliably. </p>
</div>
</div>
</body></html>