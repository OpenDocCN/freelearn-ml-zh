<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning Techniques</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign"><span><span>In this chapter, we will cover the following recipes:</span></span></p>
<ul>
<li><span><span>Weather forecasting with MDP</span></span></li>
<li><span><span>Optimizing a financial portfolio using DP</span></span></li>
<li>Finding the shortest path</li>
<li>Deciding the discount factor using Q-learning</li>
<li>Implementing a deep Q-learning algorithm</li>
<li>Developing an AI-based dynamic modeling system</li>
<li>Deep reinforcement learning with Double Q-learning</li>
<li>Deep Q-Network algorithm with dueling Q-learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span><span>To address the recipes in this chapter, you will need the following files (available on GitHub):</span></span></p>
<ul>
<li><kbd>MarkovChain.py</kbd></li>
<li><kbd>KPDP.py</kbd></li>
<li><kbd>DijkstraNX.py</kbd></li>
<li><kbd>FrozenQlearning.py</kbd></li>
<li><kbd>FrozenDeepQLearning.py</kbd></li>
<li><kbd>dqn_cartpole.py</kbd></li>
<li><kbd>DoubleDQNCartpole.py</kbd></li>
<li><kbd>DuelingDQNCartpole.py</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p><span><span>Reinforcement learning represents a family of algorithms that are able to learn and adapt to environmental changes. It is based on the concept of receiving external stimuli based on the choices of the algorithm. A correct choice will result in a reward, while a wrong choice will lead to a penalty. The goal of the system is to achieve the best possible result.</span></span></p>
<p><span><span>In supervised learning, the correct output is clearly specified (learning with a teacher). But it is not always possible to do so. Often, we only have qualitative information. The information that's available is called a <strong>reinforcement signal</strong>. In these cases, the system does not provide any information on how to update the agent's behavior (for example, weights).</span></span><span><span> You cannot define a cost function or a gradient. The goal of the system is to create the smart agents that are able to learn from their experience.</span></span></p>
<p><span><span>In the following screenshot, we can see a flowchart that displays the reinforcement learning interaction with the environment:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1076 image-border" src="assets/e7ba0357-9922-4ceb-8752-3787f92d9767.png" style="width:25.58em;height:15.00em;"/></p>
<p><span><span>Here are the steps to follow to correctly apply a reinforcement learning algorithm:</span></span></p>
<ol>
<li><span><span>Preparation of the agent</span></span></li>
<li>Observation of the environment</li>
<li>Selection of the optimal strategy</li>
<li>Execution of actions</li>
<li>Calculation of the corresponding reward (or penalty)</li>
<li>Development of updating strategies (if necessary)</li>
<li>Repetition of steps 2 to 5 iteratively until the agent learns the optimal strategies</li>
</ol>
<p><span><span>Reinforcement learning tries to maximize the rewards that are received for the execution of the action or set of actions that allow a goal to be achieved.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weather forecasting with MDP</h1>
                </header>
            
            <article>
                
<p><span><span>To avoid load problems and computational difficulties, the agent-environment interaction is considered a <strong>Markov decision process</strong> (<strong>MDP</strong>). An MDP is a discrete time stochastic control process.</span></span></p>
<p><span><span><strong>Stochastic processes</strong> are mathematical models that are used to study the evolution of phenomena following random or probabilistic laws. It is known that in all natural phenomena, both by their very nature and by observational errors, a random or accidental component is present.</span></span></p>
<p><span><span>This component causes the following: at every instance of <em>t</em>, the result of the observation of the phenomenon is a random number or random variable, <em>st</em>. It is not possible to predict with certainty what the result will be; you can only state that it will take one of several possible values, each of which has a given probability.</span></span></p>
<p><span><span>A stochastic process is called <strong>Markovian</strong> when, having chosen a certain instance of <em>t</em> for observation, the evolution of the process, starting with <em>t</em>, depends only on <em>t</em> and does not depend in any way on the previous instances. Thus, a process is Markovian when, given the moment of observation, only this instance determines the future evolution of the process, while this evolution does not depend on the past.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span><span>In this recipe, w<span>e want to build a statistical model to predict the weather. To simplify the model, we will assume that there are only two states: sunny and rainy. Let's further assume that we have made some calculations and discovered that tomorrow's time is somehow based on today's</span> <span>time.</span></span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform weather forecasting with MDP:</p>
<ol start="1">
<li><span><span>We will use the <kbd>MarkovChain.py</kbd> file that is already provided for you as a reference. To start, we import the <kbd>numpy</kbd>, <kbd>time</kbd>, and <kbd>matplotlib.pyplot</kbd> packages:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>import numpy as np<br/></span></span><span><span>import time<br/></span></span><span><span>from matplotlib import pyplot</span></span></pre>
<ol start="2">
<li><span><span>Let's set the seed of a random number generator and the state of the weather:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>np.random.seed(1)<br/></span></span><span><span>states = ["Sunny","Rainy"]</span></span></pre>
<ol start="3">
<li><span><span>At this point, we have to define the possible transitions of weather conditions:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>TransStates = [["SuSu","SuRa"],["RaRa","RaSu"]]<br/></span></span><span><span>TransnMatrix = [[0.75,0.25],[0.30,0.70]]</span></span></pre>
<ol start="4">
<li><span><span>Then, we insert the following check to verify that we did not make mistakes in defining the transition matrix:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>if sum(TransnMatrix[0])+sum(TransnMatrix[1]) != 2:<br/></span></span><span><span>     print("Warning! Probabilities MUST ADD TO 1. Wrong transition </span></span><span><span>matrix!!")<br/></span></span><span><span>     raise ValueError("Probabilities MUST ADD TO 1")</span></span></pre>
<ol start="5">
<li><span><span>Let's set the initial condition:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>WT = list()<br/></span></span><span><span>NumberDays = 200<br/></span></span><span><span>WeatherToday = states[0]<br/></span></span><span><span>print("Weather initial condition =",WeatherToday)</span></span></pre>
<ol start="6">
<li><span><span>We can now predict the weather conditions for each of the days set by the <kbd>NumberDays</kbd> variable. To do this, we will use a <kbd>while</kbd> loop, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>i = 0<br/>while i &lt; NumberDays:<br/></span></span><span><span>    if WeatherToday == "Sunny":<br/>    </span></span><span><span>TransWeather = np.random.choice(TransStates[0],replace=True,p=TransnMatrix[0])<br/>        </span></span><span><span>if TransWeather == "SuSu":<br/>            </span></span><span><span>pass<br/>        </span></span><span><span>else:<br/>            </span></span><span><span>WeatherToday = "Rainy"<br/></span></span><span><span>    elif WeatherToday == "Rainy":<br/>        </span></span><span><span>TransWeather = np.random.choice(TransStates[1],replace=True,p=TransnMatrix[1])<br/>        </span></span><span><span>if TransWeather == "RaRa":<br/>            </span></span><span><span>pass<br/>        </span></span><span><span>else:<br/>            </span></span><span><span>WeatherToday = "Sunny"<br/>    </span></span><span><span>print(WeatherToday)<br/>    </span></span><span><span>WT.append(WeatherToday)<br/>    </span></span><span><span>i += 1    <br/>    </span></span><span><span>time.sleep(0.2)</span></span></pre>
<p class="mce-root"/>
<p style="padding-left: 60px"><span><span>It consists of a control condition and a loop body. At the entrance of the cycle and every time that all the instructions contained in the body are executed, the validity of the control condition is verified. The cycle ends when the condition, consisting of a Boolean expression, returns <kbd>false</kbd>.</span></span></p>
<ol start="7">
<li><span><span>At this point, we have generated forecasts for the next 200 days. Let's plot the chart using the following code:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>pyplot.plot(WT)<br/></span></span><span><span>pyplot.show()</span></span></pre>
<p style="padding-left: 60px"><span><span>The following graph shows the weather conditions for the next 200 days, starting from the sunny condition:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1077 image-border" src="assets/08afee2a-205a-413c-b485-7eebe23692ea.png" style="width:128.58em;height:49.75em;"/></p>
<p><span><span>At first sight, it seems that sunny days prevail over the rainy ones.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p><span><span>A Markov chain is a mathematical model of a random phenomenon that evolves over time in such a way that the past influences the future only through the present. In other words, a stochastic model describes a sequence of possible events where the probability of each event depends only on the state that was attained in the previous event. So, Markov chains have the property of memorylessness.</span></span></p>
<p><span><span>The structure of a Markov chain is therefore completely represented by the following transition matrix:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2d8d7be0-63a8-4818-ae7b-717417ce5420.png" style="width:15.00em;height:6.58em;"/></p>
<p><span><span>The properties of transition probability matrices derive directly from the nature of the elements that compose them.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span><span>A very intuitive alternative to the description of a Markov chain through a transition matrix is associating an oriented graph (transition diagram) to a Markov chain. The transition matrix and transition diagram provide the same information regarding the same Markov chain.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li><em>INTRODUCTION TO MARKOV MODELS</em> (from Clemson University): <a href="http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf">http://cecas.clemson.edu/~ahoover/ece854/refs/Ramos-Intro-HMM.pdf</a></li>
<li><em>Markov Decision Processes</em> (from Carnegie Mellon University): <a href="http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf">http://egon.cheme.cmu.edu/ewo/docs/SchaeferMDP.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing a financial portfolio using DP</h1>
                </header>
            
            <article>
                
<p><span><span>The management of financial portfolios is an activity that aims to combine financial products in a manner that best represents the investor's needs. This requires an overall assessment of various characteristics, such as risk appetite, expected returns, and investor consumption, as well as an estimate of future returns and risk. <strong>Dynamic programming</strong> (<strong>DP</strong>) represents a set of algorithms that can be used to calculate an optimal policy given a perfect model of the environment in the form of an MDP. The fundamental idea of DP, as well as reinforcement learning in general, is the use of state values and actions to look for good policies.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span><span>In this recipe, we will address the <strong>knapsack problem</strong>: a thief goes into a house and wants to steal valuables. They put them in their knapsack, but they are limited by the weight. Each object has its own value and weight. They must choose the objects that are of value, but that do not have excessive weight. The thief must not exceed the weight limit in the knapsack, but, at the same time, they must optimize their gain.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can optimize a financial portfolio using DP:</p>
<ol start="1">
<li><span><span>We will use the <kbd>KPDP.py</kbd> file that is already provided for you as a reference. This algorithm starts with the definition of a <kbd>KnapSackTable()</kbd> function that will choose the optimal combination of the objects respecting the two constraints imposed by the problem: the total weight of the objects equal to 10, and the maximum value of the chosen objects, as shown in the following code:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>def KnapSackTable(weight, value, P, n):<br/></span></span><span><span>T = [[0 for w in range(P + 1)]<br/></span></span><span><span>for i in range(n + 1)]</span></span></pre>
<ol start="2">
<li><span><span>Then, we set an iterative loop on all objects and on all weight values, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>for i in range(n + 1):<br/></span></span><span><span>    for w in range(P + 1):<br/></span></span><span><span>        if i == 0 or w == 0:<br/></span></span><span><span>            T[i][w] = 0<br/></span></span><span><span>        elif weight[i - 1] &lt;= w:<br/></span></span><span><span>            T[i][w] = max(value[i - 1]<br/></span></span><span><span>                + T[i - 1][w - weight[i - 1]],<br/></span></span><span><span>                        T[i - 1][w])<br/></span></span><span><span>        else:<br/></span></span><span><span>            T[i][w] = T[i - 1][w]</span></span></pre>
<ol start="3">
<li><span><span>Now, we can memorize the result that was obtained, which represents the maximum value of the objects that can be carried in the knapsack, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>res = T[n][P]<br/></span></span><span><span>print("Total value: " ,res)</span></span></pre>
<p class="mce-root"/>
<ol start="4">
<li><span><span>The procedure we've followed so far does not indicate which subset provides the optimal solution. We must extract this information using a set procedure:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>w = P<br/></span></span><span><span>totweight=0<br/></span></span><span><span>for i in range(n, 0, -1):<br/></span></span><span><span>    if res &lt;= 0:<br/></span></span><span><span>        break</span></span></pre>
<ol start="5">
<li><span><span>If the current element is the same as the previous one, we will move on to the next one, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>if res == T[i - 1][w]:<br/>    </span></span><span><span>continue</span></span></pre>
<ol start="6">
<li><span><span>If it is not the same, then the current object will be included in the knapsack, and this item will be printed, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>else:<br/></span></span><span><span>    print("Item selected: ",weight[i - 1],value[i - 1])<br/></span></span><span><span>    totweight += weight[i - 1]<br/></span></span><span><span>    res = res - value[i - 1]<br/></span></span><span><span>    w = w - weight[i – 1]</span></span></pre>
<ol start="7">
<li><span><span>Finally, the total included weight is printed, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>print("Total weight: ",totweight)</span></span></pre>
<p style="padding-left: 60px"><span><span>In this way, we have defined the function that allows us to build the table.</span></span></p>
<ol start="8">
<li><span><span>Now, we have to define the input variables and pass them to the function, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>objects = [(5, 18),(2, 9), (4, 12), (6,25)]<br/></span></span><span><span>print("Items available: ",objects)<br/></span></span><span><span>print("***********************************")</span></span></pre>
<ol start="9">
<li><span><span>At this point, we need to extract the weight and variable values from the objects. We put them in a separate array to better understand the steps, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>value = []<br/></span></span><span><span>weight = []<br/></span></span><span><span>for item in objects:<br/></span></span><span><span>    weight.append(item[0])<br/></span></span><span><span>    value.append(item[1])</span></span></pre>
<p class="mce-root"/>
<ol start="10">
<li><span><span>Finally, the total weight that can be carried by the knapsack and the number of available items is set, as follows:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>P = 10<br/></span></span><span><span>n = len(value)</span></span></pre>
<ol start="11">
<li><span><span>Finally, we print out the results:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>KnapSackTable(weight, value, P, n)<br/></span></span><strong><span><span>The following results are returned:<br/></span></span><span><span>Items available: [(5, 18), (2, 9), (4, 12), (6, 25)]<br/></span></span><span><span>*********************************<br/></span></span><span><span>Total value: 37<br/></span></span><span><span>Item selected: 6 25<br/></span></span><span><span>Item selected: 4 12<br/></span></span><span><span>Total weight: 10</span></span></strong></pre>
<p style="padding-left: 60px"><span><span>The DP algorithm allowed us to obtain the optimal solution, saving on computational costs.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p><span><span>Consider, for example, the problem of finding the best path that joins two locations. The principle of optimality states that each sub path included in it, between any intermediate location and the final location, must in turn be optimal. Based on this principle, DP solves a problem by taking one decision at a time. At every step, the best policy for the future is determined, regardless of the past choices (it is a Markov process), assuming that the latter choices are also optimal.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p><span><span>DP is a technique for solving recursive problems more efficiently. Why is this the case? Oftentimes, in recursive procedures, we solve sub problems repeatedly. In DP, this does not happen: we memorize the solution of these sub problems so that we do not have to solve them again. This is called <strong>memoization</strong>. If the value of a variable at a given step depends on the results of previous calculations, and if the same calculations are repeated over and over, then it is convenient to store the intermediate results so as to avoid repeating computationally expensive calculations.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Refer to <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Refer to <em>Dynamic Programming</em> (from Stanford University): <a href="https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf">https://web.stanford.edu/class/cs97si/04-dynamic-programming.pdf</a></li>
<li>Refer to <em>The Knapsack Problem</em> (from Eindhoven University): <a href="http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf">http://www.es.ele.tue.nl/education/5MC10/Solutions/knapsack.pdf</a></li>
<li>Refer to <em>Memoization</em> (from Radford University): <a href="https://www.radford.edu/~nokie/classes/360/dp-memoized.html">https://www.radford.edu/~nokie/classes/360/dp-memoized.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the shortest path</h1>
                </header>
            
            <article>
                
<p><span><span>Given a weighted graph and a designated vertex <em>X</em>, we will often need to find the path from <em>X</em> to each of the other vertices in the graph. Identifying a path connecting two or more nodes of a graph appears as a sub problem of many other problems of discrete optimization and has, in addition, numerous applications in the real world.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span><span>In this recipe, we will find the shortest path between two points using the <strong>Dijkstra</strong> algorithm. We will also use the <kbd>networkx</kbd> package to represent graphs in Python.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can find the shortest path:</p>
<ol start="1">
<li><span><span>We will use the </span></span><span><kbd>DijkstraNX.py</kbd> </span><span><span>file that is already provided for you as a reference. First, we import the libraries we will use here:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>import networkx as nx<br/></span></span><span><span>import matplotlib.pyplot as plt</span></span></pre>
<p class="mce-root"/>
<ol start="2">
<li><span><span>Then, a graph object is created and the vertices are added:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>G = nx.Graph()<br/></span></span><span><span>G.add_node(1)<br/></span></span><span><span>G.add_node(2)<br/></span></span><span><span>G.add_node(3)<br/></span></span><span><span>G.add_node(4)</span></span></pre>
<ol start="3">
<li><span><span>Subsequently, the weighted edges are added:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>G.add_edge(1, 2, weight=2)<br/></span></span><span><span>G.add_edge(2, 3, weight=2)<br/></span></span><span><span>G.add_edge(3, 4, weight=3)<br/></span></span><span><span>G.add_edge(1, 3, weight=5)<br/></span></span><span><span>G.add_edge(2, 4, weight=6)</span></span></pre>
<ol start="4">
<li><span><span>At this point, we have drawn the graph by adding labels to the edges with the indication of weight:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>pos = nx.spring_layout(G, scale=3)<br/></span></span><span><span>nx.draw(G, pos,with_labels=True, font_weight='bold')<br/></span></span><span><span>edge_labels = nx.get_edge_attributes(G,'r')<br/></span></span><span><span>nx.draw_networkx_edge_labels(G, pos, labels = edge_labels)<br/></span></span><span><span>plt.show()</span></span></pre>
<p style="padding-left: 60px"><span><span><span>To do this, the</span> <kbd><span>draw_networkx_edge_labels</span></kbd> <span>function was used. The following diagram shows the results of this:</span></span></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1078 image-border" src="assets/afea934e-5e66-439a-b69e-b2c5676c5619.png" style="width:124.08em;height:49.58em;"/></p>
<p class="mce-root"/>
<ol start="5">
<li><span><span>Finally, the shortest path from</span></span> one <span><span>to</span></span> four <span><span>nodes has been calculated:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>print(nx.shortest_path(G,1,4,weight='weight'))</span></span></pre>
<ol start="6">
<li class="western"><span><span>The</span></span> <kbd><span><span>shortest_path</span></span></kbd> <span><span>function computes the shortest paths and path lengths between nodes in the graph. The following are the results:</span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span><span>[1, 2, 3, 4]</span></span></strong></pre>
<ol start="7">
<li class="western"><span><span>Finally, the length of the shortest paths has been calculated:</span></span></li>
</ol>
<pre style="padding-left: 60px"><span><span>print(nx.nx.shortest_path_length(G,1,4,weight='weight'))</span></span></pre>
<p style="padding-left: 60px" class="western"><span><span>The following is the result:</span></span></p>
<pre style="padding-left: 60px"><strong><span><span>7</span></span></strong></pre>
<p style="padding-left: 60px" class="western"><span><span>As we can verify, we have obtained the same result.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p><span><span>The Dijkstra algorithm is able to solve the problem of finding the shortest path from the source, <em>s,</em> to all of the nodes. The algorithm maintains a label <em>d(i)</em> to the nodes representing an upper bound on the length of the shortest path of the node i.</span></span></p>
<p><span><span>At each step, the algorithm partitions the nodes in <em>V</em> into two sets: the set of permanently labeled nodes and the set of nodes that are still temporarily labeled. The distance of permanently labeled nodes represents the shortest path distance from the source to these nodes, whereas the temporary labels contain a value that can be greater than or equal to the shortest path length.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p class="western"><span><span>The basic idea of the algorithm is to start from the source and try to permanently label the successor nodes. At the beginning, the algorithm places the value of the source distance to zero and initializes the other distances to an arbitrarily high value (by convention, we will set the initial value of the distances:</span></span> <em><span><span>d[i] = +</span></span> <span><span>∞</span></span><span><span>,</span></span> <span><span>∀</span></span><span><span>i</span></span> <span><span>∈</span></span></em> <span><span><em>V</em>)</span></span><span><span>.</span></span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="western"><span><span>At each iteration, the node label</span></span> <span><span>i</span></span> <span><span>is the value of the minimum distance along a path from the source that contains, apart from</span></span> <span><span>i</span></span><span><span>, only permanently labeled nodes. The algorithm selects the node whose label has the lowest value among those labeled temporarily, labels it permanently, and updates all the labels of the nodes adjacent to it. The algorithm terminates when all the nodes have been permanently labeled.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Check out <em>Solving Shortest Path Problem: Dijkstra's Algorithm</em> (from Illinois University): <a href="http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf">http://www.ifp.illinois.edu/~angelia/ge330fall09_dijkstra_l18.pdf</a></li>
<li>Check out <em>Graph Theory Tutorials</em> (from the University of Tennessee at Martin): <a href="https://primes.utm.edu/graph/index.html">https://primes.utm.edu/graph/index.html</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deciding the discount factor using Q-learning</h1>
                </header>
            
            <article>
                
<p><strong>Q-learning</strong> is one of the most used reinforcement learning algorithms. This is due to its ability to compare the expected utility of the available actions without requiring an environment model. Thanks to this technique, it is possible to find an optimal action for every given state in a finished MDP.</p>
<p>A general solution to the reinforcement learning problem is to estimate, thanks to the learning process, an evaluation function. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a particular policy. In fact, Q-learning tries to maximize the value of the Q function (the action-value function), which represents the maximum discounted future reward when we perform actions, <em>a</em>, in the state, <em>s</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will deal with the problem of controlling a character's movement in a grid world by offering a first solution based on Q-learning.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can decide on the discount factor using Q-learning:</p>
<ol>
<li>We will use the <kbd>FrozenQlearning.py</kbd> file that is already provided for you as reference. Let's start by importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>import numpy as np</pre>
<ol start="2">
<li>Then, we will move on and create the environment by calling the <kbd>make</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make('FrozenLake-v0')</pre>
<p style="padding-left: 60px">This method creates the environment that our agent will run in.</p>
<ol start="3">
<li>Now, let's initialize the parameters, starting with <kbd>QTable</kbd>:</li>
</ol>
<pre style="padding-left: 60px">QTable = np.zeros([env.observation_space.n,env.action_space.n])</pre>
<ol start="4">
<li>Let's define some parameters:</li>
</ol>
<pre style="padding-left: 60px">alpha = .80<br/>gamma = .95<br/>NumEpisodes = 2000</pre>
<p style="padding-left: 60px">Here, <kbd>alpha</kbd> is the learning rate, <kbd>gamma</kbd> is the discount factor, and <kbd>NumEpisodes</kbd> is the number of episodes.</p>
<ol start="4">
<li>Now, we will create a list to contain the total rewards:</li>
</ol>
<pre style="padding-left: 60px">RewardsList = []</pre>
<ol start="5">
<li>At this point, after setting the parameters, it is possible to start the Q-learning cycle:</li>
</ol>
<pre style="padding-left: 60px">for i in range(NumEpisodes):<br/>    CState = env.reset()<br/>    SumReward = 0<br/>    d = False<br/>    j = 0<br/>    while j &lt; 99:<br/>        j+=1<br/>        Action = np.argmax(QTable[CState,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))<br/>        NState,Rewards,d,_ = env.step(Action)<br/>        QTable[CState,Action] = QTable[CState,Action] + alpha*(Rewards + gamma*np.max(QTable[NState,:]) - QTable[CState,Action])<br/>        SumReward += Rewards<br/>        CState = NState<br/>        if d == True:<br/>            break<br/><br/>    RewardsList.append(SumReward)</pre>
<p style="padding-left: 60px">At the end of each episode, the list of rewards is enriched with a new value.</p>
<ol start="6">
<li>Finally, we print the results:</li>
</ol>
<pre style="padding-left: 60px">print ("Score: " +  str(sum(RewardsList)/NumEpisodes))<br/>print ("Final Q-Table Values")<br/>print (QTable)</pre>
<p style="padding-left: 60px">The following screenshot shows the final Q-Table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1079 image-border" src="assets/21b062e8-2b5a-4052-8cf2-9d02539736a6.png" style="width:41.08em;height:25.17em;"/></p>
<p style="padding-left: 60px">To improve the result, the retuning of the configuration parameters is required.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>The <kbd>FrozenLake</kbd> environment is a 4 × 4 grid that contains four possible areas: <strong>Safe</strong> (<strong>S</strong>), <strong>Frozen</strong> (<strong>F</strong>), <strong>Hole</strong> (<strong>H</strong>), and <strong>Goal</strong> (<strong>G</strong>). The agent controls the movement of a character in a grid world, and moves around the grid until it reaches the goal or the hole. Some tiles of the grid are walkable, and others lead to the agent falling into the water. If it falls into the hole, it has to start from the beginning and is rewarded with the value 0. Additionally, the direction in which the agent will move is uncertain and only partially depends on the chosen direction. If the agent finds a walkable path to a goal tile, it is rewarded. The agent has four possible moves: up, down, left, and right. The process continues until it learns from every mistake and reaches the goal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Q-learning estimates the function value <em>q (s, a)</em> incrementally, updating the value of the state-action pair at each step of the environment, following the logic of updating the general formula for estimating the values for the temporal difference methods. Q-learning has off-policy characteristics; that is, while the policy is improved according to the values estimated by <em>q (s, a)</em>, the value function updates the estimates following a strictly greedy secondary policy: given a state, the chosen action is always the one that maximizes the <em>max q (s, a)</em> value. However, the π policy has an important role in estimating values, because through it the state-action pairs to be visited and updated are determined.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Refer to <em>Reinforcement Learning: A Tutorial</em> (from University of Toronto): <a href="http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf">http://www.cs.toronto.edu/~zemel/documents/411/rltutorial.pdf</a></li>
<li>Check out the official site of the <kbd>gym</kbd> library: <a href="https://gym.openai.com/">https://gym.openai.com/</a></li>
<li>Check out the <em>FrozenLake-v0</em> environment: <a href="https://gym.openai.com/envs/FrozenLake-v0/">https://gym.openai.com/envs/FrozenLake-v0/</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the deep Q-learning algorithm</h1>
                </header>
            
            <article>
                
<p><strong>Deep Q-learning</strong> represents an evolution of the basic Q-learning method. The state-action is replaced by a neural network, with the aim of approximating the optimal value function. Compared to the Q-learning approaches, where it was used to structure the network in order to request both input and action and providing its expected return, deep Q-learning revolutionizes the structure to request only the state of the environment and supply as many status-action values as there are actions that can be performed in the environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will use the deep Q-learning approaches to controls a character's movement in a grid world. In this recipe, the <kbd>keras-rl</kbd> library will be used; to learn about it further, refer to the <em>Developing AI-based dynamic modeling system</em> recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can implement a deep Q-learning algorithm:</p>
<ol>
<li>We will use the <kbd>FrozenDeepQLearning.py</kbd> file that is already provided for you as a reference. Let's start by importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">import gym<br/>import numpy as np<br/>from keras.models import Sequential<br/>from keras.layers.core import Dense, Reshape<br/>from keras.layers.embeddings import Embedding<br/>from keras.optimizers import Adam<br/>from rl.agents.dqn import DQNAgent<br/>from rl.policy import BoltzmannQPolicy<br/>from rl.memory import SequentialMemory</pre>
<ol start="2">
<li>Then, we will define the environment and set the seed:</li>
</ol>
<pre style="padding-left: 60px">ENV_NAME = 'FrozenLake-v0'<br/>env = gym.make(ENV_NAME)<br/>np.random.seed(1)<br/>env.seed(1)</pre>
<ol start="3">
<li>Now, we will extract the actions that are available to the agent:</li>
</ol>
<pre style="padding-left: 60px">Actions = env.action_space.n</pre>
<p style="padding-left: 60px">The <kbd>Actions</kbd> variable now contains all the actions that are available in the selected environment. Gym will not always tell you the meaning of those actions, but only about which ones are available.</p>
<ol start="4">
<li>Now, we will build a simple neural network model using the <kbd>keras</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Embedding(16, 4, input_length=1))<br/>model.add(Reshape((4,)))<br/>print(model.summary())</pre>
<p style="padding-left: 60px">Now, the neural network model is ready to use, so let's configure and compile our agent. One problem with using the DQN is that the neural network that was used in the algorithm tends to forget previous experiences because it overwrites them with new experiences.</p>
<ol start="5">
<li>So, we need a list of previous experiences and observations to reform the model with previous experiences. For this reason, a memory variable is defined that will contain the previous experiences, and a policy will be set:</li>
</ol>
<pre style="padding-left: 60px">memory = SequentialMemory(limit=10000, window_length=1)<br/>policy = BoltzmannQPolicy()</pre>
<ol start="6">
<li>We just have to define the agent:</li>
</ol>
<pre style="padding-left: 60px">Dqn = DQNAgent(model=model, nb_actions=Actions,<br/>               memory=memory, nb_steps_warmup=500,<br/>               target_model_update=1e-2, policy=policy,<br/>               enable_double_dqn=False, batch_size=512<br/>               )</pre>
<ol start="7">
<li>Let's proceed to compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">Dqn.compile(Adam())<br/>Dqn.fit(env, nb_steps=1e5, visualize=False, verbose=1, log_interval=10000)</pre>
<ol start="8">
<li>At the end of the training, it is necessary to save the obtained weights:</li>
</ol>
<pre style="padding-left: 60px">Dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)</pre>
<ol start="9">
<li>Finally, we will evaluate our algorithm for 20 episodes:</li>
</ol>
<pre style="padding-left: 60px">Dqn.test(env, nb_episodes=20, visualize=False)</pre>
<p style="padding-left: 60px">Our agent is now able to identify the path that allows them to reach the goal.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>A general solution to the reinforcement learning problem is to estimate, thanks to the learning process, an evaluation function. This function must be able to evaluate, through the sum of the rewards, the convenience or otherwise of a particular policy. In fact, Q-learning tries to maximize the value of the <kbd>Q</kbd> function (action-value function), which represents the maximum discounted future reward when we perform actions, <em>a</em>, in the state, <em>s</em>. DQN represents an evolution of the basic Q-learning method, where the state-action is replaced by a neural network, with the aim of approximating the optimal value function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>OpenAI Gym is a library that helps us implement algorithms based on reinforcement learning. It includes a growing collection of benchmark issues that expose a common interface, and a website where people can share their results and compare algorithm performance.</p>
<p>OpenAI Gym focuses on the episodic setting of reinforced learning. In other words, the agent's experience is divided into a series of episodes. The initial state of the agent is randomly sampled by a distribution, and the interaction proceeds until the environment reaches a terminal state. This procedure is repeated for each episode, with the aim of maximizing the total reward expectation per episode and achieving a high level of performance in the fewest possible episodes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root">Refer to <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Refer to <em>Learning 2048 with Deep Reinforcement Learning</em> (from the University of Waterloo): <a href="https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf">https://cs.uwaterloo.ca/~mli/zalevine-dqn-2048.pdf</a></li>
<li>Refer to <em>Deep RL with Q-Functions</em> (from UC Berkeley): <a href="http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf">http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing an AI-based dynamic modeling system</h1>
                </header>
            
            <article>
                
<p>A <strong>Segway</strong> is a personal transport device that exploits an innovative combination of computer science, electronics, and mechanics. It functions as an extension of the body; as with a partner in a dance, it is able to anticipate every move. The operating principle is based on the <strong>reverse pendulum</strong> system. The reverse pendulum system is an example that's commonly found in textbooks on control and research literature. Its popularity derives in part from the fact that it is unstable without control and has a non-linear dynamic, but, above all, because it has several practical applications, such as controlling a rocket's take-off or a Segway.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will analyze the functioning of a physical system that's made by connecting a rigid rod to a cart, modeling the system using different approaches. The rod is connected through a pivot that's hinged on the carriage and is free to rotate around it. This mechanical system, which is called the reverse pendulum, is a classic problem in control theory.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can develop an AI-based dynamic modeling system:</p>
<ol>
<li>We will use the <kbd>dqn_cartpole.py</kbd> file that is already provided for you as a reference. Let's start by importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import gym<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Flatten<br/>from keras.optimizers import Adam<br/>from rl.agents.dqn import DQNAgent<br/>from rl.policy import BoltzmannQPolicy<br/>from rl.memory import SequentialMemory</pre>
<p class="mce-root"/>
<ol start="2">
<li>Now, we will define and load the environment:</li>
</ol>
<pre style="padding-left: 60px">ENV_NAME = 'CartPole-v0'<br/>env = gym.make(ENV_NAME)</pre>
<ol start="3">
<li>To set the <kbd>seed</kbd> value, the NumPy library's <kbd>random.seed()</kbd> function is used, as follows:</li>
</ol>
<pre style="padding-left: 60px">np.random.seed(123)<br/>env.seed(123)</pre>
<ol start="4">
<li>Now, we will extract the actions that are available to the agent:</li>
</ol>
<pre style="padding-left: 60px">nb_actions = env.action_space.n</pre>
<ol start="5">
<li>We will build a simple neural network model using the <kbd>keras</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Flatten(input_shape=(1,) + env.observation_space.shape))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())</pre>
<ol start="6">
<li>A <kbd>memory</kbd> variable and a <kbd>policy</kbd> will be set:</li>
</ol>
<pre style="padding-left: 60px">memory = SequentialMemory(limit=50000, window_length=1)<br/>policy = BoltzmannQPolicy()</pre>
<ol start="7">
<li>We just have to define the agent:</li>
</ol>
<pre style="padding-left: 60px">dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,<br/>               target_model_update=1e-2, policy=policy)</pre>
<ol start="8">
<li>Let's move on to compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">dqn.compile(Adam(lr=1e-3), metrics=['mae'])<br/>dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)</pre>
<p class="mce-root"/>
<ol start="9">
<li>At the end of the training, it is necessary to save the obtained weights:</li>
</ol>
<pre style="padding-left: 60px">dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)</pre>
<p style="padding-left: 60px">Saving the weight of a network or an entire structure takes place in an <kbd>HDF5</kbd> file, an efficient and flexible storage system that supports complex multidimensional datasets.</p>
<ol start="10">
<li>Finally, we will evaluate our algorithm for 10 episodes:</li>
</ol>
<pre style="padding-left: 60px">dqn.test(env, nb_episodes=5, visualize=True)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>In this recipe, we used the <kbd>keras–rl</kbd> package to develop an AI-based dynamic modeling system. This package implements some deep reinforcement learning algorithms in Python, and integrates seamlessly with Keras' in-depth learning library.</p>
<p>Furthermore, <kbd>keras-rl</kbd> works immediately with OpenAI Gym. OpenAI Gym includes a growing collection of benchmark issues that shows a common interface and a website where people can share their results and compare algorithm performance. This library will be adequately addressed in the next chapter—for now, we will limit ourselves to using it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>These choices do not limit the use of the <kbd>keras-rl</kbd> package, in the sense that the uses of <kbd>keras-rl</kbd> can be easily adapted to our needs. You can use the built-in Keras callbacks and metrics, or define others. For this reason, it is easy to implement your own environments, and even algorithms, simply by extending some simple abstract classes.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Check out <em>Tutorial: Deep Reinforcement Learning</em> (from Google DeepMind): <a href="https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf">https://icml.cc/2016/tutorials/deep_rl_tutorial.pdf</a></li>
<li>Refer to <em>Deep Reinforcement Learning</em> (by Xu Wang): <a href="https://pure.tue.nl/ws/files/46933213/844320-1.pdf">https://pure.tue.nl/ws/files/46933213/844320-1.pdf</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep reinforcement learning with double Q-learning</h1>
                </header>
            
            <article>
                
<p>In the Q-learning algorithm, the future maximum approximated action value is evaluated using the same Q function as the current stock selection policy. In some cases, this can overestimate the action values, slowing down learning. A variation called <strong>Double Q-learning</strong> was proposed by DeepMind researchers in the following paper: <em>Deep reinforcement learning with Double Q-learning</em><span>, H v</span>an Hasselt, A Guez, and D Silver, March, 2016, at the Thirtieth AAAI Conference on Artificial Intelligence. As a solution to this problem, the authors proposed to modify the Bellman update.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will control an inverted pendulum system using the Double Q-learning algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform deep reinforcement learning with Double Q-learning:</p>
<ol>
<li>We will use the <kbd>DoubleDQNCartpole.py</kbd> file that is already provided for you as a reference. Let's start by importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import gym<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Flatten<br/>from keras.optimizers import Adam<br/>from rl.agents.dqn import DQNAgent<br/>from rl.policy import BoltzmannQPolicy<br/>from rl.memory import SequentialMemory</pre>
<ol start="2">
<li>Now, we will define and load the environment:</li>
</ol>
<pre style="padding-left: 60px">ENV_NAME = 'CartPole-v0'<br/>env = gym.make(ENV_NAME)</pre>
<ol start="3">
<li>To set the <kbd>seed</kbd> value, the NumPy library's <kbd>random.seed()</kbd> function is used, as follows:</li>
</ol>
<pre style="padding-left: 60px">np.random.seed(1)<br/>env.seed(1)</pre>
<ol start="4">
<li>Now, we will extract the actions that are available to the agent:</li>
</ol>
<pre style="padding-left: 60px">nb_actions = env.action_space.n</pre>
<ol start="5">
<li>We will build a simple neural network model using the <kbd>keras</kbd> library:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Flatten(input_shape=(1,) + env.observation_space.shape))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())</pre>
<ol start="6">
<li>A <kbd>memory</kbd> variable and a <kbd>policy</kbd> will be set:</li>
</ol>
<pre style="padding-left: 60px">memory = SequentialMemory(limit=50000, window_length=1)<br/>policy = BoltzmannQPolicy()</pre>
<ol start="7">
<li>We just have to define the agent:</li>
</ol>
<pre style="padding-left: 60px">dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,   <br/>               nb_steps_warmup=10, enable_double_dqn=True,  <br/>               target_model_update=1e-2,policy=policy)</pre>
<p style="padding-left: 60px">To enable the double network, we have to set the <kbd>enable_double_dqn</kbd> option to <kbd>True</kbd>.</p>
<ol start="8">
<li>Let's move on to compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">dqn.compile(Adam(lr=1e-3), metrics=['mae'])<br/>dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)</pre>
<ol start="9">
<li>At the end of the training, it is necessary to save the obtained weights:</li>
</ol>
<pre style="padding-left: 60px">dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">Saving the weight of a network or an entire structure takes place in an <kbd>HDF5</kbd> file, an efficient and flexible storage system that supports complex multidimensional datasets.</p>
<ol start="10">
<li>Finally, we will evaluate our algorithm for 10 episodes:</li>
</ol>
<pre style="padding-left: 60px">dqn.test(env, nb_episodes=5, visualize=True)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>The overestimation of the action value is due to the maximum operator that is used in the Bellman equation. The max operator uses the same value for both selecting and evaluating an action. Now, if we select the best action as the one that has the maximum value, we will end up selecting a sub-optimal action (which assumes the maximum value by mistake) instead of the optimal action. We can solve this problem by having two separate Q functions, each of which learns independently. A Q1 function is used to select an action, and the other Q2 function is used to evaluate an action. To do this, simply change the objective function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Essentially, the following two networks are used:</p>
<ul>
<li>The DQN network to select what is the best action to take for the next state (the action with the highest Q value)</li>
<li>The target network, to calculate the target Q value of taking that action at the next state</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Refer to <em>Deep Reinforcement Learning with Double Q-learning</em>: <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847">https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847</a></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep Q-network algorithm with dueling Q-learning</h1>
                </header>
            
            <article>
                
<p>To improve convergence speed by making our network's architecture closer represent one of the last challenges of reinforcement learning, a definite improvement in the performance of a DQN model has been proposed by Wang and others in the following paper: <em>Dueling network architectures for deep reinforcement learning, Z </em>Wang, T Schaul, M Hessel, H van Hasselt, M Lanctot, and N de Freitas, 2015, arXiv preprint arXiv:1511.06581.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, we will control an inverted pendulum system using the dueling Q-learning algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it…</h1>
                </header>
            
            <article>
                
<p>Let's see how we can perform deep Q-network algorithm with dueling Q-learning:</p>
<ol start="1">
<li>We will use the <kbd>DuelingDQNCartpole.py</kbd> file that is already provided for you as a reference. Let's start by importing the libraries:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import gym<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Flatten<br/>from keras.optimizers import Adam<br/>from rl.agents.dqn import DQNAgent<br/>from rl.policy import BoltzmannQPolicy<br/>from rl.memory import SequentialMemory</pre>
<ol start="2">
<li>Now, we will define and load the environment:</li>
</ol>
<pre style="padding-left: 60px">ENV_NAME = 'CartPole-v0'<br/>env = gym.make(ENV_NAME)</pre>
<ol start="3">
<li>To set the <kbd>seed</kbd> value, the NumPy library's <kbd>random.seed()</kbd> function is used, as follows:</li>
</ol>
<pre style="padding-left: 60px">np.random.seed(2)<br/>env.seed(2)</pre>
<ol start="4">
<li>Now, we will extract the actions, available to the agent:</li>
</ol>
<pre style="padding-left: 60px">nb_actions = env.action_space.n</pre>
<ol start="5">
<li>We will build a simple neural network model using the Keras library:</li>
</ol>
<pre style="padding-left: 60px">model = Sequential()<br/>model.add(Flatten(input_shape=(1,) + env.observation_space.shape))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(16))<br/>model.add(Activation('relu'))<br/>model.add(Dense(nb_actions))<br/>model.add(Activation('linear'))<br/>print(model.summary())</pre>
<ol start="6">
<li>A <kbd>memory</kbd> variable and a <kbd>policy</kbd> will be set:</li>
</ol>
<pre style="padding-left: 60px">memory = SequentialMemory(limit=50000, window_length=1)<br/>policy = BoltzmannQPolicy()</pre>
<p style="padding-left: 60px">We just have to define the agent:</p>
<pre style="padding-left: 60px">dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory,<br/>               nb_steps_warmup=10, enable_dueling_network=True,  <br/>               dueling_type='avg',target_model_update=1e-2,<br/>               policy=policy)</pre>
<p style="padding-left: 60px">To enable the dueling network, we have to specify the <kbd>dueling_type</kbd> to one of the following:<kbd>'avg'</kbd>, <kbd>'max'</kbd>, or <kbd>'naive'</kbd>.</p>
<ol start="8">
<li>Let's move on to compile and fit the model:</li>
</ol>
<pre style="padding-left: 60px">dqn.compile(Adam(lr=1e-3), metrics=['mae'])<br/>dqn.fit(env, nb_steps=1000, visualize=True, verbose=2)</pre>
<ol start="9">
<li>At the end of the training, it is necessary to save the obtained weights:</li>
</ol>
<pre style="padding-left: 60px">dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)</pre>
<p style="padding-left: 60px">Saving the weight of a network or an entire structure takes place in an <kbd>HDF5</kbd> file, an efficient and flexible storage system that supports complex multidimensional datasets.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>Finally, we will evaluate our algorithm for 10 episodes:</li>
</ol>
<pre style="padding-left: 60px">dqn.test(env, nb_episodes=5, visualize=True)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works…</h1>
                </header>
            
            <article>
                
<p>In reinforcement learning, the function Q and the value function play a fundamental role:</p>
<ul>
<li>The Q function specifies how good an agent is to perform an action in the <em>s</em> state</li>
<li>The value function specifies how good it is for an agent to be in a state, <em>s</em></li>
</ul>
<p>To introduce a further improvement in the performance of a DQN, we introduce a new function called an <kbd>advantage</kbd> function, which can be defined as the difference between the <kbd>value</kbd> function and the <kbd>benefit</kbd> function. The <kbd>benefit</kbd> function specifies how good an agent is at performing an action compared to other actions.</p>
<p>Therefore, the <kbd>value</kbd> function specifies the goodness of a state and the <kbd>advantage</kbd> function specifies the goodness of an action. Then, the combination of these two functions tells us how good it is for an agent to perform an action in a state that is actually our <kbd>Q</kbd> function. So, we can define our <kbd>Q</kbd> function as the sum of a <kbd>value</kbd> function and an <kbd>advantage</kbd> function.</p>
<p>The dueling DQN is essentially a DQN, in which the fully connected final layer is divided into two streams:</p>
<ul>
<li>One calculates the <kbd>value</kbd> function</li>
<li>The other calculates the <kbd>advantage</kbd> function</li>
</ul>
<p>Finally, the two streams are combined using the aggregate level for obtaining the <kbd>Q</kbd> function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>The approximation of the <kbd>value</kbd> function via the neural network is anything but stable. To achieve convergence, the basic algorithm should be modified by introducing techniques to avoid oscillations and divergences.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The most important technique is called <kbd>experience replay</kbd>. During the episodes, at each step, the agent's experience is stored in a dataset, called <kbd>replay memory</kbd>. In the internal cycle of the algorithm, instead of performing the training on the network based on the only transition just performed, a subset of transitions is selected randomly from the replay memory, and the training takes place according to the loss calculated on the subset of transitions.</p>
<p>The experience of the <kbd>replay</kbd> technique, that is, randomly selecting transitions from <kbd>replay memory</kbd>, eliminates the problem of correlation between consecutive transitions and reduces variance among different updates.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out <em>Keras Reinforcement Learning Projects</em>, Giuseppe Ciaburro, Packt Publishing</li>
<li>Refer to <em>Dueling Network Architectures for Deep Reinforcement Learning</em> for more information: <a href="https://arxiv.org/abs/1511.06581">https://arxiv.org/abs/1511.06581</a></li>
</ul>


            </article>

            
        </section>
    </body></html>