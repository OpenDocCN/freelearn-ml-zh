- en: Equipping Your Car with a Rearview Camera and Hazard Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"Comes the morning and the headlights fade away."'
  prefs: []
  type: TYPE_NORMAL
- en: – The Living Daylights (1987)
  prefs: []
  type: TYPE_NORMAL
- en: James Bond is a car thief. The movies show that he has stolen many automobiles,
    often from innocent bystanders. We do not know whether these unfortunate people
    ever recovered their property but, even if they did, the damages from collisions,
    submersions, bullets, and rockets would have had a lasting impact on their insurance
    premiums. Bond has also stolen a propeller plane, a tank, and a moon buggy.
  prefs: []
  type: TYPE_NORMAL
- en: The man has been driving since the 1950s, and perhaps it is time that he stopped.
  prefs: []
  type: TYPE_NORMAL
- en: Be that as it may, we can break away from the old Cold War days of indifference
    to collateral damage. With modern technology, we can provide a driver with timely
    information about others who are sharing the road. This information may make it
    easier to avoid collisions and to properly aim the vehicle's rocket launchers
    so that a chase scene can be conducted in an orderly manner, without flattening
    whole city blocks. Secret agents will not lose so many cars and, thus, will not
    feel compelled to steal so many.
  prefs: []
  type: TYPE_NORMAL
- en: Since driver assistance is a broad topic, let's focus on one scenario. Twilight
    and nighttime are difficult times for drivers, including secret agents. We might
    be blinded by the lack of natural light or the glare of headlights. However, we
    can make a computer vision system that sees headlights (or rear lights) clearly
    and can estimate their distance from them. This system can also distinguish between
    lights of different colors, a feature which is relevant to identifying signals
    and types of vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: We will choose computationally inexpensive techniques, suitable for a low-powered
    computer—namely, Raspberry Pi—which we can plug into a car's cigarette lighter
    through an adapter. An LCD panel can display the relevant information, along with
    a live, rear-view video feed that is less glaring than real headlights.
  prefs: []
  type: TYPE_NORMAL
- en: 'This project presents us with several new topics and challenges, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to detect blobs of light and classify their color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to estimate the distance from the camera to a detected object whose real-world
    size is known
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up a low-budget lab where we can experiment with lights of many colors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to set up a Raspberry Pi and peripherals in a car
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Realistically, our quick, homemade project is not sufficiently robust to be
    relied upon as an automotive safety tool, so take it with a grain of salt. However,
    it is a fun introduction to analyzing signal lights and wiring up a custom in-car
    computer. The choice of Raspberry Pi as a platform challenges us to think about
    the car as an environment for rapid prototyping. We can plug in any standard peripherals,
    including a webcam, keyboard, mouse, and even a monitor, giving us a complete
    desktop Linux system with Python—on wheels! (Snakes in a car!) For more exotic
    projects, the Pi is compatible with many electronics kits, too! A smartphone or
    tablet is also a good alternative for use in a car, and is easier to power than
    a Pi with a monitor, but the Pi excels as a well-rounded prototyping tool.
  prefs: []
  type: TYPE_NORMAL
- en: All we need now is a name for our project. So, let the app be known as `The
    Living Headlights`.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter''s project has the following software dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A Python environment with the following modules**: OpenCV, NumPy, SciPy,
    wxPython'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setup instructions are covered in [Chapter 1](e3ac8266-975b-43ca-8221-482a15eb0e05.xhtml),
    *Preparing for the Mission*. Refer to the setup instructions for any version requirements.
    Basic instructions for running Python code are covered in [Appendix C](c44b1aaa-fe12-4054-85fb-37d584f15d3b.xhtml),
    *Running with Snakes (or, First Steps with Python)*.
  prefs: []
  type: TYPE_NORMAL
- en: The completed project for this chapter can be found in this book's GitHub repository, [https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-for-Secret-Agents-Second-Edition),
    in the `Chapter005` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Planning The Living Headlights app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this app, we need to return to the cross-platform wxPython framework. Optionally,
    we can also develop and test our wxPython application on a Windows, Mac, or Linux
    desktop or laptop before deploying it to our Raspberry Pi computer. With the Raspbian
    operating system, the Pi can run wxPython, just as any Linux desktop could.
  prefs: []
  type: TYPE_NORMAL
- en: 'The GUI for `The Living Headlights` includes a live video feed, a set of controls
    where the user can enter their true distance from headlights, and a label that
    initially displays a set of instructions, as seen in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ac2b3e7-2f88-40ba-96a3-6aa0bce4c04c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When a pair of headlights is detected, the user must perform a one-time calibration
    step. This step consists of entering the true distance between the camera and
    headlights (specifically, the midpoint between the headlights) and then clicking
    on the Calibrate button. Thereafter, the app continuously updates and displays
    an estimate of the headlights'' distance and color, as seen in the label at the
    bottom of the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5f11f4d-a46c-4e46-b15d-6f825222e86c.png)'
  prefs: []
  type: TYPE_IMG
- en: The calibration and the selected unit (Meters or Feet) are stored in a configuration
    file when the app closes. They are reloaded from this file when the app reopens.
    The calibration remains valid as long as the same camera and lens are used, the
    lens does not zoom, and the spacing between two headlights in a pair remains approximately
    constant for all pairs of headlights.
  prefs: []
  type: TYPE_NORMAL
- en: Atop the video feed, colored circles are drawn to mark detected lights, and
    lines are drawn between pairs of detected lights whose colors match. Such a pair
    is considered to be a set of headlights.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's consider techniques for detecting lights and classifying their colors.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting lights as blobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To the human eye, light can appear both very bright and very colorful. Imagine
    a sunny landscape or a storefront lit by a neon sign; they are bright and colorful!
    However, a camera captures a range of contrast that is much narrower and not as
    intelligently selected, so that the sunny landscape or neon-lit storefront can
    look washed out. This problem of poorly controlled contrast is especially bad
    in cheap cameras or cameras that have small sensors, such as webcams. As a result,
    bright light sources tend to be imaged as big white blobs with thin rims of color.
    These blobs also tend to mimic a lens's iris—typically, a polygon approximating
    a circle.
  prefs: []
  type: TYPE_NORMAL
- en: The thought of all lights becoming white and circular makes the world seem like
    a poorer place, if you ask me. Nonetheless, in computer vision, we can take advantage
    of such a predictable pattern. We can look for white blobs that are nearly circular
    and we can infer their human-perceptible color from a sample that includes extra
    pixels around the rim.
  prefs: []
  type: TYPE_NORMAL
- en: '**Blob detection** is actually a major branch of computer vision. Unlike the
    face detectors (or other object detectors) that we discussed in previous chapters,
    a blob detector is not trained. There is no concept of a reference image, so meaningful
    classifications such as *This blob is a light* or *This blob is skin *are more
    complicated to produce. Classification goes beyond the ken of the blob detector
    itself. We explicitly define thresholds between non-lights and lights, and between
    different human-perceptible colors of lights, based on *a priori* knowledge about
    typical shapes and colors of light sources, as imaged by a webcam.'
  prefs: []
  type: TYPE_NORMAL
- en: Other terms for a blob include a *connected component* and a *region*. However,
    in this book, we just say *blob*.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its simplest, blob detection consists of the five following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Partition the image into two or more colors. For example, this can be accomplished
    by *binary thresholding* (also called **binarization**), whereby all grayscale
    values above a threshold are converted into white and all grayscale values below
    the threshold are converted into black.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the *contour* of each contiguously colored region, that is, each blob.
    The contour is a set of points describing the region's outline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Merge blobs that are deemed to be neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, determine each blob's *features*. These arehigher-level measurements
    such as the center point, radius, and circularity. The usefulness of these features
    lies in their simplicity. For further blob-related computations and logic, it
    may be best to avoid complex representation, such as a contour's many points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reject blobs that fail to meet certain measurable criteria.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'OpenCV implements a simple blob detector in a class called `cv2.SimpleBlobDetector`
    (appropriately enough). This class''s constructor takes an instance of a helper
    class called `cv2.SimpleBlobDetector_Params`, which describes the criteria for
    accepting or rejecting a candidate blob. `SimpleBlobDetector_Params` has the following
    member variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`thresholdStep`, `minThreshold`, and `maxThreshold`: The search for blobs is
    based on a series of binarized images (analogous to the series of scaled images
    that are searched by a Haar cascade detector, as described in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml),
    *Training a Smart Alarm to Recognize the Villain and His Cat*). The thresholds
    for binarization are based on the range and step size given by these variables.
    We use `8`, `191`, and `255`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minRepeatability`: This variable minus one is the minimum number of neighbors
    that a blob must have. We use `2`, meaning that a blob must have at least one
    neighbor. If we did not require at least one neighbor, the detector would tend
    to report a large number of blobs, with a lot of overlap between blobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minDistBetweenBlobs`: Blobs must be at least this many pixels apart. Blobs
    that are closer than the minimum distance from each other are counted as neighbors.
    We use a minimum distance calculated as two percent of the image''s larger dimension
    (typically width).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filterByColor` (`True` or `False`) and `blobColor`: If `filterByColor` is
    `True`, a blob''s central pixel must exactly match `blobColor`. We use `True`
    and `255` (white), based on our assumption that light sources are white blobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filterByArea` (`True` or `False`), `minArea`, and `maxArea`: If `filterByArea`
    is `True`, a blob''s area in pixels must fall within the given range. We use `True`
    and a range calculated as 0.5 percent to 10 percent of the image''s larger dimension
    (typically width).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filterByCircularity` (`True` or `False`), `minCircularity`, and `maxCircularity`:
    If `filterByCircularity` is `True`, a blob''s circularity must fall within the
    given range, where circularity is defined as `4 * PI * area / (perimeter ^ 2)`.
    A circle''s circularity is 1.0 and a line''s circularity is 0.0\. For our approximately
    circular light sources, we use `True` and the range 0.7 to 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filterByInertia` (`True` or `False`), `minInertiaRatio`, and `maxInertiaRatio`:
    If `filterByInertia` is `True`, a blob''s inertia ratio must fall within the given
    range. A relatively high inertia ratio implies that the blob is relatively elongated
    (and would thus require more torque to rotate along its longest axis). A circle''s
    inertia ratio is 1.0 and a line''s inertia ratio is 0.0\. We use `filterByInertia=False`
    (no filtering by inertia) because the circularity test already gives sufficient
    control over the shape for our purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filterByConvexity` (`True` or `False`), `minConvexity`, and `maxConvexity`:
    If `filterByConvexity` is `True`, a blob''s convexity must fall within the given
    range, where convexity is defined as `area/hullArea`. Here, `hullArea` refers
    to the area of the convex hull—the convex polygon surrounding all the points of
    a contour with the minimum area. Convexity is always more than 0.0 and less than
    1.0\. A relatively high convexity implies that the contour is relatively smooth.
    We use `filterByConvexity=False` (no filtering by convexity) because the circularity
    test already gives sufficient control over the shape for our purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these parameters cover many useful criteria, they are designed for
    grayscale images and do not provide a practical means of filtering or classifying
    blobs based on separate criteria for hue, saturation, and luminosity. The suggested
    values in the preceding list are tuned to extract bright blobs of light. However,
    we may want to classify such blobs by subtle variations in color, especially around
    the blob's edge.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hue** refers to a color''s angle on the color wheel, where *0* degrees is
    red, *120* is green, and *240* is blue. The hue in degrees can be calculated from
    RGB values with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85334e33-6266-4d3c-b482-05437d6097d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Saturation** refers to a color''s distance from grayscale. There are several
    alternative formulations of an RGB color''s saturation. We use the following formulation,
    which some authors call **chroma** instead of saturation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e863ddc6-697d-461c-8dd0-600f505f599f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can classify a light source's human-perceptible color based on the average
    hue and saturation of the blob and some surrounding pixels. The combination of
    a low saturation and a blue or yellow hue tends to suggest that the light will
    appear white to human vision. Other light sources may appear (in order of ascending
    hue) as red, orange/amber/yellow, green (a wide range from spring green to emerald),
    blue/purple (another wide range), or pink, to give just a few examples. Threshold
    values can be chosen based on trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: Using the techniques we've mentioned, we can detect the location, pixel radius,
    and perceptual color of light sources. However, we need additional techniques
    to get an estimate of the real distance between the camera and a pair of headlights.
    Let's turn our attention to this problem now.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating distances (a cheap approach)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose we have an object sitting in front of a pinhole camera. Regardless
    of the distance between the camera and the object, the following equation holds
    true:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90fd8367-ddea-474b-a401-522814d03c63.png)'
  prefs: []
  type: TYPE_IMG
- en: We may use any unit (such as pixels) in the equation's left-hand side and any
    unit (such as meters) in its right-hand side (on each side of the equation, the
    division cancels the unit). Moreover, we may define the object's size based on
    anything linear that we can detect in the image, such as the diameter of a detected
    blob or the width of a detected face rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s rearrange the equation to illustrate that the distance to the object
    is inversely proportional to the object''s size in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/034b8d7f-f388-49a7-88af-e6cedf04b718.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s assume that the object''s real size and the camera''s focal length are
    constant (a constant focal length means that the lens does not zoom and we do
    not swap the lens for a different lens). Consider the following arrangement, which
    isolates this pair of constants on the right-hand side of the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7627267c-3d3f-464d-9942-bb924375dd37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the right-hand side of the equation is constant, so is the left. We may
    conclude that the following relationship holds true over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d50c2596-5ff5-4127-8000-20e550eece2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s solve the following equation for the new distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9898b8b0-4da5-4ae8-aa7b-649f9632fbeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s think about applying this equation in software. To provide a ground
    truth, the user must take a single, true measurement of the distance to use as
    the *old* distance in all future calculations. As well as this, we must know the
    object''s old pixel size and its subsequent new size so that we can compute the
    new distance any time there is a detection result. Let''s review the following
    assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no lens distortion; the pinhole camera model applies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focal length is constant; no zoom is applied and the lens is not swapped for
    a different lens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The object is rigid; its real-world measurements do not change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The camera is always viewing the same side of the object; the relative rotation
    of the camera and object does not change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You might wonder whether the first assumption is problematic, as webcams often
    have cheap wide angle lenses with significant distortion. Despite lens distortion,
    does the object's size in the image remain inversely proportional to the real
    distance between the camera and object? The following paper reports experimental
    results for a lens that appears to distort badly and an object that is located
    off-center (in an image region where distortion is likely to be especially bad)—M.
    N. A. Wahab, N. Sivadev, and K. Sundaraj. *Target distance estimation using monocular
    vision system for mobile robot*. **IEEE Conference on Open Systems** (**ICOS**)
    2011 Proceedings, vol. 11, no. 15, p. 25-28\. September 2011.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using exponential regression, the authors show that the following model is
    a good fit for experimental data (*R^2=0.995*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the exponent is close to *-1*, and thus the statistical model is not
    far from the ideal inverse relationship. (Even the poor-quality lens and off-center
    subject did not disprove our assumptions!)
  prefs: []
  type: TYPE_NORMAL
- en: We can also ensure that the second assumption (no zooming and no swapping of
    the lens) holds true.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider the third and fourth assumptions (rigidity and constant rotation)
    in the case of a camera and object—one in each car on a highway. Except in a crash,
    most of a car's exterior parts are rigid. Except when passing or pulling over,
    one car travels directly behind the other on a surface that is mostly flat and
    mostly straight. However, on a road that is hilly or has many turns, these assumptions
    start to fall apart. It becomes more difficult to predict which side of the object
    is currently being viewed; thus, it is more difficult to say whether our reference
    measurements apply to a particular side.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we need to define a generic car part to be our *object*. The headlights
    (and the space between them) are a decent choice, since we have a method for detecting
    them and the distance between headlights is consistent across many cars—although
    not all.
  prefs: []
  type: TYPE_NORMAL
- en: 'All distance estimation techniques in computer vision rely on some assumptions
    or calibration steps that relate to the camera, the object, the relationship between
    camera and object, or lighting. For comparison, let''s consider some of the following
    common distance estimation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: A **time-of-flight** (**ToF**) camera shines a light on objects and measures
    the intensity of any reflected light. This intensity is used to estimate the distance
    at each pixel based on the known fall-off characteristics of the light source.
    Some ToF cameras, such as Microsoft Kinect, use an infrared light source. Other,
    more expensive ToF cameras scan a scene with a laser or even use a grid of lasers.
    ToF cameras may suffer from interference if other bright lights are being imaged,
    so they are poorly suited to our application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **stereo camera** consists of two parallel cameras with a known, fixed distance
    between them. In each frame, a pair of images is captured, features are identified,
    and a *disparity* or pixel distance is calculated for each pair of corresponding
    features. We can convert disparity into real distance based on the cameras' known
    field of view and the distance between them. For our application, stereo techniques
    would be feasible, but they are also computationally expensive and use a lot of
    input bus bandwidth. Optimizing these techniques for Raspberry Pi would be a big
    challenge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structure from Motion** (**SfM**) techniques only need a single, regular
    camera, but rely on moving the camera by known distances over time. For each pair
    of images taken from neighboring locations, disparities are calculated, as with
    a stereo camera. In this scenario, as well as knowing the camera''s movements,
    we must know the object''s movements or lack thereof. Due to these limitations,
    SfM techniques are poorly suited to our application, as our camera and object
    are mounted on two freely moving vehicles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various **3D feature tracking** techniques entail estimating the rotation of
    an object, as well as its distance and other coordinates. Edges and texture details
    are also considered. The differences between models of cars make it difficult
    to define one set of features that are suitable for 3D tracking, and so 3D feature
    tracking is not well-suited to our application. Moreover, 3D tracking is computationally
    expensive, especially by the standards of a low-powered computer such as Raspberry
    Pi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information on these techniques, refer to the following books, available
    from Packt Publishing:'
  prefs: []
  type: TYPE_NORMAL
- en: Kinect and other ToF cameras are covered in the first edition of my book, *OpenCV
    Computer Vision with Python*, specifically *Chapter 5, Detecting Foreground/Background
    Regions and Depth*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3D feature tracking and SfM are covered in *Mastering OpenCV with Practical
    Computer Vision Projects*, specifically *Chapter 3, Markerless Augmented Reality,* and *Chapter
    4, Exploring Structure from Motion Using OpenCV*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stereo vision and 3D feature tracking are covered in Robert Laganière's *OpenCV
    3 Computer Vision Application Programming Cookbook*, specifically *Chapter 10,
    Estimating Projective Relations in Images*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stereo vision and 3D pose estimation are also covered in Alexey Spizhevoy and
    Aleksandr Rybnikov's *OpenCV 3 Computer Vision with Python Cookbook*, specifically *Chapter
    9, Multiple View Geometry*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On balance, the simplistic approach—based on pixel distances being inversely
    proportional to real distances—is a justifiable choice given our application and
    our intent to support the Pi.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing The Living Headlights app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`The Living Headlights` app will use the following files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LivingHeadlights.py`: This is a new file that contains our application class
    and its `main` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ColorUtils.py`: This is a new file that contains the utility functions required
    to convert colors into different representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GeomUtils.py`: This contains utility functions for geometric calculations.
    Copy or link to the version that we used in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,
    Training a Smart Alarm to Recognize the Villain and His Cat*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PyInstallerUtils.py`: This contains utility functions for accessing resources
    in a PyInstaller application bundle. Copy or link to the version that we used
    in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*, Training a Smart
    Alarm to Recognize the Villain and His Cat*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ResizeUtils.py`: This contains utility functions for resizing images, including
    camera capture dimensions. Copy or link to the version that we used in [Chapter
    3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*, Training a Smart Alarm to Recognize
    the Villain and His Cat*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`WxUtils.py`: This contains utility functions for using OpenCV images in wxPython
    apps. Copy or link to the version that we used in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,
    Training a Smart Alarm to Recognize the Villain and His Cat*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s get started with the creation of `ColorUtils.py`. Here, we need functions
    to calculate a color''s hue and saturation according to the formulae mentioned
    in the *Detecting lights as blobs* section. The module''s implementation is shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to convert an entire image (that is, every pixel) to hue, saturation,
    and either luminosity or value, we can use the following OpenCV method, `cvtColor`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: See the following Wikipedia article for definitions of saturation, luminosity,
    and value in HSV and HSL color models at [https://en.wikipedia.org/wiki/HSL_and_HSV](https://en.wikipedia.org/wiki/HSL_and_HSV).
    Our definition of saturation is called **chroma** in the Wikipedia article, which
    differs from HSL saturation, and in turn differs again from HSV saturation. Moreover,
    OpenCV represents hue in units of two degrees (a range of `0` to `180`) so that
    the hue channel fits inside a byte.
  prefs: []
  type: TYPE_NORMAL
- en: For some types of image segmentation problems, it is useful to convert the entire
    image into HSV, HSL, or another color model. For example, see Rebecca Stone's
    blog post about segmenting images of clown fish at [https://realpython.com/python-opencv-color-spaces/](https://realpython.com/python-opencv-color-spaces/),
    or Vikas Gupta's blog post about segmenting images of Rubik's cubes at [https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/](https://www.learnopencv.com/color-spaces-in-opencv-cpp-python/).
  prefs: []
  type: TYPE_NORMAL
- en: We have written our own conversion functions because, for our purposes, converting
    an entire image is unnecessary; we just need to convert a sample from each blob.
    We also prefer a more accurate floating-point representation instead of the byte-sized
    integer representation that OpenCV imposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to modify `GeomUtils.py` by adding a function to calculate the
    Euclidean distance between two 2D points, such as the pixel coordinates of two
    headlights in an image. At the top of the file, let''s add an import statement
    and implement the function, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Distances (and other magnitudes) can also be calculated using NumPy''s `linalg.norm` function,
    as seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, `a0` and `a1` can be any size and shape. However, for a low-dimensional
    space such as 2D or 3D coordinate vectors, the overhead of using NumPy arrays
    is probably not worthwhile, so a utility function such as ours is a reasonable
    alternative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code contains all the new utility functions. Now, let''s create
    a file, `LivingHeadlights.py`, for the app''s `main` class, `LivingHeadlights`.
    Like `InteractiveRecognizer` in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml)*,
    Training a Smart Alarm to Recognize the Villain and His Cat*, `LivingHeadlights`
    is a class for a wxPython app that captures and processes images on a background
    thread (to avoid blocking the GUI on the main thread), allows a user to enter
    reference data, serializes its reference data when exiting, and deserializes its
    reference data when starting up again. This time, serialization and deserialization
    is accomplished using Python''s `cPickle` module or, if `cPickle` is unavailable
    for any reason, the less-optimized `pickle` module. Let''s add the following import
    statements to the start of `LivingHeadlights.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also define some BGR color values and names at the start of the module.
    We will classify each blob as one of the following colors, depending on its hue
    and saturation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s begin implementing the class. The initializer takes several arguments
    relating to the configuration of the blob detector and the camera. Refer back
    to the *Detecting lights as blobs* section for explanations of the blob detection
    parameters supported by OpenCV''s `SimpleBlobDetector` and `SimpleBlobDetector_Params`
    classes. The class declaration and initializer declaration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We start the initializer''s implementation by setting a public Boolean variable
    that indicates to the app to display a mirrored image and a protected Boolean
    variable that ensures the app is running, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If there is any configuration file saved from a previous run of the app, we
    deserialize the reference measurements (the pixel distance between lights and
    the real distance in meters between lights and the camera), as well as the user''s
    preferred unit of measurement (`meters` or `feet`), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we initialize a `VideoCapture` object and try to configure the size of
    the captured images. If the requested size is unsupported, we fall back to the
    default size, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to declare variables for the images we will capture, process,
    and display. Initially, these are `None`. We also need to create a lock to manage
    thread-safe access to an image that will be captured and processed on one thread,
    and then drawn to the screen on another thread. The relevant declarations are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we create a `SimpleBlobDetector_Params` object and a `SimpleBlobDetector`
    object based on the arguments passed to the app''s initializer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we specify the style of the app''s window and we initialize the following
    base class, `wx.Frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need to bind the *Esc* key to a callback that closes the app, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the GUI elements, including the bitmap, the text field for
    the reference distance, radio buttons for the unit (`meters` or `feet`), and the
    Calibrate button. We also need to bind callbacks for various input events, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to ensure that the proper radio buttons start in the selected state,
    depending on the configuration data that we deserialized earlier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we stack the radio buttons vertically using a `BoxSizer`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We then line up all of our controls horizontally, again using a `BoxSizer`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To finish our layout, we place the controls below the image, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The last thing we do in the initializer is start a background thread to capture
    and process images from the camera using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'When closing the app, we first ensure that the capture thread terminates, just
    as we did for the `InteractiveRecognizer` in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml),
    *Training a Smart Alarm to Recognize the Villain and His Cat*. We also use `pickle`
    or `cPickle` to serialize the reference measurements and preferred unit (`meters`
    or `feet`) to a file. The implementation of the relevant callback is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The callback associated with the *Esc* button just closes the app, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The video panel''s erase and paint events are bound to callbacks, `_onVideoPanelEraseBackground`
    and`_onVideoPanelPaint`, which have the same implementations as `InteractiveRecognizer` in
    [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml), *Training a Smart Alarm
    to Recognize the Villain and His Cat*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When either of the radio buttons are selected, we need to record the newly
    selected unit of measurement, as seen in the following two callback methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Whenever a new character is entered in the text field, we need to call a helper
    method to validate the text as potential input, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When the Calibrate button is clicked, we parse the measurement from the text
    field, clear the text field, convert the measurement into `meters` if necessary,
    and store it. The button''s callback is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As in [Chapter 3](49c9a5fb-89a3-4c0d-bbee-021d2618168c.xhtml), *Training a
    Smart Alarm to Recognize the Villain and His Cat*, the background thread runs
    a loop, which includes capturing an image, calling a helper method to process
    the image, and then handing the image to another thread for display. Optionally,
    the image may be mirrored (flipped horizontally) before being displayed. The loop''s
    implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The helper method for processing the image is quite long, so let''s look at
    it in several chunks. First, we detect blobs in a gray version of the image and
    then initialize a dictionary to sort the blobs by color, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For each blob, we crop out a square region that is likely to include a white
    circle of light, plus some more saturated pixels around the edge, as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we find the average hue and saturation of the region and, using those
    values, we classify the blob as one of the colors we defined at the top of this
    module, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Depending on your camera's color rendition, you may need to tweak some of the
    hue and saturation thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Note that our color-matching logic is based on perceptual (subjective) similarity
    and not on the geometric distance in any color model, such as RGB, HSV, or HSL.
    Perceptually, a *green* light could be emerald green (geometrically close to cyan),
    neon green, or even spring green (geometrically close to yellow), but most people
    would never mistake a spring green light for an *amber* light, nor a yellowish-orange
    light for a *red* light. Within the reddish and yellowish ranges, most people
    perceive more abrupt distinctions between colors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, after classifying all blobs, we call a helper method that handles
    the classification results and a helper method that may enable or disable the
    Calibrate button, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the color classification results, we want to highlight the blobs in
    certain colors, draw lines that connect pairs of like-colored blobs (if any),
    and display a message about the estimated distance to the first such pair of blobs.
    We use the BGR color values and human-readable color names that we defined at
    the top of this module. The relevant code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s look at the helper method that enables or disables the Calibrate
    button. The button should be enabled only when a pixel distance between two lights
    is being measured and a number (the real distance between the lights and camera)
    is in the text field. The following code illustrates the tests for these conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The helper method that shows the instructional message is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The helper method that shows the estimated distance in either `meters` or `feet` is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the message is cleared, we need to leave an endline character so that
    the label still has the same height as when it is populated, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Showing a message simply entails changing the text of the `StaticText` object,
    as seen in the following helper method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The class is complete. Now, we just need the following `main` function (similar
    to our `main` functions for previous wxPython apps) to specify a file path for
    serialization and deserialization and to launch the app, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: There we have it! That's the whole implementation of `The Living Headlights`
    app! This project's code is short, but it does include some unusual requirements
    for setup and testing. Let's turn to these tasks now.
  prefs: []
  type: TYPE_NORMAL
- en: Testing The Living Headlights app at home
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do not run out onto the highway at night to point your laptop's webcam into
    the headlights! We can devise more convenient and safer ways to test `The Living
    Headlights`, even if you don't own a car or don't drive.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pair of LED flashlights is a good proxy for a pair of headlights. A flashlight
    with many LEDs (for example, 19) is preferable because it creates a denser circle
    of light that is more likely to be detected as exactly one blob. To ensure that
    the distance between the two flashlights remains constant, we can attach them
    to a rigid object, such as a board, using brackets, clamps, or tape. My father
    Bob Howse is great at constructing such things. Take a look at my flashlight holder
    in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/70d2121d-7996-49c6-8aad-302ceee5bcd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows a frontal view of the flashlight holder, including
    a decorative grill:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b2646bc-1b73-4a68-8467-a514a82bf34d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Set up the lights in front of the webcam (parallel to the webcam''s lens),
    run the app, and make sure that the lights are being detected. Then, using a tape
    measure, find the distance between the webcam and the center point between the
    front of the lights, as seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fecc212d-5b48-4ead-8f7d-06d1cd67abaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Type the distance into the text field and click Calibrate. Then, move the lights
    either closer to or further away from the camera, ensuring they are parallel to
    the camera's lens. Check that the app is updating the distance estimate appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simulate colored car lights, place a thick piece of colored glass in front
    of the flashlights, as close to the light source as possible. Stained glass (the
    kind used in church windows) works well, and you may find it in craft supply stores.
    Colored lens filters for photography or videography should also work. They are
    widely available, new or used, from camera stores. Colored acetate or other thin
    materials do not work as well, as the LED lights are very intense. The following
    image shows an existing light setup using an orange or amber-colored stained glass
    filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9d2840c7-1398-4453-adfb-2fcd5ec5c20f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows the app''s analysis of the lighting setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/416095d9-f470-4c71-b227-184acd6ed462.png)'
  prefs: []
  type: TYPE_IMG
- en: Check that the app is reporting the appropriate color for the detected lights.
    Depending on your particular camera's color rendition, you may find that you'll
    need to adjust some of the hue and saturation thresholds in the `detectAndEstimateDistance`
    method. You might also want to experiment with adjusting the attributes of the `SimpleBlobDetector_Params`
    object in the initializer to see their effects on the detection of lights and
    other blobs.
  prefs: []
  type: TYPE_NORMAL
- en: Once we are satisfied that the app is working well with our homemade apparatus,
    we can step up to a more realistic level of testing!
  prefs: []
  type: TYPE_NORMAL
- en: Testing The Living Headlights app in a car
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When choosing the hardware for a car-based setup, it''s important to consider
    the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Can the car's outlets power the hardware?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can the hardware fit conveniently in the car?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Raspberry Pi draws power from a 5V supply through its micro USB port. We
    can satisfy this power requirement by plugging a USB adapter into the car''s cigarette
    lighter and then connecting it to the Pi through a USB to micro USB cable. Make
    sure that your adapter''s voltage is exactly 5V and that its amperage is equal
    to or greater than the recommended amperage for your Pi model. For example, the
    official documentation at [https://www.raspberrypi.org/documentation/faqs/](https://www.raspberrypi.org/documentation/faqs/)
    recommends a 5V, 2.5A power supply for Raspberry Pi 3 Model B. The following image
    shows a setup using a first-generation Raspberry Pi Model A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7383f4d-d5de-431f-90d5-b31e8fb30f74.png)'
  prefs: []
  type: TYPE_IMG
- en: Normally, the cigarette lighter is a 12V power source, so it can power a variety
    of devices through an adapter. You could even power a chain of devices, and the
    Pi need not be the first device in the chain. Later in this section, we will discuss
    the example of a Pi drawing power from a USB port on a SunFounder LCD display,
    which in turn draws power from a cigarette lighter receptacle through an adapter.
  prefs: []
  type: TYPE_NORMAL
- en: Standard USB peripherals, such as a webcam, mouse, and keyboard, can draw enough
    power from Pi's USB ports. Although the Pi only has two USB ports, we can use
    a USB splitter to power to a webcam, mouse, and keyboard simultaneously. Alternatively,
    some keyboards have a built-in touchpad that can be used as a mouse. Another option
    is to simply make do with only using two peripherals at a time and swapping one
    of them for the third peripheral as needed. In any case, once our app has been
    started and calibrated (and once we are driving!), we no longer need the keyboard
    or mouse input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The webcam should sit against the inside of the car''s rear window. The webcam''s
    lens should be as close to the window as possible to reduce the visibility of
    grime, moisture, and reflections (for example, the reflection of the webcam''s
    *on* light). If the Raspberry Pi lies just behind the car''s front seats, the
    webcam cable should be able to reach the back window, while the power cable should
    still reach the USB adapter in the cigarette lighter receptacle. If not, use a
    longer USB to micro USB cable for the power and, if necessary, position the Pi
    farther back in the car. Alternatively, use a webcam with a longer cable. The
    following image shows the suggested positioning of the Pi:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00e2cfac-3746-40bc-9453-ddc9e96bc302.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the following image shows the suggested positioning of the camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3639c08b-2644-4899-baf9-d0e412b24505.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, it's time for the hard part—the display. For video output, the Pi supports
    HDMI (as found in new TVs and many new monitors). Some older Pi models also support
    composite RCA (as found in old TVs). For other common connectors, we can use an
    adapter, such as HDMI to DVI or HDMI to VGA. The Pi also has limited support (through
    third-party kernel extensions) for video output through DSI or SPI (as found in
    cellphone displays and prototyping kits).
  prefs: []
  type: TYPE_NORMAL
- en: Do not use a CRT television or monitor in a vehicle or in any environment where
    it is liable to be bumped. A CRT may implode if the glass is damaged. Instead,
    use an LCD television or monitor.
  prefs: []
  type: TYPE_NORMAL
- en: A small display is desirable because it can be more conveniently mounted on
    the dashboard and it consumes less power. For example, the SunFounder Raspberry
    Pi 10.1 HDMI IPS LCD Monitor requires a 12V, 1A power source. This display includes
    a USB port that can deliver 5V, 2A of power, which satisfies the recommended power
    specs for most Pi versions, including Raspberry Pi 2 Model B, but not quite Raspberry
    Pi 3 Model B. For more information, see the product's page on the SunFounder website, [https://www.sunfounder.com/10-1-inch-hdmi-lcd.html](https://www.sunfounder.com/10-1-inch-hdmi-lcd.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, though, a display needs a much higher voltage and wattage than the
    cigarette lighter can supply. Conveniently, some cars have an electrical outlet
    that resembles a wall socket, with the standard voltage for the type of socket
    but a lower maximum wattage. My car has a 110V, 150W, outlet for two-pronged North
    American plugs (NEMA 1-15P). As seen in the following image, I used an extension
    cord to convert the two-pronged connection into a three-pronged connection (NEMA
    5-15P) that my monitor cables use:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f690c8b-30ce-4df9-9e8a-0b4ce99aa708.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I tried plugging in three different monitors (one at a time, of course), with
    the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HP Pavilion 25xi (25", *1920 x 1080*)**: Does not turn on. Presumably requires
    a higher wattage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HP w2207 (22", *1680 x 1050*, 19.8 lbs)**: Does not turn on, but its weight
    and sturdy hinge make it useful as a flail to beat off hijackers—just in case
    the rocket launchers fail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Xplio XP22WD (22", *1440 x 900*)**: Turns on and works!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are unable to power a monitor from any of your car's outlets, an alternative
    is to use a battery block to power the monitor. Another alternative is to use
    a laptop or netbook as a substitute for the entire Pi-based system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The XP22WD''s ports are seen in the following image. To connect the Pi, I am
    using an HDMI to DVI cable because the monitor does not have an HDMI port:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/906fccc5-3bee-47a8-a032-4aa8cabd7b9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unfortunately, my monitors are too big to mount on a dashboard! However, for
    the purpose of testing the system on my driveway, placing the monitor in the passenger
    seat is fine, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b727ae1-5f32-4b70-af26-2bc582d4608c.png)'
  prefs: []
  type: TYPE_IMG
- en: Voilà! We've proved that a car can power a Pi, peripherals, and a desktop monitor!
    As soon as the car is turned on, our system boots and runs in exactly the same
    way as a Linux desktop. We can now launch `The Living Headlights` app from the
    command line, or from an IDE such as Geany. Our app's behavior on Pi should be
    identical to its behavior on a conventional desktop system, except that on Pi,
    we will experience a lower frame rate (less *frequent* frame updates) and greater
    lag (less *timely* frame updates). Raspberry Pi has relatively limited processing
    power; therefore, it will need more time to process each frame, and a greater
    number of camera frames will be dropped while the software processes an old frame.
  prefs: []
  type: TYPE_NORMAL
- en: Once you get your app running in a car, remember to recalibrate it so that it
    estimates distances based on the size of real headlights and not the size of a
    flashlight rig! The most practical way to perform this recalibration would be
    with two parked cars. One parked car should have its headlights on, and it should
    be behind the car that contains the Pi. Measure the distance between the parked
    cars, and use this as the calibration value.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter gave us the opportunity to scale down the complexity of our algorithms
    to support low-powered hardware. We also played with colorful lights, a homemade
    toy car, a puzzle of adapters, and a real car!
  prefs: []
  type: TYPE_NORMAL
- en: There is plenty of scope for extending the functionality of `The Living Headlights`.
    For example, we could take an average of multiple reference measurements or store
    different reference measurements for different colors of lights. We could analyze
    patterns of flashing, colored lights across multiple frames to judge whether the
    vehicle behind us is a police car or a road maintenance truck, or is even signaling
    to turn. We could try to detect the flash of rocket launchers, though testing
    might be problematic.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter's project is not something a driver should use, though! In
    the next chapter, we are going to take a pen-and-paper sketch in one hand and
    a smartphone in the other as we turn a geometric drawing into a physics simulation!
  prefs: []
  type: TYPE_NORMAL
