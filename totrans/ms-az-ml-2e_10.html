<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer206">
			<h1 id="_idParaDest-136"><em class="italic"><a id="_idTextAnchor135"/>Chapter 8</em>: Azure Machine Learning Pipelines</h1>
			<p>In the previous chapter, we learned about advanced preprocessing techniques, such as category embeddings and NLP, to extract semantic meaning from text features. In this chapter, you will learn how to use these preprocessing and transformation techniques to build reusable ML pipelines.</p>
			<p>First, you will understand the benefits of splitting your code into individual steps and wrapping those into a pipeline. Not only can you make your code blocks reusable through modularization and parameters, but you can also control the compute targets for individual steps. This helps to optimally scale your computations, save costs, and improve performance at the same time. Lastly, you can parameterize and trigger your pipelines through an HTTP endpoint or through a recurring or reactive schedule.</p>
			<p>Then, we will build a complex Azure Machine Learning pipeline in a couple of steps. We will start with a simple pipeline, add data inputs, outputs, and connections between the steps, and deploy the pipeline as a web service. You will also learn about advanced scheduling, based on the frequency and changing data, as well as how to parallelize pipeline steps for large data.</p>
			<p>In the last part, you will learn how to integrate Azure Machine Learning pipelines into other Azure services such as Azure Machine Learning designer, Azure Data Factory, and Azure DevOps. This will help you to understand the commonalities and differences between the different pipeline and workflow services and how you can trigger ML pipelines.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Using pipelines in ML workflows</li>
				<li>Building and publishing an ML pipeline</li>
				<li>Integrating pipelines with other Azure services</li>
			</ul>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor136"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create pipelines and pipeline steps:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0</strong></li>
				<li><strong class="source-inline">azureml-sdk 1.34.0</strong></li>
			</ul>
			<p>Similar to previous chapters, you can run this code using either a local Python interpreter or a notebook environment hosted in Azure Machine Learning. However, all scripts need to be scheduled to execute in Azure.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter08</a>.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor137"/>Using pipelines in ML workflows</h1>
			<p>Separating your <a id="_idIndexMarker1086"/>workflow into reusable configurable steps and combining these steps into an end-to-end pipeline provides many benefits for implementing end-to-end ML processes. Multiple teams can own and iterate on individual steps to<a id="_idIndexMarker1087"/> improve the pipeline over time, while others can easily integrate each version of the pipeline into their current setup.</p>
			<p>The pipeline itself doesn't only split code from execution; it also splits the execution from orchestration. Hence, you can configure individual compute targets that can be used to optimize your execution and provide parallel execution while you don't have to touch the ML code.</p>
			<p>We will take a quick look at Azure Machine Learning pipelines and why they are your tool of choice when implementing ML workflows in Azure. In the following section, <em class="italic">Building and publishing an ML pipeline</em>, we will dive a lot deeper and explore the individual features by building such a pipeline.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>Why build pipelines?</h2>
			<p>As a single developer doing <a id="_idIndexMarker1088"/>mostly experimentation and working simultaneously on data, infrastructure, and modeling, pipelines don't add a ton of benefits to the developer's workflow. However, as soon as you perform enterprise-grade development across multiple teams that iterate on different parts of the ML system, then you will greatly benefit from splitting your code into a pipeline of individual execution steps.</p>
			<p>This modularization will give you great flexibility, and multiple teams will be able to collaborate efficiently. Teams can integrate your models and pipelines while you are iterating and building new versions of your pipeline at the same time. By using versioned pipelines and pipeline parameters, you can control how your data or model service pipeline should be called and ensure auditing and reproducibility.</p>
			<p>Another important benefit of using workflows instead of running everything inside a single file is execution speed and cost improvements. Instead of running a single script on the same compute instance, you can run and scale the steps individually on different compute targets. This gives you greater control over potential cost savings and better optimization for performance, and you only ever have to retry the parts of the pipeline that failed and never the whole pipeline.</p>
			<p>Through scheduling pipelines, you can make sure that all your pipeline runs are executed without your manual intervention. You simply define triggers, such as the existence of new training data, that should execute your pipeline. Decoupling your code execution from triggering the execution gives you a ton of benefits, such as easy integration into many other services.</p>
			<p>Finally, the modularity of your code allows for great reusability. By splitting your script into functional steps such as cleaning, preprocessing, feature engineering, training, and hyperparameter tuning, you can version and reuse these steps for other projects as well.</p>
			<p>Therefore, as soon as you want to benefit from one of these advantages, you can start organizing your code in pipelines, which can be deployed, scheduled, versioned, scaled, and reused. Let's find out how you can achieve this in Azure Machine Learning.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/>What are Azure Machine Learning pipelines?</h2>
			<p><strong class="bold">Azure Machine Learning pipelines</strong> are workflows of executable steps in Azure Machine Learning that compose a<a id="_idIndexMarker1089"/> complete ML workflow. Hence, you can combine data import, data transformations, feature engineering, model training, optimization, and also deployment as your pipeline steps.</p>
			<p>Pipelines are resources in your Azure Machine Learning workspace that you can create, manage, version, trigger, and deploy. They integrate with all other Azure Machine Learning workspace resources such as datasets and datastores for loading data, compute instances, models, and endpoints. Each pipeline run is executed as an experiment on your Azure Machine Learning workspace and gives you the same benefits that we covered in the previous chapters, such as tracking files, logs, models, artifacts, and images while running on flexible compute clusters.</p>
			<p>Azure Machine Learning pipelines should be your first choice when implementing flexible and reusable ML workflows. By using pipelines, you can modularize your code into blocks of functionality and versions and share those blocks with other projects. This makes it easy to collaborate with other teams on complex end-to-end ML workflows.</p>
			<p>Another great integration of Azure Machine Learning pipelines is the integration with endpoints and triggers in your workspace. With a single line of code, you can publish a pipeline as a web service or web service endpoint and use this endpoint to configure and trigger the pipeline from anywhere. This opens the door for integrating Azure Machine Learning pipelines with many other Azure and third-party services.</p>
			<p>However, if you need a more complex trigger, such as continuous scheduling or reactive triggering based on changes in the source data, you can easily configure this as well. The added benefit of using pipelines is that all orchestration functionality is completely decoupled from your training code.</p>
			<p>As you can see, you get a lot of benefits by using Azure Machine Learning pipelines for your ML workflows. However, it's worth noting that this functionality does come with some extra overhead, namely wrapping each computation in a pipeline step, adding pipeline triggers, configuring environments and compute targets for each step, and exposing parameters as pipeline options. Let's start by building our first pipeline.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/>Building and publishing an ML pipeline</h1>
			<p>Let's go ahead and use all we have learned from the previous chapters and build a pipeline for data processing. We will use the Azure Machine Learning SDK for Python to define all the pipeline <a id="_idIndexMarker1090"/>steps as Python code so that it can be easily managed, reviewed, and checked into version control as an authoring script.</p>
			<p>We will define a pipeline as a linear sequence of pipeline steps. Each step will have an input and output defined as pipeline data sinks and sources. Each step will be associated with a compute target that defines both the execution environment as well as the compute resource for execution. We will set up an execution environment as a Docker container with all the required Python libraries and run the pipeline steps on a training cluster in Azure Machine Learning.</p>
			<p>A pipeline runs as an<a id="_idIndexMarker1091"/> experiment in your Azure Machine Learning workspace. We can either submit the pipeline as part of the authoring script, deploy it as a web service and hence trigger it through a webhook, schedule it as a published pipeline similar to cron jobs, or trigger it from a third-party service such as Logic Apps.</p>
			<p>In many cases, running a linear sequential pipeline is good enough. However, when the amount of data increases and pipeline steps become slower and slower, we need to find a way to speed up these large computations. A common solution for speeding up data transformations, model training, and scoring is through parallelization. Hence, we will add a parallel execution step to our data transformation pipeline.</p>
			<p>As we learned in the first section of this chapter, one of the main reasons for decoupling ML workflows into pipelines is modularity and reusability. By splitting a workflow into individual steps, we build the foundation for reusable computational blocks for common ML tasks, be it data analysis through visualizations and feature importance, feature engineering through NLP and third-party data, or simply the scoring of common ML tasks such as automatic image tagging through object detection.</p>
			<p>In Azure Machine Learning pipelines, we can use modules to create reusable computational steps from a pipeline. A module is a management layer on top of a pipeline step that allows you to version, deploy, load, and reuse pipeline steps with ease. The concept is very similar to to versioning source code or versioning datasets in ML projects.</p>
			<p>For any enterprise-grade ML workflow, the usage of pipelines is essential. Not only does it help you decouple, scale, trigger, and reuse individual computational steps, but it also provides auditability <a id="_idIndexMarker1092"/>and monitorability to your end-to-end workflow. On top, splitting computational blocks into pipeline steps will set <a id="_idIndexMarker1093"/>you up for a successful transition to MLOps – a <strong class="bold">Continuous Integration and Continuous Deployment (CI/CD)</strong> process for ML projects.</p>
			<p>Let's get started and implement our first Azure Machine Learning pipeline.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/>Creating a simple pipeline</h2>
			<p>An Azure Machine Learning pipeline is a <a id="_idIndexMarker1094"/>sequence of individual computational steps that can be executed in parallel or a series. Azure Machine Learning provides additional features on top of the pipeline, such as visualization of the computational graph, data transfer between steps, and publishing pipelines either as an endpoint or published pipeline. In this section, we will create a simple pipeline step and execute the pipeline to explore the Azure Machine Learning pipeline capabilities.</p>
			<p>Depending on the type of computation, you can schedule jobs on different compute targets such as Azure Machine Learning, Azure Batch, Databricks, Azure Synapse, and more, or run <em class="italic">automated ML</em> or <em class="italic">HyperDrive</em> experiments. Depending on the execution type, you need to provide additional configuration to each step.</p>
			<p>Let's start with a simple pipeline that consists only of a single step. We will incrementally add more functionality and steps in the subsequent sections. First, we need to define the type of execution for our pipeline step. While <strong class="source-inline">PipelineStep</strong> is the base class for any execution we can run in the pipeline, we need to choose one of the step implementations. The following steps are available at the time of writing:</p>
			<ul>
				<li><strong class="source-inline">AutoMLStep</strong>: Runs an automated ML experiment</li>
				<li><strong class="source-inline">AzureBatchStep</strong>: Runs a script on Azure Batch</li>
				<li><strong class="source-inline">DatabricksStep</strong>: Runs a Databricks notebook</li>
				<li><strong class="source-inline">DataTransferStep</strong>: Transfers data between Azure storage accounts</li>
				<li><strong class="source-inline">HyperDriveStep</strong>: Runs a HyperDrive experiment</li>
				<li><strong class="source-inline">ModuleStep</strong>: Runs a module</li>
				<li><strong class="source-inline">MpiStep</strong>: Runs <a id="_idIndexMarker1095"/>an <strong class="bold">Message Passing Interface (MPI)</strong> job</li>
				<li><strong class="source-inline">ParallelRunStep</strong>: Runs a script in parallel</li>
				<li><strong class="source-inline">PythonScriptStep</strong>: Runs a Python script</li>
				<li><strong class="source-inline">RScriptStep</strong>: Runs an R script</li>
				<li><strong class="source-inline">SynapseSparkStep</strong>: Runs a Spark script on Synapse</li>
				<li><strong class="source-inline">CommandStep</strong>: Runs a script or command</li>
				<li><strong class="source-inline">KustoStep</strong>: Runs a Kusto query on Azure Data Explorer</li>
			</ul>
			<p>For our simple example, we want to run a single Python data preprocessing script in our pipeline, so we'll choose <strong class="source-inline">PythonScriptStep</strong> from the preceding list. We are building on the same examples and code samples that we saw in the previous chapters. In this first pipeline, we will execute a single step that will load the data directly from the script – and hence doesn't require any input or output to the pipeline step. We will add these separately in the following steps:</p>
			<ol>
				<li>The pipeline steps are all attached to an Azure Machine Learning workspace. Hence, we start by loading the workspace configuration:<p class="source-code">from azureml.core import Workspace</p><p class="source-code">ws = Workspace.from_config()</p></li>
				<li>Next, we need a compute <a id="_idIndexMarker1096"/>target that we can execute our pipeline step on. Let's create an auto-scaling Azure Machine Learning training cluster as a compute target, similar to what we have created in previous chapters:<p class="source-code"># Create or get training cluster</p><p class="source-code">aml_cluster = get_aml_cluster(</p><p class="source-code">  ws, cluster_name="cpu-cluster")</p><p class="source-code">aml_cluster.wait_for_completion(show_output=True)</p></li>
				<li>In addition, we will need a run configuration that defines our training environment and Python libraries:<p class="source-code">run_conf = get_run_config(['numpy', 'pandas',</p><p class="source-code">  'scikit-learn', 'tensorflow'])</p></li>
				<li>We can now define <strong class="source-inline">PythonScriptStep</strong>, which provides all the required configuration and entry points for a target ML training script:<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p><p class="source-code">step = <strong class="bold">PythonScriptStep</strong>(name='Preprocessing',</p><p class="source-code">                        script_name="preprocess.py",</p><p class="source-code">                        source_directory="code",</p><p class="source-code">                        runconfig=run_conf,</p><p class="source-code">                        compute_target=aml_cluster)</p></li>
			</ol>
			<p>As you can see in the preceding code, we are configuring <strong class="source-inline">script_name</strong> and the <strong class="source-inline">source_directory</strong> parameter, which contain the preprocessing script. We also pass the <strong class="source-inline">runconfig</strong> runtime configuration and the <strong class="source-inline">compute_target</strong> compute target to <strong class="source-inline">PythonScriptStep</strong>.</p>
			<ol>
				<li value="5">If you recall from previous chapters, we previously submitted the <strong class="source-inline">ScriptRunConfig</strong> objects as <a id="_idIndexMarker1097"/>an experiment to the Azure Machine Learning workspace. In the case of pipelines, we first need to wrap the pipeline step in <strong class="source-inline">Pipeline</strong> and instead submit the pipeline as an experiment. While this seems counterintuitive at first, we will see how we can then parametrize the pipeline and add more computational steps to it. In the next code snippet, we define the pipeline:<p class="source-code">from azureml.pipeline.core import Pipeline</p><p class="source-code">pipeline = <strong class="bold">Pipeline</strong>(ws, steps=[step])</p></li>
			</ol>
			<p>As you can see, the pipeline is defined simply through a series of pipeline steps and linked to a workspace. In our example, we only define a single execution step. Let's also check that we didn't make any mistakes configuring our pipeline through the built-in pipeline validation:</p>
			<p class="source-code">pipeline.validate()</p>
			<ol>
				<li value="6">Once the pipeline is validated successfully, we are ready for execution. The pipeline can be executed by submitting it as an experiment to the Azure Machine Learning workspace:<p class="source-code">from azureml.core import Experiment</p><p class="source-code">exp = <strong class="bold">Experiment</strong>(ws, "azureml-pipeline")</p><p class="source-code">run = exp.submit(pipeline)</p></li>
			</ol>
			<p>Congratulations! You just ran your<a id="_idIndexMarker1098"/> first very simple Azure Machine Learning pipeline.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find many<a id="_idIndexMarker1099"/> complete and up-to-date examples for using Azure Machine Learning pipelines in the official Azure repository: <a href="https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines">https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines</a>.</p>
			<p>Once a pipeline is submitted, it is shown under the <strong class="bold">Pipelines</strong> section as well as under the <strong class="bold">Experiments</strong> section, as shown in <em class="italic">Figure 8.1</em>. A pipeline is treated as an experiment, where each pipeline run is like an experiment run. Each step of a pipeline, as well as its logs, figures, and metrics, can be accessed as a child run of the experiment:</p>
			<div>
				<div id="_idContainer198" class="IMG---Figure">
					<img src="image/B17928_08_01.jpg" alt="Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning " width="1650" height="1053"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – A pipeline run as an experiment in Azure Machine Learning</p>
			<p>While this simple pipeline <a id="_idIndexMarker1100"/>doesn't add a ton of benefits to directly submitting the script as an experiment, we can now add additional steps to the pipeline and configure data input and output. Let's take a look!</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/>Connecting data inputs and outputs between steps</h2>
			<p>Pipeline steps are computational blocks, whereas<a id="_idIndexMarker1101"/> the pipeline defines the sequence of step executions. In order to control the data flow, we need to define input and output for the pipeline as well as wire up data input and output for individual steps. The data flow between the computational blocks will ultimately define the execution order for the blocks, and hence turns a sequence of steps into a directed acyclic execution graph. This is exactly what we are going to explore in this section.</p>
			<p>In most cases, a pipeline needs external input, connections between the individual blocks, as well as persisted output. In Azure Machine Learning pipelines, we will use the following building blocks to configure this data flow:</p>
			<ul>
				<li>Pre-persisted pipeline input: <strong class="source-inline">Dataset</strong></li>
				<li>Data between pipeline steps: <strong class="source-inline">PipelineData</strong></li>
				<li>Persisting pipeline output: <strong class="source-inline">PipelineData.as_dataset()</strong></li>
			</ul>
			<p>In this section, we will look at all three types of data input and output. First, we will look at how we pass data as<a id="_idIndexMarker1102"/> input into a pipeline.</p>
			<h3>Input data to pipeline steps</h3>
			<p>Let's start with <a id="_idIndexMarker1103"/>adding a data input to the first step in a pipeline. To do so – or to pass any pre-persisted data to a pipeline step – we will use a <strong class="bold">dataset</strong>, which we saw previously in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>. In Azure Machine Learning, a dataset is an abstract reference for data stored in a specified path with specified encoding on a specified data storage system. The storage system itself is abstracted as a <strong class="bold">datastore</strong> object, a reference to the physical system with information about location, protocol, and access permissions.</p>
			<p>If you recall from the previous chapters, we can access a dataset that was previously registered in our Azure Machine Learning workspace by simply referencing it by name:</p>
			<p class="source-code">from azureml.core.dataset import Dataset</p>
			<p class="source-code">dataset = Dataset.get_by_name(ws, 'titanic')</p>
			<p>The preceding code is very convenient when your data was initially organized and registered as a dataset. As pipeline developers, we don't need to know the underlying data format (for example, CSV, ZIP, Parquet, and JSON) and on which Azure Blob storage or Azure SQL database the data is stored. Pipeline developers can consume the specified data and instead focus on pre-processing, feature engineering, and model training.</p>
			<p>However, when passing new data into an Azure Machine Learning pipeline, we often don't have the data registered as datasets. In these cases, we can create a new dataset reference. Here is an example of how to create <strong class="source-inline">Dataset</strong> from publicly available data:</p>
			<p class="source-code">path ='https://...windows.net/demo/Titanic.csv'</p>
			<p class="source-code">dataset = Dataset.Tabular.from_delimited_files(path)</p>
			<p>There are multiple ways to transform files and tabular data into <strong class="source-inline">Dataset</strong>. While this seems like a bit of complicated extra work instead of passing absolute paths to your pipelines directly, you will gain many benefits from following this convention. Most importantly, all compute instances in your Azure Machine Learning workspace will be able to access, read, and parse the data without any additional configuration. In addition, Azure Machine Learning will reference and track the dataset used for each experiment run.</p>
			<p>Once we have obtained a reference to <strong class="source-inline">Dataset</strong>, we can pass the dataset to the pipeline step as input. When <a id="_idIndexMarker1104"/>passing a dataset to the computational step, we can configure additional configurations such as the following:</p>
			<ul>
				<li>A name for the dataset reference in the script – <strong class="source-inline">as_named_input()</strong></li>
				<li>An access type for <strong class="source-inline">FileDataset</strong> – <strong class="source-inline">as_download()</strong> or <strong class="source-inline">as_mount()</strong></li>
			</ul>
			<p>First, we configure the tabular dataset as the named input:</p>
			<p class="source-code">from azureml.core.dataset import Dataset</p>
			<p class="source-code">dataset = Dataset.get_by_name(ws, 'titanic')</p>
			<p class="source-code">data_in = <strong class="bold">dataset.as_named_input('titanic')</strong></p>
			<p>Next, we will use <strong class="source-inline">PythonScriptStep</strong>, which will allow us to pass arguments to the pipeline step. We need to pass the dataset to two parameters – as an argument to the pipeline script and as an input dependency for the step. The former will allow us to pass the dataset to the Python script, whereas the latter will track the dataset as a dependency of this pipeline step:</p>
			<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p>
			<p class="source-code">step = PythonScriptStep(name='Preprocessing',</p>
			<p class="source-code">                        script_name="preprocess_input.py",</p>
			<p class="source-code">                        source_directory="code",</p>
			<p class="source-code">                        <strong class="bold">arguments=["--input", data_in],</strong></p>
			<p class="source-code">                        <strong class="bold">inputs=[data_in],</strong></p>
			<p class="source-code">                        runconfig=run_conf,</p>
			<p class="source-code">                        compute_target=aml_cluster)</p>
			<p>As you can see in the preceding example, we can pass one (or multiple) datasets to the pipeline step as the <strong class="source-inline">inputs</strong> parameter, as well as an argument to the script. Using a specific name for this dataset will help us to differentiate between multiple inputs in the pipeline. We will update the<a id="_idIndexMarker1105"/> preprocessing script to parse the dataset from the command-line arguments, as shown in the following snippet:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">preprocess_input.py</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">from azureml.core import Run, Dataset</p>
			<p class="source-code">run = Run.get_context()</p>
			<p class="source-code">ws = run.experiment.workspace</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument("--input", type=str)</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p class="source-code"><strong class="bold">dataset = Dataset.get_by_id(ws, id=args.input)</strong></p>
			<p class="source-code">df = dataset.to_pandas_dataframe()</p>
			<p>As you can see in the preceding code, the dataset gets passed as a dataset name to the Python script. We can use the <strong class="source-inline">Dataset</strong> API to retrieve the data at runtime.</p>
			<p>Once we submit the pipeline for execution, we can see the pipeline visualized in the Azure Machine Learning Studio interface, as shown in <em class="italic">Figure 8.2</em>. We can see that the dataset is passed as the <strong class="bold">titanic</strong> named input to the <strong class="bold">Preprocessing</strong> step:</p>
			<div>
				<div id="_idContainer199" class="IMG---Figure">
					<img src="image/B17928_08_02.jpg" alt="Figure 8.2 – The dataset as a pipeline step input " width="1653" height="1088"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – The dataset as a pipeline step input</p>
			<p>This is a great way to decouple a block of functionality from its input and helps you to build reusable blocks. We will see in the subsequent section, <em class="italic">Reusing pipeline steps through modularization</em>, how we<a id="_idIndexMarker1106"/> can turn these reusable blocks into shared modules.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Instead of passing datasets as input arguments to the pipeline step, we can also access named inputs from the run context using the following property on the run context object – <strong class="source-inline">Run.get_context().input_datasets['titanic']</strong>. However, setting up datasets as input and output arguments makes it easier to reuse pipeline steps and code snippets across pipelines and other experiments.</p>
			<p>Next, let's find out how to set up a data flow between individual pipeline steps.</p>
			<h3>Passing data between steps</h3>
			<p>When we define input to a<a id="_idIndexMarker1107"/> pipeline step, we often want to configure the output for the computations. By passing in input and output definitions, we separate the pipeline step from predefined data storage and avoid having to move data around as part of the computation step.</p>
			<p>While pre-persisted inputs were defined as <strong class="source-inline">Dataset</strong> objects, data connections (input and output) between pipeline steps are defined through <strong class="source-inline">PipelineData</strong> objects. Let's look at an example of a <strong class="source-inline">PipelineData</strong> object used as output for one pipeline step and input for another step:</p>
			<p class="source-code">from azureml.core import Datastore</p>
			<p class="source-code">from azureml.pipeline.core import PipelineData</p>
			<p class="source-code">datastore = Datastore.get(ws, datastore_name="mldata")</p>
			<p class="source-code">data_train = <strong class="bold">PipelineData</strong>('train', datastore=datastore)</p>
			<p class="source-code">data_test = <strong class="bold">PipelineData</strong>('test', datastore=datastore)</p>
			<p>Similar to the previous example, we pass the dataset as arguments and reference them as <strong class="source-inline">outputs</strong>. The former will allow us to retrieve the dataset in the script, whereas the latter defines the step<a id="_idIndexMarker1108"/> dependencies:</p>
			<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p>
			<p class="source-code">step_1 = PythonScriptStep(name='Preprocessing',</p>
			<p class="source-code">                          script_name= \</p>
			<p class="source-code">                            "preprocess_output.py",</p>
			<p class="source-code">                          source_directory="code",</p>
			<p class="source-code">                          arguments=[</p>
			<p class="source-code">                              "--input", data_in,</p>
			<p class="source-code">                              <strong class="bold">"--out-train", data_train</strong>,</p>
			<p class="source-code">                              <strong class="bold">"--out-test", data_test</strong>],</p>
			<p class="source-code">                          inputs=[data_in],</p>
			<p class="source-code">                          <strong class="bold">outputs=[data_train, data_test],</strong></p>
			<p class="source-code">                          runconfig=run_conf,</p>
			<p class="source-code">                          compute_target=aml_cluster)</p>
			<p>Once we pass the expected output path to the scoring file, we need to parse the command-line arguments to retrieve the path. The scoring file looks like the following snippet in order to read the output<a id="_idIndexMarker1109"/> path and output a pandas DataFrame to the desired output location. We first need to parse the command-line arguments in the training script:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">preprocess_output.py</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument("--input", type=str)</p>
			<p class="source-code">parser.add_argument("--out-train", type=str)</p>
			<p class="source-code">parser.add_argument("--out-test", type=str)</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p>The <strong class="source-inline">PipelineData</strong> arguments get interpolated at runtime and replaced with the local path for the mounted dataset directory. Therefore, we can simply write the data to this local directory, and it will be automatically registered in the dataset:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">preprocess_output.py</p>
			<p class="source-code">import os</p>
			<p class="source-code">out_train = args.out_train</p>
			<p class="source-code">os.makedirs(os.path.dirname(out_train), exist_ok=True)</p>
			<p class="source-code">out_test = args.out_test</p>
			<p class="source-code">os.makedirs(os.path.dirname(out_test), exist_ok=True)</p>
			<p class="source-code">df_train, df_test = preprocess(...)</p>
			<p class="source-code"><strong class="bold">df_train.to_csv(out_train)</strong></p>
			<p class="source-code"><strong class="bold">df_test.to_csv(out_test)</strong></p>
			<p>Once we output data to a <strong class="source-inline">PipelineData</strong> dataset, we can pass these datasets to the next pipeline step. Passing the datasets works exactly the same as we saw in the previous section – we pass them as arguments and register them as <strong class="source-inline">inputs</strong>:</p>
			<p class="source-code">from azureml.pipeline.steps import PythonScriptStep</p>
			<p class="source-code">step_2 = PythonScriptStep(name='Training',</p>
			<p class="source-code">                          script_name="train.py",</p>
			<p class="source-code">                          source_directory="code",</p>
			<p class="source-code">                          arguments=[</p>
			<p class="source-code">                              <strong class="bold">"--in-train", data_train</strong>,</p>
			<p class="source-code">                              <strong class="bold">"--in-test", data_test</strong>],</p>
			<p class="source-code">                          <strong class="bold">inputs=[data_train, data_test],</strong></p>
			<p class="source-code">                          runconfig=run_conf,</p>
			<p class="source-code">                          compute_target=aml_cluster)</p>
			<p>Now, we can load the data in the<a id="_idIndexMarker1110"/> training script. If you remember from the previous step, <strong class="source-inline">PipelineData</strong> is interpolated as paths on the local execution environment. Hence, we can read the data from the path that got interpolated in the command-line arguments:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">train.py</p>
			<p class="source-code">import argparse</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument("--in-train", type=str)</p>
			<p class="source-code">parser.add_argument("--in-test", type=str)</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p class="source-code">...</p>
			<p class="source-code">df_train = <strong class="bold">pd.read_csv(args.in_train)</strong></p>
			<p class="source-code">df_test = <strong class="bold">pd.read_csv(args.in_test)</strong></p>
			<p>Finally, we can wrap both steps as a <strong class="source-inline">Pipeline</strong> object by passing the steps using the pipeline <strong class="source-inline">steps</strong> keyword. The <strong class="source-inline">pipeline</strong> object can be passed as an experiment to Azure Machine Learning:</p>
			<p class="source-code">from azureml.pipeline.core import Pipeline</p>
			<p class="source-code">pipeline = Pipeline(ws, <strong class="bold">steps=[step_1, step_2]</strong>)</p>
			<p>As we can see in the previous example, we can read the output path from the command-line arguments and<a id="_idIndexMarker1111"/> use it in the Python script as a standard file path. Hence, we need to make sure that the file path exists and output some tabular data into the location. Next, we define the input for the second validation step that reads the newly created data:</p>
			<div>
				<div id="_idContainer200" class="IMG---Figure">
					<img src="image/B17928_08_03.jpg" alt="Figure 8.3 – Passing data between pipeline steps " width="1651" height="1023"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Passing data between pipeline steps</p>
			<p>Finally, we will take a look at how to persist the output of a pipeline step for usage outside of the pipeline.</p>
			<h3>Persisting data outputs</h3>
			<p>In this last section, we will learn <a id="_idIndexMarker1112"/>how to persist the output data of a pipeline. A common task for pipelines is building data transformations – and hence we often expect pipelines to output data.</p>
			<p>In the previous section, we learned about creating outputs from pipeline steps with <strong class="source-inline">PipelineData</strong>, mainly to connect these outputs to inputs of subsequent steps. We can use the same method to define a final persisted output of a pipeline.</p>
			<p>Doing so is very simple once you understood how to create, persist, and version datasets. The reason for this is that we can convert a <strong class="source-inline">PipelineData</strong> object into a dataset using the <strong class="source-inline">as_dataset()</strong> method. Once we have a reference to the <strong class="source-inline">Dataset</strong> object, we can go ahead and either export it to a specific datastore or register it as a dataset in the workspace.</p>
			<p>Here is a snippet of how to convert a <strong class="source-inline">PipelineData</strong> object defined as output in a pipeline step to a dataset and register it in the Azure Machine Learning workspace:</p>
			<p class="source-code">from azureml.data import OutputFileDatasetConfig</p>
			<p class="source-code">data_out = OutputFileDatasetConfig(name="predictions", </p>
			<p class="source-code">  destination=(datastore, 'titanic/predictions')) </p>
			<p>By calling the preceding authoring code, you will be able to access the resulting predictions as a dataset in any compute instance connected with your workspace:</p>
			<div>
				<div id="_idContainer201" class="IMG---Figure">
					<img src="image/B17928_08_04.jpg" alt="Figure 8.4 – A dataset as a pipeline step output " width="1647" height="1104"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – A dataset as a pipeline step output</p>
			<p>Next, we will take a look at the <a id="_idIndexMarker1113"/>different ways to trigger a pipeline execution.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Publishing, triggering, and scheduling a pipeline</h2>
			<p>After you have created your first simple pipeline, you have multiple ways of running the pipeline. One example that we already saw was submitting the pipeline as an experiment to Azure Machine Learning. This would simply execute the pipeline from the same authoring script where the pipeline was configured. While this is a good start at first to execute a pipeline, there are other ways to trigger, parametrize, and execute it.</p>
			<p>Common ways to execute a<a id="_idIndexMarker1114"/> pipeline are the following:</p>
			<ul>
				<li>Publish the pipeline as a web service.</li>
				<li>Trigger the published pipeline using a webhook.</li>
				<li>Schedule the pipeline to run periodically.</li>
			</ul>
			<p>In this section, we will look at all three methods to help you trigger and execute your pipelines with ease. Let's first start by publishing and versioning your pipeline as a web service.</p>
			<h3>Publishing a pipeline as a web service</h3>
			<p>A common reason to split an <a id="_idIndexMarker1115"/>ML workflow into a reusable pipeline is that you can parametrize and trigger it for various tasks whenever needed. Good examples are common pre-processing tasks, feature engineering steps, and batch scoring.</p>
			<p>Hence, turning a pipeline into a parametrizable web service that we can trigger from any other application is a great way of deploying our ML workflow. Let's get started and wrap and deploy the previously built pipeline as a web service.</p>
			<p>As we want our published pipeline to be configurable through HTTP parameters, we need to first create these parameter references. Let's create a parameter to control the learning rate of our training pipeline:</p>
			<p class="source-code">from azureml.pipeline.core.graph import PipelineParameter</p>
			<p class="source-code">lr_param = PipelineParameter(name="lr_arg",</p>
			<p class="source-code">                             default_value=0.01)</p>
			<p>Next, we link the pipeline parameter with the pipeline step by passing it as an argument to the training script. We extend the step from the previous section:</p>
			<p class="source-code">data = mnist_dataset.as_named_input('mnist').as_mount()</p>
			<p class="source-code">args = ["--in-train", data, "--learning-rate", lr_param]</p>
			<p class="source-code">step = PythonScriptStep(name='Training',</p>
			<p class="source-code">	script_name="train.py",</p>
			<p class="source-code">	source_directory="code",</p>
			<p class="source-code">	arguments=args,</p>
			<p class="source-code">	inputs=[data_train],</p>
			<p class="source-code">	runconfig=run_conf,</p>
			<p class="source-code">	compute_target=aml_cluster)</p>
			<p class="source-code">                     arguments=args ,</p>
			<p class="source-code">                     estimator=estimator,</p>
			<p class="source-code">                     compute_target=cpu_cluster)</p>
			<p>In the preceding example, we added the learning rate as a parameter to the list of command-line arguments. In the training script, we can parse the command-line arguments and read the parameter:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">score.py</p>
			<p class="source-code">parser = argparse.ArgumentParser()</p>
			<p class="source-code">parser.add_argument('--learning-rate', type=float, </p>
			<p class="source-code">  dest='lr')</p>
			<p class="source-code">args = parser.parse_args()</p>
			<p class="source-code"># print learning rate </p>
			<p class="source-code">print(args.lr)</p>
			<p>Now, the only step left is to<a id="_idIndexMarker1116"/> publish the pipeline. To do so, we create a pipeline and call the <strong class="source-inline">publish()</strong> method. We need to pass a name and version to the pipeline, which will now be a versioned published pipeline:</p>
			<p class="source-code">pipeline = Pipeline(ws, steps=[step])</p>
			<p class="source-code">service = pipeline.publish(name="CNN_Train_Service",</p>
			<p class="source-code">                           version="1.0")</p>
			<p class="source-code">service_id = service.id</p>
			<p class="source-code">service_endpoint = service.endpoint</p>
			<p>That's all the code you need to expose a pipeline as a parametrized web service with authentication. If you want to abstract your published pipeline from a specific endpoint – for example, to iterate on the development process of your pipeline while letting other teams integrate the web service into their application – you can also deploy pipeline webhooks as endpoints.</p>
			<p>Let's look at an example where we take the previously created pipeline service and expose it through a separate endpoint:</p>
			<p class="source-code">from azureml.pipeline.core import PipelineEndpoint</p>
			<p class="source-code">application = PipelineEndpoint.publish(ws,</p>
			<p class="source-code">  pipeline=service,</p>
			<p class="source-code">  name="CNN_Train_Endpoint")</p>
			<p class="source-code">service_id = application.id</p>
			<p class="source-code">service_endpoint = application.endpoint</p>
			<p>We have deployed and <a id="_idIndexMarker1117"/>decoupled the pipeline and the pipeline endpoint. We can finally call and trigger the endpoint through the service endpoint. Let's look at this in the next section.</p>
			<h3>Triggering a published pipeline with a webhook</h3>
			<p>The published pipeline<a id="_idIndexMarker1118"/> web service requires<a id="_idIndexMarker1119"/> authentication. Hence, let's first retrieve an Azure Active Directory token before we call the web service:</p>
			<p class="source-code">from azureml.core.authentication import AzureCliAuthentication</p>
			<p class="source-code">cli_auth = AzureCliAuthentication()</p>
			<p class="source-code">aad_token = cli_auth.get_authentication_header()</p>
			<p>Using the authentication token, we can now trigger and parametrize the pipeline by calling the service endpoint. Let's look at an example using the <strong class="source-inline">requests</strong> library. We can configure the learning rate through the <strong class="source-inline">lr_arg</strong> parameter defined in the previous section as well as the experiment name by sending a custom JSON body. If you recall, the pipeline will still run as an experiment in your Azure Machine Learning workspace:</p>
			<p class="source-code">import requests</p>
			<p class="source-code">response = requests.post(service_endpoint,</p>
			<p class="source-code">  headers=aad_token,</p>
			<p class="source-code">  json={"ExperimentName": "mnist-train",</p>
			<p class="source-code">        "ParameterAssignments": {"lr_arg": 0.05}})</p>
			<p>We can see in the preceding code snippet that we call the pipeline webhook using a <strong class="source-inline">POST</strong> request and configure the pipeline run by sending a custom JSON body. For authentication, we also need to pass the authentication as an HTTP header.</p>
			<p>In this example, we used a Python script to trigger the web service endpoint. However, you can use any other<a id="_idIndexMarker1120"/> Azure service for triggering this pipeline <a id="_idIndexMarker1121"/>now through the webhook, such as Azure Logic Apps, CI/CD pipelines in Azure DevOps, or any other custom application. If you'd prefer your pipeline to run periodically instead of triggering it manually, you can set up a pipeline schedule. Let's take a look at this in the next section.</p>
			<h3>Scheduling a published pipeline</h3>
			<p>Setting up continuous<a id="_idIndexMarker1122"/> triggers for workflows is a common use case when building pipelines. These triggers can run a pipeline and retrain a model every week or every day if new data is available. Azure Machine Learning pipelines support two types of scheduling techniques – continuous scheduling through a pre-defined frequency, and reactive scheduling and data change detection through a polling interval. In this section, we will take a look at both approaches.</p>
			<p>Before we start scheduling a pipeline, we will first explore a way to list all the previously defined pipelines of a workspace. To do so, we can use the <strong class="source-inline">PublishedPipeline.list()</strong> method, similar to the <strong class="source-inline">list()</strong> method from our Azure Machine Learning workspace resources. Let's print the name and ID of every published pipeline in the workspace:</p>
			<p class="source-code">from azureml.pipeline.core import PublishedPipeline</p>
			<p class="source-code">for pipeline in PublishedPipeline.list(ws):</p>
			<p class="source-code">  print("name: %s, id: %s" % (pipeline.name, pipeline.id))</p>
			<p>To set up a schedule for a published pipeline, we need to pass the pipeline ID as an argument. We can retrieve the desired pipeline ID from the preceding code snippet and plug it into the schedule declaration.</p>
			<p>First, we will look at continuous schedules that re-trigger a pipeline with a predefined frequency, similar to cron jobs. To define the scheduling frequency, we need to create a <strong class="source-inline">ScheduleRecurrence</strong> object. Here is an example snippet to create a recurring schedule:</p>
			<p class="source-code">from azureml.pipeline.core.schedule import \</p>
			<p class="source-code">  ScheduleRecurrence, Schedule</p>
			<p class="source-code">recurrence = ScheduleRecurrence(frequency="Minute", </p>
			<p class="source-code">                                interval=15)</p>
			<p class="source-code">schedule = Schedule.create(ws, </p>
			<p class="source-code">                           name="CNN_Train_Schedule", </p>
			<p class="source-code">                           pipeline_id=pipeline_id,</p>
			<p class="source-code">                           experiment_name="mnist-train", </p>
			<p class="source-code">                           recurrence=recurrence, </p>
			<p class="source-code">                           pipeline_parameters={})</p>
			<p>The preceding code is all you need to set up a recurring schedule that continuously triggers your pipeline. The pipeline will run as the defined experiment in your Azure Machine Learning workspace. Using the <strong class="source-inline">pipeline_parameters</strong> argument, you can pass additional parameters to the pipeline runs.</p>
			<p>Azure Machine Learning pipelines also support another type of recurring scheduling, namely polling for<a id="_idIndexMarker1123"/> changes in a datastore. This type of schedule is referred to as a<a id="_idIndexMarker1124"/> reactive schedule and requires a connection to a datastore. It will trigger your pipeline whenever data changes in your datastore. Here is an example of setting up a reactive schedule:</p>
			<p class="source-code">from azureml.core.datastore import Datastore</p>
			<p class="source-code"># use default datastore 'ws.get_default_datastore()'</p>
			<p class="source-code"># or load a custom registered datastore</p>
			<p class="source-code">datastore = Datastore.get(workspace, 'mldemodatastore')</p>
			<p class="source-code"># 5 min polling interval</p>
			<p class="source-code">polling_interval = 5</p>
			<p class="source-code">schedule = Schedule.create(</p>
			<p class="source-code">    ws, name="CNN_Train_OnChange", </p>
			<p class="source-code">    pipeline_id=pipeline_id,</p>
			<p class="source-code">    experiment_name="mnist-train",</p>
			<p class="source-code">    datastore=datastore,</p>
			<p class="source-code">    data_path_parameter_name="mnist"</p>
			<p class="source-code">    polling_interval=polling_interval,</p>
			<p class="source-code">    pipeline_parameters={})</p>
			<p>As you can see in this example, we set up the reactive schedule using a datastore reference and a polling interval in minutes. Hence, the schedule will check each polling interval to see which <a id="_idIndexMarker1125"/>blobs have changed, if any and use those to trigger the pipeline. The blob names will be passed to the pipeline using the <strong class="source-inline">data_path_parameter_name</strong> parameter. Similar to the previous schedule, you can also send additional parameters to the pipeline using the <strong class="source-inline">pipeline_parameters</strong> argument.</p>
			<p>Finally, let's take a look at how to programmatically stop a schedule once it is enabled. To do so, we need a reference to the schedule object. We can get this, similar to any other resource in Azure Machine Learning, by fetching the schedules for a specific workplace:</p>
			<p class="source-code">for schedule in Schedule.list(ws):</p>
			<p class="source-code">  print(schedule.id)</p>
			<p>We can filter this list using all the available attributes on the schedule object. Once we have found the desired schedule, we can simply disable it:</p>
			<p class="source-code">schedule.disable(wait_for_provisioning=True)</p>
			<p>Using the additional <strong class="source-inline">wait_for_provisioning</strong> argument, we ensure that we block code execution until the schedule is really disabled. You can easily re-enable the schedule using the <strong class="source-inline">Schedule.enable</strong> method. Now, you can create recurring and reactive schedules, continuously <a id="_idIndexMarker1126"/>run your Azure Machine Learning pipelines, and disable them if not needed anymore. Next, we will take a look at parallelizing execution steps.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Parallelizing steps to speed up large pipelines</h2>
			<p>It's inevitable in many <a id="_idIndexMarker1127"/>cases that the pipeline will process more and more data over time. In order to parallelize a pipeline, you can run pipeline steps in parallel or sequence, or parallelize a single pipeline step computation by using <strong class="source-inline">ParallelRunConfig</strong> and <strong class="source-inline">ParallelRunStep</strong>.</p>
			<p>Before we jump into parallelizing a single step execution, let's first discuss the control flow of a simple pipeline. We will start with a simple pipeline that is constructed using multiple steps, as shown in the following example:</p>
			<p class="source-code">pipeline = Pipeline(ws, steps=[step1, step2, step3, step4])</p>
			<p>When we submit this pipeline, how will these four steps be executed – in series, in parallel, or even in an undefined order? In order to answer the question, we need to look at the definitions of the individual steps. If all steps are independent and the compute target for each step is large enough, all steps are executed in parallel. However, if we define <strong class="source-inline">PipelineData</strong> as the output of <strong class="source-inline">step1</strong> and input it into the other steps, these steps will only be executed after <strong class="source-inline">step1</strong> has finished:</p>
			<div>
				<div id="_idContainer202" class="IMG---Figure">
					<img src="image/B17928_08_05.jpg" alt="Figure 8.5 – A pipeline with parallel steps " width="1650" height="903"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – A pipeline with parallel steps</p>
			<p>The data connections between the pipeline steps implicitly define the execution order of the steps. If no dependencies exist between the steps, all steps are scheduled in parallel.</p>
			<p>There is one exception to<a id="_idIndexMarker1128"/> the preceding statement, which is enforcing a specific execution order of pipeline steps without a dedicated data object as a dependency. In order to do this, you can define these dependencies manually, as shown in the next code snippet:</p>
			<p class="source-code">step3.run_after(step2)</p>
			<p class="source-code">step4.run_after(step3)</p>
			<p>The preceding configuration will first execute <strong class="source-inline">step1</strong> and <strong class="source-inline">step2</strong> in parallel before scheduling <strong class="source-inline">step3</strong>, thanks to your explicitly configured dependencies. This can be useful when you are accessing state or data in resources outside of the Azure Machine Learning workspace; hence, the pipeline cannot implicitly create a dependency:</p>
			<div>
				<div id="_idContainer203" class="IMG---Figure">
					<img src="image/B17928_08_06.jpg" alt="Figure 8.6 – A pipeline with a custom step order " width="1639" height="1174"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – A pipeline with a custom step order</p>
			<p>Once we have answered the question of step execution order, we want to learn how we can execute a single step in parallel rather than multiple steps. A great use case for this is batch scoring a large amount of data. Rather than partitioning your input data as input for multiple steps, you want the data as input for a single step. However, to speed up the scoring process, you<a id="_idIndexMarker1129"/> want a parallel execution of the scoring for the single step.</p>
			<p>In Azure Machine Learning pipelines, you can use a <strong class="source-inline">ParallelRunStep</strong> step to configure a parallel execution for a single step. To configure the data partitions and parallelization of the computation, you need to create a <strong class="source-inline">ParallelRunConfig</strong> object. The parallel run step is a great choice for any type of parallelized computation that helps us to split the input data into smaller partitions (also called batches or mini-batches) of data. Let's walk through an example for setting up parallel execution for a single pipeline step. We will configure both batch sizes as a pipeline parameter that can be set when calling the pipeline step:</p>
			<p class="source-code">from azureml.pipeline.core import PipelineParameter</p>
			<p class="source-code">from azureml.pipeline.steps import ParallelRunConfig</p>
			<p class="source-code">parallel_run_config = ParallelRunConfig(</p>
			<p class="source-code">  entry_script='score.py',</p>
			<p class="source-code">  mini_batch_size=PipelineParameter(</p>
			<p class="source-code">    name="batch_size", </p>
			<p class="source-code">    default_value="10"),</p>
			<p class="source-code">  output_action="append_row",</p>
			<p class="source-code">  append_row_file_name="parallel_run_step.txt",</p>
			<p class="source-code">  environment=batch_env,</p>
			<p class="source-code">  compute_target=cpu_cluster,</p>
			<p class="source-code">  process_count_per_node=2,</p>
			<p class="source-code">  node_count=2)</p>
			<p>The preceding snippet defines the run configuration for parallelizing the computation by splitting the input into mini-batches. We configure the batch size as a pipeline parameter, <strong class="source-inline">batch_size</strong>. We also configure the compute target and parallelism by the <strong class="source-inline">node_count</strong> and <strong class="source-inline">process_count_per_node</strong> parameters. Using these settings, we can score four mini-batches in parallel.</p>
			<p>The <strong class="source-inline">score.py</strong> script is a <a id="_idIndexMarker1130"/>deployment file that needs to contain an <strong class="source-inline">init()</strong> and <strong class="source-inline">run(batch)</strong> method. The <strong class="source-inline">batch</strong> argument contains a list of filenames that will get extracted from the input argument of the step configuration. We will learn more about this file structure in <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>.</p>
			<p>The <strong class="source-inline">run</strong> method in the <strong class="source-inline">score.py</strong> script should return the scoring results or write the data to an external datastore. Depending on this, the <strong class="source-inline">output_action</strong> argument needs to be set to either <strong class="source-inline">append_row</strong>, which means that all values will be collected as run in a result file, or <strong class="source-inline">summary_only</strong>, which means that the user will deal with storing the results. You can define the result file in which all the rows will get appended using the <strong class="source-inline">append_row_file_name</strong> argument.</p>
			<p>As you can see, setting up the run configuration for a parallel batch execution is not very simple and requires a bit of fiddling. However, once set up and configured properly, it can be used to scale out a computational step and run many tasks in parallel. Hence, we can now define <strong class="source-inline">ParallelRunStep</strong> with all required input and output:</p>
			<p class="source-code">from azureml.pipeline.steps import ParallelRunStep</p>
			<p class="source-code">from azureml.core.dataset import Dataset</p>
			<p class="source-code">parallelrun_step = ParallelRunStep(</p>
			<p class="source-code">  name="ScoreParallel",</p>
			<p class="source-code">  parallel_run_config=parallel_run_config,</p>
			<p class="source-code">  inputs=[Dataset.get_by_name(ws, 'mnist')],</p>
			<p class="source-code">  output=PipelineData('mnist_results', </p>
			<p class="source-code">                      datastore=datastore),</p>
			<p class="source-code">  allow_reuse=True)</p>
			<p>As you can see, we read from the input dataset that references all files on the datastore. We write the results to the <strong class="source-inline">mnist_results</strong> folder in our custom datastore. Finally, we can start the<a id="_idIndexMarker1131"/> run and look at the results. To do so, we submit the pipeline as an experiment run to Azure Machine Learning:</p>
			<p class="source-code">from azureml.pipeline.core import Pipeline</p>
			<p class="source-code">pipeline = Pipeline(workspace=ws, steps=[parallelrun_step])</p>
			<p class="source-code">run = exp.submit(pipeline)</p>
			<p>Splitting a step execution into multiple partitions will help you to speed up the computation of large amounts of data. It pays off as soon as the time of computation is significantly longer than the overhead of scheduling a step execution on a compute target. Therefore, <strong class="source-inline">ParallelRunStep</strong> is a great choice for speeding up your pipeline, with only a few changes in your pipeline configuration required. Next, we will take a look into better modularization and the reusability of pipeline steps.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>Reusing pipeline steps through modularization</h2>
			<p>By splitting your<a id="_idIndexMarker1132"/> workflow into pipeline steps, you are laying the foundation for reusable ML and data processing building blocks. However, instead of copying and pasting your pipelines, pipeline steps, and code into other projects, you might want to abstract your functionality into functional high-level modules.</p>
			<p>Let's look at an example. Suppose you are building a pipeline step that takes in a dataset of user and item ratings and outputs a recommendation of the top five items for each user. However, while you are fine-tuning the recommendation engine, you want to enable your colleagues to integrate the functionality into their pipeline. A great way would be to separate the implementation and usage of the code, define the input and output data formats, and modularize and version it. That's exactly what modules do in the scope of the Azure Machine Learning pipeline steps.</p>
			<p>Let's create a module, the container that will hold a reference to the computational step:</p>
			<p class="source-code">from azureml.pipeline.core.module import Module</p>
			<p class="source-code">module = Module.create(ws,</p>
			<p class="source-code">                       name="TopItemRecommender",</p>
			<p class="source-code">                       description="Recommend top 5 items")</p>
			<p>Next, we define input and output for the module using the <strong class="source-inline">InputPortDef</strong> and <strong class="source-inline">OutputPortDef</strong> bindings. These are<a id="_idIndexMarker1133"/> input and output references that later need to be bound to data references. We use these bindings to abstract all of our input and output:</p>
			<p class="source-code">from azureml.pipeline.core.graph import \</p>
			<p class="source-code">  InputPortDef, OutputPortDef</p>
			<p class="source-code">in1 = InputPortDef(name="in1",</p>
			<p class="source-code">                   default_datastore_mode="mount", </p>
			<p class="source-code">                   default_data_reference_name = \</p>
			<p class="source-code">                       datastore.name,</p>
			<p class="source-code">                   label="Ratings")</p>
			<p class="source-code">out1 = OutputPortDef(name="out1",</p>
			<p class="source-code">                     default_datastore_mode="mount", </p>
			<p class="source-code">                     default_datastore_name=datastore.name,</p>
			<p class="source-code">                     label="Recommendation")</p>
			<p>Finally, we can define the module functionality by publishing a Python script for this module:</p>
			<p class="source-code">module.publish_python_script("train.py",</p>
			<p class="source-code">                             source_directory="./rec",</p>
			<p class="source-code">                             params={"numTraits": 5},</p>
			<p class="source-code">                             inputs=[in1],</p>
			<p class="source-code">                             outputs=[out1],</p>
			<p class="source-code">                             version="1",</p>
			<p class="source-code">                             is_default=True)</p>
			<p>That's all you need to do to enable others to reuse your recommendation block in their Azure Machine Learning pipelines. By using versioning and default versions, you can ensure exactly which code is pulled by your users. As we can see, you can define multiple inputs and outputs for each module and define configurable parameters for this module. In addition to <a id="_idIndexMarker1134"/>publishing functionality as Python code, we can also publish an Azure Data Lake Analytics or Azure batch step.</p>
			<p>Next, we will take a look at how the module can be integrated into an Azure Machine Learning pipeline and executed together with custom steps. To do so, we will first load the module that was previously created using the following command:</p>
			<p class="source-code">from azureml.pipeline.core.module import Module</p>
			<p class="source-code">module = Module.get(ws, name="TopItemRecommender")</p>
			<p>Now, the great thing about this is that the preceding code will work in any Python interpreter or execution engine that has access to your Azure Machine Learning workspace. This is huge – no copying of code, no need for checking out dependencies, and no need for defining any additional access permissions for your application – everything is integrated with<a id="_idIndexMarker1135"/> your workspace.</p>
			<p>First, we need to write up the input and output for this pipeline step. Let's pass the input from the pipeline directly to the recommendation module and output everything to the pipeline output:</p>
			<p class="source-code">from azureml.pipeline.core import PipelineData</p>
			<p class="source-code">in1 = PipelineData("in1",</p>
			<p class="source-code">                   datastore=datastore, </p>
			<p class="source-code">                   output_mode="mount", </p>
			<p class="source-code">                   is_directory=False)</p>
			<p class="source-code">out1 = PipelineData("out1",</p>
			<p class="source-code">                    datastore=datastore, </p>
			<p class="source-code">                    output_mode="mount", </p>
			<p class="source-code">                    is_directory=False)</p>
			<p class="source-code">input_wiring = {"in1": in1}</p>
			<p class="source-code">output_wiring = {"out1": out1}</p>
			<p>Now, we parametrize the module with the use of pipeline parameters. This lets us configure a parameter in the pipeline that we can pass through to the recommendation module. In addition, we can define a default parameter for the parameter when used in this pipeline:</p>
			<p class="source-code">from azureml.pipeline.core import PipelineParameter</p>
			<p class="source-code">num_traits = PipelineParameter(name="numTraits",</p>
			<p class="source-code">                               default_value=5)</p>
			<p>We already defined the input and output for this pipeline as well as the input parameters for the pipeline<a id="_idIndexMarker1136"/> step. The only thing we are missing is bringing everything together and defining a pipeline step. Similar to the previous section, we can define a pipeline step that will execute the modularized recommendation block. To do so, instead of <strong class="source-inline">PythonScriptStep</strong>, we now use <strong class="source-inline">ModuleStep</strong>:</p>
			<p class="source-code">from azureml.core import RunConfiguration</p>
			<p class="source-code">from azureml.pipeline.steps import ModuleStep</p>
			<p class="source-code">step = ModuleStep(module= module,</p>
			<p class="source-code">                  version="1",</p>
			<p class="source-code">                  runconfig=RunConfiguration(),</p>
			<p class="source-code">                  compute_target=aml_compute,</p>
			<p class="source-code">                  inputs_map=input_wiring,</p>
			<p class="source-code">                  outputs_map=output_wiring,</p>
			<p class="source-code">                  arguments=[</p>
			<p class="source-code">                    "--output_sum", first_sum,</p>
			<p class="source-code">                    "--output_product", first_prod,</p>
			<p class="source-code">                    "--num-traits", num_traits])</p>
			<p>Finally, we can execute the pipeline by submitting it as an experiment to our Azure Machine Learning workspace. This code is very similar to what we saw already in the previous section:</p>
			<p class="source-code">from azureml.core import Experiment</p>
			<p class="source-code">from azureml.pipeline.core import Pipeline</p>
			<p class="source-code">pipeline = Pipeline(ws, steps=[step])</p>
			<p class="source-code">exp = Experiment(ws, "item-recommendation")</p>
			<p class="source-code">run = exp.submit(pipeline)</p>
			<p>The preceding step executes the modularized pipeline as an experiment in your Azure Machine Learning workspace. However, you can also choose any of the other publishing methods that we<a id="_idIndexMarker1137"/> discussed in the previous sections, such as publishing as a web service or scheduling the pipeline.</p>
			<p>Splitting pipeline steps into reusable modules is extremely helpful when working with multiple teams on the same ML projects. All teams can work in parallel, and the results can be easily integrated with a single Azure Machine Learning workspace. Let's take a look at how Azure Machine Learning pipelines integrate with other Azure services.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/>Integrating pipelines with other Azure services</h1>
			<p>It's rare that users only<a id="_idIndexMarker1138"/> use a single service to manage data flows, experimentation, training, deployment, and CI/CD in the cloud. Other services provide specific features that make them a better fit for a task, such as Azure Data Factory for loading data into Azure and Azure Pipelines for CI/CD for running automated tasks in Azure DevOps.</p>
			<p>The strongest argument for choosing a cloud provider is the strong integration of its individual services. In this section, we will see how Azure Machine Learning pipelines integrate with other Azure services. The list for this section would be a lot longer if we were to cover every possible service for integration. As we learned in this chapter, you can trigger a published pipeline by calling a REST endpoint and submitting a pipeline using standard Python code. This means that you can integrate pipelines anywhere where you can call HTTP endpoints or run Python code.</p>
			<p>We will first look into integration with Azure Machine Learning designer. The designer lets you build pipelines using graphical blocks, and these pipelines, published pipelines, and pipeline runs will show up in the workspace just like any other pipeline that we built in this chapter. Therefore, it is practical to take a quick look at the commonalities and differences.</p>
			<p>Next, we will take a <a id="_idIndexMarker1139"/>quick look at integrating Azure Machine Learning pipelines with Azure Data Factory, arguably an integration that is used the most. It's a very natural instinct to include ML pipelines with ETL pipelines for scoring, enriching, or enhancing data during the ETL process.</p>
			<p>Finally, we will compare Azure Machine Learning pipelines with Azure Pipelines for CI/CD in Azure DevOps. While Azure DevOps was used mainly for application code and app orchestration, it is now transitioning to provide fully end-to-end MLOps workflows. Let's start with the designer and jump right in.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/>Building pipelines with Azure Machine Learning designer</h2>
			<p><strong class="bold">Azure Machine Learning designer</strong> is a<a id="_idIndexMarker1140"/> graphical interface for creating complex ML pipelines through a drag and drop interface. You can choose a functionality represented as blocks for importing data, which will use a<a id="_idIndexMarker1141"/> datastore and a dataset under the hood.</p>
			<p>The following figure shows a simple pipeline to train and score a Boosted Decision Tree Regression model. As you can see, the block-based programming style requires less knowledge about the individual blocks, and it allows you to build complex pipelines without writing any code:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer204" class="IMG---Figure">
					<img src="image/B17928_08_07.jpg" alt="Figure 8.7 – The Azure Machine Learning designer pipeline " width="1274" height="782"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – The Azure Machine Learning designer pipeline</p>
			<p>Some actions, such as<a id="_idIndexMarker1142"/> connecting the output of one computation to the input of the next, are arguably more convenient to create in the <a id="_idIndexMarker1143"/>visual UI than with code. It's also easier to understand the data flow by visualizing the pipeline. Other actions, such as creating parallel executions of large data batches, are a bit easier to handle and maintain in code. However, due to our code-first approach for reproducibility, testability, and version control, we usually prefer code for authoring and execution.</p>
			<p>It's worth noting that the functionality of pipelines in the designer and pipelines using code are not the same. While you have a broad set of preconfigured abstract functional blocks, such as the <strong class="bold">Boosted Decision Tree Regression</strong> block in the previous <em class="italic">Figure 8.7</em>, you can't access these functionalities in code. However, you can use scikit-learn, PyTorch, TensorFlow, and so on to reuse an existing functionality or build your own in code.</p>
			<p>Thanks to the first-class integration of the designer into the workspace, you can access all of the files, models, and datasets of the workspace from within the designer. An important takeaway is that <a id="_idIndexMarker1144"/>all the resources that are <a id="_idIndexMarker1145"/>created in the workspace such as pipelines, published pipelines, real-time endpoints, models, datasets, and so on are stored in a common system – independently of where they were created.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/>Azure Machine Learning pipelines in Azure Data Factory</h2>
			<p>When moving data, ETL, and trigger computations in<a id="_idIndexMarker1146"/> various Azure services, you will most likely come across <strong class="bold">Azure Data Factory</strong>. It is a very popular service to move large amounts of data into <a id="_idIndexMarker1147"/>Azure, perform processing and transformations, build workflows, and trigger many other Azure or third-party services.</p>
			<p>Azure Machine Learning pipelines integrate very well with Azure Data Factory, and you can easily configure and trigger the execution of a published pipeline through Data Factory. To do so, you need to drag the <strong class="bold">ML Execute Pipeline</strong> activity to your Data Factory canvas and specify the pipeline ID of the published pipeline. In addition, you can also specify pipeline parameters as well as the experiment name for the pipeline run.</p>
			<p>The following figure shows how the <strong class="bold">ML Execute Pipeline</strong> step can be configured in Azure Data Factory. It uses a linked service to connect to your Azure Machine Learning workspace, which allows you to select the desired pipeline from a drop-down box:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer205" class="IMG---Figure">
					<img src="image/B17928_08_08.jpg" alt="Figure 8.6 – Azure Data Factory with Azure Machine Learning activity " width="1249" height="699"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Azure Data Factory with Azure Machine Learning activity</p>
			<p>If you are<a id="_idIndexMarker1148"/> configuring the computational steps using JSON, you can use the following snippet to create an <strong class="bold">ML Execute Pipeline</strong> activity with Azure <a id="_idIndexMarker1149"/>Machine Learning as a linked service. Again, you must specify the pipeline ID and can pass an experiment name, as well as pipeline parameters:</p>
			<p class="source-code">{</p>
			<p class="source-code">    "name": "Machine Learning Execute Pipeline",</p>
			<p class="source-code">    "type": "AzureMLExecutePipeline",</p>
			<p class="source-code">    "linkedServiceName": {</p>
			<p class="source-code">        "referenceName": "AzureMLService",</p>
			<p class="source-code">        "type": "LinkedServiceReference"</p>
			<p class="source-code">    },</p>
			<p class="source-code">    "typeProperties": {</p>
			<p class="source-code">        "mlPipelineId": "&lt;insert pipeline id&gt;",</p>
			<p class="source-code">        "experimentName": "data-factory-pipeline",</p>
			<p class="source-code">        "mlPipelineParameters": {</p>
			<p class="source-code">            "batch_size": "10"</p>
			<p class="source-code">        }</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>Finally, you can trigger the step by adding triggers or output into the <strong class="bold">ML Execute Pipeline</strong> activity. This<a id="_idIndexMarker1150"/> will finally trigger your published Azure Machine Learning pipeline and start the execution in your workspace. This is a <a id="_idIndexMarker1151"/>great addition and makes it easy for other teams to re-use your ML pipelines during classical ETL and data transformation processes.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/>Azure Pipelines for CI/CD</h2>
			<p>Azure Pipelines is a feature of Azure DevOps that lets you run, build, test, and deploy code as a <strong class="bold">Continuous Integration</strong> (<strong class="bold">CI</strong>) and <strong class="bold">Continuous Deployment</strong> (<strong class="bold">CD</strong>) process. Hence, they<a id="_idIndexMarker1152"/> are flexible pipelines for code and app orchestration with<a id="_idIndexMarker1153"/> many advanced features, such as approval queues and gated phases.</p>
			<p>By allowing you to run multiple blocks of code, the best way to integrate Azure Machine Learning into Azure DevOps is by using Python script blocks. If you have followed this book and used a code-first approach to author your experiments and pipelines, then this integration is very easy. Let's take a look at a small example.</p>
			<p>First, let's write a utility function that returns a published pipeline, given a workspace and pipeline ID as parameters. We will need this function in this example:</p>
			<p class="source-code">def get_pipeline(workspace, pipeline_id):</p>
			<p class="source-code">  for pipeline in PublishedPipeline.list(workspace):</p>
			<p class="source-code">    if pipeline.id == pipeline_id:</p>
			<p class="source-code">      return pipeline</p>
			<p class="source-code">  return None</p>
			<p>Next, we can go ahead and implement a very simple Python script that allows us to configure and trigger a pipeline run in Azure. We will initialize the workspace, retrieve the published pipeline, and submit the pipeline <a id="_idIndexMarker1154"/>as an experiment to the Azure Machine Learning workspace. It's all configurable and all with only a few lines of code:</p>
			<p class="source-code">ws = Workspace.get(</p>
			<p class="source-code">  name=os.environ.get("WORKSPACE_NAME"),</p>
			<p class="source-code">  subscription_id=os.environ.get("SUBSCRIPTION_ID"),</p>
			<p class="source-code">  resource_group=os.environ.get("RESOURCE_GROUP"))</p>
			<p class="source-code">pipeline = get_pipeline(args.pipeline_id)</p>
			<p class="source-code">pipeline_parameters = args.pipeline_parameters</p>
			<p class="source-code">exp = Experiment(ws, name=args.experiment_name)</p>
			<p class="source-code">run = exp.submit(pipeline,</p>
			<p class="source-code">                 pipeline_parameters=pipeline_parameters)</p>
			<p class="source-code">print("Pipeline run initiated %s" % run.id)</p>
			<p>The preceding code<a id="_idIndexMarker1155"/> shows us how we can integrate a pipeline trigger into an Azure pipeline for CI/CD. We can see that once the workspace is initialized, the code follows the exact same pattern as if we had submitted the published pipeline from our local development environment. In addition, we can configure the pipeline run through environment variables and command-line parameters. We will see this functionality in action in <a href="B17928_16_ePub.xhtml#_idTextAnchor252"><em class="italic">Chapter 16</em></a>, <em class="italic">Bringing Models into Production with MLOps</em>.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/>Summary</h1>
			<p>In this chapter, you learned how to use and configure Azure Machine Learning pipelines for splitting an ML workflow into multiple steps using pipeline and pipeline steps for estimators, Python execution, and parallel execution. You configured pipeline input and output using <strong class="source-inline">Dataset</strong> and <strong class="source-inline">PipelineData</strong> and managed to control the execution flow of the pipeline.</p>
			<p>As another milestone, you deployed the pipeline as <strong class="source-inline">PublishedPipeline</strong> to an HTTP endpoint. This lets you configure and trigger the pipeline execution with a simple HTTP call. Next, you implemented automatic scheduling based on a time frequency, as well as a reactive schedule based on changes in the underlying dataset. Now, the pipeline can rerun your workflow when the input data changes without any manual interaction.</p>
			<p>Finally, we also modularized and versioned a pipeline step so that it can be reused in other projects. We used <strong class="source-inline">InputPortDef</strong> and <strong class="source-inline">OutputPortDef</strong> to create virtual bindings for data sources and sinks. In the last step, we looked into the integration of pipelines into other Azure services, such as Azure Machine Learning designer, Azure Data Factory, and Azure DevOps.</p>
			<p>In the next chapter, we will look into building ML models in Azure using decision tree-based ensemble models.</p>
		</div>
	</div>
</div>
</body></html>