- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Cost-Sensitive Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: So far, we have studied various sampling techniques and ways to oversample or
    undersample data. However, both of these techniques have their own unique set
    of issues. For example, oversampling can easily lead to overfitting of the model
    due to the exact or very similar examples being seen repeatedly. Similarly, with
    undersampling, we lose some information (that could have been useful for the model)
    because we discard the majority class examples to balance the training dataset.
    In this chapter, weâ€™ll consider an alternative to the data-level techniques that
    we learned about previously.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº†å„ç§é‡‡æ ·æŠ€æœ¯ä»¥åŠå¦‚ä½•å¯¹æ•°æ®è¿›è¡Œè¿‡é‡‡æ ·æˆ–æ¬ é‡‡æ ·ã€‚ç„¶è€Œï¼Œè¿™ä¸¤ç§æŠ€æœ¯éƒ½æœ‰å…¶ç‹¬ç‰¹çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œè¿‡é‡‡æ ·å¯èƒ½ä¼šå› ä¸ºé‡å¤çœ‹åˆ°ç²¾ç¡®æˆ–éå¸¸ç›¸ä¼¼çš„ä¾‹å­è€Œå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆã€‚åŒæ ·ï¼Œåœ¨æ¬ é‡‡æ ·ä¸­ï¼Œæˆ‘ä»¬å¤±å»äº†ä¸€äº›ä¿¡æ¯ï¼ˆè¿™äº›ä¿¡æ¯å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨ï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬ä¸¢å¼ƒäº†å¤§å¤šæ•°ç±»åˆ«çš„ä¾‹å­æ¥å¹³è¡¡è®­ç»ƒæ•°æ®é›†ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¹‹å‰æ‰€å­¦æ•°æ®çº§æŠ€æœ¯çš„æ›¿ä»£æ–¹æ¡ˆã€‚
- en: Cost-sensitive learning is an effective strategy to tackle imbalanced data.
    We will go through this technique and learn why it can be useful. This will help
    us understand some of the details of cost functions and how machine learning models
    are not designed to deal with imbalanced datasets by default. While machine learning
    models arenâ€™t equipped to handle imbalanced datasets, we will see how modern libraries
    enable this.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•æ¥å¤„ç†ä¸å¹³è¡¡æ•°æ®ã€‚æˆ‘ä»¬å°†ä»‹ç»è¿™é¡¹æŠ€æœ¯ï¼Œå¹¶äº†è§£ä¸ºä»€ä¹ˆå®ƒå¯èƒ½æ˜¯æœ‰ç”¨çš„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬ç†è§£æˆæœ¬å‡½æ•°çš„ä¸€äº›ç»†èŠ‚ä»¥åŠæœºå™¨å­¦ä¹ æ¨¡å‹é»˜è®¤ä¸æ˜¯è®¾è®¡æ¥å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„ã€‚è™½ç„¶æœºå™¨å­¦ä¹ æ¨¡å‹æ²¡æœ‰é…å¤‡å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„èƒ½åŠ›ï¼Œä½†æˆ‘ä»¬å°†çœ‹åˆ°ç°ä»£åº“æ˜¯å¦‚ä½•å®ç°è¿™ä¸€ç‚¹çš„ã€‚
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: The concept of **cost-sensitive** **learning** (**CSL**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æˆæœ¬æ•æ„Ÿ** **å­¦ä¹ **ï¼ˆCSLï¼‰çš„æ¦‚å¿µ'
- en: Understanding costs in practice
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®è·µä¸­çš„æˆæœ¬ç†è§£
- en: Cost-sensitive learning for logistic regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’çš„æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: Cost-sensitive learning for decision trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘çš„æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: Cost-sensitive learning using `scikit-learn` and XGBoost models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`scikit-learn`å’ŒXGBoostæ¨¡å‹è¿›è¡Œæˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: MetaCost â€“ making any classification model cost-sensitive
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetaCost â€“ ä½¿ä»»ä½•åˆ†ç±»æ¨¡å‹å…·æœ‰æˆæœ¬æ•æ„Ÿæ€§
- en: Threshold adjustment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é˜ˆå€¼è°ƒæ•´
- en: By the end of this chapter, you will understand what cost means in the context
    of classification problems, how to adjust model parameters to account for such
    costs, and how to prioritize minority class predictions to mitigate the cost of
    misclassification. We will also look at a generic meta-algorithm that can make
    any algorithm cost-sensitive and a post-processing technique for adjusting prediction
    thresholds.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œæ‚¨å°†äº†è§£åœ¨åˆ†ç±»é—®é¢˜ä¸­æˆæœ¬çš„å«ä¹‰ï¼Œå¦‚ä½•è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥è€ƒè™‘è¿™äº›æˆæœ¬ï¼Œä»¥åŠå¦‚ä½•ä¼˜å…ˆè€ƒè™‘å°‘æ•°ç±»åˆ«çš„é¢„æµ‹ä»¥å‡è½»è¯¯åˆ†ç±»çš„æˆæœ¬ã€‚æˆ‘ä»¬è¿˜å°†æ¢è®¨ä¸€ä¸ªé€šç”¨çš„å…ƒç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ä½¿ä»»ä½•ç®—æ³•å…·æœ‰æˆæœ¬æ•æ„Ÿæ€§ï¼Œä»¥åŠä¸€ç§åå¤„ç†æŠ€æœ¯ï¼Œç”¨äºè°ƒæ•´é¢„æµ‹é˜ˆå€¼ã€‚
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `scikit-learn`, `xgboost`, and `imbalanced-learn`. The code and notebooks
    for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05).
    You can open this GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of this chapterâ€™s notebook or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰çš„ç« èŠ‚ç±»ä¼¼ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨å¸¸è§çš„åº“ï¼Œå¦‚`numpy`ã€`scikit-learn`ã€`xgboost`å’Œ`imbalanced-learn`ã€‚æœ¬ç« çš„ä»£ç å’Œç¬”è®°æœ¬å¯åœ¨GitHubä¸Šæ‰¾åˆ°ï¼Œç½‘å€ä¸º[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05)ã€‚æ‚¨å¯ä»¥é€šè¿‡ç‚¹å‡»æœ¬ç« ç¬”è®°æœ¬é¡¶éƒ¨çš„**åœ¨Colabä¸­æ‰“å¼€**å›¾æ ‡æˆ–é€šè¿‡ä½¿ç”¨ç¬”è®°æœ¬çš„GitHub
    URLåœ¨[https://colab.research.google.com](https://colab.research.google.com)å¯åŠ¨å®ƒæ¥æ‰“å¼€è¿™ä¸ªGitHubç¬”è®°æœ¬ã€‚
- en: The concept of Cost-Sensitive Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ ï¼ˆCSLï¼‰çš„æ¦‚å¿µ
- en: '**Cost-Sensitive Learning** (**CSL**) is a technique where the cost function
    of a machine learning model is changed to account for the imbalance in data. The
    key insight behind CSL is that we want our modelâ€™s cost function to reflect the
    relative importance of the different classes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆæœ¬æ•æ„Ÿå­¦ä¹ **ï¼ˆCSLï¼‰æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå…¶ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æˆæœ¬å‡½æ•°è¢«æ”¹å˜ä»¥è€ƒè™‘æ•°æ®çš„ä¸å¹³è¡¡ã€‚CSLèƒŒåçš„å…³é”®æ´å¯Ÿæ˜¯æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹æˆæœ¬å‡½æ•°åæ˜ ä¸åŒç±»åˆ«çš„ç›¸å¯¹é‡è¦æ€§ã€‚'
- en: Letâ€™s try to understand cost functions in machine learning and various types
    of CSL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•ç†è§£æœºå™¨å­¦ä¹ ä¸­çš„æˆæœ¬å‡½æ•°å’Œå„ç§ç±»å‹çš„æˆæœ¬æ•æ„Ÿå­¦ä¹ ï¼ˆCSLï¼‰ã€‚
- en: Costs and cost functions
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆæœ¬å’Œæˆæœ¬å‡½æ•°
- en: 'A cost function estimates the difference between the actual outcome and the
    predicted outcome from a model. For example, the cost function of the logistic
    regression model is given by the log loss function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬å‡½æ•°ä¼°è®¡äº†å®é™…ç»“æœä¸æ¨¡å‹é¢„æµ‹ç»“æœä¹‹é—´çš„å·®å¼‚ã€‚ä¾‹å¦‚ï¼Œé€»è¾‘å›å½’æ¨¡å‹çš„æˆæœ¬å‡½æ•°ç”±å¯¹æ•°æŸå¤±å‡½æ•°ç»™å‡ºï¼š
- en: LogLoss = âˆ’ Â 1Â _Â N * âˆ‘Â i=1 Â NÂ Â ( yÂ i * log(Â Ë†Â yÂ Â i) + (1 âˆ’ yÂ i)* log(1 âˆ’ Â Ë†Â yÂ Â i))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LogLoss = âˆ’ 1 * N * âˆ‘ i=1 N (y_i * log(Ë†y_i) + (1 âˆ’ y_i) * log(1 âˆ’ Ë†y_i))
- en: Here, N is the total number of observations, yÂ i is the true label (0 or 1),
    and Â Ë†Â yÂ Â i is the probability value (between 0 and 1) predicted from the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼ŒN æ˜¯è§‚å¯Ÿçš„æ€»æ•°ï¼Œy_i æ˜¯çœŸå®æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰ï¼ŒË†y_i æ˜¯ä»æ¨¡å‹é¢„æµ‹å‡ºçš„æ¦‚ç‡å€¼ï¼ˆä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼‰ã€‚
- en: One type of cost is called the cost of misclassification errors [1] â€“ that is,
    the cost of predicting the majority class instead of the minority class or vice
    versa.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§ç±»å‹çš„æˆæœ¬è¢«ç§°ä¸ºè¯¯åˆ†ç±»é”™è¯¯æˆæœ¬ [1] - å³ï¼Œé¢„æµ‹å¤šæ•°ç±»è€Œä¸æ˜¯å°‘æ•°ç±»æˆ–åä¹‹çš„æˆæœ¬ã€‚
- en: 'In practice, there can be other types of costs that we may incur, such as the
    following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé‡åˆ°å…¶ä»–ç±»å‹çš„æˆæœ¬ï¼Œä¾‹å¦‚ä»¥ä¸‹è¿™äº›ï¼š
- en: Cost of labeling the dataset
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡è®°æ•°æ®é›†çš„æˆæœ¬
- en: Cost of training or evaluating the model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæˆ–è¯„ä¼°æ¨¡å‹çš„æˆæœ¬
- en: Cost of training data collection
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®æ”¶é›†çš„æˆæœ¬
- en: 'Letâ€™s consider the confusion matrix:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘æ··æ·†çŸ©é˜µï¼š
- en: '|  | **Predicted Negative** | **Predicted Positive** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | **é¢„æµ‹ä¸ºè´Ÿ** | **é¢„æµ‹ä¸ºæ­£** |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Actual Negative** | True Negative | False Positive |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **å®é™…ä¸ºè´Ÿ** | çœŸé˜´æ€§ | å‡é˜³æ€§ |'
- en: '| **Actual Positive** | False Negative | True Positive |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **å®é™…ä¸ºæ­£** | å‡é˜´æ€§ | çœŸé˜³æ€§ |'
- en: Table 5.1 â€“ Confusion matrix for understanding the cost of classification errors
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨ 5.1 â€“ ç†è§£åˆ†ç±»é”™è¯¯æˆæœ¬çš„æ··æ·†çŸ©é˜µ
- en: Psychological studies have suggested that loss hurts twice as much as gain.
    Similarly, in machine learning, the â€œcostâ€ captures whenever the model makes a
    mistake (False Positive and False Negative) and does not worry about when itâ€™s
    right (True Positive and True Negative). This cost is the cost of misclassification
    errors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¿ƒç†å­¦ç ”ç©¶è¡¨æ˜ï¼ŒæŸå¤±å¸¦æ¥çš„ç—›è‹¦æ˜¯æ”¶ç›Šçš„ä¸¤å€ã€‚åŒæ ·ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œâ€œæˆæœ¬â€æ•æ‰äº†æ¨¡å‹å‡ºé”™ï¼ˆå‡é˜³æ€§ä¸å‡é˜´æ€§ï¼‰çš„æ—¶åˆ»ï¼Œè€Œä¸å…³å¿ƒå®ƒæ­£ç¡®çš„æ—¶å€™ï¼ˆçœŸé˜³æ€§ä¸çœŸé˜´æ€§ï¼‰ã€‚è¿™ç§æˆæœ¬æ˜¯è¯¯åˆ†ç±»é”™è¯¯çš„æˆæœ¬ã€‚
- en: Not all misclassifications are created equal. For instance, suppose weâ€™re attempting
    to predict whether a patient has cancer. If our model incorrectly indicates that
    the patient has cancer (a false positive), this could lead to additional testing.
    However, if our model incorrectly suggests that the patient is cancer-free (a
    false negative), the consequences could be far more severe as the disease could
    progress undiagnosed. Therefore, a false negative is significantly more detrimental
    than a false positive. Our cost function should take this discrepancy into account.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰è¯¯åˆ†ç±»éƒ½æ˜¯ç›¸åŒçš„ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æ­£åœ¨å°è¯•é¢„æµ‹æ‚£è€…æ˜¯å¦æ‚£æœ‰ç™Œç—‡ã€‚å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹é”™è¯¯åœ°æŒ‡ç¤ºæ‚£è€…æ‚£æœ‰ç™Œç—‡ï¼ˆå‡é˜³æ€§ï¼‰ï¼Œè¿™å¯èƒ½å¯¼è‡´é¢å¤–çš„æµ‹è¯•ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çš„æ¨¡å‹é”™è¯¯åœ°å»ºè®®æ‚£è€…æ— ç—…ï¼ˆå‡é˜´æ€§ï¼‰ï¼Œåæœå¯èƒ½æ›´ä¸ºä¸¥é‡ï¼Œå› ä¸ºç–¾ç—…å¯èƒ½ä¼šæœªè¯Šæ–­è€Œè¿›å±•ã€‚å› æ­¤ï¼Œå‡é˜´æ€§æ¯”å‡é˜³æ€§æ›´å…·ç ´åæ€§ã€‚æˆ‘ä»¬çš„æˆæœ¬å‡½æ•°åº”è¯¥è€ƒè™‘è¿™ç§å·®å¼‚ã€‚
- en: Unfortunately, most models treat the majority and minority classes equally by
    default. However, modern ML frameworks such as `scikit-learn`, Keras/TensorFlow,
    and PyTorch provide a way to weigh the various classes differently across a variety
    of learning algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå¤§å¤šæ•°æ¨¡å‹é»˜è®¤å°†å¤šæ•°ç±»å’Œå°‘æ•°ç±»åŒç­‰å¯¹å¾…ã€‚ç„¶è€Œï¼Œç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶å¦‚ `scikit-learn`ã€Keras/TensorFlow å’Œ PyTorch
    æä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥åœ¨å„ç§å­¦ä¹ ç®—æ³•ä¸­ä¸åŒåœ°æƒè¡¡å„ç§ç±»ã€‚
- en: Types of cost-sensitive learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ ç±»å‹
- en: There are two major types of CSL approaches, namely weighting and meta-learning.
    In weighting approaches, we update the cost function of the machine learning model
    to reflect the importance of the different classes. In meta-learning, we can make
    the model cost-sensitive without changing its cost function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•æ„Ÿå­¦ä¹ ï¼ˆCSLï¼‰æœ‰ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼Œå³åŠ æƒæ³•å’Œå…ƒå­¦ä¹ ã€‚åœ¨åŠ æƒæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ›´æ–°æœºå™¨å­¦ä¹ æ¨¡å‹çš„æˆæœ¬å‡½æ•°ï¼Œä»¥åæ˜ ä¸åŒç±»çš„é‡è¦æ€§ã€‚åœ¨å…ƒå­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿æ¨¡å‹å¯¹æˆæœ¬æ•æ„Ÿï¼Œè€Œæ— éœ€æ›´æ”¹å…¶æˆæœ¬å‡½æ•°ã€‚
- en: 'In MetaCost, a type of meta-learning technique, for example, we alter the labels
    of training instances to minimize expected misclassification costs. Similarly,
    in the threshold adjustment method, we determine a probability threshold that
    minimizes total misclassification costs for predictions. *Figure 5**.1* categorizes
    these methods at a high level [2][3]:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…ƒå­¦ä¹ æŠ€æœ¯ MetaCost ä¸­ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜è®­ç»ƒå®ä¾‹çš„æ ‡ç­¾ï¼Œä»¥æœ€å°åŒ–é¢„æœŸçš„è¯¯åˆ†ç±»æˆæœ¬ã€‚åŒæ ·ï¼Œåœ¨é˜ˆå€¼è°ƒæ•´æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ç¡®å®šä¸€ä¸ªæ¦‚ç‡é˜ˆå€¼ï¼Œä»¥æœ€å°åŒ–é¢„æµ‹çš„æ€»è¯¯åˆ†ç±»æˆæœ¬ã€‚*å›¾
    5.1* ä»é«˜å±‚æ¬¡ä¸Šå¯¹è¿™äº›æ–¹æ³•è¿›è¡Œäº†åˆ†ç±» [2][3]ï¼š
- en: '![](img/B17259_05_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_05_01.jpg)'
- en: Figure 5.1 â€“ Categorization of cost-sensitive learning methods
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.1 â€“ æˆæœ¬æ•æ„Ÿå­¦ä¹ æ–¹æ³•çš„åˆ†ç±»
- en: Difference between CSL and resampling
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSLä¸é‡é‡‡æ ·çš„åŒºåˆ«
- en: The key difference between previously discussed data-level techniques and CSL
    is that the data-level techniques adjust the frequency of the different error
    types, but they treat all misclassification errors the same. In certain cases,
    as we encountered earlier, the cost of misclassifying observations of different
    classes is not the same. For example, in cancer detection, the cost of misclassifying
    a patient who has cancer as healthy (False Negative) is much higher, as the patient
    is at high risk if not detected or treated early. Similarly, misclassifying a
    fraudulent booking as non-fraudulent can cost more money than wrongly classifying
    a legitimate transaction as fraud. Why? Because in the latter case, we can just
    call and verify with the user the legitimacy of the transaction. By applying resampling
    techniques such as upsampling or downsampling, we are implicitly changing the
    cost of different types of errors. So, CSL and resampling techniques can be considered
    to have an equivalent effect on the model at the end of the day.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰è®¨è®ºçš„æ•°æ®çº§åˆ«æŠ€æœ¯ç›¸æ¯”ï¼ŒCSLçš„å…³é”®åŒºåˆ«åœ¨äºæ•°æ®çº§åˆ«æŠ€æœ¯è°ƒæ•´ä¸åŒé”™è¯¯ç±»å‹çš„é¢‘ç‡ï¼Œä½†å®ƒä»¬å¯¹å¾…æ‰€æœ‰è¯¯åˆ†ç±»é”™è¯¯éƒ½æ˜¯ä¸€æ ·çš„ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰é‡åˆ°çš„ï¼Œä¸åŒç±»åˆ«çš„è§‚æµ‹å€¼è¢«è¯¯åˆ†ç±»çš„æˆæœ¬å¹¶ä¸ç›¸åŒã€‚ä¾‹å¦‚ï¼Œåœ¨ç™Œç—‡æ£€æµ‹ä¸­ï¼Œå°†æ‚£æœ‰ç™Œç—‡çš„æ‚£è€…è¯¯åˆ†ç±»ä¸ºå¥åº·ï¼ˆå‡é˜´æ€§ï¼‰çš„æˆæœ¬è¦é«˜å¾—å¤šï¼Œå› ä¸ºå¦‚æœæœªæ£€æµ‹æˆ–æœªæ—©æœŸæ²»ç–—ï¼Œæ‚£è€…é£é™©å¾ˆé«˜ã€‚åŒæ ·ï¼Œå°†æ¬ºè¯ˆé¢„è®¢è¯¯åˆ†ç±»ä¸ºéæ¬ºè¯ˆå¯èƒ½ä¼šæ¯”å°†åˆæ³•äº¤æ˜“è¯¯åˆ†ç±»ä¸ºæ¬ºè¯ˆçš„æˆæœ¬æ›´é«˜ã€‚ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºåœ¨åä¸€ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªéœ€è”ç³»ç”¨æˆ·å¹¶éªŒè¯äº¤æ˜“çš„åˆæ³•æ€§å³å¯ã€‚é€šè¿‡åº”ç”¨é‡é‡‡æ ·æŠ€æœ¯ï¼Œå¦‚ä¸Šé‡‡æ ·æˆ–ä¸‹é‡‡æ ·ï¼Œæˆ‘ä»¬éšå¼åœ°æ”¹å˜äº†ä¸åŒç±»å‹é”™è¯¯çš„æˆæœ¬ã€‚å› æ­¤ï¼ŒCSLå’Œé‡é‡‡æ ·æŠ€æœ¯æœ€ç»ˆå¯ä»¥å¯¹æ¨¡å‹äº§ç”Ÿç­‰æ•ˆçš„å½±å“ã€‚
- en: 'However, resampling techniques may be problematic in certain cases, as we will
    discuss in the next section. In such cases, CSL can be more practical:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé‡é‡‡æ ·æŠ€æœ¯å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒCSLå¯èƒ½æ›´å®ç”¨ï¼š
- en: '![](img/B17259_05_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_05_02.jpg)'
- en: Figure 5.2 â€“ Comic re-emphasizing the idea of misclassification errors
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.2 â€“ æ¼«ç”»å†æ¬¡å¼ºè°ƒè¯¯åˆ†ç±»é”™è¯¯çš„æ¦‚å¿µ
- en: Problems with rebalancing techniques
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡å¹³è¡¡æŠ€æœ¯çš„ç¼ºé™·
- en: 'In the previous chapters, we briefly touched on why in some cases, we would
    prefer not to apply any data sampling techniques. This could be because of the
    following reasons:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç®€è¦åœ°æåˆ°äº†ä¸ºä»€ä¹ˆåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½æ›´å–œæ¬¢ä¸åº”ç”¨ä»»ä½•æ•°æ®é‡‡æ ·æŠ€æœ¯ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºä»¥ä¸‹åŸå› ï¼š
- en: We already have too much training data, and it might be quite expensive to deal
    with more data, or the training time can increase by many folds due to having
    more training data.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ‹¥æœ‰å¤ªå¤šçš„è®­ç»ƒæ•°æ®ï¼Œå¤„ç†æ›´å¤šçš„æ•°æ®å¯èƒ½ç›¸å½“æ˜‚è´µï¼Œæˆ–è€…ç”±äºæœ‰æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼Œè®­ç»ƒæ—¶é—´å¯èƒ½å¢åŠ æ•°å€ã€‚
- en: Sometimes, we may not get the best results using sampling or data rebalancing
    techniques because of the dataset we are using.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ‰æ—¶ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•é€šè¿‡é‡‡æ ·æˆ–æ•°æ®é‡å¹³è¡¡æŠ€æœ¯è·å¾—æœ€ä½³ç»“æœã€‚
- en: An additional consideration is that upon rebalancing the dataset, our modelâ€™s
    predictive scores may become miscalibrated, necessitating a recalibration process.
    We will cover this topic in [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*, where we will learn about various model calibration techniques.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦è€ƒè™‘çš„å› ç´ æ˜¯ï¼Œåœ¨é‡æ–°å¹³è¡¡æ•°æ®é›†åï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„é¢„æµ‹åˆ†æ•°å¯èƒ½å˜å¾—ä¸å‡†ç¡®ï¼Œéœ€è¦é‡æ–°æ ¡å‡†ã€‚æˆ‘ä»¬å°†åœ¨[*ç¬¬10ç« *](B17259_10.xhtml#_idTextAnchor279)â€œæ¨¡å‹æ ¡å‡†â€ä¸­ä»‹ç»è¿™ä¸€ä¸»é¢˜ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å„ç§æ¨¡å‹æ ¡å‡†æŠ€æœ¯ã€‚
- en: Rebalancing techniques can lead to model overfitting or underfitting issues.
    Overfitting can especially happen when using oversampling since they produce repeated
    or similar training examples. Similarly, the model may be underfitted when using
    undersampling because the model did not get trained on the data thrown away during
    undersampling.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é‡å¹³è¡¡æŠ€æœ¯å¯èƒ½å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆé—®é¢˜ã€‚ç‰¹åˆ«æ˜¯å½“ä½¿ç”¨è¿‡é‡‡æ ·æ—¶ï¼Œå®ƒä»¬ä¼šäº§ç”Ÿé‡å¤æˆ–ç›¸ä¼¼çš„è®­ç»ƒç¤ºä¾‹ï¼Œè¿™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆã€‚åŒæ ·ï¼Œå½“ä½¿ç”¨æ¬ é‡‡æ ·æ—¶ï¼Œæ¨¡å‹å¯èƒ½æ¬ æ‹Ÿåˆï¼Œå› ä¸ºæ¨¡å‹æ²¡æœ‰åœ¨æ¬ é‡‡æ ·è¿‡ç¨‹ä¸­ä¸¢å¼ƒçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: Next, letâ€™s try to understand what costs really mean.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å°è¯•ç†è§£æˆæœ¬ç©¶ç«Ÿæ„å‘³ç€ä»€ä¹ˆã€‚
- en: Understanding costs in practice
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®è·µä¸­ç†è§£æˆæœ¬
- en: We need to understand the various types of costs involved while creating weights
    for different classes. These costs change on a case-by-case basis. Letâ€™s discuss
    an example of cost calculations to understand what we should consider while thinking
    about cost calculations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸ºä¸åŒç±»åˆ«åˆ›å»ºæƒé‡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æ¶‰åŠçš„å„ç§æˆæœ¬ç±»å‹ã€‚è¿™äº›æˆæœ¬æ ¹æ®å…·ä½“æƒ…å†µè€Œå˜åŒ–ã€‚è®©æˆ‘ä»¬è®¨è®ºä¸€ä¸ªæˆæœ¬è®¡ç®—çš„ä¾‹å­ï¼Œä»¥äº†è§£åœ¨è€ƒè™‘æˆæœ¬è®¡ç®—æ—¶åº”è€ƒè™‘ä»€ä¹ˆã€‚
- en: Letâ€™s take the example of pediatric pneumonia. According to UNICEF, a child
    dies of pneumonia every 43 seconds [4]. Imagine we are creating a new test for
    pediatric pneumonia â€“ how will we decide the cost of different errors?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»¥å„¿ç«¥è‚ºç‚ä¸ºä¾‹ã€‚æ ¹æ®è”åˆå›½å„¿ç«¥åŸºé‡‘ä¼šï¼Œæ¯43ç§’å°±æœ‰ä¸€ä¸ªå­©å­æ­»äºè‚ºç‚[4]ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨ä¸ºå„¿ç«¥è‚ºç‚å¼€å‘ä¸€ä¸ªæ–°çš„æµ‹è¯•â€”â€”æˆ‘ä»¬å°†å¦‚ä½•å†³å®šä¸åŒé”™è¯¯çš„æˆæœ¬ï¼Ÿ
- en: Letâ€™s review the confusion matrix from *Table 5.1*. There will usually be no
    extra cost for True Negatives and True Positives. But using a False Negative â€“
    that is, when a child has pneumonia and predicting the child to be healthy â€“ will
    have a very high cost. On the flip side, when a healthy child is predicted as
    being affected by pneumonia, there will be a cost associated with the troubles
    the family of the child may have to go through, but there will be much less cost
    than in the previous case. Furthermore, the cost of misclassification can vary
    depending on the childâ€™s age. For example, younger kids will be at a higher risk
    than older kids. Thus, we will aim to penalize the model more if it makes an error
    in the case of younger kids.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹è¡¨5.1ä¸­çš„æ··æ·†çŸ©é˜µã€‚é€šå¸¸ï¼Œå¯¹äºçœŸæ­£çš„è´Ÿä¾‹å’ŒçœŸæ­£çš„æ­£ä¾‹ï¼Œä¸ä¼šæœ‰é¢å¤–çš„æˆæœ¬ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨é”™è¯¯çš„è´Ÿä¾‹â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ä¸€ä¸ªå­©å­æ‚£æœ‰è‚ºç‚ï¼Œå´é¢„æµ‹è¯¥å­©å­å¥åº·æ—¶â€”â€”å°†ä¼šæœ‰éå¸¸é«˜çš„æˆæœ¬ã€‚å¦ä¸€æ–¹é¢ï¼Œå½“ä¸€ä¸ªå¥åº·çš„å„¿ç«¥è¢«é¢„æµ‹ä¸ºæ‚£æœ‰è‚ºç‚æ—¶ï¼Œå°†ä¼šæœ‰ä¸å­©å­å®¶åº­å¯èƒ½é‡åˆ°çš„éº»çƒ¦ç›¸å…³çš„æˆæœ¬ï¼Œä½†è¿™ä¸ªæˆæœ¬æ¯”å‰ä¸€ç§æƒ…å†µè¦ä½å¾—å¤šã€‚æ­¤å¤–ï¼Œè¯¯åˆ†ç±»çš„æˆæœ¬å¯èƒ½å› å­©å­çš„å¹´é¾„è€Œå¼‚ã€‚ä¾‹å¦‚ï¼Œå¹´å¹¼çš„å­©å­æ¯”å¹´é•¿çš„å­©å­é£é™©æ›´é«˜ã€‚å› æ­¤ï¼Œå¦‚æœæ¨¡å‹åœ¨å¹´å¹¼å­©å­çš„æ¡ˆä¾‹ä¸­å‡ºé”™ï¼Œæˆ‘ä»¬å°†å¯¹æ¨¡å‹è¿›è¡Œæ›´å¤šçš„æƒ©ç½šã€‚
- en: 'The cost can vary depending on the duration of the symptoms. Consider it this
    way: if we make an error and misdiagnose a child who has only had flu symptoms
    for a day, itâ€™s not ideal, but itâ€™s not disastrous. However, if that child has
    been enduring flu symptoms for 2 weeks, thatâ€™s a different scenario. That mistake
    will cost us significantly more.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆæœ¬å¯èƒ½å› ç—‡çŠ¶çš„æŒç»­æ—¶é—´è€Œå¼‚ã€‚å¯ä»¥è¿™æ ·è€ƒè™‘ï¼šå¦‚æœæˆ‘ä»¬çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œé”™è¯¯åœ°è¯Šæ–­äº†ä¸€ä¸ªåªæœ‰ä¸€å¤©æµæ„Ÿç—‡çŠ¶çš„å­©å­ï¼Œè¿™å¹¶ä¸ç†æƒ³ï¼Œä½†ä¹Ÿä¸æ˜¯ç¾éš¾æ€§çš„ã€‚ç„¶è€Œï¼Œå¦‚æœé‚£ä¸ªå­©å­å·²ç»å¿å—äº†2å‘¨çš„æµæ„Ÿç—‡çŠ¶ï¼Œé‚£å°†æ˜¯ä¸€ä¸ªä¸åŒçš„åœºæ™¯ã€‚è¿™ä¸ªé”™è¯¯å°†ç»™æˆ‘ä»¬å¸¦æ¥æ›´å¤§çš„æˆæœ¬ã€‚
- en: 'While weâ€™ve discussed real-world problems so far, this chapter will pivot to
    utilize a synthetic dataset. This approach is intended to reinforce concepts and
    methods in a controlled environment, thus enhancing the learning process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢è®¨è®ºäº†ç°å®ä¸–ç•Œçš„é—®é¢˜ï¼Œä½†æœ¬ç« å°†è½¬å‘ä½¿ç”¨åˆæˆæ•°æ®é›†ã€‚è¿™ç§æ–¹æ³•æ—¨åœ¨åœ¨å—æ§ç¯å¢ƒä¸­å¼ºåŒ–æ¦‚å¿µå’Œæ–¹æ³•ï¼Œä»è€Œå¢å¼ºå­¦ä¹ è¿‡ç¨‹ï¼š
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `make_classification` function produces some overlapping points that we
    cleaned up. To keep things simple, weâ€™ve omitted that cleanup code here. You can
    refer to the full notebook on GitHub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_classification`å‡½æ•°äº§ç”Ÿäº†ä¸€äº›é‡å çš„ç‚¹ï¼Œæˆ‘ä»¬æ¸…ç†äº†è¿™äº›ç‚¹ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçœç•¥äº†æ¸…ç†ä»£ç ã€‚æ‚¨å¯ä»¥åœ¨GitHubä¸Šçš„å®Œæ•´ç¬”è®°æœ¬ä¸­æŸ¥é˜…ã€‚'
- en: 'The preceding code produces the following output and scatter plot (*Figure
    5**.3*):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„ä»£ç äº§ç”Ÿäº†ä»¥ä¸‹è¾“å‡ºå’Œæ•£ç‚¹å›¾ï¼ˆ*å›¾5**.3*ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/B17259_05_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_03.jpg)'
- en: Figure 5.3 â€“ Scatter plot showing the training datasetâ€™s distribution
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.3 â€“ æ˜¾ç¤ºè®­ç»ƒæ•°æ®é›†åˆ†å¸ƒçš„æ•£ç‚¹å›¾
- en: Weâ€™ll dive into how to apply CSL to logistic regression models next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å¦‚ä½•å°†CSLåº”ç”¨äºé€»è¾‘å›å½’æ¨¡å‹ã€‚
- en: Cost-Sensitive Learning for logistic regression
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’çš„æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: Logistic regression is a simple classification algorithm. We train a model as
    a linear combination of the features. Then, we pass the result of that linear
    combination into a sigmoid function to predict the class probabilities for different
    classes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’æ˜¯ä¸€ç§ç®€å•çš„åˆ†ç±»ç®—æ³•ã€‚æˆ‘ä»¬é€šè¿‡å°†ç‰¹å¾è¿›è¡Œçº¿æ€§ç»„åˆæ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†çº¿æ€§ç»„åˆçš„ç»“æœä¼ é€’ç»™sigmoidå‡½æ•°ï¼Œä»¥é¢„æµ‹ä¸åŒç±»åˆ«çš„ç±»åˆ«æ¦‚ç‡ã€‚
- en: 'The `sigmoid` function (also called a `logit` function) is a mathematical tool
    capable of converting any real number into a value between 0 and 1\. This value
    can be interpreted as a probability estimate:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`å‡½æ•°ï¼ˆä¹Ÿç§°ä¸º`logit`å‡½æ•°ï¼‰æ˜¯ä¸€ç§å¯ä»¥å°†ä»»ä½•å®æ•°è½¬æ¢ä¸º0åˆ°1ä¹‹é—´å€¼çš„æ•°å­¦å·¥å…·ã€‚è¿™ä¸ªå€¼å¯ä»¥è§£é‡Šä¸ºæ¦‚ç‡ä¼°è®¡ï¼š'
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The graph of the sigmoid function has an S-shaped curve, and it appears like
    this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoidå‡½æ•°çš„å›¾åƒå‘ˆSå½¢æ›²çº¿ï¼Œçœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ï¼š
- en: '![](img/B17259_05_04.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_04.jpg)'
- en: Figure 5.4 â€“ Sigmoid function
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.4 â€“ Sigmoidå‡½æ•°
- en: The class with the highest predicted probability is taken as the prediction
    for a given sample.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å…·æœ‰æœ€é«˜é¢„æµ‹æ¦‚ç‡çš„ç±»åˆ«è¢«ç”¨ä½œç»™å®šæ ·æœ¬çš„é¢„æµ‹ã€‚
- en: 'Letâ€™s say we have an email to be classified as spam or non-spam, and our logistic
    regression model outputs the probabilities of 0.25 for non-spam and 0.75 for spam.
    Here, the class with the highest predicted probability is â€œspamâ€ (1) since 0.75
    is greater than 0.25\. Therefore, the model would predict that this email is spam
    (*Figure 5**.5*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¦åˆ†ç±»ä¸ºåƒåœ¾é‚®ä»¶æˆ–éåƒåœ¾é‚®ä»¶çš„ç”µå­é‚®ä»¶ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„é€»è¾‘å›å½’æ¨¡å‹è¾“å‡ºéåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡ä¸º0.25ï¼Œåƒåœ¾é‚®ä»¶çš„æ¦‚ç‡ä¸º0.75ã€‚åœ¨è¿™é‡Œï¼Œå…·æœ‰æœ€é«˜é¢„æµ‹æ¦‚ç‡çš„ç±»åˆ«æ˜¯â€œåƒåœ¾é‚®ä»¶â€ï¼ˆ1ï¼‰ï¼Œå› ä¸º0.75å¤§äº0.25ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¼šé¢„æµ‹è¿™å°é‚®ä»¶æ˜¯åƒåœ¾é‚®ä»¶ï¼ˆ*å›¾5**.5*ï¼‰ï¼š
- en: '![](img/B17259_05_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_05.jpg)'
- en: Figure 5.5 â€“ Higher class probability determining the class for binary classification
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.5 â€“ é«˜ç±»åˆ«æ¦‚ç‡å†³å®šäºŒåˆ†ç±»çš„ç±»åˆ«
- en: For two-class classification, we just predict the probability of one class.
    The probability of the other class is one minus the probability of the first class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºäºŒåˆ†ç±»ï¼Œæˆ‘ä»¬åªé¢„æµ‹ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚å¦ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡æ˜¯ç¬¬ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡å‡å»1ã€‚
- en: 'The logistic regression model is trained using a loss function. The loss function
    for one example from a dataset with two classes would look like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ•°é€»è¾‘å›å½’æ¨¡å‹ä½¿ç”¨æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒã€‚æ¥è‡ªå…·æœ‰ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®é›†çš„ä¸€ä¸ªç¤ºä¾‹çš„æŸå¤±å‡½æ•°çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: cost = âˆ’ y * log(classProbability) âˆ’ (1 âˆ’ y)* log(1 âˆ’ classProbability)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: cost = âˆ’ y * log(classProbability) âˆ’ (1 âˆ’ y)* log(1 âˆ’ classProbability)
- en: 'For true positives and true negatives, this loss will be very low. For a false
    positive, y, the actual value would be 0; therefore, the first term will be 0,
    but the second term will be very high as the class probability approaches 1, and
    the term will approach negative infinity (since, log(0) â†’ âˆ’ âˆ). Since there is
    a negative sign at the front, the cost will approach positive infinity. A similar
    analysis can be done for the false negative case. One part of the cost can be
    seen as the false positive part, and another part of the cost can be seen as the
    false negative part:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºçœŸæ­£çš„é˜³æ€§å’ŒçœŸæ­£çš„é˜´æ€§ï¼Œè¿™ä¸ªæŸå¤±å°†éå¸¸ä½ã€‚å¯¹äºè¯¯æŠ¥ï¼Œyï¼Œå®é™…å€¼å°†æ˜¯0ï¼›å› æ­¤ï¼Œç¬¬ä¸€ä¸ªé¡¹å°†æ˜¯0ï¼Œä½†ç¬¬äºŒä¸ªé¡¹å°†éå¸¸é«˜ï¼Œå› ä¸ºç±»åˆ«æ¦‚ç‡æ¥è¿‘1ï¼Œè¿™ä¸ªé¡¹å°†æ¥è¿‘è´Ÿæ— ç©·å¤§ï¼ˆå› ä¸ºï¼Œlog(0)
    â†’ âˆ’ âˆï¼‰ã€‚ç”±äºå‰é¢æœ‰ä¸€ä¸ªè´Ÿå·ï¼Œæˆæœ¬å°†æ¥è¿‘æ­£æ— ç©·å¤§ã€‚å¯¹è¯¯æ£€æƒ…å†µå¯ä»¥è¿›è¡Œç±»ä¼¼çš„åˆ†æã€‚æˆæœ¬çš„ä¸€éƒ¨åˆ†å¯ä»¥çœ‹ä½œæ˜¯è¯¯æŠ¥éƒ¨åˆ†ï¼Œå¦ä¸€éƒ¨åˆ†å¯ä»¥çœ‹ä½œæ˜¯è¯¯æ£€éƒ¨åˆ†ï¼š
- en: cost = falsePositiveCost + falseNegativeCost
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: cost = falsePositiveCost + falseNegativeCost
- en: 'As discussed earlier, we donâ€™t want to weigh the two types of costs equally.
    So, all we do is add weights, WÂ FP and WÂ FN, for the respective costs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›ä¸¤ç§ç±»å‹çš„æˆæœ¬åŒç­‰é‡è¦ã€‚æ‰€ä»¥æˆ‘ä»¬åªæ˜¯ä¸ºå„è‡ªçš„æˆæœ¬æ·»åŠ æƒé‡ï¼ŒW FPå’ŒW FNï¼š
- en: cost = WÂ FP * falsePositiveCost + WÂ FN * falseNegativeCost
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: cost = W FP * falsePositiveCost + W FN * falseNegativeCost
- en: 'This is the crux of CSL with logistic regression. To get the overall costs
    of the model, we take the average cost across all the data points:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¯¹æ•°é€»è¾‘å›å½’ä¸­CSLçš„æ ¸å¿ƒã€‚ä¸ºäº†å¾—åˆ°æ¨¡å‹çš„æ€»ä½“æˆæœ¬ï¼Œæˆ‘ä»¬å–æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡æˆæœ¬ï¼š
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When all errors are equally costly, the modelâ€™s decision boundary and the modelâ€™s
    **Precision-Recall** (**PR**) curve will look like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‰€æœ‰é”™è¯¯æˆæœ¬ç›¸ç­‰æ—¶ï¼Œæ¨¡å‹çš„å†³ç­–è¾¹ç•Œå’Œæ¨¡å‹çš„**ç²¾åº¦-å¬å›ç‡**ï¼ˆ**PR**ï¼‰æ›²çº¿å°†çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](img/B17259_05_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_06.jpg)'
- en: Figure 5.6 â€“ The decision boundary (left) and PR curve (right) of the baseline
    regression model
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.6 â€“ åŸºçº¿å›å½’æ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼ˆå·¦ï¼‰å’ŒPRæ›²çº¿ï¼ˆå³ï¼‰
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The previous code outputs the following F2 score, precision, and recall values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„ä»£ç è¾“å‡ºäº†ä»¥ä¸‹F2åˆ†æ•°ã€ç²¾åº¦å’Œå¬å›å€¼ï¼š
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this chapter, we will use the F2 score as our primary metric. What is the
    F2 score? In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to
    Data Imbalance in Machine Learning*, we studied the F-beta score. The F2 score
    is the F-beta score with beta=2, while the F1 score is the F-beta score with beta=1\.
    Itâ€™s useful when recall is more important than precision â€“ that is, false negatives
    are more costly (important) than false positives:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨F2åˆ†æ•°ä½œä¸ºæˆ‘ä»¬çš„ä¸»è¦æŒ‡æ ‡ã€‚F2åˆ†æ•°æ˜¯ä»€ä¹ˆï¼Ÿåœ¨[*ç¬¬1ç« *](B17259_01.xhtml#_idTextAnchor015)ï¼Œ*æœºå™¨å­¦ä¹ ä¸­æ•°æ®ä¸å¹³è¡¡çš„ä»‹ç»*ï¼Œæˆ‘ä»¬å­¦ä¹ äº†F-betaåˆ†æ•°ã€‚F2åˆ†æ•°æ˜¯beta=2çš„F-betaåˆ†æ•°ï¼Œè€ŒF1åˆ†æ•°æ˜¯beta=1çš„F-betaåˆ†æ•°ã€‚å½“å¬å›ç‡æ¯”ç²¾åº¦æ›´é‡è¦æ—¶ï¼Œå®ƒå¾ˆæœ‰ç”¨â€”â€”ä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¯æŠ¥æ¯”è¯¯æ£€ï¼ˆé‡è¦ï¼‰çš„æˆæœ¬æ›´é«˜ï¼š
- en: FÂ Î² = Â (1 + Î²Â 2) Ã— (precision Ã— recall)Â Â ____________________Â Â (Î²Â 2 Ã— precision)
    + recallÂ  = Â (5 Ã— precision Ã— recall)Â Â ________________Â Â (4 Ã— precision) + recall
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: F Î² = Â (1 + Î² 2) Ã— (precision Ã— recall)Â Â ____________________Â Â (Î² 2 Ã— precision)
    + recallÂ  = Â (5 Ã— precision Ã— recall)Â Â ________________Â Â (4 Ã— precision) + recall
- en: '`LogisticRegression` from the `scikit-learn` library provides a `class_weight`
    parameter. When the value of this parameter is set to â€œbalanced,â€ the weight of
    each class is automatically computed by the following formula:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression`æ¥è‡ª`scikit-learn`åº“æä¾›äº†ä¸€ä¸ª`class_weight`å‚æ•°ã€‚å½“æ­¤å‚æ•°çš„å€¼è®¾ç½®ä¸ºâ€œbalancedâ€æ—¶ï¼Œæ¯ä¸ªç±»åˆ«çš„æƒé‡å°†è‡ªåŠ¨é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š'
- en: weightOfClass = Â totalNumberOfSamplesÂ Â Â ________________________________Â Â Â numberOfClasses
    * numberOfSamplesPerClass
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: weightOfClass =  totalNumberOfSamplesÂ Â Â ________________________________Â Â Â numberOfClasses
    * numberOfSamplesPerClass
- en: 'For example, we have 100 examples in the dataset â€“ 80 in class 0 and 20 in
    class 1\. The weights of each class are computed as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨æ•°æ®é›†ä¸­æˆ‘ä»¬æœ‰100ä¸ªç¤ºä¾‹ â€“ 80ä¸ªå±äºç±»åˆ«0ï¼Œ20ä¸ªå±äºç±»åˆ«1ã€‚æ¯ä¸ªç±»åˆ«çš„æƒé‡è®¡ç®—å¦‚ä¸‹ï¼š
- en: Weight for class 0 = 100/(2*80) = 0.625
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ«0çš„æƒé‡ = 100/(2*80) = 0.625
- en: Weight for class 1 = 100/(2*20) = 2.5
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç±»åˆ«1çš„æƒé‡ = 100/(2*20) = 2.5
- en: Given that the number of class 0 examples is four times that of class 1, the
    weight of class 1 is 2.5, which is four times the weight of class 0 â€“ that is,
    0.625\. This makes sense since we would want to give more weight to class 1, which
    is smaller in number.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç±»åˆ«0çš„ç¤ºä¾‹æ•°é‡æ˜¯ç±»åˆ«1çš„å››å€ï¼Œç±»åˆ«1çš„æƒé‡æ˜¯2.5ï¼Œæ˜¯ç±»åˆ«0æƒé‡çš„å››å€ï¼Œå³0.625ã€‚è¿™å¾ˆæœ‰é“ç†ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ç»™ç±»åˆ«1æ›´å¤šçš„æƒé‡ï¼Œå› ä¸ºå®ƒçš„æ•°é‡è¾ƒå°‘ã€‚
- en: 'We can mention `class_weight` as a dictionary as well:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†`class_weight`ä½œä¸ºä¸€ä¸ªå­—å…¸æ¥æåŠï¼š
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Letâ€™s try to use the `class_weight` parameter in the `LogisticRegression` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•åœ¨`LogisticRegression`å‡½æ•°ä¸­ä½¿ç”¨`class_weight`å‚æ•°ï¼š
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B17259_05_07.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_07.jpg)'
- en: Figure 5.7 â€“ The decision boundary (left) and PR curve (right) of the â€œbalancedâ€
    class-weighted logistic regression model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.7 â€“ â€œå¹³è¡¡â€ç±»åˆ«åŠ æƒé€»è¾‘å›å½’æ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼ˆå·¦ï¼‰å’ŒPRæ›²çº¿ï¼ˆå³ï¼‰
- en: 'Letâ€™s calculate the F2 score, precision, and recall scores:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¡ç®—F2åˆ†æ•°ã€ç²¾ç¡®ç‡å’Œå¬å›ç‡åˆ†æ•°ï¼š
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The scores of the â€œbalancedâ€ class-weighted logistic regression model are as
    follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: â€œå¹³è¡¡â€ç±»åˆ«åŠ æƒé€»è¾‘å›å½’æ¨¡å‹çš„åˆ†æ•°å¦‚ä¸‹ï¼š
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Upon analyzing the results, we can see a decision boundary that correctly classifies
    most of the positive class examples. The precision comes down while the recall
    goes up. The decline in the F2 score can be attributed to changes in the recall
    and precision values. The model exhibits an improvement in recall, indicating
    its enhanced ability to correctly identify all positive class examples. However,
    this advancement results in a simultaneous drop in precision, suggesting an increased
    rate of mistakes made on the negative class examples (which we donâ€™t really care
    about as much!).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†æç»“æœåï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªå†³ç­–è¾¹ç•Œï¼Œå®ƒæ­£ç¡®åœ°åˆ†ç±»äº†å¤§å¤šæ•°æ­£ç±»ç¤ºä¾‹ã€‚ç²¾ç¡®ç‡ä¸‹é™ï¼Œè€Œå¬å›ç‡ä¸Šå‡ã€‚F2åˆ†æ•°çš„ä¸‹é™å¯ä»¥å½’å› äºå¬å›ç‡å’Œç²¾ç¡®ç‡å€¼çš„å˜åŒ–ã€‚æ¨¡å‹åœ¨å¬å›ç‡æ–¹é¢æœ‰æ‰€æé«˜ï¼Œè¡¨æ˜å…¶è¯†åˆ«æ‰€æœ‰æ­£ç±»ç¤ºä¾‹çš„èƒ½åŠ›å¾—åˆ°äº†å¢å¼ºã€‚ç„¶è€Œï¼Œè¿™ç§è¿›æ­¥å¯¼è‡´ç²¾ç¡®ç‡åŒæ—¶ä¸‹é™ï¼Œè¿™è¡¨æ˜åœ¨è´Ÿç±»ç¤ºä¾‹ï¼ˆæˆ‘ä»¬å¹¶ä¸ç‰¹åˆ«å…³å¿ƒï¼‰ä¸ŠçŠ¯é”™çš„é€Ÿç‡å¢åŠ ã€‚
- en: 'Letâ€™s try to tune the `class_weight` parameter using a grid search that optimizes
    our F2 score. We can always try to optimize any other objective, such as average
    precision, precision, or recall, and so on. The `np.linspace(0.05, 0.95, 20)`
    function is a `numpy` function that generates an array of 20 evenly spaced numbers
    between 0.05 and 0.95:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ç½‘æ ¼æœç´¢è°ƒæ•´`class_weight`å‚æ•°ï¼Œä»¥ä¼˜åŒ–æˆ‘ä»¬çš„F2åˆ†æ•°ã€‚æˆ‘ä»¬å§‹ç»ˆå¯ä»¥å°è¯•ä¼˜åŒ–ä»»ä½•å…¶ä»–ç›®æ ‡ï¼Œä¾‹å¦‚å¹³å‡ç²¾ç¡®ç‡ã€ç²¾ç¡®ç‡æˆ–å¬å›ç‡ç­‰ã€‚`np.linspace(0.05,
    0.95, 20)`å‡½æ•°æ˜¯ä¸€ä¸ª`numpy`å‡½æ•°ï¼Œå®ƒç”Ÿæˆä¸€ä¸ªä»‹äº0.05å’Œ0.95ä¹‹é—´çš„20ä¸ªå‡åŒ€åˆ†å¸ƒçš„æ•°å­—æ•°ç»„ï¼š
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our standard metrics are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ ‡å‡†æŒ‡æ ‡å¦‚ä¸‹ï¼š
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After incorporating these class weights, our decision boundary attempts to
    strike a better balance between misclassifying positive and negative class examples,
    as illustrated in *Figure 5**.8*. This results in a superior F2 score of 0.93,
    increasing the precision value while maintaining a modest recall:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼•å…¥è¿™äº›ç±»åˆ«æƒé‡åï¼Œæˆ‘ä»¬çš„å†³ç­–è¾¹ç•Œè¯•å›¾åœ¨é”™è¯¯åˆ†ç±»æ­£ç±»å’Œè´Ÿç±»ç¤ºä¾‹ä¹‹é—´å–å¾—æ›´å¥½çš„å¹³è¡¡ï¼Œå¦‚å›¾*å›¾5**.8*æ‰€ç¤ºã€‚è¿™å¯¼è‡´F2åˆ†æ•°è¾¾åˆ°0.93ï¼Œæé«˜äº†ç²¾ç¡®ç‡å€¼ï¼ŒåŒæ—¶ä¿æŒé€‚åº¦çš„å¬å›ç‡ï¼š
- en: '![](img/B17259_05_08.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_08.jpg)'
- en: Figure 5.8 â€“ The decision boundary (left) and PR curve (right) of the class-weighted
    logistic regression model
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5.8 â€“ ç±»åˆ«åŠ æƒé€»è¾‘å›å½’æ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼ˆå·¦ï¼‰å’ŒPRæ›²çº¿ï¼ˆå³ï¼‰
- en: ğŸš€ Cost-sensitive learning in production at Microsoft
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ å¾®è½¯åœ¨ç”Ÿäº§ä¸­çš„æˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: In a practical application at Microsoft, the primary objective was to improve
    the **Click-Through Rate** (**CTR**) prediction for Bing ads [5]. Achieving accurate
    CTR prediction is vital for optimizing both user experience and revenue streams.
    A marginal improvement of just 0.1% in prediction accuracy has the potential to
    elevate profits by hundreds of millions of dollars. Through rigorous testing,
    an ensemble model that combines **Neural Networks** (**NNs**) and **Gradient-Boosted
    Decision Trees** (**GBDTs**) emerged as the most effective solution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾®è½¯çš„ä¸€ä¸ªå®é™…åº”ç”¨ä¸­ï¼Œä¸»è¦ç›®æ ‡æ˜¯æé«˜Bingå¹¿å‘Šçš„**ç‚¹å‡»ç‡**ï¼ˆ**CTR**ï¼‰é¢„æµ‹[5]ã€‚å®ç°å‡†ç¡®çš„CTRé¢„æµ‹å¯¹äºä¼˜åŒ–ç”¨æˆ·ä½“éªŒå’Œæ”¶å…¥æµè‡³å…³é‡è¦ã€‚é¢„æµ‹å‡†ç¡®ç‡ä»…æé«˜0.1%ï¼Œå°±æœ‰å¯èƒ½ä½¿åˆ©æ¶¦å¢åŠ æ•°äº¿ç¾å…ƒã€‚é€šè¿‡ä¸¥æ ¼çš„æµ‹è¯•ï¼Œä¸€ä¸ªç»“åˆ**ç¥ç»ç½‘ç»œ**ï¼ˆ**NNs**ï¼‰å’Œ**æ¢¯åº¦æå‡å†³ç­–æ ‘**ï¼ˆ**GBDTs**ï¼‰çš„é›†æˆæ¨¡å‹æˆä¸ºæœ€æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
- en: For the training dataset, 56 million samples were randomly chosen from a monthâ€™s
    log data, each containing hundreds of statistical features. To reduce training
    expenses, non-click cases were **downsampled** by 50% and assigned a **class weight**
    of 2 to maintain the original distribution. Model performance was then assessed
    using a test dataset of 40 million samples randomly drawn from the subsequent
    weekâ€™s logs. Instead of recalibrating the model, class weighting was used to maintain
    the average CTR after downsampling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®­ç»ƒæ•°æ®é›†ï¼Œä»ä¸€ä¸ªæœˆçš„æ—¥å¿—æ•°æ®ä¸­éšæœºé€‰æ‹©äº†5600ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«æ•°ç™¾ä¸ªç»Ÿè®¡ç‰¹å¾ã€‚ä¸ºäº†å‡å°‘è®­ç»ƒæˆæœ¬ï¼Œéç‚¹å‡»æ¡ˆä¾‹è¢«**é™é‡‡æ ·**äº†50%ï¼Œå¹¶åˆ†é…äº†2çš„**ç±»åˆ«æƒé‡**ä»¥ä¿æŒåŸå§‹åˆ†å¸ƒã€‚ç„¶åä½¿ç”¨ä»éšåä¸€å‘¨æ—¥å¿—ä¸­éšæœºæŠ½å–çš„4000ä¸‡ä¸ªæ ·æœ¬çš„æµ‹è¯•æ•°æ®é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è€Œä¸æ˜¯é‡æ–°æ ¡å‡†æ¨¡å‹ï¼Œä½¿ç”¨äº†ç±»åˆ«æƒé‡æ¥åœ¨é™é‡‡æ ·åä¿æŒå¹³å‡CTRã€‚
- en: In the next section, we will discuss how to do CSL with decision trees.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•ä½¿ç”¨å†³ç­–æ ‘è¿›è¡Œä»£ä»·æ•æ„Ÿå­¦ä¹ ã€‚
- en: Cost-Sensitive Learning for decision trees
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘çš„ä»£ä»·æ•æ„Ÿå­¦ä¹ 
- en: Decision trees are binary trees that use conditional decision-making to predict
    the class of the samples. Every tree node represents a set of samples corresponding
    to a chain of conditional statements based on the features. We divide the node
    into two children based on a feature and a threshold value. Imagine a set of students
    with height, weight, age, class, and location. We can divide the set into two
    parts according to the features of age and with a threshold of 8\. Now, all the
    students with ages less than 8 will go into the left child, and all those with
    ages greater than or equal to 8 will go into the right child.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯ä½¿ç”¨æ¡ä»¶å†³ç­–æ¥é¢„æµ‹æ ·æœ¬ç±»åˆ«çš„äºŒå‰æ ‘ã€‚æ¯ä¸ªæ ‘èŠ‚ç‚¹ä»£è¡¨ä¸€ç»„ä¸åŸºäºç‰¹å¾çš„è¿ç»­æ¡ä»¶è¯­å¥ç›¸å¯¹åº”çš„æ ·æœ¬ã€‚æˆ‘ä»¬æ ¹æ®ç‰¹å¾å’Œé˜ˆå€¼å€¼å°†èŠ‚ç‚¹åˆ†ä¸ºä¸¤ä¸ªå­èŠ‚ç‚¹ã€‚æƒ³è±¡ä¸€ä¸‹æœ‰ä¸€ç»„å­¦ç”Ÿï¼Œä»–ä»¬çš„èº«é«˜ã€ä½“é‡ã€å¹´é¾„ã€ç­çº§å’Œä½ç½®ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®å¹´é¾„ç‰¹å¾å’Œ8çš„é˜ˆå€¼å°†è¿™ä¸ªé›†åˆåˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚ç°åœ¨ï¼Œæ‰€æœ‰å¹´é¾„å°äº8å²çš„å­¦ç”Ÿå°†è¿›å…¥å·¦å­èŠ‚ç‚¹ï¼Œè€Œæ‰€æœ‰å¹´é¾„å¤§äºæˆ–ç­‰äº8å²çš„å­¦ç”Ÿå°†è¿›å…¥å³å­èŠ‚ç‚¹ã€‚
- en: This way, we can create a tree by successively choosing features and threshold
    values. Every leaf node of the tree will contain nodes from only one class, respectively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿ç»­é€‰æ‹©ç‰¹å¾å’Œé˜ˆå€¼å€¼æ¥åˆ›å»ºæ ‘ã€‚æ ‘çš„æ¯ä¸ªå¶èŠ‚ç‚¹å°†åªåŒ…å«æ¥è‡ªä¸€ä¸ªç±»åˆ«çš„èŠ‚ç‚¹ã€‚
- en: 'A question often arises during the construction of a decision tree: â€œWhich
    feature and threshold pair should be selected to partition the set of samples
    at a given node?â€ The answer is straightforward: we opt for the pair that produces
    the most uniform (or homogeneous) subsets of data. Ideally, the two resulting
    subsets â€“ referred to as the left and right children â€“ should each contain elements
    predominantly from a single class.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„å»ºå†³ç­–æ ‘çš„è¿‡ç¨‹ä¸­ï¼Œç»å¸¸ä¼šé‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼šâ€œåº”è¯¥é€‰æ‹©å“ªä¸ªç‰¹å¾å’Œé˜ˆå€¼å¯¹æ¥åˆ†å‰²ç»™å®šèŠ‚ç‚¹çš„æ ·æœ¬é›†ï¼Ÿâ€ç­”æ¡ˆå¾ˆç®€å•ï¼šæˆ‘ä»¬é€‰æ‹©äº§ç”Ÿæœ€å‡åŒ€ï¼ˆæˆ–åŒè´¨ï¼‰æ•°æ®å­é›†çš„å¯¹ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œäº§ç”Ÿçš„ä¸¤ä¸ªç»“æœå­é›†â€”â€”è¢«ç§°ä¸ºå·¦å³å­èŠ‚ç‚¹â€”â€”åº”è¯¥å„è‡ªä¸»è¦åŒ…å«æ¥è‡ªå•ä¸ªç±»åˆ«çš„å…ƒç´ ã€‚
- en: 'The degree to which the nodes have a mixture of samples from different classes
    is known as the **impurity** of the node, which can be considered to be a measure
    of loss for decision trees. The more the impurity, the more heterogeneous the
    set of samples. Here are the two most common ways of calculating the impurity:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ä»ä¸åŒç±»åˆ«ä¸­æ··åˆæ ·æœ¬çš„ç¨‹åº¦è¢«ç§°ä¸ºèŠ‚ç‚¹çš„**ä¸çº¯åº¦**ï¼Œè¿™å¯ä»¥è¢«è§†ä¸ºå†³ç­–æ ‘çš„æŸå¤±åº¦é‡ã€‚ä¸çº¯åº¦è¶Šé«˜ï¼Œæ ·æœ¬é›†çš„å¼‚è´¨æ€§å°±è¶Šå¤§ã€‚ä»¥ä¸‹æ˜¯è®¡ç®—ä¸çº¯åº¦çš„ä¸¤ç§æœ€å¸¸è§æ–¹æ³•ï¼š
- en: Gini coefficient
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Giniç³»æ•°
- en: Entropy
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Entropy
- en: 'Letâ€™s look at the formula for the Gini coefficient and entropy for two classes,
    cÂ 1 and cÂ 2:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹Giniç³»æ•°å’Œç†µçš„ä¸¤ä¸ªç±»c1å’Œc2çš„å…¬å¼ï¼š
- en: Gini = 1âˆ’ ProportionÂ c1Â 2âˆ’ ProportionÂ c2Â 2
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Gini = 1âˆ’ ProportionÂ c1Â 2âˆ’ ProportionÂ c2Â 2
- en: 'We will get the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: Entropy = âˆ’ ProportionÂ c1 * log (ProportionÂ c1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Entropy = âˆ’ ProportionÂ c1 * log (ProportionÂ c1)
- en: âˆ’ ProportionÂ c2 * log (ProportionÂ c2)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ’ æ¯”ä¾‹c2 * log (æ¯”ä¾‹c2)
- en: 'To do CSL with decision trees, we just multiply the class weights with the
    terms for each of the classes in the calculation of the Gini and entropy. If the
    weights for the two classes are WÂ 1and WÂ 2, Gini and entropy will look as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨å†³ç­–æ ‘è¿›è¡ŒCSLï¼Œæˆ‘ä»¬åªéœ€å°†ç±»æƒé‡ä¸Giniå’Œç†µè®¡ç®—ä¸­æ¯ä¸ªç±»çš„é¡¹ç›¸ä¹˜ã€‚å¦‚æœä¸¤ä¸ªç±»çš„æƒé‡æ˜¯W1å’ŒW2ï¼ŒGiniå’Œç†µå°†å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: Gini = 1 âˆ’ WÂ 1 * ProportionÂ c1Â 2 âˆ’ WÂ 2 * ProportionÂ c2Â 2
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Gini = 1 âˆ’ W1 * æ¯”ä¾‹c1^2 âˆ’ W2 * æ¯”ä¾‹c2^2
- en: Entropy = âˆ’ WÂ 1 * ProportionÂ c1 * log(ProportionÂ c1)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Entropy = âˆ’ W1 * æ¯”ä¾‹c1 * log(æ¯”ä¾‹c1)
- en: âˆ’ WÂ 2 * ProportionÂ c2 * log(ProportionÂ c2)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ’ W2 * æ¯”ä¾‹c2 * log(æ¯”ä¾‹c2)
- en: Now, the model prioritizes the class with a higher weight over the class with
    a lower weight. If we give more weight to the minority class, the model will make
    the decision that will prioritize nodes with homogeneous minority class samples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ¨¡å‹ä¼˜å…ˆè€ƒè™‘æƒé‡è¾ƒé«˜çš„ç±»ï¼Œè€Œä¸æ˜¯æƒé‡è¾ƒä½çš„ç±»ã€‚å¦‚æœæˆ‘ä»¬ç»™å°‘æ•°ç±»æ›´å¤šçš„æƒé‡ï¼Œæ¨¡å‹å°†åšå‡ºä¼˜å…ˆè€ƒè™‘å…·æœ‰åŒè´¨å°‘æ•°ç±»æ ·æœ¬çš„èŠ‚ç‚¹çš„å†³ç­–ã€‚
- en: In this section, we got some idea of how class weights can be accommodated into
    the loss function of decision trees to account for the misclassification error.
    In the next section, we will see how `scikit-learn` simplifies this process by
    integrating it into the model creation API, eliminating the need for us to manually
    adjust the loss function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†ä¸€äº›å¦‚ä½•å°†ç±»æƒé‡çº³å…¥å†³ç­–æ ‘çš„æŸå¤±å‡½æ•°ä¸­ï¼Œä»¥è€ƒè™‘è¯¯åˆ†ç±»é”™è¯¯ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°`scikit-learn`å¦‚ä½•é€šè¿‡å°†å…¶é›†æˆåˆ°æ¨¡å‹åˆ›å»ºAPIä¸­æ¥ç®€åŒ–æ­¤è¿‡ç¨‹ï¼Œä»è€Œæ¶ˆé™¤æˆ‘ä»¬æ‰‹åŠ¨è°ƒæ•´æŸå¤±å‡½æ•°çš„éœ€è¦ã€‚
- en: Cost-Sensitive Learning using scikit-learn and XGBoost models
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨scikit-learnå’ŒXGBoostæ¨¡å‹è¿›è¡Œæˆæœ¬æ•æ„Ÿå­¦ä¹ 
- en: '`scikit-learn` provides a `class_weight` hyperparameter to adjust the weights
    of various classes for most models. This parameter can be specified in various
    ways for different learning algorithms in `scikit-learn`. However, the main idea
    is that this parameter specifies the weights to use for each class in the loss
    calculation formula. For example, this parameter specifies the values of weightÂ FP
    and weightÂ FN mentioned previously for logistic regression.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`æä¾›äº†ä¸€ä¸ª`class_weight`è¶…å‚æ•°æ¥è°ƒæ•´å¤§å¤šæ•°æ¨¡å‹ä¸­å„ç§ç±»çš„æƒé‡ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥æ ¹æ®`scikit-learn`ä¸­ä¸åŒå­¦ä¹ ç®—æ³•çš„ä¸åŒæ–¹å¼æŒ‡å®šã€‚ç„¶è€Œï¼Œä¸»è¦æ€æƒ³æ˜¯è¿™ä¸ªå‚æ•°æŒ‡å®šäº†æŸå¤±è®¡ç®—å…¬å¼ä¸­æ¯ä¸ªç±»çš„æƒé‡ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªå‚æ•°æŒ‡å®šäº†ä¹‹å‰æåˆ°çš„é€»è¾‘å›å½’ä¸­çš„æƒé‡FPå’Œæƒé‡FNçš„å€¼ã€‚'
- en: 'Similar to the `LogisticRegression` function, for `DecisionTreeClassifier`,
    we could use `DecisionTreeClassifier(class_weight=''balanced'')` or `DecisionTreeClassifier(class_weight={0:
    0.5,` `1: 0.5})`.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸`LogisticRegression`å‡½æ•°ç±»ä¼¼ï¼Œå¯¹äº`DecisionTreeClassifier`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`DecisionTreeClassifier(class_weight=''balanced'')`æˆ–`DecisionTreeClassifier(class_weight={0:
    0.5, 1: 0.5})`ã€‚'
- en: 'Regarding SVM, it can even be extended to multi-class classification by specifying
    a weight value for each class label:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºSVMï¼Œå®ƒç”šè‡³å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªç±»æ ‡ç­¾æŒ‡å®šä¸€ä¸ªæƒé‡å€¼æ¥æ‰©å±•åˆ°å¤šç±»åˆ†ç±»ï¼š
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The general guidance about coming up with the `class_weight` values is to use
    the inverse of the ratio of the majority class to the minority class. We can find
    even more optimal `class_weight` values by performing hyperparameter tuning using
    the GridSearch algorithm (use the `GridSearchCV` function from `scikit-learn`).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºç¡®å®š`class_weight`å€¼çš„é€šç”¨æŒ‡å¯¼åŸåˆ™æ˜¯ä½¿ç”¨å¤šæ•°ç±»ä¸å°‘æ•°ç±»æ¯”ä¾‹çš„å€’æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ç½‘æ ¼æœç´¢ç®—æ³•ï¼ˆä½¿ç”¨`scikit-learn`ä¸­çš„`GridSearchCV`å‡½æ•°ï¼‰è¿›è¡Œè¶…å‚æ•°è°ƒæ•´æ¥æ‰¾åˆ°æ›´ä¼˜çš„`class_weight`å€¼ã€‚
- en: 'Similarly, XGBoost has the `scale_pos_weight` parameter to control the balance
    of positive and negative weights:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼ŒXGBoostä¹Ÿæœ‰`scale_pos_weight`å‚æ•°æ¥æ§åˆ¶æ­£è´Ÿæƒé‡çš„å¹³è¡¡ï¼š
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The default value of `scale_pos_weight` is 1\. A recommended `scale_pos_weight`
    value is `sum(negative_instances)/sum(positive_instances)`, which can be computed
    as `float(np.sum(label == 0)) /` `np.sum(label==1)`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`çš„é»˜è®¤å€¼æ˜¯1ã€‚ä¸€ä¸ªæ¨èçš„`scale_pos_weight`å€¼æ˜¯`sum(negative_instances)/sum(positive_instances)`ï¼Œè¿™å¯ä»¥è®¡ç®—ä¸º`float(np.sum(label
    == 0)) / np.sum(label==1)`ã€‚'
- en: XGBoost has a few other parameters, such as `max_delta_step` and `min_child_weight`,
    that can be tuned for imbalanced datasets. During the optimization process, `max_delta_step`
    determines the step size of updates, affecting learning speed and stability. `min_child_weight`
    controls overfitting and enhances generalization by influencing the size of leaf
    nodes in the decision tree. When dealing with imbalanced data scenarios, adjusting
    these parameters can strategically improve algorithm performance.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'First, letâ€™s use `DecisionTreeClassifier` to solve our classification problem:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output decision boundary is more complex than that of logistic regression
    (*Figure 5**.9*), separating the two classes better and giving an F2 score of
    0.932:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_09.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 â€“ The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'We have reproduced the decision boundary and PR curve of the logistic regression
    model for comparison in *Figure 5**.10*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_10.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 â€“ The decision boundary (left) and PR curve (right) of logistic
    regression (for comparison)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Our standard metrics for the decision tree classifier are as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, letâ€™s use the `class_weight=''balanced''` parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After utilizing the code from before to plot the decision boundary, the PR
    curve, and compute the scores, the outputs are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 â€“ The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The tuned weights improve the F2 score and recall values.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular frameworks such as `scikit-learn` also let us specify `sample_weight`
    as a list of weights for each observation in the dataset. The `sample_weight`
    and `class_weight` parameters can be quite confusing, and their purpose may not
    be very clear from their documentation on when to use what. The following table
    clarifies the difference between the two:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `sample_weight` | `class_weight` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| **Purpose** | Used to specify weights for individual examples.Can be useful
    when some examples are more important than others, regardless of their class.When
    some data is more trustworthy (say labeled using in-house human labelers), it
    can receive a higher weight.Can be useful when you donâ€™t have equal confidence
    in the samples in your batch. | Used to correct class imbalance.Should be used
    when the importance of examples depends on their class. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| **Usage** | Can be used in training as well as testing.Especially useful
    when comparing multiple models on different test sets with metrics such as AUC,
    where itâ€™s often desirable to balance the test set:`sklearn.metrics.confusion_matrix(â€¦,
    sample_weight)``sklearn.linear_model``.``LogisticRegression()``.``score(â€¦,sample_weight)`
    | Mainly used during training to guide the training.Accounts for misclassification
    errors because certain classes are more important than others:`sklearn.linear_model``.``LogisticRegression(``class_weight)`
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| **Effect of setting the value to 0 during** **model training** | Model will
    not take into account the examples for which `samples_weight=0` (irrespective
    of the exampleâ€™s class). | The model will not consider any example belonging to
    the class for which `class_weight = 0`. Also, the model will never predict that
    class. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| **Use** **case example** | When predicting customer churn, if losing certain
    customers would have a larger impact on business because they tend to purchase
    more often or spend more, we would want to give these customers a higher weight
    using `sample_weight`. | If we have a dataset where one class significantly outnumbers
    the other(s), using `class_weight` can help the model pay more attention to the
    underrepresented class(es). |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: Table 5.2 â€“ sample_weight versus class_weight in the scikit-learn library
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: If we use `sample_weight` along with `class_weight`, both will be multiplied,
    and we will see the effect of both parameters. The two can still be used together
    to balance class importance and individual instance importance with their intended
    purposes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `numpy` makes it easier to create the list of weight values that are
    required by `sample_weight`: `sample_weight = np.where(label==1, 80, 20)`. However,
    `scikit-learn` has a function called `sklearn.utils.class_weight.compute_sample_weight()`
    that can be used to estimate the value of `sample_weight` automatically from `class_weight`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight` can also be a dict of values for each label or balanced. If
    we set it to balanced, class weights are determined by `n_samples/(n_classes *`
    `np.bincount(y))`.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The returned value from `class_weight` is a dictionary: `{class_label: weight}`
    for each `class_label` value.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can use `sklearn.utils.class_weight.compute_sample_weight` if
    you have to do multi-label classification.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Cost-sensitive learning in production at Airbnb
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Airbnb [6], the main problem to solve was improving
    the search and discoverability as well as personalization of their Experiences
    (handcrafted activities) platform. As the number of experiences grew, it became
    crucial to effectively rank these experiences to match user preferences and improve
    bookings.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Airbnb aimed to improve its search ranking to provide users with the most relevant
    and high-quality experiences. To promote the quality of their ranking model, they
    used sample weights (discussed in the previous section) in their objective function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: The data imbalance in terms of quality tiers was addressed by using sample weighting
    (discussed in the previous section) in the training data. High-quality experiences
    were given higher weights, and low-quality experiences were given lower weights
    in the objective function. This was done to promote high-quality experiences in
    the search rankings, and they successfully improved the ranking of high-quality
    experiences and reduced low-quality ones without affecting overall bookings, as
    confirmed by A/B tests.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Airbnb iteratively developed and tested its machine learning model, eventually
    integrating it into its production system to rank â€œExperiencesâ€ in real time.
    They went through multiple stages, from building a strong baseline to personalization
    and online scoring to handle various business rules.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about a technique that can convert any model
    into its cost-sensitive version without us knowing about its loss function or
    the inner workings of the model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: MetaCost â€“ making any classification model cost-sensitive
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MetaCost was first introduced in a paper by Pedro Domingos [7] in 1999\. MetaCost
    acts as a wrapper around machine learning algorithms that converts the underlying
    algorithm into a cost-sensitive version of itself. It treats the underlying algorithm
    as a black box and works best with unstable algorithms (defined below). When MetaCost
    was first proposed, CSL was in its early stages. Only a few algorithms, such as
    decision trees, had been converted into their cost-sensitive versions. For some
    models, creating a cost-sensitive version turned out to be easy while for others
    it was a non-trivial task. For algorithms where defining cost-sensitive versions
    of the model turned out to be difficult, people mostly relied upon data sampling
    techniques such as oversampling or undersampling. This was when Domingos came
    up with an approach for converting a large range of algorithms into their cost-sensitive
    versions. MetaCost can work for multi-class classification and with all types
    of cost matrices.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Unstable algorithms
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm is called unstable [8] if a slight change in its initial conditions
    (for example, training data or initial weights) can create a big change in the
    model. Assume you are given a dataset of 1,000 items. A stable model such as a
    **K-Nearest Neighbor** (**KNN**) will not change much if you remove one item from
    the dataset. However, a model such as a decision tree might get completely restructured
    if you train it on 999 items instead of 1,000 items.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s delve into the mechanics of the MetaCost algorithm, as illustrated in
    *Figure 5**.12*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_12.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 â€“ The MetaCost algorithm
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'MetaCost works by combining the concept of bagging with a misclassification
    cost matrix:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: First, we create multiple bootstrap samples of the original data.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train one new copy of the given model for each bootstrap sample. So far,
    the process is the same as bagging. You can see the first two steps in *Figure
    5**.12* on the left-hand side. First, we create bootstrap samples S1, S2, and
    S3 from the original data. Then, we train models L1, L2, and L3 on the samples
    (S1, S2, and S3), respectively.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we send the original data, S, into the ensemble of L1, L2, and L3.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply the misclassification costs obtained from the cost matrix with the
    class probabilities predicted by the ensemble to get the actual cost. This is
    shown on the right-hand side of *Figure 5**.12*.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we relabel the data so that the new class labels minimize the actual cost.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we train a new copy of the model on the relabeled data. This copy of
    the model is used as the final model.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can see the process of relabeling data using MetaCost in *Figure 5**.13*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_13.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 â€“ Process of relabeling data using MetaCost
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: On the left of *Figure 5**.13*, we have the original data. Stars are the minority
    class examples and squares are the majority class examples. Here, all the samples
    inside the oval are predicted as stars, and all the samples outside it are predicted
    as squares. The oval on the left is drawn by assuming the same misclassification
    cost for all errors. In the center, we create a new class boundary based on the
    actual misclassification cost drawn as an elongated oval. Notice that all the
    stars are now classified correctly. Also, notice that some squares are now misclassified
    as stars. This is expected as the misclassification cost for the stars is much
    higher than that of squares. At this point, MetaCost relabels these misclassified
    squares as stars. Finally, MetaCost trains a model on the relabeled data. Because
    the majority class examples that are easily mistaken for the minority class have
    been relabeled as belonging to the minority class, the final model is less likely
    to mislabel instances of the minority class.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: To save space, we have omitted the implementation of the MetaCost algorithm.
    You can find it in the GitHub repository for this chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply the algorithm to the logistic regression model. MetaCost uses
    a cost matrix, which is a hyperparameter. The values in the cost matrix correspond
    to the weight or cost of items in the confusion matrix (the transpose of the confusion
    matrix from *Table 5.1*):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: C = (TNÂ FNÂ FPÂ TPÂ )
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s say we use a cost matrix with equal costs for false positives and false
    negatives (that is, an identity matrix):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 5**.14* shows the decision boundary and metrics, which are very close
    to the ones from the logistic regression classifier (*Figure 5**.10*):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_14.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 â€“ The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with an identity cost matrix
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'We can estimate the cost matrix based on the imbalance ratio of the training
    data:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Figure 5**.15* shows the output decision function and PR curve:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_15.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 â€“ The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with a more optimal cost matrix
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Although the F2 score dropped compared to the baseline, the recall did improve
    drastically.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The various steps in the MetaCost algorithm, such as relabeling the whole training
    set, can be quite an expensive operation, and that might deter us from using this
    technique when our training dataset is large.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive ensemble techniques
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: AdaCost [9], AdaUBoost [10], and AsymBoost [11] are cost-sensitive modifications
    of the AdaBoost model. AdaCost minimizes misclassification costs during iterative
    training. AdaUBoost handles imbalanced datasets by emphasizing the minority class.
    AsymBoost focuses on reducing the costliest misclassifications. They all adjust
    weights while considering misclassification costs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The underlying principle behind these algorithms is that besides allocating
    high initial weights to instances where the cost of misclassification is large,
    the rule for updating weights should also consider costs. This means that the
    weights of expensive misclassifications should be increased while the weights
    of correct classifications should be reduced.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about another cost-sensitive meta-learning
    technique, called threshold adjustment.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Threshold adjustment
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The decision threshold is a very important concept to keep track of. By default,
    we have the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Prediction probability >= 0.5 implies Class 1
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction probability < 0.5 implies Class 0
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the threshold is a powerful meta-parameter that we are free to adjust.
    *Table 5.3* shows predictions from a model versus the true labels.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the default threshold of 0.5, the accuracy is 2/4 = 50%. If, on the
    other hand, the threshold chosen is 0.80, the accuracy is 100%. This shows how
    important the chosen threshold can be:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predicted Output** | **True Output** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| 0.65 | 0 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 0 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| 0.85 | 1 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| 0.95 | 1 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: Table 5.3 â€“ A table showing the predicted output from a model versus the true
    output (labels)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Most of the metrics, such as accuracy, precision, recall, and F1 score, are
    all threshold-dependent metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, metrics such as the ROC curve and the PR curve are threshold-independent,
    which means that these plots evaluate the performance of a model at all possible
    thresholds rather than a single, fixed threshold.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with machine learning metrics such as F1 or accuracy, itâ€™s important
    to understand the role of the threshold value. These metrics, by default, utilize
    a threshold of 0.5\. Therefore, a misconception arises, particularly among novice
    and intermediate machine learning practitioners, that these metrics are inevitably
    linked to this particular threshold.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: However, this can lead to an inaccurate interpretation of the modelâ€™s performance,
    particularly in scenarios involving imbalanced datasets. The selection of the
    metric and the decision threshold are separate choices and should be treated as
    such. Establishing an appropriate threshold is a crucial step in the process,
    which should be considered independently of the chosen metric.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, relying solely on the default threshold of 0.5 can be misleading.
    The threshold should be set based on the specific requirements of the project
    and the nature of the data. Therefore, itâ€™s integral that machine learning practitioners
    understand the interplay between the threshold and the selected metric to accurately
    assess the performance of their models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: In binary classification, altering the threshold will easily change the threshold-dependent
    metrics such as accuracy, F1 score, TPR, or FPR. Many pieces of research [12][13]
    have mentioned the value of threshold adjustment, especially in the case when
    training data is imbalanced. A paper by Provost [14] states that using models
    without adjusting the output thresholds may be a critical mistake. Among deep
    learning domains, Buda et al. [15] show that using **random oversampling** (**ROS**)
    along with thresholding outperforms plain ROS on imbalanced datasets created from
    CIFAR and MNIST. Regardless of whether the data is imbalanced or not, choosing
    an optimal threshold can make a lot of difference in the performance of the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Many times, we would want to find the threshold that optimizes our threshold-dependent
    metric, say F1 score. Here, find the threshold at which the F1 score is the maximum
    (*Figure 5**.16*):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_16.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 â€“ A PR curve with the best threshold that finds the max F1 score
    (see the notebook in this chapterâ€™s GitHub repository)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.17* presents a plot that illustrates the impact of modifying the
    decision threshold on various classification metrics for an imbalanced dataset:
    **True Positive Rate** (**TPR** or recall), **True Negative Rate** (**TNR**),
    **False Positive Rate** (**FPR**), and precision. The model that was used was
    logistic regression without any class weighting or sensitivity to the minority
    class. For the full notebook, please refer to the GitHub repository for this chapter:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_17.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 â€“ A plot of the different classification metrics (TPR, TNR, FPR,
    precision, F1 score, and accuracy) as a function of the decision threshold
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some observations about these plots:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: If the threshold is increased, precision ( Â TPÂ _________________Â Â Total
    number of positive predictions ) typically goes up as well. Why? Because as the
    threshold is increased, the total number of positive predictions would come down,
    and hence, as the denominator decreases, precision increases. Similarly, the opposite
    is true as well: if the threshold goes down, the precision goes down too.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Letâ€™s see the impact of threshold change on recall. The recall
    is defined as Â TPÂ ____________Â Â Total number of positives and the denominator
    is a constant value. As the threshold is lowered, TP may increase and would typically
    increase the recall.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative Rate** (**TNR**): TNR measures the proportion of actual negatives
    that are correctly identified as such. In imbalanced datasets, where the negative
    class is the majority, a naive or poorly performing classifier might have a high
    TNR simply because it predicts the majority class for all or most instances. In
    such cases, the TNR could be misleadingly high.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive Rate** (**FPR**): This is the rate at which negative instances
    are incorrectly classified as positive. In imbalanced datasets, a naive classifier
    that predicts everything as the majority (negative) class would have an FPR close
    to 0.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, we have a trade-off between TPR and TNR that must be taken into account
    while selecting an optimal decision threshold, as shown in the plot in *Figure
    5**.17*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Cost-sensitive learning in production at Shopify
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Shopify [16], the platform faced the challenge
    of categorizing products for millions of merchants selling a diverse array of
    items. Accurate product categorization was vital for functionalities such as enhanced
    search and discovery, as well as providing personalized marketing insights to
    merchants. Given the immense volume and variety of products, manual categorization
    was not feasible. Machine learning techniques were employed to automate the categorization
    process, adapting to the ever-expanding and diversifying product range. The dataset
    that was utilized was highly imbalanced, particularly due to the hierarchical
    structure of the **Google Product Taxonomy** (**GPT**) that Shopify employs. With
    over 5,500 categories, the GPT added complexity to an already challenging problem.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: To address the issue of data imbalance, class weights were implemented. By assigning
    the class weights, the model could impose higher penalties for incorrect predictions
    in underrepresented classes, effectively mitigating the lack of data in those
    categories. The model was fine-tuned to strike a balance between hierarchical
    precision and recall. This fine-tuning was informed by specific business use cases
    and aimed at enhancing the merchant experience by minimizing negative interactions
    and friction. Manual adjustments were made to the confidence thresholds (this
    shows that threshold tuning is so relevant in the real world!) to ensure optimal
    performance in sensitive categories such as â€œReligious and Ceremonial.â€ Various
    metrics such as hierarchical accuracy, precision, recall, and F1 score were balanced
    to tailor the model to business requirements. The model is now actively used by
    multiple internal teams and partner ecosystems to develop derivative data products.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Next, weâ€™ll look at various ways of tuning these thresholds.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Methods for threshold tuning
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, we aim to optimize specific business metrics or standard
    machine learning metrics, requiring us to select a threshold that maximizes the
    metric of interest. In literature, various methods for threshold tuning are discussed,
    such as setting a threshold equal to the priority probability of observing a positive
    example, using the ROC curve to optimize for high TPR and low FPR, or employing
    the PR curve to maximize the F1 score or Fbeta score (see *Figure 5**.18*):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_18.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 â€“ Popular ways of tuning the threshold
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss these methods one by one.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use the same dataset we created earlier. The following
    code block fits a logistic regression model and obtains predicted probabilities
    for the test set:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Threshold tuning using the prior threshold
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An obvious threshold that can be used is equal to the probability of the positive
    class in the training dataset [10]. Letâ€™s implement this idea:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This prints the following threshold:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Threshold tuning using the ROC curve
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the ROC curve, Youdenâ€™s J statistic [17] can be used to find the optimal
    threshold. Youdenâ€™s J statistic has roots in the clinical field and is a single
    statistic that captures the performance of a diagnostic test. In the context of
    binary classification, the statistic, J, is defined as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: J = Sensitivity + Specificity âˆ’ 1 = TPR + TNR âˆ’ 1 = TPR âˆ’ FPR
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Its value can range from -1 (TPR=0 and TNR=0 â€“ that is, always wrong results)
    to 1 (TPR=1 and FPR=0 â€“ that is, perfect results). This is a common choice for
    selecting a threshold in an ROC analysis since it balances both sensitivity (TPR)
    and specificity (TNR). Please note that TNR = 1-FPR.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The reason that maximizing Youdenâ€™s J is equivalent to choosing the optimal
    threshold is that it essentially finds the point on the ROC curve that is farthest
    from the line of no discrimination (the diagonal). This means that it selects
    a threshold that achieves a balance between TPR and FPR, which is often what we
    want in a classifier.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'The â€œoptimalâ€ threshold can depend heavily on the cost of false positives versus
    false negatives in our specific application. The following code block identifies
    the optimal classification threshold using the Youden index, which is calculated
    from the ROC curve:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This outputs the following values:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Another threshold adjustment method that uses ROC curves thatâ€™s often used
    in literature is maximizing the geometric mean of TPR (also known as sensitivity)
    and TNR (also known as specificity):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: G âˆ’ mean = âˆšÂ __________________Â Â Sensitivity * SpecificityÂ  = âˆšÂ _Â TPR * TNRÂ 
    = âˆšÂ ______________Â Â TPR * (1 âˆ’ FPR)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximizing the geometric mean is equivalent to finding a good balance between
    TPR and TNR. The following code block calculates the best threshold for classification
    using the G-mean metric, along with its corresponding TPR, FPR, and TNR values.
    We import `roc_curve` from `sklearn.metrics`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This outputs the following optimal values:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Threshold tuning using the PR curve
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction
    to Data Imbalance in Machine Learning*, the PR curve is generally preferred over
    ROC curves for imbalanced datasets when the positive class is more important than
    the negative class. As a reminder, the simple reason for this is that the PR curve
    ignores the true negatives, and hence, it can represent a stark difference between
    model performance when using imbalanced datasets in comparison to a balanced dataset.
    While ROC curves wonâ€™t change much as an imbalance in the data increases, they
    can be a preferred option if both classes are equally important.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: We want the maximum possible values for both precision and recall, ideally both
    being 1\. Thus, the point of optimality on a PR curve is (1,1).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: When we move away from this point of optimality, both the precision and recall
    values decrease, and we are less optimal.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 'One measure that captures this trade-off between precision and recall is the
    F1 score. The F1 score is defined as the harmonic mean of the precision and recall:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: F1 = 2 * (Precision * Recall) / (Precision + Recall)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: If we analyze this equation, we will see that the F1 score is highest (reaching
    its maximum at 1) when both precision and recall are 1, which is exactly the optimal
    point we defined on the PR curve.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, optimizing for the maximum F1 score would ensure that we are striving
    to maximize both precision and recall, effectively pushing us toward the optimal
    point on the PR curve.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximizing the F1 score is a common and effective method for determining the
    optimal threshold. The following code block calculates the best threshold using
    the F1 score metric derived from the PR curve:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This outputs the following optimal values:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Letâ€™s plot the PR curve with the optimal threshold value:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The PR curve looks like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_19.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 â€“ The PR curve with the best F1 score threshold value
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: General threshold tuning
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a general method, we may have to optimize any metric, such as average precision,
    accuracy, and so on, or any other business metric. In such cases, we can write
    a function to optimize that metric directly. Letâ€™s take the example of the **Index
    of Union** (**IU**) metric defined by I. Unal et al. in their research [18], where
    the metric is defined by the threshold value, c, at which IU(c) is minimized:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: IU(c) = (|Sensitivity(c) âˆ’ ROC _ AUC| + |Specificity(c) âˆ’ ROC _ AUC|)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: Here, Sensitivity(c) is the sensitivity at c, Specificity is the specificity
    at c, and ROC _ AUC is the **Area Under the Curve** (**AUC**) of the ROC plot.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s implement the IU metric, as defined here, as a custom metric to find
    the optimal threshold that minimizes it:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This produces the following optimal values:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This wraps up our discussion on classical modeling techniques. We are now ready
    to venture into studying data imbalance in the realm of deep learning. Weâ€™ll explore
    how the insights gained from the general techniques learned from previous chapters
    can be adapted to enhance our deep learning models when dealing with imbalanced
    data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into CSL, an alternative to oversampling and undersampling.
    Unlike data-level techniques that treat all misclassification errors equally,
    CSL adjusts the cost function of a model to account for the significance of different
    classes. It includes class weighting and meta-learning techniques.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Libraries such as `scikit-learn`, Keras/TensorFlow, and PyTorch support cost-sensitive
    learning. For instance, `scikit-learn` offers a `class_weight` hyperparameter
    to adjust class weights in loss calculation. XGBoost has a `scale_pos_weight`
    parameter for balancing positive and negative weights. MetaCost transforms any
    algorithm into its cost-sensitive version using bagging and a misclassification
    cost matrix. Additionally, threshold adjustment techniques can enhance metrics
    such as F1 score, precision, and recall by post-processing model predictions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with various data sampling and CSL techniques can help determine
    the best approach. Weâ€™ll extend these concepts to deep learning models in [*Chapter
    8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level Deep Learning Techniques*.
    This concludes our discussion of classical machine learning models, and we have
    graduated to move on to deep learning techniques. In the next chapter, we will
    briefly introduce deep learning concepts and see how imbalanced datasets could
    be a problem in the deep learning world.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply the CSL technique to the SVM model from `scikit-learn` while utilizing
    the dataset that was used in this chapter. Use the `class_weight` and `sample_weight`
    parameters, similar to how we used them for other models in this chapter. Compare
    the performance of this model with the ones that we already encountered in this
    chapter.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LightGBM is another gradient-boosting framework similar to XGBoost. Apply the
    cost-sensitive learning technique to a LightGBM model while utilizing the dataset
    we used in this chapter. Use the `class_weight` and `sample_weight` parameters
    similar to how we used them for other models in this chapter. Compare the performance
    of this model with the ones that we already encountered in this chapter.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaCost [10] is a variant of AdaBoost that combines boosting with CSL. It updates
    the training distribution for successive boosting rounds by utilizing the misclassification
    cost. Extend `AdaBoostClassifier` from `scikit-learn` to implement the AdaCost
    algorithm. Compare the performance of AdaCost with MetaCost on the dataset that
    was used in this chapter.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters, specifically `max_depth`, `max_delta_step`, and `min_child_weight`,
    for the XGBoost model using the dataset that we used in this chapter. After tuning,
    evaluate whether the weighted XGBoost model outperforms the non-weighted version.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: P. Turney, *Types of cost in inductive concept learning*, Proc. Workshop on
    CostSensitive Learning at the 17th Int. Conf. Mach. Learn., Stanford University,
    CA (2000), pp. 15â€“21.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C. X. Ling and V. S. Sheng, *Cost-Sensitive Learning and the Class* *Imbalance
    Problem*.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sheng, V. S., & Ling, C. X. (2006). *Thresholding for making classifiers cost-sensitive*.
    AAAIâ€™06: Proceedings of the 21st national conference on artificial intelligence,
    vol. 6, pp. 476â€“481.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Pneumonia in Children Statistics* â€“ UNICEF data: [https://data.unicef.org/topic/child-health/pneumonia/](https://data.unicef.org/topic/child-health/pneumonia/).'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, Model Ensemble for Click
    Prediction in Bing Search Ads, in Proceedings of the 26th International Conference
    on World Wide Web Companion â€“ WWW â€™17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689â€“698\. doi: 10.1145/3041021.3054192.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Machine Learning-Powered Search Ranking of Airbnb Experiences* (2019), [https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789).'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Domingos, MetaCost: A general method for making classifiers cost-sensitive,
    in Proceedings of International Conference on Knowledge Discovery and Data Mining,
    pp. 155â€“164, 1999.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Unstable Learner*. In: Sammut, C., Webb, G.I. (eds) Encyclopedia of Machine
    Learning and Data Mining. Springer, Boston, MA. doi: [https://doi.org/10.1007/978-1-4899-7687-1_866](https://doi.org/10.1007/978-1-4899-7687-1_866).'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, *AdaCost: Misclassiï¬cation*
    *Cost-sensitive Boosting*.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. I. Karakoulas and J. Shawe-Taylor, *Optimizing Classifiers for Imbalanced*
    *Training Sets*.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. Viola and M. Jones, *Fast and Robust Classification using Asymmetric AdaBoost
    and a* *Detector Cascade*.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Output Thresholding for Ensemble Learners
    and Imbalanced Big Data*, in 2021 IEEE 33rd International Conference on Tools
    with Artificial Intelligence (ICTAI), Washington, DC, USA: IEEE, Nov. 2021, pp.
    1449â€“1454\. doi: 10.1109/ICTAI52525.2021.00230.'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Deep Learning and Thresholding with
    Class-Imbalanced Big Data*, in 2019 18th IEEE International Conference On Machine
    Learning And Applications (ICMLA), Boca Raton, FL, USA, Dec. 2019, pp. 755â€“762\.
    doi: 10.1109/ICMLA.2019.00134.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. Provost, *Machine Learning from Imbalanced Data* *Sets 101*.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*, Neural Networks, vol. 106, pp. 249â€“259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Using Rich Image and Text Data to Categorize Products at Scale* (2021), [https://shopify.engineering/using-rich-image-text-data-categorize-products](https://shopify.engineering/using-rich-image-text-data-categorize-products).'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. J. Youden, *Index for rating diagnostic tests*, Cancer, vol. 3, no. 1, pp.
    32â€“35, 1950, doi: 10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I. Unal, *Defining an Optimal Cut-Point Value in ROC Analysis: An Alternative
    Approach*, Computational and Mathematical Methods in Medicine, vol. 2017, pp.
    1â€“14, 2017, doi: 10.1155/2017/3762651.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, *Model Ensemble for Click
    Prediction in Bing Search Ads*, in Proceedings of the 26th International Conference
    on World Wide Web Companion â€“ WWW â€™17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689â€“698\. doi: 10.1145/3041021.3054192.'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
