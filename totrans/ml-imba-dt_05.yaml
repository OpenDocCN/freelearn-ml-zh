- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cost-Sensitive Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have studied various sampling techniques and ways to oversample or
    undersample data. However, both of these techniques have their own unique set
    of issues. For example, oversampling can easily lead to overfitting of the model
    due to the exact or very similar examples being seen repeatedly. Similarly, with
    undersampling, we lose some information (that could have been useful for the model)
    because we discard the majority class examples to balance the training dataset.
    In this chapter, we’ll consider an alternative to the data-level techniques that
    we learned about previously.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning is an effective strategy to tackle imbalanced data.
    We will go through this technique and learn why it can be useful. This will help
    us understand some of the details of cost functions and how machine learning models
    are not designed to deal with imbalanced datasets by default. While machine learning
    models aren’t equipped to handle imbalanced datasets, we will see how modern libraries
    enable this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The concept of **cost-sensitive** **learning** (**CSL**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding costs in practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning for logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning for decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning using `scikit-learn` and XGBoost models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetaCost – making any classification model cost-sensitive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threshold adjustment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand what cost means in the context
    of classification problems, how to adjust model parameters to account for such
    costs, and how to prioritize minority class predictions to mitigate the cost of
    misclassification. We will also look at a generic meta-algorithm that can make
    any algorithm cost-sensitive and a post-processing technique for adjusting prediction
    thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `scikit-learn`, `xgboost`, and `imbalanced-learn`. The code and notebooks
    for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05).
    You can open this GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of this chapter’s notebook or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of Cost-Sensitive Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Cost-Sensitive Learning** (**CSL**) is a technique where the cost function
    of a machine learning model is changed to account for the imbalance in data. The
    key insight behind CSL is that we want our model’s cost function to reflect the
    relative importance of the different classes.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try to understand cost functions in machine learning and various types
    of CSL.
  prefs: []
  type: TYPE_NORMAL
- en: Costs and cost functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A cost function estimates the difference between the actual outcome and the
    predicted outcome from a model. For example, the cost function of the logistic
    regression model is given by the log loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: LogLoss = −  1 _ N * ∑ i=1  N  ( y i * log( ˆ y  i) + (1 − y i)* log(1 −  ˆ y  i))
  prefs: []
  type: TYPE_NORMAL
- en: Here, N is the total number of observations, y i is the true label (0 or 1),
    and  ˆ y  i is the probability value (between 0 and 1) predicted from the model.
  prefs: []
  type: TYPE_NORMAL
- en: One type of cost is called the cost of misclassification errors [1] – that is,
    the cost of predicting the majority class instead of the minority class or vice
    versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, there can be other types of costs that we may incur, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost of labeling the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost of training or evaluating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost of training data collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Predicted Negative** | **Predicted Positive** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual Negative** | True Negative | False Positive |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual Positive** | False Negative | True Positive |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – Confusion matrix for understanding the cost of classification errors
  prefs: []
  type: TYPE_NORMAL
- en: Psychological studies have suggested that loss hurts twice as much as gain.
    Similarly, in machine learning, the “cost” captures whenever the model makes a
    mistake (False Positive and False Negative) and does not worry about when it’s
    right (True Positive and True Negative). This cost is the cost of misclassification
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Not all misclassifications are created equal. For instance, suppose we’re attempting
    to predict whether a patient has cancer. If our model incorrectly indicates that
    the patient has cancer (a false positive), this could lead to additional testing.
    However, if our model incorrectly suggests that the patient is cancer-free (a
    false negative), the consequences could be far more severe as the disease could
    progress undiagnosed. Therefore, a false negative is significantly more detrimental
    than a false positive. Our cost function should take this discrepancy into account.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, most models treat the majority and minority classes equally by
    default. However, modern ML frameworks such as `scikit-learn`, Keras/TensorFlow,
    and PyTorch provide a way to weigh the various classes differently across a variety
    of learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Types of cost-sensitive learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two major types of CSL approaches, namely weighting and meta-learning.
    In weighting approaches, we update the cost function of the machine learning model
    to reflect the importance of the different classes. In meta-learning, we can make
    the model cost-sensitive without changing its cost function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In MetaCost, a type of meta-learning technique, for example, we alter the labels
    of training instances to minimize expected misclassification costs. Similarly,
    in the threshold adjustment method, we determine a probability threshold that
    minimizes total misclassification costs for predictions. *Figure 5**.1* categorizes
    these methods at a high level [2][3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Categorization of cost-sensitive learning methods
  prefs: []
  type: TYPE_NORMAL
- en: Difference between CSL and resampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key difference between previously discussed data-level techniques and CSL
    is that the data-level techniques adjust the frequency of the different error
    types, but they treat all misclassification errors the same. In certain cases,
    as we encountered earlier, the cost of misclassifying observations of different
    classes is not the same. For example, in cancer detection, the cost of misclassifying
    a patient who has cancer as healthy (False Negative) is much higher, as the patient
    is at high risk if not detected or treated early. Similarly, misclassifying a
    fraudulent booking as non-fraudulent can cost more money than wrongly classifying
    a legitimate transaction as fraud. Why? Because in the latter case, we can just
    call and verify with the user the legitimacy of the transaction. By applying resampling
    techniques such as upsampling or downsampling, we are implicitly changing the
    cost of different types of errors. So, CSL and resampling techniques can be considered
    to have an equivalent effect on the model at the end of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, resampling techniques may be problematic in certain cases, as we will
    discuss in the next section. In such cases, CSL can be more practical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Comic re-emphasizing the idea of misclassification errors
  prefs: []
  type: TYPE_NORMAL
- en: Problems with rebalancing techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapters, we briefly touched on why in some cases, we would
    prefer not to apply any data sampling techniques. This could be because of the
    following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: We already have too much training data, and it might be quite expensive to deal
    with more data, or the training time can increase by many folds due to having
    more training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, we may not get the best results using sampling or data rebalancing
    techniques because of the dataset we are using.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An additional consideration is that upon rebalancing the dataset, our model’s
    predictive scores may become miscalibrated, necessitating a recalibration process.
    We will cover this topic in [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*, where we will learn about various model calibration techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rebalancing techniques can lead to model overfitting or underfitting issues.
    Overfitting can especially happen when using oversampling since they produce repeated
    or similar training examples. Similarly, the model may be underfitted when using
    undersampling because the model did not get trained on the data thrown away during
    undersampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s try to understand what costs really mean.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding costs in practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We need to understand the various types of costs involved while creating weights
    for different classes. These costs change on a case-by-case basis. Let’s discuss
    an example of cost calculations to understand what we should consider while thinking
    about cost calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the example of pediatric pneumonia. According to UNICEF, a child
    dies of pneumonia every 43 seconds [4]. Imagine we are creating a new test for
    pediatric pneumonia – how will we decide the cost of different errors?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the confusion matrix from *Table 5.1*. There will usually be no
    extra cost for True Negatives and True Positives. But using a False Negative –
    that is, when a child has pneumonia and predicting the child to be healthy – will
    have a very high cost. On the flip side, when a healthy child is predicted as
    being affected by pneumonia, there will be a cost associated with the troubles
    the family of the child may have to go through, but there will be much less cost
    than in the previous case. Furthermore, the cost of misclassification can vary
    depending on the child’s age. For example, younger kids will be at a higher risk
    than older kids. Thus, we will aim to penalize the model more if it makes an error
    in the case of younger kids.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost can vary depending on the duration of the symptoms. Consider it this
    way: if we make an error and misdiagnose a child who has only had flu symptoms
    for a day, it’s not ideal, but it’s not disastrous. However, if that child has
    been enduring flu symptoms for 2 weeks, that’s a different scenario. That mistake
    will cost us significantly more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While we’ve discussed real-world problems so far, this chapter will pivot to
    utilize a synthetic dataset. This approach is intended to reinforce concepts and
    methods in a controlled environment, thus enhancing the learning process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `make_classification` function produces some overlapping points that we
    cleaned up. To keep things simple, we’ve omitted that cleanup code here. You can
    refer to the full notebook on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code produces the following output and scatter plot (*Figure
    5**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Scatter plot showing the training dataset’s distribution
  prefs: []
  type: TYPE_NORMAL
- en: We’ll dive into how to apply CSL to logistic regression models next.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-Sensitive Learning for logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a simple classification algorithm. We train a model as
    a linear combination of the features. Then, we pass the result of that linear
    combination into a sigmoid function to predict the class probabilities for different
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `sigmoid` function (also called a `logit` function) is a mathematical tool
    capable of converting any real number into a value between 0 and 1\. This value
    can be interpreted as a probability estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph of the sigmoid function has an S-shaped curve, and it appears like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: The class with the highest predicted probability is taken as the prediction
    for a given sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have an email to be classified as spam or non-spam, and our logistic
    regression model outputs the probabilities of 0.25 for non-spam and 0.75 for spam.
    Here, the class with the highest predicted probability is “spam” (1) since 0.75
    is greater than 0.25\. Therefore, the model would predict that this email is spam
    (*Figure 5**.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Higher class probability determining the class for binary classification
  prefs: []
  type: TYPE_NORMAL
- en: For two-class classification, we just predict the probability of one class.
    The probability of the other class is one minus the probability of the first class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logistic regression model is trained using a loss function. The loss function
    for one example from a dataset with two classes would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: cost = − y * log(classProbability) − (1 − y)* log(1 − classProbability)
  prefs: []
  type: TYPE_NORMAL
- en: 'For true positives and true negatives, this loss will be very low. For a false
    positive, y, the actual value would be 0; therefore, the first term will be 0,
    but the second term will be very high as the class probability approaches 1, and
    the term will approach negative infinity (since, log(0) → − ∞). Since there is
    a negative sign at the front, the cost will approach positive infinity. A similar
    analysis can be done for the false negative case. One part of the cost can be
    seen as the false positive part, and another part of the cost can be seen as the
    false negative part:'
  prefs: []
  type: TYPE_NORMAL
- en: cost = falsePositiveCost + falseNegativeCost
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed earlier, we don’t want to weigh the two types of costs equally.
    So, all we do is add weights, W FP and W FN, for the respective costs:'
  prefs: []
  type: TYPE_NORMAL
- en: cost = W FP * falsePositiveCost + W FN * falseNegativeCost
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the crux of CSL with logistic regression. To get the overall costs
    of the model, we take the average cost across all the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'When all errors are equally costly, the model’s decision boundary and the model’s
    **Precision-Recall** (**PR**) curve will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – The decision boundary (left) and PR curve (right) of the baseline
    regression model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code outputs the following F2 score, precision, and recall values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this chapter, we will use the F2 score as our primary metric. What is the
    F2 score? In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to
    Data Imbalance in Machine Learning*, we studied the F-beta score. The F2 score
    is the F-beta score with beta=2, while the F1 score is the F-beta score with beta=1\.
    It’s useful when recall is more important than precision – that is, false negatives
    are more costly (important) than false positives:'
  prefs: []
  type: TYPE_NORMAL
- en: F β =  (1 + β 2) × (precision × recall)  ____________________  (β 2 × precision)
    + recall  =  (5 × precision × recall)  ________________  (4 × precision) + recall
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegression` from the `scikit-learn` library provides a `class_weight`
    parameter. When the value of this parameter is set to “balanced,” the weight of
    each class is automatically computed by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: weightOfClass =  totalNumberOfSamples   ________________________________   numberOfClasses
    * numberOfSamplesPerClass
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we have 100 examples in the dataset – 80 in class 0 and 20 in
    class 1\. The weights of each class are computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight for class 0 = 100/(2*80) = 0.625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight for class 1 = 100/(2*20) = 2.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that the number of class 0 examples is four times that of class 1, the
    weight of class 1 is 2.5, which is four times the weight of class 0 – that is,
    0.625\. This makes sense since we would want to give more weight to class 1, which
    is smaller in number.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can mention `class_weight` as a dictionary as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try to use the `class_weight` parameter in the `LogisticRegression` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The decision boundary (left) and PR curve (right) of the “balanced”
    class-weighted logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s calculate the F2 score, precision, and recall scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The scores of the “balanced” class-weighted logistic regression model are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Upon analyzing the results, we can see a decision boundary that correctly classifies
    most of the positive class examples. The precision comes down while the recall
    goes up. The decline in the F2 score can be attributed to changes in the recall
    and precision values. The model exhibits an improvement in recall, indicating
    its enhanced ability to correctly identify all positive class examples. However,
    this advancement results in a simultaneous drop in precision, suggesting an increased
    rate of mistakes made on the negative class examples (which we don’t really care
    about as much!).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try to tune the `class_weight` parameter using a grid search that optimizes
    our F2 score. We can always try to optimize any other objective, such as average
    precision, precision, or recall, and so on. The `np.linspace(0.05, 0.95, 20)`
    function is a `numpy` function that generates an array of 20 evenly spaced numbers
    between 0.05 and 0.95:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Our standard metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After incorporating these class weights, our decision boundary attempts to
    strike a better balance between misclassifying positive and negative class examples,
    as illustrated in *Figure 5**.8*. This results in a superior F2 score of 0.93,
    increasing the precision value while maintaining a modest recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – The decision boundary (left) and PR curve (right) of the class-weighted
    logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Cost-sensitive learning in production at Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: In a practical application at Microsoft, the primary objective was to improve
    the **Click-Through Rate** (**CTR**) prediction for Bing ads [5]. Achieving accurate
    CTR prediction is vital for optimizing both user experience and revenue streams.
    A marginal improvement of just 0.1% in prediction accuracy has the potential to
    elevate profits by hundreds of millions of dollars. Through rigorous testing,
    an ensemble model that combines **Neural Networks** (**NNs**) and **Gradient-Boosted
    Decision Trees** (**GBDTs**) emerged as the most effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: For the training dataset, 56 million samples were randomly chosen from a month’s
    log data, each containing hundreds of statistical features. To reduce training
    expenses, non-click cases were **downsampled** by 50% and assigned a **class weight**
    of 2 to maintain the original distribution. Model performance was then assessed
    using a test dataset of 40 million samples randomly drawn from the subsequent
    week’s logs. Instead of recalibrating the model, class weighting was used to maintain
    the average CTR after downsampling.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss how to do CSL with decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-Sensitive Learning for decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are binary trees that use conditional decision-making to predict
    the class of the samples. Every tree node represents a set of samples corresponding
    to a chain of conditional statements based on the features. We divide the node
    into two children based on a feature and a threshold value. Imagine a set of students
    with height, weight, age, class, and location. We can divide the set into two
    parts according to the features of age and with a threshold of 8\. Now, all the
    students with ages less than 8 will go into the left child, and all those with
    ages greater than or equal to 8 will go into the right child.
  prefs: []
  type: TYPE_NORMAL
- en: This way, we can create a tree by successively choosing features and threshold
    values. Every leaf node of the tree will contain nodes from only one class, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'A question often arises during the construction of a decision tree: “Which
    feature and threshold pair should be selected to partition the set of samples
    at a given node?” The answer is straightforward: we opt for the pair that produces
    the most uniform (or homogeneous) subsets of data. Ideally, the two resulting
    subsets – referred to as the left and right children – should each contain elements
    predominantly from a single class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The degree to which the nodes have a mixture of samples from different classes
    is known as the **impurity** of the node, which can be considered to be a measure
    of loss for decision trees. The more the impurity, the more heterogeneous the
    set of samples. Here are the two most common ways of calculating the impurity:'
  prefs: []
  type: TYPE_NORMAL
- en: Gini coefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the formula for the Gini coefficient and entropy for two classes,
    c 1 and c 2:'
  prefs: []
  type: TYPE_NORMAL
- en: Gini = 1− Proportion c1 2− Proportion c2 2
  prefs: []
  type: TYPE_NORMAL
- en: 'We will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy = − Proportion c1 * log (Proportion c1)
  prefs: []
  type: TYPE_NORMAL
- en: − Proportion c2 * log (Proportion c2)
  prefs: []
  type: TYPE_NORMAL
- en: 'To do CSL with decision trees, we just multiply the class weights with the
    terms for each of the classes in the calculation of the Gini and entropy. If the
    weights for the two classes are W 1and W 2, Gini and entropy will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Gini = 1 − W 1 * Proportion c1 2 − W 2 * Proportion c2 2
  prefs: []
  type: TYPE_NORMAL
- en: Entropy = − W 1 * Proportion c1 * log(Proportion c1)
  prefs: []
  type: TYPE_NORMAL
- en: − W 2 * Proportion c2 * log(Proportion c2)
  prefs: []
  type: TYPE_NORMAL
- en: Now, the model prioritizes the class with a higher weight over the class with
    a lower weight. If we give more weight to the minority class, the model will make
    the decision that will prioritize nodes with homogeneous minority class samples.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we got some idea of how class weights can be accommodated into
    the loss function of decision trees to account for the misclassification error.
    In the next section, we will see how `scikit-learn` simplifies this process by
    integrating it into the model creation API, eliminating the need for us to manually
    adjust the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-Sensitive Learning using scikit-learn and XGBoost models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn` provides a `class_weight` hyperparameter to adjust the weights
    of various classes for most models. This parameter can be specified in various
    ways for different learning algorithms in `scikit-learn`. However, the main idea
    is that this parameter specifies the weights to use for each class in the loss
    calculation formula. For example, this parameter specifies the values of weight FP
    and weight FN mentioned previously for logistic regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the `LogisticRegression` function, for `DecisionTreeClassifier`,
    we could use `DecisionTreeClassifier(class_weight=''balanced'')` or `DecisionTreeClassifier(class_weight={0:
    0.5,` `1: 0.5})`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding SVM, it can even be extended to multi-class classification by specifying
    a weight value for each class label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The general guidance about coming up with the `class_weight` values is to use
    the inverse of the ratio of the majority class to the minority class. We can find
    even more optimal `class_weight` values by performing hyperparameter tuning using
    the GridSearch algorithm (use the `GridSearchCV` function from `scikit-learn`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, XGBoost has the `scale_pos_weight` parameter to control the balance
    of positive and negative weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The default value of `scale_pos_weight` is 1\. A recommended `scale_pos_weight`
    value is `sum(negative_instances)/sum(positive_instances)`, which can be computed
    as `float(np.sum(label == 0)) /` `np.sum(label==1)`.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost has a few other parameters, such as `max_delta_step` and `min_child_weight`,
    that can be tuned for imbalanced datasets. During the optimization process, `max_delta_step`
    determines the step size of updates, affecting learning speed and stability. `min_child_weight`
    controls overfitting and enhances generalization by influencing the size of leaf
    nodes in the decision tree. When dealing with imbalanced data scenarios, adjusting
    these parameters can strategically improve algorithm performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s use `DecisionTreeClassifier` to solve our classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output decision boundary is more complex than that of logistic regression
    (*Figure 5**.9*), separating the two classes better and giving an F2 score of
    0.932:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  prefs: []
  type: TYPE_NORMAL
- en: 'We have reproduced the decision boundary and PR curve of the logistic regression
    model for comparison in *Figure 5**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – The decision boundary (left) and PR curve (right) of logistic
    regression (for comparison)
  prefs: []
  type: TYPE_NORMAL
- en: 'Our standard metrics for the decision tree classifier are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s use the `class_weight=''balanced''` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'After utilizing the code from before to plot the decision boundary, the PR
    curve, and compute the scores, the outputs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The tuned weights improve the F2 score and recall values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Popular frameworks such as `scikit-learn` also let us specify `sample_weight`
    as a list of weights for each observation in the dataset. The `sample_weight`
    and `class_weight` parameters can be quite confusing, and their purpose may not
    be very clear from their documentation on when to use what. The following table
    clarifies the difference between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `sample_weight` | `class_weight` |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Purpose** | Used to specify weights for individual examples.Can be useful
    when some examples are more important than others, regardless of their class.When
    some data is more trustworthy (say labeled using in-house human labelers), it
    can receive a higher weight.Can be useful when you don’t have equal confidence
    in the samples in your batch. | Used to correct class imbalance.Should be used
    when the importance of examples depends on their class. |'
  prefs: []
  type: TYPE_TB
- en: '| **Usage** | Can be used in training as well as testing.Especially useful
    when comparing multiple models on different test sets with metrics such as AUC,
    where it’s often desirable to balance the test set:`sklearn.metrics.confusion_matrix(…,
    sample_weight)``sklearn.linear_model``.``LogisticRegression()``.``score(…,sample_weight)`
    | Mainly used during training to guide the training.Accounts for misclassification
    errors because certain classes are more important than others:`sklearn.linear_model``.``LogisticRegression(``class_weight)`
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Effect of setting the value to 0 during** **model training** | Model will
    not take into account the examples for which `samples_weight=0` (irrespective
    of the example’s class). | The model will not consider any example belonging to
    the class for which `class_weight = 0`. Also, the model will never predict that
    class. |'
  prefs: []
  type: TYPE_TB
- en: '| **Use** **case example** | When predicting customer churn, if losing certain
    customers would have a larger impact on business because they tend to purchase
    more often or spend more, we would want to give these customers a higher weight
    using `sample_weight`. | If we have a dataset where one class significantly outnumbers
    the other(s), using `class_weight` can help the model pay more attention to the
    underrepresented class(es). |'
  prefs: []
  type: TYPE_TB
- en: Table 5.2 – sample_weight versus class_weight in the scikit-learn library
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: If we use `sample_weight` along with `class_weight`, both will be multiplied,
    and we will see the effect of both parameters. The two can still be used together
    to balance class importance and individual instance importance with their intended
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `numpy` makes it easier to create the list of weight values that are
    required by `sample_weight`: `sample_weight = np.where(label==1, 80, 20)`. However,
    `scikit-learn` has a function called `sklearn.utils.class_weight.compute_sample_weight()`
    that can be used to estimate the value of `sample_weight` automatically from `class_weight`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`class_weight` can also be a dict of values for each label or balanced. If
    we set it to balanced, class weights are determined by `n_samples/(n_classes *`
    `np.bincount(y))`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The returned value from `class_weight` is a dictionary: `{class_label: weight}`
    for each `class_label` value.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can use `sklearn.utils.class_weight.compute_sample_weight` if
    you have to do multi-label classification.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Cost-sensitive learning in production at Airbnb
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Airbnb [6], the main problem to solve was improving
    the search and discoverability as well as personalization of their Experiences
    (handcrafted activities) platform. As the number of experiences grew, it became
    crucial to effectively rank these experiences to match user preferences and improve
    bookings.
  prefs: []
  type: TYPE_NORMAL
- en: Airbnb aimed to improve its search ranking to provide users with the most relevant
    and high-quality experiences. To promote the quality of their ranking model, they
    used sample weights (discussed in the previous section) in their objective function.
  prefs: []
  type: TYPE_NORMAL
- en: The data imbalance in terms of quality tiers was addressed by using sample weighting
    (discussed in the previous section) in the training data. High-quality experiences
    were given higher weights, and low-quality experiences were given lower weights
    in the objective function. This was done to promote high-quality experiences in
    the search rankings, and they successfully improved the ranking of high-quality
    experiences and reduced low-quality ones without affecting overall bookings, as
    confirmed by A/B tests.
  prefs: []
  type: TYPE_NORMAL
- en: Airbnb iteratively developed and tested its machine learning model, eventually
    integrating it into its production system to rank “Experiences” in real time.
    They went through multiple stages, from building a strong baseline to personalization
    and online scoring to handle various business rules.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about a technique that can convert any model
    into its cost-sensitive version without us knowing about its loss function or
    the inner workings of the model.
  prefs: []
  type: TYPE_NORMAL
- en: MetaCost – making any classification model cost-sensitive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MetaCost was first introduced in a paper by Pedro Domingos [7] in 1999\. MetaCost
    acts as a wrapper around machine learning algorithms that converts the underlying
    algorithm into a cost-sensitive version of itself. It treats the underlying algorithm
    as a black box and works best with unstable algorithms (defined below). When MetaCost
    was first proposed, CSL was in its early stages. Only a few algorithms, such as
    decision trees, had been converted into their cost-sensitive versions. For some
    models, creating a cost-sensitive version turned out to be easy while for others
    it was a non-trivial task. For algorithms where defining cost-sensitive versions
    of the model turned out to be difficult, people mostly relied upon data sampling
    techniques such as oversampling or undersampling. This was when Domingos came
    up with an approach for converting a large range of algorithms into their cost-sensitive
    versions. MetaCost can work for multi-class classification and with all types
    of cost matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Unstable algorithms
  prefs: []
  type: TYPE_NORMAL
- en: An algorithm is called unstable [8] if a slight change in its initial conditions
    (for example, training data or initial weights) can create a big change in the
    model. Assume you are given a dataset of 1,000 items. A stable model such as a
    **K-Nearest Neighbor** (**KNN**) will not change much if you remove one item from
    the dataset. However, a model such as a decision tree might get completely restructured
    if you train it on 999 items instead of 1,000 items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s delve into the mechanics of the MetaCost algorithm, as illustrated in
    *Figure 5**.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – The MetaCost algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'MetaCost works by combining the concept of bagging with a misclassification
    cost matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create multiple bootstrap samples of the original data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train one new copy of the given model for each bootstrap sample. So far,
    the process is the same as bagging. You can see the first two steps in *Figure
    5**.12* on the left-hand side. First, we create bootstrap samples S1, S2, and
    S3 from the original data. Then, we train models L1, L2, and L3 on the samples
    (S1, S2, and S3), respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we send the original data, S, into the ensemble of L1, L2, and L3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We multiply the misclassification costs obtained from the cost matrix with the
    class probabilities predicted by the ensemble to get the actual cost. This is
    shown on the right-hand side of *Figure 5**.12*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we relabel the data so that the new class labels minimize the actual cost.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we train a new copy of the model on the relabeled data. This copy of
    the model is used as the final model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can see the process of relabeling data using MetaCost in *Figure 5**.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Process of relabeling data using MetaCost
  prefs: []
  type: TYPE_NORMAL
- en: On the left of *Figure 5**.13*, we have the original data. Stars are the minority
    class examples and squares are the majority class examples. Here, all the samples
    inside the oval are predicted as stars, and all the samples outside it are predicted
    as squares. The oval on the left is drawn by assuming the same misclassification
    cost for all errors. In the center, we create a new class boundary based on the
    actual misclassification cost drawn as an elongated oval. Notice that all the
    stars are now classified correctly. Also, notice that some squares are now misclassified
    as stars. This is expected as the misclassification cost for the stars is much
    higher than that of squares. At this point, MetaCost relabels these misclassified
    squares as stars. Finally, MetaCost trains a model on the relabeled data. Because
    the majority class examples that are easily mistaken for the minority class have
    been relabeled as belonging to the minority class, the final model is less likely
    to mislabel instances of the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: To save space, we have omitted the implementation of the MetaCost algorithm.
    You can find it in the GitHub repository for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will apply the algorithm to the logistic regression model. MetaCost uses
    a cost matrix, which is a hyperparameter. The values in the cost matrix correspond
    to the weight or cost of items in the confusion matrix (the transpose of the confusion
    matrix from *Table 5.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: C = (TN FN FP TP )
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we use a cost matrix with equal costs for false positives and false
    negatives (that is, an identity matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 5**.14* shows the decision boundary and metrics, which are very close
    to the ones from the logistic regression classifier (*Figure 5**.10*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with an identity cost matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'We can estimate the cost matrix based on the imbalance ratio of the training
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 5**.15* shows the output decision function and PR curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with a more optimal cost matrix
  prefs: []
  type: TYPE_NORMAL
- en: Although the F2 score dropped compared to the baseline, the recall did improve
    drastically.
  prefs: []
  type: TYPE_NORMAL
- en: The various steps in the MetaCost algorithm, such as relabeling the whole training
    set, can be quite an expensive operation, and that might deter us from using this
    technique when our training dataset is large.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive ensemble techniques
  prefs: []
  type: TYPE_NORMAL
- en: AdaCost [9], AdaUBoost [10], and AsymBoost [11] are cost-sensitive modifications
    of the AdaBoost model. AdaCost minimizes misclassification costs during iterative
    training. AdaUBoost handles imbalanced datasets by emphasizing the minority class.
    AsymBoost focuses on reducing the costliest misclassifications. They all adjust
    weights while considering misclassification costs.
  prefs: []
  type: TYPE_NORMAL
- en: The underlying principle behind these algorithms is that besides allocating
    high initial weights to instances where the cost of misclassification is large,
    the rule for updating weights should also consider costs. This means that the
    weights of expensive misclassifications should be increased while the weights
    of correct classifications should be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about another cost-sensitive meta-learning
    technique, called threshold adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: Threshold adjustment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The decision threshold is a very important concept to keep track of. By default,
    we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction probability >= 0.5 implies Class 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction probability < 0.5 implies Class 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the threshold is a powerful meta-parameter that we are free to adjust.
    *Table 5.3* shows predictions from a model versus the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the default threshold of 0.5, the accuracy is 2/4 = 50%. If, on the
    other hand, the threshold chosen is 0.80, the accuracy is 100%. This shows how
    important the chosen threshold can be:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Predicted Output** | **True Output** |'
  prefs: []
  type: TYPE_TB
- en: '| 0.65 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.85 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.95 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 5.3 – A table showing the predicted output from a model versus the true
    output (labels)
  prefs: []
  type: TYPE_NORMAL
- en: Most of the metrics, such as accuracy, precision, recall, and F1 score, are
    all threshold-dependent metrics.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, metrics such as the ROC curve and the PR curve are threshold-independent,
    which means that these plots evaluate the performance of a model at all possible
    thresholds rather than a single, fixed threshold.
  prefs: []
  type: TYPE_NORMAL
- en: When dealing with machine learning metrics such as F1 or accuracy, it’s important
    to understand the role of the threshold value. These metrics, by default, utilize
    a threshold of 0.5\. Therefore, a misconception arises, particularly among novice
    and intermediate machine learning practitioners, that these metrics are inevitably
    linked to this particular threshold.
  prefs: []
  type: TYPE_NORMAL
- en: However, this can lead to an inaccurate interpretation of the model’s performance,
    particularly in scenarios involving imbalanced datasets. The selection of the
    metric and the decision threshold are separate choices and should be treated as
    such. Establishing an appropriate threshold is a crucial step in the process,
    which should be considered independently of the chosen metric.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, relying solely on the default threshold of 0.5 can be misleading.
    The threshold should be set based on the specific requirements of the project
    and the nature of the data. Therefore, it’s integral that machine learning practitioners
    understand the interplay between the threshold and the selected metric to accurately
    assess the performance of their models.
  prefs: []
  type: TYPE_NORMAL
- en: In binary classification, altering the threshold will easily change the threshold-dependent
    metrics such as accuracy, F1 score, TPR, or FPR. Many pieces of research [12][13]
    have mentioned the value of threshold adjustment, especially in the case when
    training data is imbalanced. A paper by Provost [14] states that using models
    without adjusting the output thresholds may be a critical mistake. Among deep
    learning domains, Buda et al. [15] show that using **random oversampling** (**ROS**)
    along with thresholding outperforms plain ROS on imbalanced datasets created from
    CIFAR and MNIST. Regardless of whether the data is imbalanced or not, choosing
    an optimal threshold can make a lot of difference in the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many times, we would want to find the threshold that optimizes our threshold-dependent
    metric, say F1 score. Here, find the threshold at which the F1 score is the maximum
    (*Figure 5**.16*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – A PR curve with the best threshold that finds the max F1 score
    (see the notebook in this chapter’s GitHub repository)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.17* presents a plot that illustrates the impact of modifying the
    decision threshold on various classification metrics for an imbalanced dataset:
    **True Positive Rate** (**TPR** or recall), **True Negative Rate** (**TNR**),
    **False Positive Rate** (**FPR**), and precision. The model that was used was
    logistic regression without any class weighting or sensitivity to the minority
    class. For the full notebook, please refer to the GitHub repository for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – A plot of the different classification metrics (TPR, TNR, FPR,
    precision, F1 score, and accuracy) as a function of the decision threshold
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some observations about these plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: If the threshold is increased, precision (  TP _________________  Total
    number of positive predictions ) typically goes up as well. Why? Because as the
    threshold is increased, the total number of positive predictions would come down,
    and hence, as the denominator decreases, precision increases. Similarly, the opposite
    is true as well: if the threshold goes down, the precision goes down too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Let’s see the impact of threshold change on recall. The recall
    is defined as  TP ____________  Total number of positives and the denominator
    is a constant value. As the threshold is lowered, TP may increase and would typically
    increase the recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative Rate** (**TNR**): TNR measures the proportion of actual negatives
    that are correctly identified as such. In imbalanced datasets, where the negative
    class is the majority, a naive or poorly performing classifier might have a high
    TNR simply because it predicts the majority class for all or most instances. In
    such cases, the TNR could be misleadingly high.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive Rate** (**FPR**): This is the rate at which negative instances
    are incorrectly classified as positive. In imbalanced datasets, a naive classifier
    that predicts everything as the majority (negative) class would have an FPR close
    to 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, we have a trade-off between TPR and TNR that must be taken into account
    while selecting an optimal decision threshold, as shown in the plot in *Figure
    5**.17*.
  prefs: []
  type: TYPE_NORMAL
- en: 🚀 Cost-sensitive learning in production at Shopify
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Shopify [16], the platform faced the challenge
    of categorizing products for millions of merchants selling a diverse array of
    items. Accurate product categorization was vital for functionalities such as enhanced
    search and discovery, as well as providing personalized marketing insights to
    merchants. Given the immense volume and variety of products, manual categorization
    was not feasible. Machine learning techniques were employed to automate the categorization
    process, adapting to the ever-expanding and diversifying product range. The dataset
    that was utilized was highly imbalanced, particularly due to the hierarchical
    structure of the **Google Product Taxonomy** (**GPT**) that Shopify employs. With
    over 5,500 categories, the GPT added complexity to an already challenging problem.
  prefs: []
  type: TYPE_NORMAL
- en: To address the issue of data imbalance, class weights were implemented. By assigning
    the class weights, the model could impose higher penalties for incorrect predictions
    in underrepresented classes, effectively mitigating the lack of data in those
    categories. The model was fine-tuned to strike a balance between hierarchical
    precision and recall. This fine-tuning was informed by specific business use cases
    and aimed at enhancing the merchant experience by minimizing negative interactions
    and friction. Manual adjustments were made to the confidence thresholds (this
    shows that threshold tuning is so relevant in the real world!) to ensure optimal
    performance in sensitive categories such as “Religious and Ceremonial.” Various
    metrics such as hierarchical accuracy, precision, recall, and F1 score were balanced
    to tailor the model to business requirements. The model is now actively used by
    multiple internal teams and partner ecosystems to develop derivative data products.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at various ways of tuning these thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Methods for threshold tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, we aim to optimize specific business metrics or standard
    machine learning metrics, requiring us to select a threshold that maximizes the
    metric of interest. In literature, various methods for threshold tuning are discussed,
    such as setting a threshold equal to the priority probability of observing a positive
    example, using the ROC curve to optimize for high TPR and low FPR, or employing
    the PR curve to maximize the F1 score or Fbeta score (see *Figure 5**.18*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Popular ways of tuning the threshold
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss these methods one by one.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use the same dataset we created earlier. The following
    code block fits a logistic regression model and obtains predicted probabilities
    for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Threshold tuning using the prior threshold
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An obvious threshold that can be used is equal to the probability of the positive
    class in the training dataset [10]. Let’s implement this idea:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Threshold tuning using the ROC curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the ROC curve, Youden’s J statistic [17] can be used to find the optimal
    threshold. Youden’s J statistic has roots in the clinical field and is a single
    statistic that captures the performance of a diagnostic test. In the context of
    binary classification, the statistic, J, is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: J = Sensitivity + Specificity − 1 = TPR + TNR − 1 = TPR − FPR
  prefs: []
  type: TYPE_NORMAL
- en: Its value can range from -1 (TPR=0 and TNR=0 – that is, always wrong results)
    to 1 (TPR=1 and FPR=0 – that is, perfect results). This is a common choice for
    selecting a threshold in an ROC analysis since it balances both sensitivity (TPR)
    and specificity (TNR). Please note that TNR = 1-FPR.
  prefs: []
  type: TYPE_NORMAL
- en: The reason that maximizing Youden’s J is equivalent to choosing the optimal
    threshold is that it essentially finds the point on the ROC curve that is farthest
    from the line of no discrimination (the diagonal). This means that it selects
    a threshold that achieves a balance between TPR and FPR, which is often what we
    want in a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “optimal” threshold can depend heavily on the cost of false positives versus
    false negatives in our specific application. The following code block identifies
    the optimal classification threshold using the Youden index, which is calculated
    from the ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Another threshold adjustment method that uses ROC curves that’s often used
    in literature is maximizing the geometric mean of TPR (also known as sensitivity)
    and TNR (also known as specificity):'
  prefs: []
  type: TYPE_NORMAL
- en: G − mean = √ __________________  Sensitivity * Specificity  = √ _ TPR * TNR 
    = √ ______________  TPR * (1 − FPR)
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximizing the geometric mean is equivalent to finding a good balance between
    TPR and TNR. The following code block calculates the best threshold for classification
    using the G-mean metric, along with its corresponding TPR, FPR, and TNR values.
    We import `roc_curve` from `sklearn.metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Threshold tuning using the PR curve
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction
    to Data Imbalance in Machine Learning*, the PR curve is generally preferred over
    ROC curves for imbalanced datasets when the positive class is more important than
    the negative class. As a reminder, the simple reason for this is that the PR curve
    ignores the true negatives, and hence, it can represent a stark difference between
    model performance when using imbalanced datasets in comparison to a balanced dataset.
    While ROC curves won’t change much as an imbalance in the data increases, they
    can be a preferred option if both classes are equally important.
  prefs: []
  type: TYPE_NORMAL
- en: We want the maximum possible values for both precision and recall, ideally both
    being 1\. Thus, the point of optimality on a PR curve is (1,1).
  prefs: []
  type: TYPE_NORMAL
- en: When we move away from this point of optimality, both the precision and recall
    values decrease, and we are less optimal.
  prefs: []
  type: TYPE_NORMAL
- en: 'One measure that captures this trade-off between precision and recall is the
    F1 score. The F1 score is defined as the harmonic mean of the precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 = 2 * (Precision * Recall) / (Precision + Recall)
  prefs: []
  type: TYPE_NORMAL
- en: If we analyze this equation, we will see that the F1 score is highest (reaching
    its maximum at 1) when both precision and recall are 1, which is exactly the optimal
    point we defined on the PR curve.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, optimizing for the maximum F1 score would ensure that we are striving
    to maximize both precision and recall, effectively pushing us toward the optimal
    point on the PR curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximizing the F1 score is a common and effective method for determining the
    optimal threshold. The following code block calculates the best threshold using
    the F1 score metric derived from the PR curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the PR curve with the optimal threshold value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The PR curve looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – The PR curve with the best F1 score threshold value
  prefs: []
  type: TYPE_NORMAL
- en: General threshold tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a general method, we may have to optimize any metric, such as average precision,
    accuracy, and so on, or any other business metric. In such cases, we can write
    a function to optimize that metric directly. Let’s take the example of the **Index
    of Union** (**IU**) metric defined by I. Unal et al. in their research [18], where
    the metric is defined by the threshold value, c, at which IU(c) is minimized:'
  prefs: []
  type: TYPE_NORMAL
- en: IU(c) = (|Sensitivity(c) − ROC _ AUC| + |Specificity(c) − ROC _ AUC|)
  prefs: []
  type: TYPE_NORMAL
- en: Here, Sensitivity(c) is the sensitivity at c, Specificity is the specificity
    at c, and ROC _ AUC is the **Area Under the Curve** (**AUC**) of the ROC plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement the IU metric, as defined here, as a custom metric to find
    the optimal threshold that minimizes it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following optimal values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This wraps up our discussion on classical modeling techniques. We are now ready
    to venture into studying data imbalance in the realm of deep learning. We’ll explore
    how the insights gained from the general techniques learned from previous chapters
    can be adapted to enhance our deep learning models when dealing with imbalanced
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into CSL, an alternative to oversampling and undersampling.
    Unlike data-level techniques that treat all misclassification errors equally,
    CSL adjusts the cost function of a model to account for the significance of different
    classes. It includes class weighting and meta-learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries such as `scikit-learn`, Keras/TensorFlow, and PyTorch support cost-sensitive
    learning. For instance, `scikit-learn` offers a `class_weight` hyperparameter
    to adjust class weights in loss calculation. XGBoost has a `scale_pos_weight`
    parameter for balancing positive and negative weights. MetaCost transforms any
    algorithm into its cost-sensitive version using bagging and a misclassification
    cost matrix. Additionally, threshold adjustment techniques can enhance metrics
    such as F1 score, precision, and recall by post-processing model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with various data sampling and CSL techniques can help determine
    the best approach. We’ll extend these concepts to deep learning models in [*Chapter
    8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level Deep Learning Techniques*.
    This concludes our discussion of classical machine learning models, and we have
    graduated to move on to deep learning techniques. In the next chapter, we will
    briefly introduce deep learning concepts and see how imbalanced datasets could
    be a problem in the deep learning world.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apply the CSL technique to the SVM model from `scikit-learn` while utilizing
    the dataset that was used in this chapter. Use the `class_weight` and `sample_weight`
    parameters, similar to how we used them for other models in this chapter. Compare
    the performance of this model with the ones that we already encountered in this
    chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LightGBM is another gradient-boosting framework similar to XGBoost. Apply the
    cost-sensitive learning technique to a LightGBM model while utilizing the dataset
    we used in this chapter. Use the `class_weight` and `sample_weight` parameters
    similar to how we used them for other models in this chapter. Compare the performance
    of this model with the ones that we already encountered in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AdaCost [10] is a variant of AdaBoost that combines boosting with CSL. It updates
    the training distribution for successive boosting rounds by utilizing the misclassification
    cost. Extend `AdaBoostClassifier` from `scikit-learn` to implement the AdaCost
    algorithm. Compare the performance of AdaCost with MetaCost on the dataset that
    was used in this chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters, specifically `max_depth`, `max_delta_step`, and `min_child_weight`,
    for the XGBoost model using the dataset that we used in this chapter. After tuning,
    evaluate whether the weighted XGBoost model outperforms the non-weighted version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: P. Turney, *Types of cost in inductive concept learning*, Proc. Workshop on
    CostSensitive Learning at the 17th Int. Conf. Mach. Learn., Stanford University,
    CA (2000), pp. 15–21.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: C. X. Ling and V. S. Sheng, *Cost-Sensitive Learning and the Class* *Imbalance
    Problem*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sheng, V. S., & Ling, C. X. (2006). *Thresholding for making classifiers cost-sensitive*.
    AAAI’06: Proceedings of the 21st national conference on artificial intelligence,
    vol. 6, pp. 476–481.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Pneumonia in Children Statistics* – UNICEF data: [https://data.unicef.org/topic/child-health/pneumonia/](https://data.unicef.org/topic/child-health/pneumonia/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, Model Ensemble for Click
    Prediction in Bing Search Ads, in Proceedings of the 26th International Conference
    on World Wide Web Companion – WWW ’17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689–698\. doi: 10.1145/3041021.3054192.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Machine Learning-Powered Search Ranking of Airbnb Experiences* (2019), [https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Domingos, MetaCost: A general method for making classifiers cost-sensitive,
    in Proceedings of International Conference on Knowledge Discovery and Data Mining,
    pp. 155–164, 1999.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Unstable Learner*. In: Sammut, C., Webb, G.I. (eds) Encyclopedia of Machine
    Learning and Data Mining. Springer, Boston, MA. doi: [https://doi.org/10.1007/978-1-4899-7687-1_866](https://doi.org/10.1007/978-1-4899-7687-1_866).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, *AdaCost: Misclassiﬁcation*
    *Cost-sensitive Boosting*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: G. I. Karakoulas and J. Shawe-Taylor, *Optimizing Classifiers for Imbalanced*
    *Training Sets*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: P. Viola and M. Jones, *Fast and Robust Classification using Asymmetric AdaBoost
    and a* *Detector Cascade*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Output Thresholding for Ensemble Learners
    and Imbalanced Big Data*, in 2021 IEEE 33rd International Conference on Tools
    with Artificial Intelligence (ICTAI), Washington, DC, USA: IEEE, Nov. 2021, pp.
    1449–1454\. doi: 10.1109/ICTAI52525.2021.00230.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Deep Learning and Thresholding with
    Class-Imbalanced Big Data*, in 2019 18th IEEE International Conference On Machine
    Learning And Applications (ICMLA), Boca Raton, FL, USA, Dec. 2019, pp. 755–762\.
    doi: 10.1109/ICMLA.2019.00134.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: F. Provost, *Machine Learning from Imbalanced Data* *Sets 101*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*, Neural Networks, vol. 106, pp. 249–259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Using Rich Image and Text Data to Categorize Products at Scale* (2021), [https://shopify.engineering/using-rich-image-text-data-categorize-products](https://shopify.engineering/using-rich-image-text-data-categorize-products).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'W. J. Youden, *Index for rating diagnostic tests*, Cancer, vol. 3, no. 1, pp.
    32–35, 1950, doi: 10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I. Unal, *Defining an Optimal Cut-Point Value in ROC Analysis: An Alternative
    Approach*, Computational and Mathematical Methods in Medicine, vol. 2017, pp.
    1–14, 2017, doi: 10.1155/2017/3762651.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, *Model Ensemble for Click
    Prediction in Bing Search Ads*, in Proceedings of the 26th International Conference
    on World Wide Web Companion – WWW ’17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689–698\. doi: 10.1145/3041021.3054192.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
