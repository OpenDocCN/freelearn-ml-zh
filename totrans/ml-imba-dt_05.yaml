- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Cost-Sensitive Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本敏感学习
- en: So far, we have studied various sampling techniques and ways to oversample or
    undersample data. However, both of these techniques have their own unique set
    of issues. For example, oversampling can easily lead to overfitting of the model
    due to the exact or very similar examples being seen repeatedly. Similarly, with
    undersampling, we lose some information (that could have been useful for the model)
    because we discard the majority class examples to balance the training dataset.
    In this chapter, we’ll consider an alternative to the data-level techniques that
    we learned about previously.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了各种采样技术以及如何对数据进行过采样或欠采样。然而，这两种技术都有其独特的问题。例如，过采样可能会因为重复看到精确或非常相似的例子而导致模型过拟合。同样，在欠采样中，我们失去了一些信息（这些信息可能对模型有用），因为我们丢弃了大多数类别的例子来平衡训练数据集。在本章中，我们将考虑之前所学数据级技术的替代方案。
- en: Cost-sensitive learning is an effective strategy to tackle imbalanced data.
    We will go through this technique and learn why it can be useful. This will help
    us understand some of the details of cost functions and how machine learning models
    are not designed to deal with imbalanced datasets by default. While machine learning
    models aren’t equipped to handle imbalanced datasets, we will see how modern libraries
    enable this.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 成本敏感学习是一种有效的方法来处理不平衡数据。我们将介绍这项技术，并了解为什么它可能是有用的。这将帮助我们理解成本函数的一些细节以及机器学习模型默认不是设计来处理不平衡数据集的。虽然机器学习模型没有配备处理不平衡数据集的能力，但我们将看到现代库是如何实现这一点的。
- en: 'We will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: The concept of **cost-sensitive** **learning** (**CSL**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本敏感** **学习**（CSL）的概念'
- en: Understanding costs in practice
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践中的成本理解
- en: Cost-sensitive learning for logistic regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归的成本敏感学习
- en: Cost-sensitive learning for decision trees
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的成本敏感学习
- en: Cost-sensitive learning using `scikit-learn` and XGBoost models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`scikit-learn`和XGBoost模型进行成本敏感学习
- en: MetaCost – making any classification model cost-sensitive
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetaCost – 使任何分类模型具有成本敏感性
- en: Threshold adjustment
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值调整
- en: By the end of this chapter, you will understand what cost means in the context
    of classification problems, how to adjust model parameters to account for such
    costs, and how to prioritize minority class predictions to mitigate the cost of
    misclassification. We will also look at a generic meta-algorithm that can make
    any algorithm cost-sensitive and a post-processing technique for adjusting prediction
    thresholds.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解在分类问题中成本的含义，如何调整模型参数以考虑这些成本，以及如何优先考虑少数类别的预测以减轻误分类的成本。我们还将探讨一个通用的元算法，该算法可以使任何算法具有成本敏感性，以及一种后处理技术，用于调整预测阈值。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `numpy`, `scikit-learn`, `xgboost`, and `imbalanced-learn`. The code and notebooks
    for this chapter are available on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05).
    You can open this GitHub notebook using Google Colab by clicking on the **Open
    in Colab** icon at the top of this chapter’s notebook or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com) using the
    GitHub URL of the notebook.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节类似，我们将继续使用常见的库，如`numpy`、`scikit-learn`、`xgboost`和`imbalanced-learn`。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter05)。您可以通过点击本章笔记本顶部的**在Colab中打开**图标或通过使用笔记本的GitHub
    URL在[https://colab.research.google.com](https://colab.research.google.com)启动它来打开这个GitHub笔记本。
- en: The concept of Cost-Sensitive Learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本敏感学习（CSL）的概念
- en: '**Cost-Sensitive Learning** (**CSL**) is a technique where the cost function
    of a machine learning model is changed to account for the imbalance in data. The
    key insight behind CSL is that we want our model’s cost function to reflect the
    relative importance of the different classes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本敏感学习**（CSL）是一种技术，其中机器学习模型的成本函数被改变以考虑数据的不平衡。CSL背后的关键洞察是我们希望我们的模型成本函数反映不同类别的相对重要性。'
- en: Let’s try to understand cost functions in machine learning and various types
    of CSL.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解机器学习中的成本函数和各种类型的成本敏感学习（CSL）。
- en: Costs and cost functions
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本和成本函数
- en: 'A cost function estimates the difference between the actual outcome and the
    predicted outcome from a model. For example, the cost function of the logistic
    regression model is given by the log loss function:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数估计了实际结果与模型预测结果之间的差异。例如，逻辑回归模型的成本函数由对数损失函数给出：
- en: LogLoss = −  1 _ N * ∑ i=1  N  ( y i * log( ˆ y  i) + (1 − y i)* log(1 −  ˆ y  i))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LogLoss = − 1 * N * ∑ i=1 N (y_i * log(ˆy_i) + (1 − y_i) * log(1 − ˆy_i))
- en: Here, N is the total number of observations, y i is the true label (0 or 1),
    and  ˆ y  i is the probability value (between 0 and 1) predicted from the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，N 是观察的总数，y_i 是真实标签（0 或 1），ˆy_i 是从模型预测出的概率值（介于 0 和 1 之间）。
- en: One type of cost is called the cost of misclassification errors [1] – that is,
    the cost of predicting the majority class instead of the minority class or vice
    versa.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类型的成本被称为误分类错误成本 [1] - 即，预测多数类而不是少数类或反之的成本。
- en: 'In practice, there can be other types of costs that we may incur, such as the
    following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可能会遇到其他类型的成本，例如以下这些：
- en: Cost of labeling the dataset
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记数据集的成本
- en: Cost of training or evaluating the model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练或评估模型的成本
- en: Cost of training data collection
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据收集的成本
- en: 'Let’s consider the confusion matrix:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑混淆矩阵：
- en: '|  | **Predicted Negative** | **Predicted Positive** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | **预测为负** | **预测为正** |'
- en: '| --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Actual Negative** | True Negative | False Positive |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **实际为负** | 真阴性 | 假阳性 |'
- en: '| **Actual Positive** | False Negative | True Positive |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **实际为正** | 假阴性 | 真阳性 |'
- en: Table 5.1 – Confusion matrix for understanding the cost of classification errors
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 理解分类错误成本的混淆矩阵
- en: Psychological studies have suggested that loss hurts twice as much as gain.
    Similarly, in machine learning, the “cost” captures whenever the model makes a
    mistake (False Positive and False Negative) and does not worry about when it’s
    right (True Positive and True Negative). This cost is the cost of misclassification
    errors.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 心理学研究表明，损失带来的痛苦是收益的两倍。同样，在机器学习中，“成本”捕捉了模型出错（假阳性与假阴性）的时刻，而不关心它正确的时候（真阳性与真阴性）。这种成本是误分类错误的成本。
- en: Not all misclassifications are created equal. For instance, suppose we’re attempting
    to predict whether a patient has cancer. If our model incorrectly indicates that
    the patient has cancer (a false positive), this could lead to additional testing.
    However, if our model incorrectly suggests that the patient is cancer-free (a
    false negative), the consequences could be far more severe as the disease could
    progress undiagnosed. Therefore, a false negative is significantly more detrimental
    than a false positive. Our cost function should take this discrepancy into account.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有误分类都是相同的。例如，假设我们正在尝试预测患者是否患有癌症。如果我们的模型错误地指示患者患有癌症（假阳性），这可能导致额外的测试。然而，如果我们的模型错误地建议患者无病（假阴性），后果可能更为严重，因为疾病可能会未诊断而进展。因此，假阴性比假阳性更具破坏性。我们的成本函数应该考虑这种差异。
- en: Unfortunately, most models treat the majority and minority classes equally by
    default. However, modern ML frameworks such as `scikit-learn`, Keras/TensorFlow,
    and PyTorch provide a way to weigh the various classes differently across a variety
    of learning algorithms.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，大多数模型默认将多数类和少数类同等对待。然而，现代机器学习框架如 `scikit-learn`、Keras/TensorFlow 和 PyTorch
    提供了一种方法，可以在各种学习算法中不同地权衡各种类。
- en: Types of cost-sensitive learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本敏感学习类型
- en: There are two major types of CSL approaches, namely weighting and meta-learning.
    In weighting approaches, we update the cost function of the machine learning model
    to reflect the importance of the different classes. In meta-learning, we can make
    the model cost-sensitive without changing its cost function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 成本敏感学习（CSL）有两种主要方法，即加权法和元学习。在加权方法中，我们更新机器学习模型的成本函数，以反映不同类的重要性。在元学习中，我们可以使模型对成本敏感，而无需更改其成本函数。
- en: 'In MetaCost, a type of meta-learning technique, for example, we alter the labels
    of training instances to minimize expected misclassification costs. Similarly,
    in the threshold adjustment method, we determine a probability threshold that
    minimizes total misclassification costs for predictions. *Figure 5**.1* categorizes
    these methods at a high level [2][3]:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在元学习技术 MetaCost 中，例如，我们可以改变训练实例的标签，以最小化预期的误分类成本。同样，在阈值调整方法中，我们确定一个概率阈值，以最小化预测的总误分类成本。*图
    5.1* 从高层次上对这些方法进行了分类 [2][3]：
- en: '![](img/B17259_05_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_01.jpg)'
- en: Figure 5.1 – Categorization of cost-sensitive learning methods
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 成本敏感学习方法的分类
- en: Difference between CSL and resampling
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSL与重采样的区别
- en: The key difference between previously discussed data-level techniques and CSL
    is that the data-level techniques adjust the frequency of the different error
    types, but they treat all misclassification errors the same. In certain cases,
    as we encountered earlier, the cost of misclassifying observations of different
    classes is not the same. For example, in cancer detection, the cost of misclassifying
    a patient who has cancer as healthy (False Negative) is much higher, as the patient
    is at high risk if not detected or treated early. Similarly, misclassifying a
    fraudulent booking as non-fraudulent can cost more money than wrongly classifying
    a legitimate transaction as fraud. Why? Because in the latter case, we can just
    call and verify with the user the legitimacy of the transaction. By applying resampling
    techniques such as upsampling or downsampling, we are implicitly changing the
    cost of different types of errors. So, CSL and resampling techniques can be considered
    to have an equivalent effect on the model at the end of the day.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前讨论的数据级别技术相比，CSL的关键区别在于数据级别技术调整不同错误类型的频率，但它们对待所有误分类错误都是一样的。在某些情况下，正如我们之前遇到的，不同类别的观测值被误分类的成本并不相同。例如，在癌症检测中，将患有癌症的患者误分类为健康（假阴性）的成本要高得多，因为如果未检测或未早期治疗，患者风险很高。同样，将欺诈预订误分类为非欺诈可能会比将合法交易误分类为欺诈的成本更高。为什么？因为在后一种情况下，我们只需联系用户并验证交易的合法性即可。通过应用重采样技术，如上采样或下采样，我们隐式地改变了不同类型错误的成本。因此，CSL和重采样技术最终可以对模型产生等效的影响。
- en: 'However, resampling techniques may be problematic in certain cases, as we will
    discuss in the next section. In such cases, CSL can be more practical:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，重采样技术可能存在问题，我们将在下一节讨论。在这种情况下，CSL可能更实用：
- en: '![](img/B17259_05_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_02.jpg)'
- en: Figure 5.2 – Comic re-emphasizing the idea of misclassification errors
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 漫画再次强调误分类错误的概念
- en: Problems with rebalancing techniques
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重平衡技术的缺陷
- en: 'In the previous chapters, we briefly touched on why in some cases, we would
    prefer not to apply any data sampling techniques. This could be because of the
    following reasons:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们简要地提到了为什么在某些情况下，我们可能更喜欢不应用任何数据采样技术。这可能是因为以下原因：
- en: We already have too much training data, and it might be quite expensive to deal
    with more data, or the training time can increase by many folds due to having
    more training data.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经拥有太多的训练数据，处理更多的数据可能相当昂贵，或者由于有更多的训练数据，训练时间可能增加数倍。
- en: Sometimes, we may not get the best results using sampling or data rebalancing
    techniques because of the dataset we are using.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，由于我们使用的数据集，我们可能无法通过采样或数据重平衡技术获得最佳结果。
- en: An additional consideration is that upon rebalancing the dataset, our model’s
    predictive scores may become miscalibrated, necessitating a recalibration process.
    We will cover this topic in [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279),
    *Model Calibration*, where we will learn about various model calibration techniques.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是，在重新平衡数据集后，我们的模型的预测分数可能变得不准确，需要重新校准。我们将在[*第10章*](B17259_10.xhtml#_idTextAnchor279)“模型校准”中介绍这一主题，我们将学习各种模型校准技术。
- en: Rebalancing techniques can lead to model overfitting or underfitting issues.
    Overfitting can especially happen when using oversampling since they produce repeated
    or similar training examples. Similarly, the model may be underfitted when using
    undersampling because the model did not get trained on the data thrown away during
    undersampling.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重平衡技术可能导致模型过拟合或欠拟合问题。特别是当使用过采样时，它们会产生重复或相似的训练示例，这可能导致过拟合。同样，当使用欠采样时，模型可能欠拟合，因为模型没有在欠采样过程中丢弃的数据上进行训练。
- en: Next, let’s try to understand what costs really mean.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试理解成本究竟意味着什么。
- en: Understanding costs in practice
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中理解成本
- en: We need to understand the various types of costs involved while creating weights
    for different classes. These costs change on a case-by-case basis. Let’s discuss
    an example of cost calculations to understand what we should consider while thinking
    about cost calculations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在为不同类别创建权重时，我们需要了解涉及的各种成本类型。这些成本根据具体情况而变化。让我们讨论一个成本计算的例子，以了解在考虑成本计算时应考虑什么。
- en: Let’s take the example of pediatric pneumonia. According to UNICEF, a child
    dies of pneumonia every 43 seconds [4]. Imagine we are creating a new test for
    pediatric pneumonia – how will we decide the cost of different errors?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以儿童肺炎为例。根据联合国儿童基金会，每43秒就有一个孩子死于肺炎[4]。想象一下，我们正在为儿童肺炎开发一个新的测试——我们将如何决定不同错误的成本？
- en: Let’s review the confusion matrix from *Table 5.1*. There will usually be no
    extra cost for True Negatives and True Positives. But using a False Negative –
    that is, when a child has pneumonia and predicting the child to be healthy – will
    have a very high cost. On the flip side, when a healthy child is predicted as
    being affected by pneumonia, there will be a cost associated with the troubles
    the family of the child may have to go through, but there will be much less cost
    than in the previous case. Furthermore, the cost of misclassification can vary
    depending on the child’s age. For example, younger kids will be at a higher risk
    than older kids. Thus, we will aim to penalize the model more if it makes an error
    in the case of younger kids.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下表5.1中的混淆矩阵。通常，对于真正的负例和真正的正例，不会有额外的成本。但是，使用错误的负例——也就是说，当一个孩子患有肺炎，却预测该孩子健康时——将会有非常高的成本。另一方面，当一个健康的儿童被预测为患有肺炎时，将会有与孩子家庭可能遇到的麻烦相关的成本，但这个成本比前一种情况要低得多。此外，误分类的成本可能因孩子的年龄而异。例如，年幼的孩子比年长的孩子风险更高。因此，如果模型在年幼孩子的案例中出错，我们将对模型进行更多的惩罚。
- en: 'The cost can vary depending on the duration of the symptoms. Consider it this
    way: if we make an error and misdiagnose a child who has only had flu symptoms
    for a day, it’s not ideal, but it’s not disastrous. However, if that child has
    been enduring flu symptoms for 2 weeks, that’s a different scenario. That mistake
    will cost us significantly more.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 成本可能因症状的持续时间而异。可以这样考虑：如果我们犯了一个错误，错误地诊断了一个只有一天流感症状的孩子，这并不理想，但也不是灾难性的。然而，如果那个孩子已经忍受了2周的流感症状，那将是一个不同的场景。这个错误将给我们带来更大的成本。
- en: 'While we’ve discussed real-world problems so far, this chapter will pivot to
    utilize a synthetic dataset. This approach is intended to reinforce concepts and
    methods in a controlled environment, thus enhancing the learning process:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们迄今为止讨论了现实世界的问题，但本章将转向使用合成数据集。这种方法旨在在受控环境中强化概念和方法，从而增强学习过程：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `make_classification` function produces some overlapping points that we
    cleaned up. To keep things simple, we’ve omitted that cleanup code here. You can
    refer to the full notebook on GitHub.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_classification`函数产生了一些重叠的点，我们清理了这些点。为了简化，我们在这里省略了清理代码。您可以在GitHub上的完整笔记本中查阅。'
- en: 'The preceding code produces the following output and scatter plot (*Figure
    5**.3*):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码产生了以下输出和散点图（*图5**.3*）：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/B17259_05_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_03.jpg)'
- en: Figure 5.3 – Scatter plot showing the training dataset’s distribution
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 显示训练数据集分布的散点图
- en: We’ll dive into how to apply CSL to logistic regression models next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨如何将CSL应用于逻辑回归模型。
- en: Cost-Sensitive Learning for logistic regression
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归的成本敏感学习
- en: Logistic regression is a simple classification algorithm. We train a model as
    a linear combination of the features. Then, we pass the result of that linear
    combination into a sigmoid function to predict the class probabilities for different
    classes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种简单的分类算法。我们通过将特征进行线性组合来训练模型。然后，我们将线性组合的结果传递给sigmoid函数，以预测不同类别的类别概率。
- en: 'The `sigmoid` function (also called a `logit` function) is a mathematical tool
    capable of converting any real number into a value between 0 and 1\. This value
    can be interpreted as a probability estimate:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid`函数（也称为`logit`函数）是一种可以将任何实数转换为0到1之间值的数学工具。这个值可以解释为概率估计：'
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The graph of the sigmoid function has an S-shaped curve, and it appears like
    this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数的图像呈S形曲线，看起来是这样的：
- en: '![](img/B17259_05_04.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_04.jpg)'
- en: Figure 5.4 – Sigmoid function
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – Sigmoid函数
- en: The class with the highest predicted probability is taken as the prediction
    for a given sample.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具有最高预测概率的类别被用作给定样本的预测。
- en: 'Let’s say we have an email to be classified as spam or non-spam, and our logistic
    regression model outputs the probabilities of 0.25 for non-spam and 0.75 for spam.
    Here, the class with the highest predicted probability is “spam” (1) since 0.75
    is greater than 0.25\. Therefore, the model would predict that this email is spam
    (*Figure 5**.5*):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个要分类为垃圾邮件或非垃圾邮件的电子邮件，并且我们的逻辑回归模型输出非垃圾邮件的概率为0.25，垃圾邮件的概率为0.75。在这里，具有最高预测概率的类别是“垃圾邮件”（1），因为0.75大于0.25。因此，模型会预测这封邮件是垃圾邮件（*图5**.5*）：
- en: '![](img/B17259_05_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_05.jpg)'
- en: Figure 5.5 – Higher class probability determining the class for binary classification
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 高类别概率决定二分类的类别
- en: For two-class classification, we just predict the probability of one class.
    The probability of the other class is one minus the probability of the first class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类，我们只预测一个类别的概率。另一个类别的概率是第一个类别的概率减去1。
- en: 'The logistic regression model is trained using a loss function. The loss function
    for one example from a dataset with two classes would look like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对数逻辑回归模型使用损失函数进行训练。来自具有两个类别的数据集的一个示例的损失函数看起来像这样：
- en: cost = − y * log(classProbability) − (1 − y)* log(1 − classProbability)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: cost = − y * log(classProbability) − (1 − y)* log(1 − classProbability)
- en: 'For true positives and true negatives, this loss will be very low. For a false
    positive, y, the actual value would be 0; therefore, the first term will be 0,
    but the second term will be very high as the class probability approaches 1, and
    the term will approach negative infinity (since, log(0) → − ∞). Since there is
    a negative sign at the front, the cost will approach positive infinity. A similar
    analysis can be done for the false negative case. One part of the cost can be
    seen as the false positive part, and another part of the cost can be seen as the
    false negative part:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于真正的阳性和真正的阴性，这个损失将非常低。对于误报，y，实际值将是0；因此，第一个项将是0，但第二个项将非常高，因为类别概率接近1，这个项将接近负无穷大（因为，log(0)
    → − ∞）。由于前面有一个负号，成本将接近正无穷大。对误检情况可以进行类似的分析。成本的一部分可以看作是误报部分，另一部分可以看作是误检部分：
- en: cost = falsePositiveCost + falseNegativeCost
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: cost = falsePositiveCost + falseNegativeCost
- en: 'As discussed earlier, we don’t want to weigh the two types of costs equally.
    So, all we do is add weights, W FP and W FN, for the respective costs:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们不希望两种类型的成本同等重要。所以我们只是为各自的成本添加权重，W FP和W FN：
- en: cost = W FP * falsePositiveCost + W FN * falseNegativeCost
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: cost = W FP * falsePositiveCost + W FN * falseNegativeCost
- en: 'This is the crux of CSL with logistic regression. To get the overall costs
    of the model, we take the average cost across all the data points:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对数逻辑回归中CSL的核心。为了得到模型的总体成本，我们取所有数据点的平均成本：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When all errors are equally costly, the model’s decision boundary and the model’s
    **Precision-Recall** (**PR**) curve will look like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有错误成本相等时，模型的决策边界和模型的**精度-召回率**（**PR**）曲线将看起来像这样：
- en: '![](img/B17259_05_06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_06.jpg)'
- en: Figure 5.6 – The decision boundary (left) and PR curve (right) of the baseline
    regression model
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 基线回归模型的决策边界（左）和PR曲线（右）
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The previous code outputs the following F2 score, precision, and recall values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码输出了以下F2分数、精度和召回值：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this chapter, we will use the F2 score as our primary metric. What is the
    F2 score? In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to
    Data Imbalance in Machine Learning*, we studied the F-beta score. The F2 score
    is the F-beta score with beta=2, while the F1 score is the F-beta score with beta=1\.
    It’s useful when recall is more important than precision – that is, false negatives
    are more costly (important) than false positives:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用F2分数作为我们的主要指标。F2分数是什么？在[*第1章*](B17259_01.xhtml#_idTextAnchor015)，*机器学习中数据不平衡的介绍*，我们学习了F-beta分数。F2分数是beta=2的F-beta分数，而F1分数是beta=1的F-beta分数。当召回率比精度更重要时，它很有用——也就是说，误报比误检（重要）的成本更高：
- en: F β =  (1 + β 2) × (precision × recall)  ____________________  (β 2 × precision)
    + recall  =  (5 × precision × recall)  ________________  (4 × precision) + recall
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: F β =  (1 + β 2) × (precision × recall)  ____________________  (β 2 × precision)
    + recall  =  (5 × precision × recall)  ________________  (4 × precision) + recall
- en: '`LogisticRegression` from the `scikit-learn` library provides a `class_weight`
    parameter. When the value of this parameter is set to “balanced,” the weight of
    each class is automatically computed by the following formula:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression`来自`scikit-learn`库提供了一个`class_weight`参数。当此参数的值设置为“balanced”时，每个类别的权重将自动通过以下公式计算：'
- en: weightOfClass =  totalNumberOfSamples   ________________________________   numberOfClasses
    * numberOfSamplesPerClass
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: weightOfClass =  totalNumberOfSamples   ________________________________   numberOfClasses
    * numberOfSamplesPerClass
- en: 'For example, we have 100 examples in the dataset – 80 in class 0 and 20 in
    class 1\. The weights of each class are computed as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在数据集中我们有100个示例 – 80个属于类别0，20个属于类别1。每个类别的权重计算如下：
- en: Weight for class 0 = 100/(2*80) = 0.625
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别0的权重 = 100/(2*80) = 0.625
- en: Weight for class 1 = 100/(2*20) = 2.5
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别1的权重 = 100/(2*20) = 2.5
- en: Given that the number of class 0 examples is four times that of class 1, the
    weight of class 1 is 2.5, which is four times the weight of class 0 – that is,
    0.625\. This makes sense since we would want to give more weight to class 1, which
    is smaller in number.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别0的示例数量是类别1的四倍，类别1的权重是2.5，是类别0权重的四倍，即0.625。这很有道理，因为我们希望给类别1更多的权重，因为它的数量较少。
- en: 'We can mention `class_weight` as a dictionary as well:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将`class_weight`作为一个字典来提及：
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s try to use the `class_weight` parameter in the `LogisticRegression` function:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在`LogisticRegression`函数中使用`class_weight`参数：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B17259_05_07.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_07.jpg)'
- en: Figure 5.7 – The decision boundary (left) and PR curve (right) of the “balanced”
    class-weighted logistic regression model
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – “平衡”类别加权逻辑回归模型的决策边界（左）和PR曲线（右）
- en: 'Let’s calculate the F2 score, precision, and recall scores:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算F2分数、精确率和召回率分数：
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The scores of the “balanced” class-weighted logistic regression model are as
    follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: “平衡”类别加权逻辑回归模型的分数如下：
- en: '[PRE9]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Upon analyzing the results, we can see a decision boundary that correctly classifies
    most of the positive class examples. The precision comes down while the recall
    goes up. The decline in the F2 score can be attributed to changes in the recall
    and precision values. The model exhibits an improvement in recall, indicating
    its enhanced ability to correctly identify all positive class examples. However,
    this advancement results in a simultaneous drop in precision, suggesting an increased
    rate of mistakes made on the negative class examples (which we don’t really care
    about as much!).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析结果后，我们可以看到一个决策边界，它正确地分类了大多数正类示例。精确率下降，而召回率上升。F2分数的下降可以归因于召回率和精确率值的变化。模型在召回率方面有所提高，表明其识别所有正类示例的能力得到了增强。然而，这种进步导致精确率同时下降，这表明在负类示例（我们并不特别关心）上犯错的速率增加。
- en: 'Let’s try to tune the `class_weight` parameter using a grid search that optimizes
    our F2 score. We can always try to optimize any other objective, such as average
    precision, precision, or recall, and so on. The `np.linspace(0.05, 0.95, 20)`
    function is a `numpy` function that generates an array of 20 evenly spaced numbers
    between 0.05 and 0.95:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用网格搜索调整`class_weight`参数，以优化我们的F2分数。我们始终可以尝试优化任何其他目标，例如平均精确率、精确率或召回率等。`np.linspace(0.05,
    0.95, 20)`函数是一个`numpy`函数，它生成一个介于0.05和0.95之间的20个均匀分布的数字数组：
- en: '[PRE10]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our standard metrics are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标准指标如下：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After incorporating these class weights, our decision boundary attempts to
    strike a better balance between misclassifying positive and negative class examples,
    as illustrated in *Figure 5**.8*. This results in a superior F2 score of 0.93,
    increasing the precision value while maintaining a modest recall:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入这些类别权重后，我们的决策边界试图在错误分类正类和负类示例之间取得更好的平衡，如图*图5**.8*所示。这导致F2分数达到0.93，提高了精确率值，同时保持适度的召回率：
- en: '![](img/B17259_05_08.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_08.jpg)'
- en: Figure 5.8 – The decision boundary (left) and PR curve (right) of the class-weighted
    logistic regression model
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 类别加权逻辑回归模型的决策边界（左）和PR曲线（右）
- en: 🚀 Cost-sensitive learning in production at Microsoft
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 微软在生产中的成本敏感学习
- en: In a practical application at Microsoft, the primary objective was to improve
    the **Click-Through Rate** (**CTR**) prediction for Bing ads [5]. Achieving accurate
    CTR prediction is vital for optimizing both user experience and revenue streams.
    A marginal improvement of just 0.1% in prediction accuracy has the potential to
    elevate profits by hundreds of millions of dollars. Through rigorous testing,
    an ensemble model that combines **Neural Networks** (**NNs**) and **Gradient-Boosted
    Decision Trees** (**GBDTs**) emerged as the most effective solution.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在微软的一个实际应用中，主要目标是提高Bing广告的**点击率**（**CTR**）预测[5]。实现准确的CTR预测对于优化用户体验和收入流至关重要。预测准确率仅提高0.1%，就有可能使利润增加数亿美元。通过严格的测试，一个结合**神经网络**（**NNs**）和**梯度提升决策树**（**GBDTs**）的集成模型成为最有效的解决方案。
- en: For the training dataset, 56 million samples were randomly chosen from a month’s
    log data, each containing hundreds of statistical features. To reduce training
    expenses, non-click cases were **downsampled** by 50% and assigned a **class weight**
    of 2 to maintain the original distribution. Model performance was then assessed
    using a test dataset of 40 million samples randomly drawn from the subsequent
    week’s logs. Instead of recalibrating the model, class weighting was used to maintain
    the average CTR after downsampling.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据集，从一个月的日志数据中随机选择了5600万个样本，每个样本包含数百个统计特征。为了减少训练成本，非点击案例被**降采样**了50%，并分配了2的**类别权重**以保持原始分布。然后使用从随后一周日志中随机抽取的4000万个样本的测试数据集评估模型性能。而不是重新校准模型，使用了类别权重来在降采样后保持平均CTR。
- en: In the next section, we will discuss how to do CSL with decision trees.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何使用决策树进行代价敏感学习。
- en: Cost-Sensitive Learning for decision trees
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的代价敏感学习
- en: Decision trees are binary trees that use conditional decision-making to predict
    the class of the samples. Every tree node represents a set of samples corresponding
    to a chain of conditional statements based on the features. We divide the node
    into two children based on a feature and a threshold value. Imagine a set of students
    with height, weight, age, class, and location. We can divide the set into two
    parts according to the features of age and with a threshold of 8\. Now, all the
    students with ages less than 8 will go into the left child, and all those with
    ages greater than or equal to 8 will go into the right child.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是使用条件决策来预测样本类别的二叉树。每个树节点代表一组与基于特征的连续条件语句相对应的样本。我们根据特征和阈值值将节点分为两个子节点。想象一下有一组学生，他们的身高、体重、年龄、班级和位置。我们可以根据年龄特征和8的阈值将这个集合分为两部分。现在，所有年龄小于8岁的学生将进入左子节点，而所有年龄大于或等于8岁的学生将进入右子节点。
- en: This way, we can create a tree by successively choosing features and threshold
    values. Every leaf node of the tree will contain nodes from only one class, respectively.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以通过连续选择特征和阈值值来创建树。树的每个叶节点将只包含来自一个类别的节点。
- en: 'A question often arises during the construction of a decision tree: “Which
    feature and threshold pair should be selected to partition the set of samples
    at a given node?” The answer is straightforward: we opt for the pair that produces
    the most uniform (or homogeneous) subsets of data. Ideally, the two resulting
    subsets – referred to as the left and right children – should each contain elements
    predominantly from a single class.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建决策树的过程中，经常会遇到一个问题：“应该选择哪个特征和阈值对来分割给定节点的样本集？”答案很简单：我们选择产生最均匀（或同质）数据子集的对。理想情况下，产生的两个结果子集——被称为左右子节点——应该各自主要包含来自单个类别的元素。
- en: 'The degree to which the nodes have a mixture of samples from different classes
    is known as the **impurity** of the node, which can be considered to be a measure
    of loss for decision trees. The more the impurity, the more heterogeneous the
    set of samples. Here are the two most common ways of calculating the impurity:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 节点从不同类别中混合样本的程度被称为节点的**不纯度**，这可以被视为决策树的损失度量。不纯度越高，样本集的异质性就越大。以下是计算不纯度的两种最常见方法：
- en: Gini coefficient
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gini系数
- en: Entropy
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Entropy
- en: 'Let’s look at the formula for the Gini coefficient and entropy for two classes,
    c 1 and c 2:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Gini系数和熵的两个类c1和c2的公式：
- en: Gini = 1− Proportion c1 2− Proportion c2 2
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Gini = 1− Proportion c1 2− Proportion c2 2
- en: 'We will get the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下结果：
- en: Entropy = − Proportion c1 * log (Proportion c1)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Entropy = − Proportion c1 * log (Proportion c1)
- en: − Proportion c2 * log (Proportion c2)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: − 比例c2 * log (比例c2)
- en: 'To do CSL with decision trees, we just multiply the class weights with the
    terms for each of the classes in the calculation of the Gini and entropy. If the
    weights for the two classes are W 1and W 2, Gini and entropy will look as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用决策树进行CSL，我们只需将类权重与Gini和熵计算中每个类的项相乘。如果两个类的权重是W1和W2，Gini和熵将如下所示：
- en: Gini = 1 − W 1 * Proportion c1 2 − W 2 * Proportion c2 2
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Gini = 1 − W1 * 比例c1^2 − W2 * 比例c2^2
- en: Entropy = − W 1 * Proportion c1 * log(Proportion c1)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Entropy = − W1 * 比例c1 * log(比例c1)
- en: − W 2 * Proportion c2 * log(Proportion c2)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: − W2 * 比例c2 * log(比例c2)
- en: Now, the model prioritizes the class with a higher weight over the class with
    a lower weight. If we give more weight to the minority class, the model will make
    the decision that will prioritize nodes with homogeneous minority class samples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型优先考虑权重较高的类，而不是权重较低的类。如果我们给少数类更多的权重，模型将做出优先考虑具有同质少数类样本的节点的决策。
- en: In this section, we got some idea of how class weights can be accommodated into
    the loss function of decision trees to account for the misclassification error.
    In the next section, we will see how `scikit-learn` simplifies this process by
    integrating it into the model creation API, eliminating the need for us to manually
    adjust the loss function.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了一些如何将类权重纳入决策树的损失函数中，以考虑误分类错误。在下一节中，我们将看到`scikit-learn`如何通过将其集成到模型创建API中来简化此过程，从而消除我们手动调整损失函数的需要。
- en: Cost-Sensitive Learning using scikit-learn and XGBoost models
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn和XGBoost模型进行成本敏感学习
- en: '`scikit-learn` provides a `class_weight` hyperparameter to adjust the weights
    of various classes for most models. This parameter can be specified in various
    ways for different learning algorithms in `scikit-learn`. However, the main idea
    is that this parameter specifies the weights to use for each class in the loss
    calculation formula. For example, this parameter specifies the values of weight FP
    and weight FN mentioned previously for logistic regression.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`提供了一个`class_weight`超参数来调整大多数模型中各种类的权重。这个参数可以根据`scikit-learn`中不同学习算法的不同方式指定。然而，主要思想是这个参数指定了损失计算公式中每个类的权重。例如，这个参数指定了之前提到的逻辑回归中的权重FP和权重FN的值。'
- en: 'Similar to the `LogisticRegression` function, for `DecisionTreeClassifier`,
    we could use `DecisionTreeClassifier(class_weight=''balanced'')` or `DecisionTreeClassifier(class_weight={0:
    0.5,` `1: 0.5})`.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '与`LogisticRegression`函数类似，对于`DecisionTreeClassifier`，我们可以使用`DecisionTreeClassifier(class_weight=''balanced'')`或`DecisionTreeClassifier(class_weight={0:
    0.5, 1: 0.5})`。'
- en: 'Regarding SVM, it can even be extended to multi-class classification by specifying
    a weight value for each class label:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SVM，它甚至可以通过为每个类标签指定一个权重值来扩展到多类分类：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The general guidance about coming up with the `class_weight` values is to use
    the inverse of the ratio of the majority class to the minority class. We can find
    even more optimal `class_weight` values by performing hyperparameter tuning using
    the GridSearch algorithm (use the `GridSearchCV` function from `scikit-learn`).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于确定`class_weight`值的通用指导原则是使用多数类与少数类比例的倒数。我们可以通过使用网格搜索算法（使用`scikit-learn`中的`GridSearchCV`函数）进行超参数调整来找到更优的`class_weight`值。
- en: 'Similarly, XGBoost has the `scale_pos_weight` parameter to control the balance
    of positive and negative weights:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，XGBoost也有`scale_pos_weight`参数来控制正负权重的平衡：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The default value of `scale_pos_weight` is 1\. A recommended `scale_pos_weight`
    value is `sum(negative_instances)/sum(positive_instances)`, which can be computed
    as `float(np.sum(label == 0)) /` `np.sum(label==1)`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`scale_pos_weight`的默认值是1。一个推荐的`scale_pos_weight`值是`sum(negative_instances)/sum(positive_instances)`，这可以计算为`float(np.sum(label
    == 0)) / np.sum(label==1)`。'
- en: XGBoost has a few other parameters, such as `max_delta_step` and `min_child_weight`,
    that can be tuned for imbalanced datasets. During the optimization process, `max_delta_step`
    determines the step size of updates, affecting learning speed and stability. `min_child_weight`
    controls overfitting and enhances generalization by influencing the size of leaf
    nodes in the decision tree. When dealing with imbalanced data scenarios, adjusting
    these parameters can strategically improve algorithm performance.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 有几个其他参数，例如 `max_delta_step` 和 `min_child_weight`，这些参数可以针对不平衡数据集进行调整。在优化过程中，`max_delta_step`
    决定了更新步长的大小，影响学习速度和稳定性。`min_child_weight` 通过影响决策树中叶节点的大小来控制过拟合并增强泛化。在处理不平衡数据场景时，调整这些参数可以策略性地提高算法性能。
- en: 'First, let’s use `DecisionTreeClassifier` to solve our classification problem:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `DecisionTreeClassifier` 解决我们的分类问题：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output decision boundary is more complex than that of logistic regression
    (*Figure 5**.9*), separating the two classes better and giving an F2 score of
    0.932:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的决策边界比逻辑回归的更复杂（*图 5.9*），更好地分离了两个类别，并给出了 F2 分数为 0.932：
- en: '![](img/B17259_05_09.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_09.jpg)'
- en: Figure 5.9 – The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 决策树分类器模型的决策边界（左）和 PR 曲线（右）
- en: 'We have reproduced the decision boundary and PR curve of the logistic regression
    model for comparison in *Figure 5**.10*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *图 5.10* 中重现了逻辑回归模型的决策边界和 PR 曲线，以进行比较：
- en: '![](img/B17259_05_10.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_10.jpg)'
- en: Figure 5.10 – The decision boundary (left) and PR curve (right) of logistic
    regression (for comparison)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 用于比较的逻辑回归（左）和 PR 曲线（右）
- en: 'Our standard metrics for the decision tree classifier are as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对决策树分类器的标准指标如下：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, let’s use the `class_weight=''balanced''` parameter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `class_weight='balanced'` 参数：
- en: '[PRE17]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After utilizing the code from before to plot the decision boundary, the PR
    curve, and compute the scores, the outputs are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用之前的代码绘制决策边界、PR 曲线和计算分数后，输出如下：
- en: '![](img/B17259_05_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_11.jpg)'
- en: Figure 5.11 – The decision boundary (left) and PR curve (right) of the decision
    tree classifier model
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 决策树分类器模型的决策边界（左）和 PR 曲线（右）
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The tuned weights improve the F2 score and recall values.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的权重提高了 F2 分数和召回值。
- en: 'Popular frameworks such as `scikit-learn` also let us specify `sample_weight`
    as a list of weights for each observation in the dataset. The `sample_weight`
    and `class_weight` parameters can be quite confusing, and their purpose may not
    be very clear from their documentation on when to use what. The following table
    clarifies the difference between the two:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 流行框架如 `scikit-learn` 也允许我们指定 `sample_weight` 作为数据集中每个观察值的权重列表。`sample_weight`
    和 `class_weight` 参数可能相当令人困惑，并且它们的目的可能不会从它们的文档中非常清楚地了解何时使用什么。以下表格说明了两者之间的区别：
- en: '|  | `sample_weight` | `class_weight` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | `sample_weight` | `class_weight` |'
- en: '| --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Purpose** | Used to specify weights for individual examples.Can be useful
    when some examples are more important than others, regardless of their class.When
    some data is more trustworthy (say labeled using in-house human labelers), it
    can receive a higher weight.Can be useful when you don’t have equal confidence
    in the samples in your batch. | Used to correct class imbalance.Should be used
    when the importance of examples depends on their class. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **目的** | 用于指定单个示例的权重。当某些示例比其他示例更重要时，无论它们的类别如何，都可能很有用。当某些数据更可信（例如使用内部人工标注器标注）时，它可以获得更高的权重。当您对批处理中的样本没有相同的信心时，这可能很有用。
    | 用于纠正类别不平衡。当示例的重要性取决于它们的类别时应该使用。 |'
- en: '| **Usage** | Can be used in training as well as testing.Especially useful
    when comparing multiple models on different test sets with metrics such as AUC,
    where it’s often desirable to balance the test set:`sklearn.metrics.confusion_matrix(…,
    sample_weight)``sklearn.linear_model``.``LogisticRegression()``.``score(…,sample_weight)`
    | Mainly used during training to guide the training.Accounts for misclassification
    errors because certain classes are more important than others:`sklearn.linear_model``.``LogisticRegression(``class_weight)`
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| **用法** | 可以在训练和测试中使用。特别适用于在具有 AUC 等指标的不同测试集上比较多个模型时，通常希望平衡测试集：`sklearn.metrics.confusion_matrix(…,
    sample_weight)` `sklearn.linear_model` `.` `LogisticRegression()` `.` `score(…,sample_weight)`
    | 主要在训练期间使用，以指导训练。考虑到某些类别比其他类别更重要，因此会计算误分类错误：`sklearn.linear_model` `.` `LogisticRegression(``class_weight`)
    |'
- en: '| **Effect of setting the value to 0 during** **model training** | Model will
    not take into account the examples for which `samples_weight=0` (irrespective
    of the example’s class). | The model will not consider any example belonging to
    the class for which `class_weight = 0`. Also, the model will never predict that
    class. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| **在模型训练期间设置值为0的影响** | 模型将不会考虑`samples_weight=0`的示例（无论示例的类别如何）。 | 模型将不会考虑属于`class_weight
    = 0`的类别的任何示例。此外，模型永远不会预测该类别。 |'
- en: '| **Use** **case example** | When predicting customer churn, if losing certain
    customers would have a larger impact on business because they tend to purchase
    more often or spend more, we would want to give these customers a higher weight
    using `sample_weight`. | If we have a dataset where one class significantly outnumbers
    the other(s), using `class_weight` can help the model pay more attention to the
    underrepresented class(es). |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| **应用案例** | 当预测客户流失时，如果失去某些客户会对业务产生更大的影响，因为他们倾向于更频繁地购买或花费更多，我们希望使用`sample_weight`给这些客户更高的权重。
    | 如果我们有一个数据集，其中一个类别明显多于其他类别，使用`class_weight`可以帮助模型更多地关注代表性不足的类别。 |'
- en: Table 5.2 – sample_weight versus class_weight in the scikit-learn library
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 – scikit-learn库中的sample_weight与class_weight
- en: Warning
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: If we use `sample_weight` along with `class_weight`, both will be multiplied,
    and we will see the effect of both parameters. The two can still be used together
    to balance class importance and individual instance importance with their intended
    purposes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`sample_weight`和`class_weight`，两者都会相乘，我们将看到这两个参数的效果。这两个参数仍然可以一起使用，以平衡类别重要性和个体实例重要性，并实现其预期目的。
- en: 'Using `numpy` makes it easier to create the list of weight values that are
    required by `sample_weight`: `sample_weight = np.where(label==1, 80, 20)`. However,
    `scikit-learn` has a function called `sklearn.utils.class_weight.compute_sample_weight()`
    that can be used to estimate the value of `sample_weight` automatically from `class_weight`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numpy`可以更容易地创建`sample_weight`所需的权重值列表：`sample_weight = np.where(label==1,
    80, 20)`。然而，`scikit-learn`有一个名为`sklearn.utils.class_weight.compute_sample_weight()`的函数，可以用来自动从`class_weight`估计`sample_weight`的值。
- en: '`class_weight` can also be a dict of values for each label or balanced. If
    we set it to balanced, class weights are determined by `n_samples/(n_classes *`
    `np.bincount(y))`.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`class_weight`也可以是一个字典，包含每个标签的值或平衡值。如果我们将其设置为平衡，类别权重将由`n_samples/(n_classes
    * np.bincount(y))`确定。'
- en: 'The returned value from `class_weight` is a dictionary: `{class_label: weight}`
    for each `class_label` value.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`class_weight`返回的值是一个字典：`{class_label: weight}`，对于每个`class_label`值。'
- en: Similarly, you can use `sklearn.utils.class_weight.compute_sample_weight` if
    you have to do multi-label classification.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，如果你需要进行多标签分类，你也可以使用`sklearn.utils.class_weight.compute_sample_weight`。
- en: 🚀 Cost-sensitive learning in production at Airbnb
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Airbnb生产中的成本敏感学习
- en: In a real-world application at Airbnb [6], the main problem to solve was improving
    the search and discoverability as well as personalization of their Experiences
    (handcrafted activities) platform. As the number of experiences grew, it became
    crucial to effectively rank these experiences to match user preferences and improve
    bookings.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airbnb的一个实际应用中[6]，要解决的主要问题是提高搜索和可发现性以及他们体验（手工活动）平台的个性化。随着体验数量的增加，有效地对这些体验进行排名以匹配用户偏好并提高预订变得至关重要。
- en: Airbnb aimed to improve its search ranking to provide users with the most relevant
    and high-quality experiences. To promote the quality of their ranking model, they
    used sample weights (discussed in the previous section) in their objective function.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Airbnb旨在提高其搜索排名，为用户提供最相关和高质量的经历。为了提升其排名模型的品质，他们在目标函数中使用了样本权重（在上一节中讨论过）。
- en: The data imbalance in terms of quality tiers was addressed by using sample weighting
    (discussed in the previous section) in the training data. High-quality experiences
    were given higher weights, and low-quality experiences were given lower weights
    in the objective function. This was done to promote high-quality experiences in
    the search rankings, and they successfully improved the ranking of high-quality
    experiences and reduced low-quality ones without affecting overall bookings, as
    confirmed by A/B tests.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用样本权重（在上一节中讨论过）解决训练数据中的数据不平衡问题。在目标函数中，高质量的经历被赋予更高的权重，而低质量的经历被赋予较低的权重。这样做是为了在搜索排名中推广高质量的经历，并且他们成功地提高了高质量经历的排名，减少了低质量经历，而没有影响整体预订，如A/B测试所证实。
- en: Airbnb iteratively developed and tested its machine learning model, eventually
    integrating it into its production system to rank “Experiences” in real time.
    They went through multiple stages, from building a strong baseline to personalization
    and online scoring to handle various business rules.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Airbnb迭代地开发和测试其机器学习模型，最终将其集成到其生产系统中，以实时对“体验”进行排名。他们经历了多个阶段，从建立强大的基线到个性化、在线评分以及处理各种业务规则。
- en: In the next section, we will learn about a technique that can convert any model
    into its cost-sensitive version without us knowing about its loss function or
    the inner workings of the model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解一种可以将任何模型转换为成本敏感版本的技术，而无需我们知道其损失函数或模型的内部工作原理。
- en: MetaCost – making any classification model cost-sensitive
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MetaCost – 使任何分类模型成本敏感
- en: MetaCost was first introduced in a paper by Pedro Domingos [7] in 1999\. MetaCost
    acts as a wrapper around machine learning algorithms that converts the underlying
    algorithm into a cost-sensitive version of itself. It treats the underlying algorithm
    as a black box and works best with unstable algorithms (defined below). When MetaCost
    was first proposed, CSL was in its early stages. Only a few algorithms, such as
    decision trees, had been converted into their cost-sensitive versions. For some
    models, creating a cost-sensitive version turned out to be easy while for others
    it was a non-trivial task. For algorithms where defining cost-sensitive versions
    of the model turned out to be difficult, people mostly relied upon data sampling
    techniques such as oversampling or undersampling. This was when Domingos came
    up with an approach for converting a large range of algorithms into their cost-sensitive
    versions. MetaCost can work for multi-class classification and with all types
    of cost matrices.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MetaCost首次在1999年由Pedro Domingos [7]发表的一篇论文中提出。MetaCost作为机器学习算法的包装器，将底层算法转换为自身的成本敏感版本。它将底层算法视为黑盒，与不稳定算法（如下定义）配合最佳。当MetaCost首次提出时，CSL还处于早期阶段。只有少数算法，如决策树，已被转换为它们的成本敏感版本。对于某些模型，创建成本敏感版本变得很容易，而对于其他模型，则是一项非平凡的任务。对于定义模型成本敏感版本变得困难的算法，人们主要依赖于数据采样技术，如过采样或欠采样。这就是Domingos提出了一种将大量算法转换为它们的成本敏感版本的方法。MetaCost可以用于多类分类和所有类型的成本矩阵。
- en: Unstable algorithms
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 不稳定算法
- en: An algorithm is called unstable [8] if a slight change in its initial conditions
    (for example, training data or initial weights) can create a big change in the
    model. Assume you are given a dataset of 1,000 items. A stable model such as a
    **K-Nearest Neighbor** (**KNN**) will not change much if you remove one item from
    the dataset. However, a model such as a decision tree might get completely restructured
    if you train it on 999 items instead of 1,000 items.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个算法的初始条件（例如，训练数据或初始权重）的微小变化可以导致模型产生大的变化，则称该算法为不稳定[8]。假设你有一个包含1,000个项目的数据集。一个稳定的模型，如**K-最近邻**（**KNN**），如果你从数据集中移除一个项目，它不会改变太多。然而，如果你用999个项目而不是1,000个项目来训练一个决策树模型，它可能会完全重构。
- en: 'Let’s delve into the mechanics of the MetaCost algorithm, as illustrated in
    *Figure 5**.12*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨MetaCost算法的机制，如图5.12所示：
- en: '![](img/B17259_05_12.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_12.jpg)'
- en: Figure 5.12 – The MetaCost algorithm
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – MetaCost算法
- en: 'MetaCost works by combining the concept of bagging with a misclassification
    cost matrix:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: MetaCost通过结合bagging的概念与误分类成本矩阵来工作：
- en: First, we create multiple bootstrap samples of the original data.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建原始数据的多个bootstrap样本。
- en: We train one new copy of the given model for each bootstrap sample. So far,
    the process is the same as bagging. You can see the first two steps in *Figure
    5**.12* on the left-hand side. First, we create bootstrap samples S1, S2, and
    S3 from the original data. Then, we train models L1, L2, and L3 on the samples
    (S1, S2, and S3), respectively.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个bootstrap样本，我们训练给定模型的一个新副本。到目前为止，这个过程与bagging相同。你可以在图5.12左侧看到前两个步骤。首先，我们从原始数据中创建bootstrap样本S1、S2和S3。然后，我们分别在样本（S1、S2和S3）上训练模型L1、L2和L3。
- en: Next, we send the original data, S, into the ensemble of L1, L2, and L3.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将原始数据S送入由L1、L2和L3组成的集成中。
- en: We multiply the misclassification costs obtained from the cost matrix with the
    class probabilities predicted by the ensemble to get the actual cost. This is
    shown on the right-hand side of *Figure 5**.12*.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将来自成本矩阵的误分类成本与集成预测的类别概率相乘，以获得实际成本。这显示在图5.12的右侧。
- en: Then, we relabel the data so that the new class labels minimize the actual cost.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们重新标记数据，使得新的类别标签最小化实际成本。
- en: Finally, we train a new copy of the model on the relabeled data. This copy of
    the model is used as the final model.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在重新标记的数据上训练了一个新模型的副本。这个模型副本被用作最终模型。
- en: 'We can see the process of relabeling data using MetaCost in *Figure 5**.13*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 *图5.13* 中看到使用MetaCost重新标记数据的过程：
- en: '![](img/B17259_05_13.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_13.jpg)'
- en: Figure 5.13 – Process of relabeling data using MetaCost
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – 使用MetaCost重新标记数据的过程
- en: On the left of *Figure 5**.13*, we have the original data. Stars are the minority
    class examples and squares are the majority class examples. Here, all the samples
    inside the oval are predicted as stars, and all the samples outside it are predicted
    as squares. The oval on the left is drawn by assuming the same misclassification
    cost for all errors. In the center, we create a new class boundary based on the
    actual misclassification cost drawn as an elongated oval. Notice that all the
    stars are now classified correctly. Also, notice that some squares are now misclassified
    as stars. This is expected as the misclassification cost for the stars is much
    higher than that of squares. At this point, MetaCost relabels these misclassified
    squares as stars. Finally, MetaCost trains a model on the relabeled data. Because
    the majority class examples that are easily mistaken for the minority class have
    been relabeled as belonging to the minority class, the final model is less likely
    to mislabel instances of the minority class.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图5.13* 的左侧，我们有原始数据。星星是少数类示例，正方形是多数类示例。在这里，所有在椭圆形内的样本都被预测为星星，所有在椭圆形外的样本都被预测为正方形。左侧的椭圆形是根据对所有错误具有相同误分类成本的假设绘制的。在中间，我们根据实际误分类成本绘制的一个拉长的椭圆形创建了一个新的类别边界。请注意，现在所有的星星都被正确分类了。同时，请注意，现在一些正方形被错误地分类为星星。这是预期的，因为星星的误分类成本远高于正方形。在此阶段，MetaCost将这些被错误分类的正方形重新标记为星星。最后，MetaCost在重新标记的数据上训练了一个模型。由于容易被误认为是少数类的多数类示例已被重新标记为属于少数类，因此最终的模型不太可能将少数类的实例标记错误。
- en: To save space, we have omitted the implementation of the MetaCost algorithm.
    You can find it in the GitHub repository for this chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们省略了MetaCost算法的实现。您可以在本章的GitHub仓库中找到它。
- en: 'We will apply the algorithm to the logistic regression model. MetaCost uses
    a cost matrix, which is a hyperparameter. The values in the cost matrix correspond
    to the weight or cost of items in the confusion matrix (the transpose of the confusion
    matrix from *Table 5.1*):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用该算法到逻辑回归模型中。MetaCost使用一个代价矩阵，这是一个超参数。代价矩阵中的值对应于混淆矩阵（*表5.1*中混淆矩阵的转置）中各项的权重或成本：
- en: C = (TN FN FP TP )
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: C = (TN FN FP TP )
- en: 'Let’s say we use a cost matrix with equal costs for false positives and false
    negatives (that is, an identity matrix):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用一个对假阳性和假阴性具有相同成本的代价矩阵（即单位矩阵）：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Figure 5**.14* shows the decision boundary and metrics, which are very close
    to the ones from the logistic regression classifier (*Figure 5**.10*):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.14* 展示了决策边界和指标，它们与逻辑回归分类器的结果（*图5.10*）非常接近：'
- en: '![](img/B17259_05_14.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_14.jpg)'
- en: Figure 5.14 – The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with an identity cost matrix
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 具有单位代价矩阵的逻辑回归模型MetaCost变种的决策边界（左侧）和PR曲线（右侧）
- en: 'We can estimate the cost matrix based on the imbalance ratio of the training
    data:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据训练数据的失衡比率估计代价矩阵：
- en: '[PRE20]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Figure 5**.15* shows the output decision function and PR curve:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.15* 展示了输出决策函数和PR曲线：'
- en: '![](img/B17259_05_15.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_15.jpg)'
- en: Figure 5.15 – The decision boundary (left) and PR curve (right) of the MetaCost
    variant of the logistic regression model with a more optimal cost matrix
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 – 具有更优代价矩阵的逻辑回归模型MetaCost变种的决策边界（左侧）和PR曲线（右侧）
- en: Although the F2 score dropped compared to the baseline, the recall did improve
    drastically.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然与基线相比F2分数有所下降，但召回率确实大幅提高。
- en: The various steps in the MetaCost algorithm, such as relabeling the whole training
    set, can be quite an expensive operation, and that might deter us from using this
    technique when our training dataset is large.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: MetaCost算法中的各种步骤，如重新标记整个训练集，可能相当昂贵，这可能会阻止我们在训练数据集很大时使用这种技术。
- en: Cost-sensitive ensemble techniques
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 成本敏感的集成技术
- en: AdaCost [9], AdaUBoost [10], and AsymBoost [11] are cost-sensitive modifications
    of the AdaBoost model. AdaCost minimizes misclassification costs during iterative
    training. AdaUBoost handles imbalanced datasets by emphasizing the minority class.
    AsymBoost focuses on reducing the costliest misclassifications. They all adjust
    weights while considering misclassification costs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: AdaCost [9]、AdaUBoost [10]和AsymBoost [11]是AdaBoost模型的成本敏感修改。AdaCost在迭代训练过程中最小化误分类成本。AdaUBoost通过强调少数类来处理不平衡数据集。AsymBoost专注于减少最昂贵的误分类。它们都在考虑误分类成本的同时调整权重。
- en: The underlying principle behind these algorithms is that besides allocating
    high initial weights to instances where the cost of misclassification is large,
    the rule for updating weights should also consider costs. This means that the
    weights of expensive misclassifications should be increased while the weights
    of correct classifications should be reduced.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法背后的基本原理是，除了对误分类成本大的实例分配高初始权重外，更新权重的规则也应考虑成本。这意味着应该增加昂贵误分类的权重，同时减少正确分类的权重。
- en: In the next section, we will learn about another cost-sensitive meta-learning
    technique, called threshold adjustment.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解另一种成本敏感的元学习技术，称为阈值调整。
- en: Threshold adjustment
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阈值调整
- en: 'The decision threshold is a very important concept to keep track of. By default,
    we have the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 决策阈值是一个非常重要的概念，需要密切关注。默认情况下，我们有以下情况：
- en: Prediction probability >= 0.5 implies Class 1
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测概率 >= 0.5表示类别1
- en: Prediction probability < 0.5 implies Class 0
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测概率 < 0.5表示类别0
- en: However, the threshold is a powerful meta-parameter that we are free to adjust.
    *Table 5.3* shows predictions from a model versus the true labels.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，阈值是一个强大的元参数，我们可以自由调整。*表5.3*显示了模型预测与真实标签的预测。
- en: 'If we use the default threshold of 0.5, the accuracy is 2/4 = 50%. If, on the
    other hand, the threshold chosen is 0.80, the accuracy is 100%. This shows how
    important the chosen threshold can be:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用默认的阈值0.5，准确率为2/4 = 50%。另一方面，如果我们选择的阈值是0.80，准确率为100%。这表明所选阈值的重要性：
- en: '| **Predicted Output** | **True Output** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **预测输出** | **真实输出** |'
- en: '| 0.65 | 0 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 0.65 | 0 |'
- en: '| 0.75 | 0 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 0.75 | 0 |'
- en: '| 0.85 | 1 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 0.85 | 1 |'
- en: '| 0.95 | 1 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 0.95 | 1 |'
- en: Table 5.3 – A table showing the predicted output from a model versus the true
    output (labels)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.3 – 一个显示模型预测输出与真实输出（标签）的表格
- en: Most of the metrics, such as accuracy, precision, recall, and F1 score, are
    all threshold-dependent metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数指标，如准确率、精确率、召回率和F1分数，都是阈值相关指标。
- en: On the other hand, metrics such as the ROC curve and the PR curve are threshold-independent,
    which means that these plots evaluate the performance of a model at all possible
    thresholds rather than a single, fixed threshold.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如ROC曲线和PR曲线这样的指标是阈值无关的，这意味着这些图表评估的是模型在所有可能的阈值下的性能，而不是单个固定的阈值。
- en: When dealing with machine learning metrics such as F1 or accuracy, it’s important
    to understand the role of the threshold value. These metrics, by default, utilize
    a threshold of 0.5\. Therefore, a misconception arises, particularly among novice
    and intermediate machine learning practitioners, that these metrics are inevitably
    linked to this particular threshold.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理机器学习指标，如F1或准确率时，了解阈值值的作用很重要。这些指标默认使用0.5的阈值。因此，尤其是对于新手和中级机器学习从业者，会产生一种误解，认为这些指标不可避免地与这个特定的阈值相关联。
- en: However, this can lead to an inaccurate interpretation of the model’s performance,
    particularly in scenarios involving imbalanced datasets. The selection of the
    metric and the decision threshold are separate choices and should be treated as
    such. Establishing an appropriate threshold is a crucial step in the process,
    which should be considered independently of the chosen metric.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能导致对模型性能的不准确解释，尤其是在涉及不平衡数据集的场景中。指标的选择和决策阈值的确定是独立的选择，应该这样处理。确定适当的阈值是过程中的一个关键步骤，应该独立于所选的指标来考虑。
- en: Furthermore, relying solely on the default threshold of 0.5 can be misleading.
    The threshold should be set based on the specific requirements of the project
    and the nature of the data. Therefore, it’s integral that machine learning practitioners
    understand the interplay between the threshold and the selected metric to accurately
    assess the performance of their models.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，仅仅依赖于默认的阈值0.5可能会产生误导。阈值应根据项目的具体要求和数据的性质来设置。因此，机器学习从业者理解阈值与所选指标之间的相互作用，以准确评估其模型性能至关重要。
- en: In binary classification, altering the threshold will easily change the threshold-dependent
    metrics such as accuracy, F1 score, TPR, or FPR. Many pieces of research [12][13]
    have mentioned the value of threshold adjustment, especially in the case when
    training data is imbalanced. A paper by Provost [14] states that using models
    without adjusting the output thresholds may be a critical mistake. Among deep
    learning domains, Buda et al. [15] show that using **random oversampling** (**ROS**)
    along with thresholding outperforms plain ROS on imbalanced datasets created from
    CIFAR and MNIST. Regardless of whether the data is imbalanced or not, choosing
    an optimal threshold can make a lot of difference in the performance of the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类中，改变阈值将很容易改变阈值相关的指标，如准确度、F1分数、TPR或FPR。许多研究[12][13]提到了阈值调整的价值，尤其是在训练数据不平衡的情况下。Provost
    [14]的一篇论文指出，使用未调整输出阈值的模型可能是一个严重的错误。在深度学习领域，Buda等人[15]表明，使用**随机过采样**（**ROS**）并结合阈值处理，在由CIFAR和MNIST创建的不平衡数据集上优于简单的ROS。无论数据是否不平衡，选择一个最优的阈值可以在模型的性能上产生很大的差异。
- en: 'Many times, we would want to find the threshold that optimizes our threshold-dependent
    metric, say F1 score. Here, find the threshold at which the F1 score is the maximum
    (*Figure 5**.16*):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，我们希望找到优化我们的阈值相关指标（例如F1分数）的阈值。在这里，找到F1分数最大的阈值（*图5.16*）：
- en: '![](img/B17259_05_16.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_16.jpg)'
- en: Figure 5.16 – A PR curve with the best threshold that finds the max F1 score
    (see the notebook in this chapter’s GitHub repository)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 – 一个具有最佳阈值的PR曲线，该阈值找到最大的F1分数（请参阅本章GitHub仓库中的笔记本）
- en: '*Figure 5**.17* presents a plot that illustrates the impact of modifying the
    decision threshold on various classification metrics for an imbalanced dataset:
    **True Positive Rate** (**TPR** or recall), **True Negative Rate** (**TNR**),
    **False Positive Rate** (**FPR**), and precision. The model that was used was
    logistic regression without any class weighting or sensitivity to the minority
    class. For the full notebook, please refer to the GitHub repository for this chapter:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.17* 展示了一个图表，说明了修改决策阈值对不平衡数据集的各种分类指标的影响：**真正例率**（**TPR**或召回率）、**真负例率**（**TNR**）、**假正例率**（**FPR**）和精确度。所使用的模型是没有任何类别权重或对少数类敏感性的逻辑回归。有关完整笔记本，请参阅本章的GitHub仓库：'
- en: '![](img/B17259_05_17.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_17.jpg)'
- en: Figure 5.17 – A plot of the different classification metrics (TPR, TNR, FPR,
    precision, F1 score, and accuracy) as a function of the decision threshold
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 – 不同分类指标（TPR、TNR、FPR、精确度、F1分数和准确度）作为决策阈值的函数的图表
- en: 'Here are some observations about these plots:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些图表，有以下几点观察：
- en: '**Precision**: If the threshold is increased, precision (  TP _________________  Total
    number of positive predictions ) typically goes up as well. Why? Because as the
    threshold is increased, the total number of positive predictions would come down,
    and hence, as the denominator decreases, precision increases. Similarly, the opposite
    is true as well: if the threshold goes down, the precision goes down too.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：如果阈值增加，精确度（  TP _________________  总预测为正的次数）通常会上升。为什么？因为随着阈值的增加，总预测为正的次数会减少，因此，随着分母的减少，精确度增加。同样，相反的情况也是正确的：如果阈值降低，精确度也会降低。'
- en: '**Recall**: Let’s see the impact of threshold change on recall. The recall
    is defined as  TP ____________  Total number of positives and the denominator
    is a constant value. As the threshold is lowered, TP may increase and would typically
    increase the recall.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：让我们看看阈值变化对召回率的影响。召回率定义为  TP ____________  总正例数，分母是一个常数。随着阈值的降低，TP可能会增加，通常会提高召回率。'
- en: '**True Negative Rate** (**TNR**): TNR measures the proportion of actual negatives
    that are correctly identified as such. In imbalanced datasets, where the negative
    class is the majority, a naive or poorly performing classifier might have a high
    TNR simply because it predicts the majority class for all or most instances. In
    such cases, the TNR could be misleadingly high.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例率**（**TNR**）：TNR衡量的是实际负例中被正确识别为负例的比例。在不平衡的数据集中，如果负类是多数类，一个简单或表现不佳的分类器可能因为对所有或大多数实例预测多数类而具有很高的TNR。在这种情况下，TNR可能会误导性地很高。'
- en: '**False Positive Rate** (**FPR**): This is the rate at which negative instances
    are incorrectly classified as positive. In imbalanced datasets, a naive classifier
    that predicts everything as the majority (negative) class would have an FPR close
    to 0.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误报率**（**FPR**）：这是将负实例错误地分类为正实例的比率。在不平衡的数据集中，一个简单的分类器，如果将所有实例都预测为多数类（负类），其FPR将接近0。'
- en: Usually, we have a trade-off between TPR and TNR that must be taken into account
    while selecting an optimal decision threshold, as shown in the plot in *Figure
    5**.17*.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在选择最佳决策阈值时，需要在TPR（真正例率）和TNR（真负例率）之间进行权衡，如图5**.17**所示。
- en: 🚀 Cost-sensitive learning in production at Shopify
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Shopify在生产中的成本敏感学习
- en: In a real-world application at Shopify [16], the platform faced the challenge
    of categorizing products for millions of merchants selling a diverse array of
    items. Accurate product categorization was vital for functionalities such as enhanced
    search and discovery, as well as providing personalized marketing insights to
    merchants. Given the immense volume and variety of products, manual categorization
    was not feasible. Machine learning techniques were employed to automate the categorization
    process, adapting to the ever-expanding and diversifying product range. The dataset
    that was utilized was highly imbalanced, particularly due to the hierarchical
    structure of the **Google Product Taxonomy** (**GPT**) that Shopify employs. With
    over 5,500 categories, the GPT added complexity to an already challenging problem.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在Shopify的实际应用中[16]，平台面临着为成百万的商家对各种商品进行分类的挑战。准确的产品分类对于增强搜索和发现功能以及向商家提供个性化营销洞察至关重要。鉴于产品数量庞大且种类繁多，手动分类是不可行的。机器学习技术被用于自动化分类过程，以适应不断扩展和多样化的产品范围。所使用的数据集高度不平衡，这主要是由于Shopify采用的**谷歌产品分类法**（**GPT**）的层级结构。GPT拥有超过5,500个类别，这给一个已经具有挑战性的问题增加了复杂性。
- en: To address the issue of data imbalance, class weights were implemented. By assigning
    the class weights, the model could impose higher penalties for incorrect predictions
    in underrepresented classes, effectively mitigating the lack of data in those
    categories. The model was fine-tuned to strike a balance between hierarchical
    precision and recall. This fine-tuning was informed by specific business use cases
    and aimed at enhancing the merchant experience by minimizing negative interactions
    and friction. Manual adjustments were made to the confidence thresholds (this
    shows that threshold tuning is so relevant in the real world!) to ensure optimal
    performance in sensitive categories such as “Religious and Ceremonial.” Various
    metrics such as hierarchical accuracy, precision, recall, and F1 score were balanced
    to tailor the model to business requirements. The model is now actively used by
    multiple internal teams and partner ecosystems to develop derivative data products.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据不平衡的问题，实施了类权重。通过分配类权重，模型可以对代表性不足的类别中的错误预测施加更高的惩罚，从而有效地缓解这些类别中数据不足的问题。模型经过微调，以在层级精度和召回率之间取得平衡。这种微调是基于特定的业务用例，旨在通过最小化负面交互和摩擦来提升商家体验。对置信阈值进行了手动调整（这表明阈值调整在现实世界中是多么相关！），以确保在“宗教和仪式”等敏感类别中实现最佳性能。平衡了各种指标，如层级准确率、精确率、召回率和F1分数，以使模型满足业务需求。该模型现在被多个内部团队和合作伙伴生态系统积极用于开发衍生数据产品。
- en: Next, we’ll look at various ways of tuning these thresholds.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨调整这些阈值的各种方法。
- en: Methods for threshold tuning
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阈值调整方法
- en: 'Most of the time, we aim to optimize specific business metrics or standard
    machine learning metrics, requiring us to select a threshold that maximizes the
    metric of interest. In literature, various methods for threshold tuning are discussed,
    such as setting a threshold equal to the priority probability of observing a positive
    example, using the ROC curve to optimize for high TPR and low FPR, or employing
    the PR curve to maximize the F1 score or Fbeta score (see *Figure 5**.18*):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，我们旨在优化特定的业务指标或标准机器学习指标，这要求我们选择一个最大化感兴趣指标的阈值。在文献中，讨论了各种阈值调整方法，例如将阈值设置为观察正例的优先概率，使用ROC曲线优化高TPR和低FPR，或使用PR曲线最大化F1分数或Fbeta分数（见*图5**.18*）：
- en: '![](img/B17259_05_18.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_05_18.jpg)'
- en: Figure 5.18 – Popular ways of tuning the threshold
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 – 调整阈值的流行方法
- en: We will discuss these methods one by one.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一讨论这些方法。
- en: 'We will continue to use the same dataset we created earlier. The following
    code block fits a logistic regression model and obtains predicted probabilities
    for the test set:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用我们之前创建的相同数据集。以下代码块拟合逻辑回归模型并获取测试集的预测概率：
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Threshold tuning using the prior threshold
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用先验阈值进行阈值调整
- en: 'An obvious threshold that can be used is equal to the probability of the positive
    class in the training dataset [10]. Let’s implement this idea:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显的阈值可以使用的是训练数据集中正类概率的值[10]。让我们实现这个想法：
- en: '[PRE22]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This prints the following threshold:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下阈值：
- en: '[PRE23]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Threshold tuning using the ROC curve
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用ROC曲线进行阈值调整
- en: 'For the ROC curve, Youden’s J statistic [17] can be used to find the optimal
    threshold. Youden’s J statistic has roots in the clinical field and is a single
    statistic that captures the performance of a diagnostic test. In the context of
    binary classification, the statistic, J, is defined as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ROC曲线，可以使用尤登的J统计量[17]来找到最优阈值。尤登的J统计量在临床领域有根源，是一个捕获诊断测试性能的单个统计量。在二元分类的背景下，统计量J定义为以下：
- en: J = Sensitivity + Specificity − 1 = TPR + TNR − 1 = TPR − FPR
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: J = 灵敏度 + 特异性 − 1 = TPR + TNR − 1 = TPR − FPR
- en: Its value can range from -1 (TPR=0 and TNR=0 – that is, always wrong results)
    to 1 (TPR=1 and FPR=0 – that is, perfect results). This is a common choice for
    selecting a threshold in an ROC analysis since it balances both sensitivity (TPR)
    and specificity (TNR). Please note that TNR = 1-FPR.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 其值可以从-1（TPR=0和TNR=0 – 即，总是错误的结果）到1（TPR=1和FPR=0 – 即，完美结果）不等。这是在ROC分析中选择阈值的一个常见选择，因为它平衡了灵敏度（TPR）和特异性（TNR）。请注意，TNR
    = 1-FPR。
- en: The reason that maximizing Youden’s J is equivalent to choosing the optimal
    threshold is that it essentially finds the point on the ROC curve that is farthest
    from the line of no discrimination (the diagonal). This means that it selects
    a threshold that achieves a balance between TPR and FPR, which is often what we
    want in a classifier.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化尤登指数J的原因等同于选择最优阈值，因为它本质上是在ROC曲线上找到离无歧视线（对角线）最远的点。这意味着它选择一个在TPR和FPR之间取得平衡的阈值，这通常是我们在分类器中想要的。
- en: 'The “optimal” threshold can depend heavily on the cost of false positives versus
    false negatives in our specific application. The following code block identifies
    the optimal classification threshold using the Youden index, which is calculated
    from the ROC curve:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: “最优”阈值可能严重依赖于我们特定应用中假阳性与假阴性的成本。以下代码块使用尤登指数识别最优分类阈值，该指数是从ROC曲线计算得出的：
- en: '[PRE24]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This outputs the following values:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下值：
- en: '[PRE25]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Another threshold adjustment method that uses ROC curves that’s often used
    in literature is maximizing the geometric mean of TPR (also known as sensitivity)
    and TNR (also known as specificity):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用于文献中的阈值调整方法，它使用ROC曲线，即最大化TPR（也称为灵敏度）和TNR（也称为特异性）的几何平均值：
- en: G − mean = √ __________________  Sensitivity * Specificity  = √ _ TPR * TNR 
    = √ ______________  TPR * (1 − FPR)
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: G − mean = √ __________________  灵敏度 * 特异性  = √ _TPR * TNR  = √ ______________  TPR
    * (1 − FPR)
- en: 'Maximizing the geometric mean is equivalent to finding a good balance between
    TPR and TNR. The following code block calculates the best threshold for classification
    using the G-mean metric, along with its corresponding TPR, FPR, and TNR values.
    We import `roc_curve` from `sklearn.metrics`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化几何平均值等同于在TPR和TNR之间找到一个良好的平衡。以下代码块使用G-mean指标计算分类的最佳阈值，以及其相应的TPR、FPR和TNR值。我们从`sklearn.metrics`导入`roc_curve`：
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This outputs the following optimal values:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下最优值：
- en: '[PRE27]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Threshold tuning using the PR curve
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PR曲线进行阈值调整
- en: As we discussed in [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction
    to Data Imbalance in Machine Learning*, the PR curve is generally preferred over
    ROC curves for imbalanced datasets when the positive class is more important than
    the negative class. As a reminder, the simple reason for this is that the PR curve
    ignores the true negatives, and hence, it can represent a stark difference between
    model performance when using imbalanced datasets in comparison to a balanced dataset.
    While ROC curves won’t change much as an imbalance in the data increases, they
    can be a preferred option if both classes are equally important.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第一章*](B17259_01.xhtml#_idTextAnchor015)，“机器学习中的数据不平衡介绍”中讨论的那样，当正类比负类更重要时，PR曲线通常比ROC曲线更适合不平衡数据集。作为提醒，简单的原因是PR曲线忽略了真正的负例，因此，它可以在使用不平衡数据集与平衡数据集相比时，表示模型性能的显著差异。虽然随着数据不平衡的增加，ROC曲线不会变化太多，但如果两个类别都同样重要，它们可以是一个更好的选择。
- en: We want the maximum possible values for both precision and recall, ideally both
    being 1\. Thus, the point of optimality on a PR curve is (1,1).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望精确率和召回率都达到最大值，理想情况下都是1。因此，PR曲线上的优化点是（1，1）。
- en: When we move away from this point of optimality, both the precision and recall
    values decrease, and we are less optimal.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们偏离这个优化点时，精确率和召回率的值都会降低，我们就不那么优化了。
- en: 'One measure that captures this trade-off between precision and recall is the
    F1 score. The F1 score is defined as the harmonic mean of the precision and recall:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一种捕捉精确率和召回率之间权衡的度量是F1分数。F1分数被定义为精确率和召回率的调和平均值：
- en: F1 = 2 * (Precision * Recall) / (Precision + Recall)
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: F1 = 2 * (Precision * Recall) / (Precision + Recall)
- en: If we analyze this equation, we will see that the F1 score is highest (reaching
    its maximum at 1) when both precision and recall are 1, which is exactly the optimal
    point we defined on the PR curve.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析这个方程，我们会看到当精确率和召回率都是1时，F1分数最高（在1处达到最大值），这正是我们在PR曲线上定义的优化点。
- en: Therefore, optimizing for the maximum F1 score would ensure that we are striving
    to maximize both precision and recall, effectively pushing us toward the optimal
    point on the PR curve.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化最大F1分数将确保我们努力最大化精确率和召回率，有效地推动我们向PR曲线上的优化点靠近。
- en: 'Maximizing the F1 score is a common and effective method for determining the
    optimal threshold. The following code block calculates the best threshold using
    the F1 score metric derived from the PR curve:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化F1分数是确定最佳阈值的一种常见且有效的方法。以下代码块使用从PR曲线派生的F1分数度量计算最佳阈值：
- en: '[PRE28]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This outputs the following optimal values:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出以下优化值：
- en: '[PRE29]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s plot the PR curve with the optimal threshold value:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制具有最佳阈值值的PR曲线：
- en: '[PRE30]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The PR curve looks like this:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: PR曲线看起来是这样的：
- en: '![](img/B17259_05_19.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_05_19.jpg)'
- en: Figure 5.19 – The PR curve with the best F1 score threshold value
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 – 最佳F1分数阈值值的PR曲线
- en: General threshold tuning
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一般阈值调整
- en: 'As a general method, we may have to optimize any metric, such as average precision,
    accuracy, and so on, or any other business metric. In such cases, we can write
    a function to optimize that metric directly. Let’s take the example of the **Index
    of Union** (**IU**) metric defined by I. Unal et al. in their research [18], where
    the metric is defined by the threshold value, c, at which IU(c) is minimized:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种一般方法，我们可能需要优化任何度量，如平均精确度、准确度等，或任何其他业务度量。在这种情况下，我们可以编写一个函数来直接优化该度量。让我们以I.
    Unal等人在其研究[18]中定义的**联合指数**（**IU**）度量为例，其中度量由使IU(c)最小化的阈值值c定义：
- en: IU(c) = (|Sensitivity(c) − ROC _ AUC| + |Specificity(c) − ROC _ AUC|)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: IU(c) = (|Sensitivity(c) − ROC _ AUC| + |Specificity(c) − ROC _ AUC|)
- en: Here, Sensitivity(c) is the sensitivity at c, Specificity is the specificity
    at c, and ROC _ AUC is the **Area Under the Curve** (**AUC**) of the ROC plot.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Sensitivity(c)是c处的灵敏度，Specificity是c处的特异性，ROC _ AUC是ROC图的**曲线下面积**（**AUC**）。
- en: 'Let’s implement the IU metric, as defined here, as a custom metric to find
    the optimal threshold that minimizes it:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现IU度量，如这里定义的，作为一个自定义度量来找到最小化它的最佳阈值：
- en: '[PRE31]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This produces the following optimal values:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下优化值：
- en: '[PRE32]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This wraps up our discussion on classical modeling techniques. We are now ready
    to venture into studying data imbalance in the realm of deep learning. We’ll explore
    how the insights gained from the general techniques learned from previous chapters
    can be adapted to enhance our deep learning models when dealing with imbalanced
    data.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对经典建模技术的讨论结束。我们现在准备进入研究深度学习领域中的数据不平衡。我们将探讨从上一章学习的一般技术如何适应以增强我们处理不平衡数据时的深度学习模型。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into CSL, an alternative to oversampling and undersampling.
    Unlike data-level techniques that treat all misclassification errors equally,
    CSL adjusts the cost function of a model to account for the significance of different
    classes. It includes class weighting and meta-learning techniques.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了CSL（组合过采样和欠采样），作为一种替代方案。与将所有误分类错误同等对待的数据级别技术不同，CSL调整模型的成本函数，以考虑不同类别的显著性。它包括类别加权和元学习技术。
- en: Libraries such as `scikit-learn`, Keras/TensorFlow, and PyTorch support cost-sensitive
    learning. For instance, `scikit-learn` offers a `class_weight` hyperparameter
    to adjust class weights in loss calculation. XGBoost has a `scale_pos_weight`
    parameter for balancing positive and negative weights. MetaCost transforms any
    algorithm into its cost-sensitive version using bagging and a misclassification
    cost matrix. Additionally, threshold adjustment techniques can enhance metrics
    such as F1 score, precision, and recall by post-processing model predictions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`scikit-learn`、Keras/TensorFlow和PyTorch这样的库支持成本敏感学习。例如，`scikit-learn`提供了一个`class_weight`超参数来调整损失计算中的类别权重。XGBoost有一个`scale_pos_weight`参数用于平衡正负权重。MetaCost通过使用袋装和误分类成本矩阵将任何算法转换为成本敏感版本。此外，阈值调整技术可以通过后处理模型预测来增强F1分数、精确度和召回率等指标。
- en: Experiments with various data sampling and CSL techniques can help determine
    the best approach. We’ll extend these concepts to deep learning models in [*Chapter
    8*](B17259_08.xhtml#_idTextAnchor235), *Algorithm-Level Deep Learning Techniques*.
    This concludes our discussion of classical machine learning models, and we have
    graduated to move on to deep learning techniques. In the next chapter, we will
    briefly introduce deep learning concepts and see how imbalanced datasets could
    be a problem in the deep learning world.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用各种数据采样和CSL技术的实验可以帮助确定最佳方法。我们将在[*第8章*](B17259_08.xhtml#_idTextAnchor235)中扩展这些概念，即*算法级深度学习技术*。这标志着我们对经典机器学习模型的讨论结束，我们已经毕业到深入学习技术。在下一章中，我们将简要介绍深度学习概念，并看看不平衡数据集在深度学习世界中可能成为问题。
- en: Questions
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Apply the CSL technique to the SVM model from `scikit-learn` while utilizing
    the dataset that was used in this chapter. Use the `class_weight` and `sample_weight`
    parameters, similar to how we used them for other models in this chapter. Compare
    the performance of this model with the ones that we already encountered in this
    chapter.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用本章中使用的相同数据集的同时，将CSL技术应用于`scikit-learn`中的SVM模型。使用`class_weight`和`sample_weight`参数，类似于我们在本章中用于其他模型的方式。比较此模型与本章中已经遇到的模型的表现。
- en: LightGBM is another gradient-boosting framework similar to XGBoost. Apply the
    cost-sensitive learning technique to a LightGBM model while utilizing the dataset
    we used in this chapter. Use the `class_weight` and `sample_weight` parameters
    similar to how we used them for other models in this chapter. Compare the performance
    of this model with the ones that we already encountered in this chapter.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LightGBM是另一个类似于XGBoost的梯度提升框架。在本章使用的数据集上应用成本敏感学习技术到LightGBM模型。使用`class_weight`和`sample_weight`参数，类似于我们在本章中用于其他模型的方式。比较此模型与本章中已经遇到的模型的表现。
- en: AdaCost [10] is a variant of AdaBoost that combines boosting with CSL. It updates
    the training distribution for successive boosting rounds by utilizing the misclassification
    cost. Extend `AdaBoostClassifier` from `scikit-learn` to implement the AdaCost
    algorithm. Compare the performance of AdaCost with MetaCost on the dataset that
    was used in this chapter.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AdaCost [10] 是一种结合了提升和CSL的AdaBoost变体。它通过利用误分类成本来更新连续提升轮次的训练分布。将`scikit-learn`中的`AdaBoostClassifier`扩展以实现AdaCost算法。比较AdaCost与MetaCost在本章使用的数据集上的性能。
- en: Tune the hyperparameters, specifically `max_depth`, `max_delta_step`, and `min_child_weight`,
    for the XGBoost model using the dataset that we used in this chapter. After tuning,
    evaluate whether the weighted XGBoost model outperforms the non-weighted version.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整XGBoost模型的超参数，特别是`max_depth`、`max_delta_step`和`min_child_weight`，使用本章中使用的数据集。调整后，评估加权XGBoost模型是否优于非加权版本。
- en: References
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: P. Turney, *Types of cost in inductive concept learning*, Proc. Workshop on
    CostSensitive Learning at the 17th Int. Conf. Mach. Learn., Stanford University,
    CA (2000), pp. 15–21.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Turney, *归纳概念学习中的成本类型*, 在第17届国际机器学习会议的成本敏感学习研讨会论文集，斯坦福大学，加利福尼亚州（2000年），第15–21页。
- en: C. X. Ling and V. S. Sheng, *Cost-Sensitive Learning and the Class* *Imbalance
    Problem*.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C. X. Ling 和 V. S. Sheng, *成本敏感学习和类* *不平衡问题*.
- en: 'Sheng, V. S., & Ling, C. X. (2006). *Thresholding for making classifiers cost-sensitive*.
    AAAI’06: Proceedings of the 21st national conference on artificial intelligence,
    vol. 6, pp. 476–481.'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sheng, V. S.，& Ling, C. X. (2006). *为了使分类器成本敏感的阈值方法*. AAAI’06: 第21届全国人工智能会议论文集，第6卷，第476–481页。'
- en: '*Pneumonia in Children Statistics* – UNICEF data: [https://data.unicef.org/topic/child-health/pneumonia/](https://data.unicef.org/topic/child-health/pneumonia/).'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*儿童肺炎统计数据* – 联合国儿童基金会数据：[https://data.unicef.org/topic/child-health/pneumonia/](https://data.unicef.org/topic/child-health/pneumonia/).'
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, Model Ensemble for Click
    Prediction in Bing Search Ads, in Proceedings of the 26th International Conference
    on World Wide Web Companion – WWW ’17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689–698\. doi: 10.1145/3041021.3054192.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, 和 F. Sun, 在第26届国际万维网会议（WWW ''17）的伴随会议论文集中，Bing搜索广告的点击预测模型集成，珀斯，澳大利亚：ACM出版社，2017年，第689–698页。doi:
    10.1145/3041021.3054192.'
- en: '*Machine Learning-Powered Search Ranking of Airbnb Experiences* (2019), [https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789).'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*机器学习驱动的Airbnb体验搜索排名*（2019年），[https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789).'
- en: 'P. Domingos, MetaCost: A general method for making classifiers cost-sensitive,
    in Proceedings of International Conference on Knowledge Discovery and Data Mining,
    pp. 155–164, 1999.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'P. Domingos, MetaCost: 一种使分类器成本敏感的通用方法，在知识发现与数据挖掘国际会议论文集中，第155–164页，1999年。'
- en: '*Unstable Learner*. In: Sammut, C., Webb, G.I. (eds) Encyclopedia of Machine
    Learning and Data Mining. Springer, Boston, MA. doi: [https://doi.org/10.1007/978-1-4899-7687-1_866](https://doi.org/10.1007/978-1-4899-7687-1_866).'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*不稳定的学习者*. 在：Sammut, C.，Webb, G.I. (eds) 机器学习与数据挖掘百科全书。Springer，波士顿，马萨诸塞州。doi:
    [https://doi.org/10.1007/978-1-4899-7687-1_866](https://doi.org/10.1007/978-1-4899-7687-1_866).'
- en: 'W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan, *AdaCost: Misclassiﬁcation*
    *Cost-sensitive Boosting*.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'W. Fan, S. J. Stolfo, J. Zhang, 和 P. K. Chan, *AdaCost: 误分类* *成本敏感的Boosting*.'
- en: G. I. Karakoulas and J. Shawe-Taylor, *Optimizing Classifiers for Imbalanced*
    *Training Sets*.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: G. I. Karakoulas 和 J. Shawe-Taylor, *优化不平衡* *训练集* 的分类器*.
- en: P. Viola and M. Jones, *Fast and Robust Classification using Asymmetric AdaBoost
    and a* *Detector Cascade*.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Viola 和 M. Jones, *使用非对称AdaBoost和* *检测级联* *进行快速和鲁棒的分类*.
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Output Thresholding for Ensemble Learners
    and Imbalanced Big Data*, in 2021 IEEE 33rd International Conference on Tools
    with Artificial Intelligence (ICTAI), Washington, DC, USA: IEEE, Nov. 2021, pp.
    1449–1454\. doi: 10.1109/ICTAI52525.2021.00230.'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'J. M. Johnson 和 T. M. Khoshgoftaar, *集成学习者的输出阈值和类不平衡大数据*, 在2021年IEEE第33届国际人工智能工具会议（ICTAI）中，华盛顿特区，美国：IEEE，2021年11月，第1449–1454页。doi:
    10.1109/ICTAI52525.2021.00230.'
- en: 'J. M. Johnson and T. M. Khoshgoftaar, *Deep Learning and Thresholding with
    Class-Imbalanced Big Data*, in 2019 18th IEEE International Conference On Machine
    Learning And Applications (ICMLA), Boca Raton, FL, USA, Dec. 2019, pp. 755–762\.
    doi: 10.1109/ICMLA.2019.00134.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'J. M. Johnson 和 T. M. Khoshgoftaar, *在2019年第18届IEEE国际机器学习与应用会议（ICMLA）中，关于类不平衡大数据的深度学习和阈值处理*,
    佛罗里达州博卡拉顿，美国，2019年12月，第755–762页。doi: 10.1109/ICMLA.2019.00134.'
- en: F. Provost, *Machine Learning from Imbalanced Data* *Sets 101*.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: F. Provost, *不平衡数据集的机器学习* *入门*.
- en: 'M. Buda, A. Maki, and M. A. Mazurowski, *A systematic study of the class imbalance
    problem in convolutional neural networks*, Neural Networks, vol. 106, pp. 249–259,
    Oct. 2018, doi: 10.1016/j.neunet.2018.07.011.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'M. Buda, A. Maki, 和 M. A. Mazurowski, *卷积神经网络中类别不平衡问题的系统研究*, 神经网络，第106卷，第249–259页，2018年10月，doi:
    10.1016/j.neunet.2018.07.011.'
- en: '*Using Rich Image and Text Data to Categorize Products at Scale* (2021), [https://shopify.engineering/using-rich-image-text-data-categorize-products](https://shopify.engineering/using-rich-image-text-data-categorize-products).'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用丰富图像和文本数据大规模分类产品*（2021年），[https://shopify.engineering/using-rich-image-text-data-categorize-products](https://shopify.engineering/using-rich-image-text-data-categorize-products).'
- en: 'W. J. Youden, *Index for rating diagnostic tests*, Cancer, vol. 3, no. 1, pp.
    32–35, 1950, doi: 10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'W. J. Youden, *诊断测试评分指数*，癌症，第3卷，第1期，第32–35页，1950年，doi: 10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3.'
- en: 'I. Unal, *Defining an Optimal Cut-Point Value in ROC Analysis: An Alternative
    Approach*, Computational and Mathematical Methods in Medicine, vol. 2017, pp.
    1–14, 2017, doi: 10.1155/2017/3762651.'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'I. Unal, *在ROC分析中定义最佳切割点值：一种替代方法*，计算与数学在医学中的应用，第2017卷，第1–14页，2017年，doi: 10.1155/2017/3762651.'
- en: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, and F. Sun, *Model Ensemble for Click
    Prediction in Bing Search Ads*, in Proceedings of the 26th International Conference
    on World Wide Web Companion – WWW ’17 Companion, Perth, Australia: ACM Press,
    2017, pp. 689–698\. doi: 10.1145/3041021.3054192.'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X. Ling, W. Deng, C. Gu, H. Zhou, C. Li, 和 F. Sun, *用于Bing搜索广告点击预测的模型集成*, 在第26届国际万维网大会伴随会议——WWW
    ''17伴随会议论文集中，珀斯，澳大利亚：ACM出版社，2017年，第689–698页。doi: 10.1145/3041021.3054192.'
