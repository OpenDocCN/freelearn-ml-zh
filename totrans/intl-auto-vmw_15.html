<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Virtualizing Big Data on vSphere</h1>
                </header>
            
            <article>
                
<p>In this chapter, you will learn to leverage shared storage in modern big data platforms. We'll evaluate current in-memory big data apps on vSphere virtualization platform. The in-memory feature of these platforms makes them less dependent on I/O and storage protocols. We will go through with administrator productivity and his control while creation of a Hadoop cluster and show the use of a Hadoop management tool for installation of software onto virtual machines. Further, we will learn about the ability to scale in and out, such that any workloads on the platform can expand to utilize all available cluster resources by pooling of resources to be shared by multiple virtual Hadoop clusters, resulting in higher average resource utilization.</p>
<p>We will cover the following topics in detail:</p>
<ul>
<li class="h1">Big data infrastructure</li>
<li class="h1">Open source software</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can download VMware vSphere Big Data Extensions 2.3.2 from <a href="https://my.vmware.com/web/vmware/details?downloadGroup=BDE_232&amp;productId=676&amp;rPId=28154">https://my.vmware.com/web/vmware/details?downloadGroup=BDE_232&amp;productId=676&amp;rPId=28154</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Big data infrastructure</h1>
                </header>
            
            <article>
                
<p>A cloud implementation always has a service catalog with all the services that are available for consumption. It also has service-design, catalog-management, and knowledge-management systems. These services will provide an organization with the ability to accelerate the operations and build an agile cloud services framework. We have to define some roles, responsibilities, and functionalities to manage the process:</p>
<ul>
<li><strong>Service owner</strong>: Responsible for the value of a service and managing the service backlog</li>
<li><strong>Service backlog manager</strong>: Responsible for defining the service's priority with all backlogs, including functional, non-functional, and technical requirements</li>
<li><strong>Service release manager</strong>: Responsible for planning, scheduling and controlling the builds, tests, and releases by delivering new features as well as taking care of existing services</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop as a service</h1>
                </header>
            
            <article>
                
<p>VMware vSphere <strong>Big Data Extensions</strong> (<strong>BDE</strong>) is the platform that runs scale-out clustered Hadoop applications. It provides the agility to change the configuration through a single console and the flexibility to scale up and out for both compute and storage resources with better reliability and security on the vSphere platform:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ceab812f-bdaa-4231-84d1-23f5e3bd2b6a.png" style="width:29.67em;height:16.33em;"/></p>
<p>We categorize the Hadoop journey in three stages:</p>
<ul>
<li><strong>Stage 1</strong>: This is what we call the piloting stage; typically the size of the cluster we see is under 20 nodes. In this stage, the customer is dabbling with Hadoop, trying to understand the value of Hadoop, and in many cases, prove the value of Hadoop in providing new business insights. Usually, the journey starts with a line of business, where someone wants to apply Hadoop on one or two use cases, often on data the enterprise is collecting but not doing much with. For example, one of the oil and gas companies we spoke with was collecting all these sensor data from oil wells and drill platforms. With Hadoop, they were to do some interesting analyses and get some very interesting insights. </li>
<li><strong>Stage 2</strong>: Once the initial value is proven for Hadoop on big data, enterprises typically move into codifying these use cases and running them regularly as a production workload. One common phenomenon we see at this stage is that, as people hear about this production Hadoop cluster, they want to leverage it to explore their data; more and more jobs are added to the cluster, and the cluster starts to expand and grow. Another common thing we see is that it's not just about core Hadoop components of MapReduce and <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>). The other non-core Hadoop components of Hive, Pig, HBase, and others are often added to the cluster. Typically, we see the production cluster ranging from dozens of nodes to hundreds of nodes, and it may grow very rapidly. Here, there are typically dedicated Hadoop administrators to ensure the health of the system.</li>
<li><strong>Stage 3</strong>: At this stage, customers are using Hadoop extensively throughout their organization, and have built mission-critical business workflows around it. For example, for an e-commerce retailer, the recommendation engine is now a critical part of their business, and Hadoop is a critical part of the workflow. Often, at this stage, we see enterprises expanding beyond Hadoop, and adding other big data technology and services into the mix. Often, <strong>massively-parallel processing</strong> (<strong>MPP</strong>) databases, NoSQL databases, and more non-core Hadoop components are part of the big data production system. In terms of Hadoop nodes, typically we see hundreds to thousands of nodes. On the extreme end, companies such as Yahoo and Facebook have several thousands of nodes.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying the BDE appliance</h1>
                </header>
            
            <article>
                
<p>VMware enables you to easily and efficiently deploy and use Hadoop on existing virtual infrastructure through vSphere BDE. BDE makes Hadoop virtualization-aware, improves performance in virtual environments, and enables the deployment of highly-available Hadoop clusters in minutes. vSphere BDE automates the deployment of a Hadoop cluster, and thus provides better Hadoop manageability and usability.</p>
<p>Let's get started with these steps:</p>
<ol>
<li>Select <span class="packt_screen">File</span> in the VMware vSphere Client and go to <span class="packt_screen">Deploy VMware-BigDataExtensions-x.x_OVF10.ova</span>.</li>
<li>In the <span class="packt_screen">Select the source location</span> dialog box, click the <span class="packt_screen">Local file</span> radio button, click <span class="packt_screen">B</span><span class="packt_screen">rowse...</span>, browse to the location of the identity appliance, click <span class="packt_screen">Open</span>, and then click <span class="packt_screen">Next</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1002aa18-4bad-4c78-a098-3777c38cabeb.png" style="width:48.58em;height:30.50em;"/></p>
<ol start="3">
<li>In the <span class="packt_screen">Review details</span> dialog box, review the summary details and click <span class="packt_screen">Next</span>.</li>
<li>In the <span class="packt_screen">Accept EULAs</span> dialog box, accept the license agreement by clicking the <span class="packt_screen">Accept</span> button and then click <span class="packt_screen">Next</span>.</li>
<li>In the <span class="packt_screen">Select name and folder</span> dialog box, enter a unique name for the virtual appliance in the <span class="packt_screen">Name</span> text box.</li>
<li>Select the folder or data center location where we have to deploy virtual appliances and then click <span class="packt_screen">Next</span>. For QA deployment, <span class="packt_screen">East FP</span> | <span class="packt_screen">Pumulus</span> | QA folder is selected.</li>
<li>In the <span class="packt_screen">Select a resource</span> dialog box, select the cluster where you want to deploy the virtual appliance and click <span class="packt_screen">Next</span>. For QA deployment, the <span class="packt_screen">ECHADMIN01</span> cluster is selected.</li>
<li><span>Select the required resource pool i</span>n the <span class="packt_screen">Resource Pool</span> dialog box.</li>
<li>Select the <span class="packt_screen">QumulusQA VMs</span> resource group.</li>
<li>In the <span class="packt_screen">Select storage</span> dialog box, select the disk format that you want to use for the virtual appliance from the <span class="packt_screen">Select virtual disk format</span> drop-down list.</li>
<li>Select the datastore you wish to place the virtual appliance on by clicking on it in the list. Click <span class="packt_screen">Next</span>. For QA deployment, the <span class="packt_screen">ECHADMIN01-DEV and QA-VMX</span> datastore cluster is selected.</li>
<li>In the <span class="packt_screen">Disk Format</span> dialog box, select <span class="packt_screen">Thin Provision</span> and click <span class="packt_screen">Next</span>.</li>
<li>In the <span class="packt_screen">Setup networks</span> dialog box, select the network that you want to connect the virtual appliance to using the <span class="packt_screen">Destination</span> drop-down list and then click <span class="packt_screen">Next</span>.</li>
<li>For <span class="packt_screen">QA deployment</span>, <span class="packt_screen">xx.xxx.0.0/22</span> is<span class="packt_screen"> </span>selected.</li>
<li>In the <span class="packt_screen">Ready to complete</span> dialog box, select the <span class="packt_screen">Power on after deployment</span> check box and click <span class="packt_screen">Finish</span>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the VMware BDE</h1>
                </header>
            
            <article>
                
<p>We will deploy the vApp, power on, and then browse the console of the management server. There are four Hadoop clusters configured in this vSphere environment. The columnar view on the right indicates each cluster's name, status, which Hadoop distribution is running, the resource pool it belongs to, and the list of nodes. Resource pools manage how Hadoop consumes the underlying physical resources.</p>
<p>Steps to configure BDE on vSphere are given below :</p>
<ol>
<li>Log in as Serengeti and change the Serengeti user password with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>run sudo /opt/serengeti/sbin/set-password –u</strong></pre>
<ol start="2">
<li>Close the management console and SSH with the serengeti user. Configure the YUM repository by running the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong># cd /opt/serengeti/www/yum/repos/centos/6/base/RPMS/</strong><br/><strong>wget http://mirror.centos.org/centos/6/os/x86_64/Packages/mailx-12.4-7.el6.x86_64.rpm</strong><br/><strong>wget http://mirror.centos.org/centos/6/os/x86_64/Packages/wsdl4j-1.5.2-7.8.el6.noarch.rpm</strong></pre>
<ol start="3">
<li>If we can't connect using <kbd>wget</kbd>, download <kbd>.rpm</kbd> and then winscp ( open source tool ) them over. To create the repo, run the following command: </li>
</ol>
<pre style="padding-left: 60px"><strong>createrepo ..</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The BDE plugin</h1>
                </header>
            
            <article>
                
<p><span>We will get to the</span> BDE <span>plugin by clicking the</span> Home <span>icon and then choosing</span> <span class="packt_screen">Big Data Extensions</span>:</p>
<ol>
<li>
<p><span>Open a web browser and navigate to <kbd>https://xx.xxx.x.xx:8443/register-plugin</kbd>. Remember that the IP address will be user-specific.</span></p>
</li>
<li>Select the <span class="packt_screen">Install</span> radial button, fill out the form with vCenter information, and click <span class="packt_screen">Submit</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/257e099c-f4b7-4fa5-88ba-74afd6d25265.png" style="width:37.00em;height:22.42em;"/></p>
<ol start="3">
<li>Click on <span class="packt_screen">Big Data Extensions</span> in the vSphere Web Client, then click on the <span class="packt_screen">Connect Server…</span> hyperlink in the <span class="packt_screen">Summary</span> tab and navigate through the inventory tree to find the management server:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8c4d6d31-8975-4da9-92f4-015bdc74c8d0.png" style="width:57.83em;height:25.50em;"/></p>
<ol start="4">
<li>Click on <span class="packt_screen">OK</span> to accept the certificate<span>. </span>The server is now connected in the <span class="packt_screen">Summary</span> tab.</li>
<li>To set up the Hadoop YUM repository, SSH into the YUM repo server as the root user. Type the commands shown in the <span>VMware KB article (<a href="https://kb.vmware.com/s/article/2091054" target="_blank">https://kb.vmware.com/s/article/2091054</a>) </span>to configure <strong>Hortonworks Data Platform</strong> (<strong>HDP</strong>) 2 YUM. </li>
<li>Browse to the new repo at <kbd>http://puppet2.qvcdev.qvc.net/hdp/2/</kbd>. We will utilize an existing YUM repo server for this environment.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring distributions on BDE</h1>
                </header>
            
            <article>
                
<p>We will now log in through SSH into the Serengeti Management Server with the Serengeti user account:</p>
<ol>
<li>Use PuTTY to SSH to the management server, then double-click the <span class="packt_screen">PuTTY</span> <span>icon on the desktop</span></li>
<li>Click the <span class="packt_screen">SerengetiCLI</span> <span>session and then click</span> <span class="packt_screen">Open</span></li>
<li><span>Run the following commands:</span><br/></li>
</ol>
<pre style="padding-left: 60px"># navigating to the scripts directory<br/><br/><strong>cd /opt/serengeti/sbin</strong><br/><strong>[root@10 sbin]# ./config-dist</strong><br/><strong>ro.rb \</strong><br/><strong>&gt; --name qumulusqahdp \</strong><br/><strong>&gt; --vendor HDP \</strong><br/><strong>&gt; --version 2.x.x \</strong><br/><strong>&gt; --repos http://puppet2.qvcdev.qvc.net/hdp/2/hortonworks-hdp2.repo</strong></pre>
<p style="padding-left: 60px">The <kbd>qumulushdp</kbd> distro is added into <kbd>/opt/ erengeti/www/distros/manifest</kbd> successfully.</p>
<p style="padding-left: 60px">The old manifest is backed up to <kbd>/opt/ serengeti/www/distros/manifest.bak</kbd>.</p>
<pre style="padding-left: 60px"><strong>[root@10 sbin]# sudo service tomcat restart</strong></pre>
<ol start="4">
<li>Log into the vCenter Web Client and click on <span class="packt_screen">Big Data Extension</span> from the tree on the left</li>
<li>Click on <span class="packt_screen">Big Data Clusters</span> and then click the icon to add a new cluster (a green +)</li>
</ol>
<p>We can see the name of the new HDP distro now in the Hadoop distribution. Keep in mind that the name will match the parameter specified when we ran <kbd>./config-distro.rb</kbd> (<kbd>pumulushdp</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Hadoop plugin in vRO</h1>
                </header>
            
            <article>
                
<p>We can now see how vRO integrates the BDE plugin and runs the workflow:</p>
<ol>
<li>Log into the vRO configuration page at <kbd>https://xx.xxx.x.xx:8283/config_general/General.action</kbd>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/41961ed7-a408-4250-a7e4-d932ad0da7b5.png" style="width:37.17em;height:18.08em;"/></p>
<ol start="2">
<li class="mce-root">Click on the <span class="packt_screen">Plug-ins</span> tab on the left:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/39a36f0d-fcd9-413e-8fbb-f71197e8c5e3.png" style="width:20.08em;height:11.00em;"/></p>
<ol start="3">
<li class="mce-root">Scroll toward the bottom and click the magnifying glass. Look for the magnifying glass and select the required plugin file:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fd9733ed-f90c-4b6a-a16d-2eec203608ab.png" style="width:20.25em;height:10.92em;"/></p>
<ol start="4">
<li class="mce-root">Click the <span class="packt_screen">Upload and install</span> button.</li>
<li class="mce-root">Accept the license agreement.</li>
</ol>
<p style="padding-left: 60px"><span>This is the <span class="packt_screen">VMware vRealize Orchestrator</span> console through which we can manage tasks:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c54b84cf-5511-4560-8b72-cd52da709eb9.png" style="width:32.33em;height:30.83em;"/></p>
<ol start="6">
<li>Click <span class="packt_screen">Startup Options</span> to restart the vRO service and restart the vRO configuration server.</li>
<li>Log into the vRO<span class="packt_screen"> </span>client, and, under <span class="packt_screen">Run</span>, select <span class="packt_screen">Workflows</span>.</li>
<li>Under <span class="packt_screen">Library</span>, you should see <span class="packt_screen">Hadoop Cluster As A Service</span>.</li>
<li>Expand <span class="packt_screen">Hadoop Cluster As A Service</span> and then expand <span class="packt_screen">Configuration</span>. Right-click <span class="packt_screen">Configure</span> T<span class="packt_screen">he Serengeti Host</span> and click <span class="packt_screen">Start Workflow</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb006753-f8a1-48e3-a100-9e160e1198ba.png" style="width:23.00em;height:23.67em;"/></p>
<ol start="10">
<li>Type in the URL for the Serengeti Management Server as <kbd>https://xx.xxx.x.xx:8443/serengeti</kbd>.</li>
<li>Enter the username for an administrative user on vCenter in UPN format (for example, <kbd>user@domain.com</kbd>). Enter the password for the administrative user and click <span class="packt_screen">Submit</span>:
<ul>
<li><span class="packt_screen">Connection url for a Serengeti host</span>: For example, <kbd><kbd>https://10103.3.18:8443/serengeti</kbd></kbd></li>
<li><span class="packt_screen">vCenter Server username</span>: For example, <kbd>vrasvcqa@qvcdev.qvc.net</kbd>:</li>
</ul>
</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2b1f469a-2c88-4416-b023-306587c51eeb.png" style="width:34.75em;height:19.08em;"/></p>
<ol start="12">
<li>We get a question about importing the certificate. On the last page of the form, select <span class="packt_screen">Install certificate…</span> from the dropdown:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ca86e5e8-7f67-4f63-be1a-28aab0a7db43.png" style="width:42.75em;height:18.08em;"/></p>
<ol start="13">
<li>Click <span class="packt_screen">Next</span> and then click <span class="packt_screen">Submit</span>:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6ac7d6cc-7075-4a14-a7f4-c33323bdefdc.png" style="width:37.17em;height:15.17em;"/></p>
<p><span>The Serengeti host is now fully configured.</span></p>
<p>We can provision the cluster using VRO as the workflow "<span class="packt_screen">Configure The Serengeti Host "</span> has rest host connection and operation timeout value hardcoded as 30.</p>
<p><span>The following screenshot shows the</span> <span class="packt_screen">Workflow</span> c<span>reation settings; the user can create different workflows as per their requirements:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b3761d3-385c-41f4-a371-ca37d145ef63.png" style="width:36.17em;height:29.00em;"/></p>
<p>We have to select the network or datastore resource option in BDE Cluster Blueprint in vRA. There should be a drop-down option to select a certain BDE Resource on the Web Client side. This needs to be customized on the vRA blueprint forms. Configure the Serengeti host to add a timeout value for the connection and operation. We also have an option to select BDE cluster sizes (<span class="packt_screen">Small</span>, <span class="packt_screen">Medium</span>, <span class="packt_screen">Large</span>) from the vSphere web console. This needs to be customized on the blueprint side from vRA Blueprint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Open source software</h1>
                </header>
            
            <article>
                
<p>Organizations need great skill sets to accept open source compared to traditional proprietary solutions as there's a big difference between building a solution from scratch with all integrated support and utilizing tried-and-tested vendor solutions. For many enterprises, these challenges are too daunting, and erode the value of the open source choice for them. Business strategies, investments, and many other factors come into play. In these situations, enterprises find that a commercially-supported distribution of open source solutions, or a proprietary solution, better supports their strategy. Customers build digital and online selling channels as a pillar of their go to market strategy, develop their own proprietary implementation of OpenStack aligned to the unique demands of their business use cases.</p>
<p><span><span>Customers have </span></span>invested in the time, talent, and resources to refine OpenStack to meet their specific needs. A major sports retailer had chosen open source based solution rather then to implement a commercial distribution of OpenStack .VMware Integrated OpenStack help customers to save their time and resources, investing their technical talent in refining the outbound customer-facing portions of their strategy. Open source is undeniably a strategic part of every company's software portfolio today. While open source software has its strong suits, being production-ready is not one of its top attributes. There's still a lot of work to do: getting that code to meet the standards of a commercial, sold product is not an insignificant investment, and requires specialized skills.</p>
<p>From selection to testing, integration, and security, some assembly is required. For most enterprises, that's not an investment they want to make; they are better served investing in their core competence, not in becoming an expert in a single implementation of an open source project. That's where commercial providers, such as VMware, step in to provide the pragmatic, practical open-source-based software that enterprises can rely on.</p>
<p><strong>Open vSwitch</strong> (<strong>OVS</strong>) is another example of VMware's contributions. The code was transferred to the Linux Foundation collaborative projects for ongoing community support with VMware, and continues to play an active role as VMware engineers are responsible for as much as 70% of the active commits to OVS. These contributions are considered personal, and community support across the industry continues to grow. VMware is making strategic investments in the IoT space with EdgeX and <strong>n</strong><span><strong>etwork functions virtualization</strong> (</span><strong>NFV</strong>) with <span><span>expertise</span></span> in the <strong>o</strong><span><strong>pen network automation platform</strong> (</span><strong>ONAP</strong>). </p>
<p><strong>Clarity</strong> is an excellent example of creating software internally and choosing to open source it to benefit a broader community. Clarity is a UX/UI design framework as it helps both developers and designers with the visual aspects of applications. Clarity was developed <span><span>internally in VMware</span></span> to meet the UI/UX needs of products but it's not contingent upon VMware products to work or to deliver value. It can be applied and used in nearly any environment, so the choice was made to open source it. Clarity has taken off as it has an active community, has been downloaded more than 100,000 times, and has nearly 1,000,000 views on its homepage. Our open source projects also include tools and kits that help a developer to be more efficient.</p>
<p>C<span>hallenge handshake authentication protocol (</span>CHAP<span>) </span> is a tool that analyzes un-instrumented ELF core files for leaks, memory growth, and corruption.</p>
<p>VMware products are based in open source, which we support and contribute to but we are not an open source software company. VMware software, whether propriety or based on open source, is production-ready: it is fully supported, fully tested, and optimized—<span>it's</span> secure and ready to deploy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considering solutions with CapEx and OpEx</h1>
                </header>
            
            <article>
                
<p>We can see with open source solution that CapEx cost reduces as license costs potentially diminish, while OpEx cost rises as<br/>
the support and skilled technical manpower required to deploy and maintain the open source solution. We see CapEx rise in popularity, which reflects the license and support contract costs for a <strong>commercial off-the-shelf</strong> (<strong>COTS</strong>) software solution, while OpEx falls as the burden for patching, upgrading, enhancing, and securing the software rests with the vendor, not the enterprise IT department.</p>
<p>This is not a 1:1 tradeoff, but something you must consider across your enterprise and it's not a short-term decision; it has long-term, structural, and strategic implications. If you are struggling to hire or retain staff, converting to an open source solution where you are relying on your intellectual property and technical prowess to make the solution work could put you in a very precarious situation. You may be held captive by consultants or outsourcing companies that promise to <em>hold your hand</em> as you approach production operations. Those costs rarely fall over time. Another option to consider is a hybrid solution: commercially-supported open source distributions or commercialized versions of open source projects. Another option to explore is the two-tiered option: some companies offer a <em>community version</em>, which is their open source project offered at no cost, and offer a second version, typically labeled <em>enterprise,</em><span> </span>which is a sold product that offers a more robust version of the software and is fully supported. We can <span><span>go with </span></span>open source to build our strategy and make the right decisions for business. Therefore, to start with the basics, we must know where and how our application developers or IT staff are leveraging open source and understand the decisions behind their choice, including the benefits as well as the gaps.</p>
<p>As our team starts to engage in open source projects, arm them with guidelines so that they feel confident in their contributions. We should have a single point of contact for questions about IP, license types and compliance, and best practices with a safety first option. If we want to create a new open source project or engage at a deeper level in an existing project, be sure to understand the strategic intent. This is a long-term commitment and requires an investment in talent and time, or our efforts will flounder as they're time-consuming, distracting, cost money, and can be annoying. We have to evaluate the choice between an open source solution and a proprietary, vendor-backed and -sold solution, as it is a strategic choice, not just a purchasing decision. We need to weigh the pros and cons of CapEx and OpEx, and carefully evaluate our long-term commitment and ability to hire staff. We can discuss to understand the costs and benefits along with the technology curve.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Benefits of virtualizing Hadoop</h1>
                </header>
            
            <article>
                
<p>The benefits of virtualizing Hadoop are as follows:</p>
<ul>
<li><strong>On-demand provisioning</strong>: Automate the cluster-deployment process as per the defined policy</li>
<li><strong>Continuous availability</strong>: vSphere's built-in HA protection protects the single point of failure</li>
<li><strong>Flexibility</strong>: Resources (CPU, memory, network, and storage) can be scaled up and down on demand as per your requirements</li>
<li><strong>Multi-tenant environment</strong>: Different tenants running Hadoop can be isolated within a shared infrastructure as per security compliance</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case – security and configuration isolation</h1>
                </header>
            
            <article>
                
<p>The Hadoop authentication and authorization model is weak. Sensitive data is hard to protect. It has multiple MapReduce workloads for production batch analysis, ad hoc analysis, and experiment tasks with different SLAs for different jobs.</p>
<p>We need to take the following into consideration:</p>
<ul>
<li>Where it makes sense, HDFS is consolidated to minimize data duplication</li>
<li>High-priority jobs get more resources to ensure they are completed on time</li>
<li>Each type of job can get as many resources as possible at any time</li>
<li>Avoid CPU and memory contention so better utilize resources to get a job done on time</li>
</ul>
<p>Our objective is to integrate Hadoop workloads and other workloads by having a big data shared infrastructure. The Hadoop MapReduce framework uses HDFS as an underlying filesystem to process large sets of data and use their own storage mechanism. We also have other technologies, such as HBase and Pivotal.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Case study – automating application delivery for a major media provider</h1>
                </header>
            
            <article>
                
<p>The following are the challenges:</p>
<ul>
<li>Customer had a mandate that any application must be deployable to any number of backend infrastructures, spanning multiple private clouds</li>
<li>A specific application (with a footprint of over 10,000 servers) needed better provisioning procedures and tools to ensure the mandated goal was met</li>
<li>Customer's current provisioning model required an extensive overhaul, as spin-up time was several weeks if not months, largely consisting of manual procedures</li>
</ul>
<p>The following is the solution:</p>
<ul>
<li>Implemented an automated workflow-based approach using a version-controlled, software-defined infrastructure via the <strong>Business Process Management</strong> (<strong>BPM</strong>) platform/workflow engine and underlying Java services</li>
<li>Leveraged Puppet architecture for build processes and packaging, and bakery workflow for images</li>
<li>Provided insight into the operations via a Ruby-based console and reporting UX</li>
</ul>
<ul>
<li>Integrating Jira into the provisioning workflow will make delivery ease-of-use from familiar tools</li>
<li>Provisioned application servers and requisite number of Memcached and related instances</li>
<li>The new system validates a newly-provisioned infrastructure, provides an automated cleanup of any failures, and automatically switches routing rules to serve up the new infrastructure</li>
<li>Provided customer with tools and patterns necessary for repeatable operation</li>
<li>Created better build procedures and processes, which yield more stable infrastructure changes</li>
<li>Deployment time for the target infrastructure was decreased from weeks/months to 90 minutes for 270 servers</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Hadoop is still quite new for many enterprises, and different companies are at different stages in their Hadoop adoption journey. Having worked with several customers on this, it's evident that, depending on the stage the customer is at, there are distinct Hadoop use cases and requirements. Virtualization can help to address all the key<span> </span>requirements of each stages. Multiple Hadoop clusters can be used by different departments within a company.</p>
<p>It is difficult to manage multiple clusters for different departments in a company and keep them all running well. We have multiple use cases running data mining, recommendation engines, and for our online service, we have one shared dataset, rather than duplicated data everywhere. We are now managing a single cluster rather than multiple clusters. </p>
<p>In the next chapter, we will learn how to support cloud-native application development by providing developers with access to traditional and modern application-development frameworks and resources, including container services and open APIs, on a common vSphere platform. This enables microservice-based architectures for faster and frequent development without compromising security, reliability, or governance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>Check out the following resources for more information on the topics covered in this chapter:</p>
<ul>
<li><em>Adobe Deploys Hadoop as a Service on VMware vSphere</em> at <a href="http://www.vmware.com/files/pdf/products/vsphere/VMware-vSphere-Adobe-Deploys-HAAS-CS.pdf">http://www.vmware.com/files/pdf/products/vsphere/VMware-vSphere-Adobe-Deploys-HAAS-CS.pdf</a></li>
<li><em>Virtualizing Hadoop in Large-Scale Infrastructures, technical white paper by EMC</em> at <a href="https://community.emc.com/docs/DOC-41473">https://community.emc.com/docs/DOC-41473</a></li>
<li><em>Virtualized Hadoop Performance with VMware vSphere 6 on High-Performance Servers</em> at <a href="http://www.vmware.com/resources/techresources/10452">http://www.vmware.com/resources/techresources/10452</a></li>
<li><em>A Benchmarking Case Study of Virtualized Hadoop Performance on vSphere</em> at <a href="http://www.vmware.com/resources/techresources/10222">http://www.vmware.com/resources/techresources/10222</a></li>
<li><em>Transaction Processing Council—TPCx-HS Benchmark Results (Cloudera on VMware performance, submitted by Dell)</em> at <a href="http://www.tpc.org/tpcx-hs/results/tpcxhs_results.asp">http://www.tpc.org/tpcx-hs/results/tpcxhs_results.asp</a></li>
<li><em>ESG Lab Review: VCE vBlock/systems with EMC Isilon for Enterprise Hadoop</em> at <a href="http://www.esg-global.com/lab-reports/esg-lab-review-vce-vblock-systems-with-emc-isilon-for-enterprise-hadoop/">http://www.esg-global.com/lab-reports/esg-lab-review-vce-vblock-systems-with-emc-isilon-for-enterprise-hadoop/</a></li>
<li><em>VMware BDE Documentation site: vSphere Big Data Extensions (BDE)</em> at <a href="https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html">https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html</a></li>
<li><em>VMware vSphere Big Data Extensions—Administrator's and User's Guide and Command-line Interface User's Guide</em> at <a href="https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html">https://www.vmware.com/support/pubs/vsphere-big-data-extensions-pubs.html</a></li>
<li><em>Blog articles on BDE Version 2.1</em> at <a href="http://blogs.vmware.com/vsphere/2014/10/whats-new-vsphere-big-data-extensions-version-2-1.html">http://blogs.vmware.com/vsphere/2014/10/whats-new-vsphere-big-data-extensions-version-2-1.html</a></li>
<li><em>VMware Big Data Extensions (BDE) Community Discussion</em> at <a href="https://communities.vmware.com/message/2308400">https://communities.vmware.com/message/2308400</a></li>
<li><em>Apache Hadoop Storage Provisioning Using VMware vSphere Big Data Extensions</em> at <a href="https://www.vmware.com/files/pdf/VMware-vSphere-BDE-Storage-Provisioning.pdf">https://www.vmware.com/files/pdf/VMware-vSphere-BDE-Storage-Provisioning.pdf</a></li>
<li><em>Hadoop Virtualization Extensions</em> at <a href="http://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf">http://www.vmware.com/files/pdf/Hadoop-Virtualization-Extensions-on-VMware-vSphere-5.pdf</a></li>
<li><em>Container Orchestration on vSphere with Big Data Extensions</em> at <a href="https://labs.vmware.com/flings/big-data-extensions-for-vsphere-standard-edition">https://labs.vmware.com/flings/big-data-extensions-for-vsphere-standard-edition</a><a href="https://venturebeat.com/2015/12/06/its-actually-open-source-software-thats-eating-the-world/"/></li>
</ul>


            </article>

            
        </section>
    </body></html>