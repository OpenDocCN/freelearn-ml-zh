- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Micro-Targeting with Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces the advanced capabilities of **retrieval-augmented generation**
    (**RAG**) and its strategic application in precision marketing, building on the
    foundations laid by **zero-shot learning** (**ZSL**) and **few-shot learning**
    (**FSL**) discussed in the previous two chapters. Unlike ZSL, which operates without
    prior examples, and FSL, which relies on a minimal dataset, RAG leverages a real-time
    retrieval system to enhance generative models, enabling them to access and incorporate
    the most current and specific information available. This ability allows RAG to
    surpass the limitations of ZSL and FSL by providing personalized content tailored
    to individual consumer profiles or current market conditions – capabilities crucial
    for micro-targeting in marketing.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter will detail the operational framework of RAG, emphasizing its hybrid
    structure, which merges generative AI with dynamic information retrieval. This
    synthesis not only ensures the generation of contextually appropriate and accurate
    content but also introduces a level of personalization that was previously unattainable
    with either ZSL or FSL alone. We will explore how RAG’s real-time data retrieval
    component plays a critical role in adapting marketing strategies swiftly to align
    with consumer behavior and preferences using the consumer interaction data from
    a multi-product e-commerce platform as an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the conclusion of this chapter, you will be equipped with the knowledge
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand the integration of RAG with traditional generative models and its
    superiority in handling real-time, relevant data compared to ZSL and FSL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize the enhanced capability of RAG in micro-targeting and personalizing
    content, which can dramatically improve consumer engagement and conversion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply RAG concepts to develop cutting-edge marketing strategies that are not
    only data-driven but also highly adaptable to the nuances of consumers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RAG for precision marketing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models, particularly those developed on transformer frameworks like
    **generative pre-trained transformer** (**GPT**), have revolutionized how machines
    understand and generate human-like text. These models are trained on vast corpora
    of text data and are capable of learning complex patterns and structures of language
    that enable them to predict and generate coherent and contextually appropriate
    text sequences. However, despite their sophistication, pure generative models
    often lack the ability to incorporate real-time, specific information that isn’t
    explicitly present in their training data.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the “retrieval” component of RAG comes into play. RAG is a fusion
    of **Generative AI** (**GenAI**) with information retrieval systems, forming a
    hybrid model designed to enhance the quality and relevance of generated content.
    RAG achieves this by incorporating a dynamic retrieval component that pulls relevant
    information from an external dataset or knowledge base during the generation process.
    The retrieval system in RAG is designed to query a structured or unstructured
    database to fetch information that is relevant to the context of the generation
    task. This approach ensures that the generative model has access to the most current
    and relevant data to enhance the accuracy and relevance of the content it produces.
  prefs: []
  type: TYPE_NORMAL
- en: How RAG works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to its role in enhancing content relevance, RAG enables a bidirectional
    flow of information between generative models and external databases. As illustrated
    in*Figure 11.1*, the process of RAG can be broken down into several key steps,
    with each step discussed further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The key components of the RAG workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore these steps further:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input prompt**: In this step, the generative model is fed the user’s input
    prompt or the ongoing (fine-tuned) generation context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query generation**: Initially, the generative model produces a query based
    on the input provided in step 1\. This query is written to retrieve the most relevant
    information from the knowledge base. This bidirectional flow of information between
    the generative model and the external database fosters a symbiotic relationship,
    enriching both data utilization and model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Information retrieval**: In step 3, the query is then processed by the retrieval
    system, which searches through the database to find matches or closely related
    information. The sophistication of this system can vary, from simple keyword-based
    searches to more complex neural network models that understand semantic meanings.
    The retrieved data is then integrated into the content generation process in the
    next step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Content generation**: Armed with the retrieved data, in step 4, the generative
    model then incorporates this information into its ongoing text generation process.
    This step is crucial as it involves blending the newly retrieved data with the
    generated content to maintain coherence and flow. This content may serve as the
    basis for an initiative like a marketing campaign, as illustrated above.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative refinement**: Often, the process is iterative, with the generative
    model adjusting the queries based on the feedback loop between what has been generated
    and what needs to be generated next. This is captured in step 5, where the iterative
    refinement loop ensures the continuous adaptation and optimization of the generative
    process based on feedback such as marketing KPIs or customer feedback.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we will discuss how the query generation and retrieval steps work from
    a mathematical perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical model of RAG retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mathematical model of RAG captures the relationship between a document query
    and its retrieval components (steps 2 and 3 in *Figure 11.1*, respectively), laying
    the groundwork for its dynamic content generation process. At its core, RAG combines
    probabilistic frameworks to integrate text generation with information retrieval,
    which can be expressed through equations that quantify the probabilities involved
    in generating text given an input and retrieved documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process begins with the generative model creating a query *q* based on
    the input prompt or ongoing context. Simultaneously, the retrieval system processes
    this query to fetch relevant documents from the database. The retrieval process
    is governed by a probability model that estimates the relevance of each document
    in the database given the query *q*. This estimation incorporates factors such
    as semantic similarity and document recency, ensuring that the retrieved information
    aligns closely with the context of the generation task. Mathematically, the probability
    of selecting document *r* from the database given the query *q* can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *D* represents the set of all possible documents in the database and *score(q,r)*
    is a function that computes the relevance of document *r* to the query *q*. The
    score function is typically learned through training and can involve complex embeddings
    that capture the semantic similarity between the query and the documents. The
    denominator in the equation ensures that the probability of selecting any document
    from the database sums to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how the model works, let’s discuss the critical importance
    of the external data used in RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of data in RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'External data acts as the cornerstone of the RAG framework, influencing both
    the quality of the content generated and the accuracy of the information retrieved.
    In marketing, where precision and relevance directly correlate with consumer engagement
    and conversion, the proper application and management of data within a RAG system
    becomes critical. There are two fundamental components embedded within the retrieval
    framework that warrant deeper exploration and should be prioritized due to their
    profound impact on the resulting outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data freshness**: Keeping data up to date allows the model to capitalize
    on current trends, events, and behaviors, enhancing consumer engagement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data specificity**: By incorporating detailed consumer data, content can
    be precisely tailored to individual preferences and behaviors. This specificity
    not only increases the relevance of marketing messages but also can significantly
    boost conversion rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s look at these concepts in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data freshness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In practical terms, data freshness means the system consistently accesses and
    prioritizes the most recent information available. This is particularly significant
    in marketing, where information can quickly become outdated due to rapidly changing
    market conditions or consumer preferences. For example, during a major sales event
    like Black Friday, having the latest data allows RAG systems to produce content
    that highlights current promotions, available stock levels, or last-minute deals,
    greatly enhancing the effectiveness of marketing campaigns. Unlike previously
    discussed GenAI approaches such as ZSL and FSL, which primarily rely on pre-existing
    datasets and may not update their knowledge base in real time, RAG systems integrate
    a dynamic retrieval mechanism that actively fetches and utilizes the most current
    data available.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure content remains relevant, RAG systems can adjust their retrieval
    processes to favor newer documents. Mathematically, this adjustment can be represented
    by modifying the relevance score to include a term that increases the weight of
    more recent documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B30999_11_003.png) is a parameter that determines the impact of
    a document’s recency on its score. By prioritizing recent data, RAG systems can
    respond more dynamically to the latest trends, ensuring that the content they
    generate resonates with current consumer behaviors and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: Data specificity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data specificity refers to how detailed and relevant the information is in relation
    to a specific query. In RAG systems, high data specificity ensures that the content
    retrieved and generated is directly aligned with the user’s current needs or questions,
    therefore enhancing user engagement and satisfaction. While other methods like
    transfer learning also allow access to detailed datasets, RAG integrates real-time
    data retrieval with generative capabilities, making it particularly well-suited
    for applications where up-to-the-minute information is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 'In technical terms, RAG systems can use advanced semantic matching algorithms
    to ensure that the content they retrieve matches the user’s query not just by
    keywords but in overall meaning and context. This approach involves adjusting
    the scoring mechanism to prioritize documents that are not only relevant but also
    contextually specific to the user’s inquiry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_004.png)'
  prefs: []
  type: TYPE_IMG
- en: For example, consider a user searching for “best skincare for sensitive skin”.
    A RAG system with high data specificity would be able to pull and generate content
    that not only mentions skincare products but specifically addresses products designed
    for sensitive skin, potentially including user reviews, expert advice, and the
    latest product innovations. This level of detail in content generation can significantly
    improve the effectiveness of personalized marketing campaigns, enhancing customer
    conversion rates and building brand loyalty.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the retrieval index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **retrieval index** is a fundamental component of any RAG system, acting
    as the bedrock on which the retrieval functionality operates. It ensures that
    the queries processed by the system are matched with accurate and contextually
    relevant responses. Managing this index effectively is crucial for the system’s
    performance and can involve several key strategies. Let’s look at some management
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Indexing strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Effective indexing is vital for the swift and accurate retrieval of information.
    The backbone of a robust RAG system lies in its ability to quickly sift through
    vast amounts of data and find information that best matches the user’s query.
    This is achieved through sophisticated indexing strategies that organize data
    in a way that optimizes search processes, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inverted indices**: These are used to store a mapping from content keywords
    to their locations in the database. For example, in an e-commerce setting, an
    inverted index might link terms like “wireless headphones” or “thermal jacket”
    directly to the relevant product listings. This allows the system to quickly gather
    all relevant documents during a query, enhancing search efficiency and response
    speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector space models**: This approach involves converting text into vector
    forms or embeddings that are easy to compare and analyze. Using algorithms like
    TF-IDF or neural network-based embeddings, such as those introduced in *Chapters
    5*, *9*, and *10*, these models help in understanding and comparing the semantic
    similarity between the user’s query and available documents, leading to more nuanced
    and contextually appropriate responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-based indexing**: This approach is useful for applications like social
    media analytics where understanding complex user relationships enhances content
    targeting. More generally, this approach is valuable when relationships between
    data points are as important as the data itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-dimensional indexing**: This technique is beneficial in cases such
    as geographic data applications such as real estate analysis where efficient searches
    across multiple attributes like location and time are needed. This method is particularly
    valuable for queries that involve ranges or require simultaneous consideration
    of several dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the appropriate indexing strategy depends on the application’s data
    needs and query complexity. Inverted indices are optimized for quick keyword lookups,
    ideal for environments where speed is crucial. Vector space models offer richer
    semantic analysis and are better suited for contexts requiring a deeper understanding
    of content. For handling more complex data relationships or multiple query dimensions,
    graph-based and multi-dimensional indexing provide valuable solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Data curation and updating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To maintain its efficacy, the retrieval index must be regularly updated and
    curated. The following are a couple of concepts to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic updating**: As new information becomes available or old information
    becomes obsolete, the index needs to be updated to reflect these changes. This
    ensures that the RAG system remains effective and relevant over time. For instance,
    in a marketing campaign, promotional content might need frequent updates to reflect
    the most current offers by the company so that the RAG system can adjust its responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated monitoring**: Implementing automated systems to continuously monitor
    and update indices can greatly enhance a RAG system’s responsiveness to changes.
    In digital marketing, this could include adjusting strategies based on new consumer
    trend analyses to make sure the marketing content is aligned with the economic
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proper data updating and curation are key for keeping RAG systems useful, especially
    in marketing, given that these systems currently do not include advanced data
    management features and often need separate practices to manage data curation
    and updates.
  prefs: []
  type: TYPE_NORMAL
- en: RAG implementation challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several challenges in RAG implementation that are worth highlighting.
    As discussed in the previous section, ensuring the quality and relevance of the
    underlying external database is crucial, and often separate data engineering practices
    are needed to achieve this. The following are a couple of other major issues that
    RAG systems can face:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling long documents**: RAG systems need to break retrieval documents
    into smaller chunks to avoid memory issues and slow response times. However, breaking
    longer documents into smaller chunks can lead to a loss of context. To address
    this, solutions such as summarization algorithms and context-aware chunking methods
    can be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieving relevant data from multiple sources**: Combining data from various
    sources can be challenging due to inconsistencies and differences in data formats.
    For instance, if customer data comes from both chat history and tabular purchase
    data, the system might face issues linking the two sources and generate marketing
    messages based on inconsistent information. Techniques such as multi-modal AI
    models (see *Chapter 12*) and knowledge graphs can help address this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these challenges, major organizations such as Meta AI Research, Google
    AI, and Microsoft have successfully implemented RAG systems. For instance, Google
    has integrated RAG to enhance the relevance of its search results and Microsoft
    integrates RAG within Azure AI services to improve virtual assistants’ capabilities.
    RAG is also becoming increasingly accessible to smaller organizations through
    services that offer to curate their existing enterprise data into formats that
    are compatible with RAG applications.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of RAG in marketing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG systems that uniquely combine real-time data retrieval with advanced content
    generation offer a number of transformative applications in marketing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that these systems enable marketers to craft personalized and contextually
    aware content by integrating the most relevant data into the content creation
    process, there are some pivotal areas where RAG systems can significantly enhance
    marketing strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dynamic personalized content creation**: RAG systems excel in generating
    advertising content that adapts in real time to changes in user interactions,
    browsing behaviors, or purchase histories. By accessing data specific to an individual’s
    recent activities, RAG can tailor advertisements to match personal preferences
    and interests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Imagine a user recently explored camping gear online. A RAG system
    could use this data to dynamically create ads for related items like hiking boots
    or travel guides. This targeted advertising not only increases the relevance and
    appeal of the ads but also boosts engagement rates and potential conversions.
    We will discuss other examples of this in the hands-on exercise presented later
    in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Segment-specific content generation**: RAG enables marketers to produce content
    finely targeted to specific demographic segments, enriched with the latest data
    relevant to these groups. This strategy ensures the content deeply resonates with
    its intended audience, therefore enhancing reader engagement and brand loyalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: A RAG system might generate blog posts for first-time home buyers
    that include up-to-date mortgage rates and real-time housing market trends. This
    not only provides valuable information but also establishes the brand as a credible
    resource in the home-buying journey.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced customer engagement:** By harnessing the power of real-time data
    retrieval and advanced content generation, these systems enable the production
    of highly personalized content that resonates deeply with individual customers.
    Whether through personalized emails, targeted social media posts, or bespoke newsletters,
    RAG systems ensure that all communications are timely, relevant, and tailored
    to each customer’s unique profile.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: For a customer who recently celebrated a significant event like
    an anniversary, RAG can generate personalized greetings or offers that reflect
    the customer’s preferences and past interactions with the brand. This could include
    curated content, special promotions, or personalized messages from brand ambassadors,
    creating a highly personalized and memorable customer experience.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated response generation for customers**: RAG enhances customer support
    by generating informative and contextually relevant responses to inquiries. By
    pulling information from FAQs, product manuals, or customer databases, RAG systems
    deliver precise, customized answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: When a customer inquires about the return policy for an online
    purchase, RAG can generate a response that includes details tailored to the item’s
    category or the date of purchase. This application of RAG in customer support
    not only improves response times but also boosts overall customer satisfaction.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the theoretical knowledge in place, let’s move on to the application
    and get familiar with the process of building a knowledge retrieval system.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you would like to explore RAG further, here are a couple of valuable resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* ([https://arxiv.org/pdf/2005.11401](https://arxiv.org/pdf/2005.11401))
    is a seminal paper by researchers from Meta AI that explores the integration of
    retrieval and generation to enhance language models for knowledge-intensive tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Searching for Best Practices in Retrieval-Augmented Generation* ([https://arxiv.org/abs/2407.01219](https://arxiv.org/abs/2407.01219))
    is a more recent review/survey of the literature covering key advancements and
    current challenges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a knowledge retrieval system for marketing with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will explore the practical aspects of setting up a knowledge retrieval
    system specifically tailored for marketing purposes using LangChain, a library
    designed to facilitate the integration of language models with retrieval systems.
    We will do this via an example that readers can follow to construct their own
    retrieval systems for enhancing the capabilities of generative AI in their own
    marketing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain is a framework that significantly enhances the capabilities of language
    models by integrating them with retrieval systems. Its flexible design allows
    it to support a variety of backend databases and customizable retrieval strategies
    using modules like `langchain-retrieval` and `langchain-storage`.
  prefs: []
  type: TYPE_NORMAL
- en: '**LangChain essentials**'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain facilitates the combination of language models with robust retrieval
    systems. This integration captures the principles of RAG, enabling applications
    to generate content that is not only human-like but also highly relevant and informed
    by the latest data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further documentation and resources can be found on LangChain’s official documentation
    page: [https://python.langchain.com/docs/get_started/introduction/](https://python.langchain.com/docs/get_started/introduction/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain stands out as a top choice for Python developers looking to integrate
    advanced AI capabilities into their marketing strategies. While other tools such
    as LlamaIndex specialize in data indexing and retrieval for quick access to large
    datasets, LangChain offers a more versatile platform for creating complex NLP
    applications. Some of the key features making LangChain ideal for Python developers
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ease of integration**: LangChain simplifies the incorporation of complex
    AI models with retrieval databases through high-level abstraction, allowing you
    to focus more on creating unique application logic rather than wrestling with
    backend complexities, speeding up development and deployment processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modularity**: The framework’s high modularity supports a diverse range of
    language models and retrieval databases, alongside custom logic for specific interactions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: LangChain is designed to scale effortlessly from handling
    small tasks to managing large-scale applications, such as real-time personalized
    advertising and extensive email marketing campaigns. This scalability ensures
    that as marketing strategies grow and evolve, their technological infrastructure
    can grow without needing a complete overhaul.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will dive deeper into setting up LangChain, designing
    a retrieval model, and integrating this system with generative models to demonstrate
    its application in real-world marketing scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the external dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset that we’ll be using as the source of external data for our retrieval
    model comes from a large multi-category e-commerce platform and captures user
    interactions over the course of one month (December 2019). It includes different
    types of events related to product interactions by users, such as viewing, adding
    to the cart, removing from the cart, and purchasing.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this as a basis for our setup process of a knowledge retrieval system
    with LangChain as it will set the stage for the hands-on example we will walk
    through in the final part of the chapter, where we will be using the user interactions
    from this same database for the purpose of RAG micro-targeting of consumers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Source code and data**: [https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.11](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.11)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data source**: [https://www.kaggle.com/code/annettecatherinepaul/ecommerce-analysis-and-recommendations/input](https://www.kaggle.com/code/annettecatherinepaul/ecommerce-analysis-and-recommendations/input)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before incorporating this dataset into our retrieval model, it’s important
    to thoroughly understand its characteristics to ensure effective data ingestion
    and utilization in the retrieval system. A preliminary examination of the data
    can tailor the retrieval architecture and refine the indexing strategy, facilitating
    optimized query responses. To start, let’s examine the first few entries to understand
    the type of data each column contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: First three rows from the Kaggle e-commerce user interactions
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset features columns such as `event_time`, which marks when an event
    occurred, `event_type`, indicating the nature of the interaction (viewing, purchasing,
    or adding or removing from the cart), and other identifiers such as `product_id`,
    `category_id`, `brand`, and `price`. Taken together, this will help us understand
    the user’s journey through the e-commerce platform, including their actions, the
    products involved, and the pricing dynamics during their sessions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To deepen our understanding, we can perform basic statistical analyses that
    highlight the distribution and range of key data points by running the `describe`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the dataset comprises over 3.5 million events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Key statistics for numeric columns in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The product prices range significantly from as low as -$79.37 (possibly indicating
    returns or pricing errors) to a high of $327.80, with a mean of around $8.87\.
    Such variability suggests diverse consumer behavior and purchasing power, which
    are crucial for segmenting and targeting marketing campaigns.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also obtain a deeper understanding of two more key columns that are
    not included in the above summary, `event_type` and `brand`, which help us identify
    consumer preferences and buying patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Counts of event_type values in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This first output shows that views are the most common `event_type` (approximately
    1.73 million instances), followed by additions to the cart, removals from the
    cart, and purchases. The substantial drop from cart interactions to actual purchases
    (over 700,000 fewer purchases than cart additions) is a point of interest for
    improving conversion rates. More generally, the reduction of counts from views
    to cart interactions to overall purchases highlights a typical e-commerce sales
    funnel.
  prefs: []
  type: TYPE_NORMAL
- en: 'By running our second command of `value_counts` on the `brand` column, we can
    also see which brands appear most commonly throughout the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Counts of brand values in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also important to understand the null values in the data. This gives
    us a way to understand both the data quality and key information around missing
    values that will need to be accounted for when indexing data for our retrieval
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result indicates significant gaps in `category_code` (almost
    3.5 million missing values) and `brand` (over 1.5 million missing values) data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Counts of missing values across columns in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: This could hinder our ability to perform detailed product category analysis
    and brand-specific marketing using this dataset. The minor missing count in `user_session`
    (`779`) suggests nearly complete tracking of user sessions, which is most vital
    for our goal of analyzing user behavior throughout their sessions.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the retrieval model with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we move forward in constructing our knowledge retrieval system using LangChain,
    the next step involves the effective indexing of our e-commerce dataset. Indexing
    is a foundational process that enables the efficient retrieval of data. This section
    outlines the setup of an indexing system to support the queries that will allow
    us to achieve our goal of micro-targeting based on their shopping behaviors. To
    implement our retrieval system, we will utilize Elasticsearch for our backend
    system due to its robust full-text search capabilities and suitability for handling
    large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Install and connect to Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before creating the index, we need to ensure that Elasticsearch is installed
    and running on our system. Follow these steps based on your operating system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps to be followed by Windows users:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download**: Visit the Elasticsearch official download page ([https://www.elastic.co/downloads/elasticsearch](https://www.elastic.co/downloads/elasticsearch))
    and download the latest version for Windows.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Install**: Extract the downloaded ZIP file to your desired location and navigate
    to the extracted folder.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Start**: Open Command Prompt as `Administrator`, change directory to where
    Elasticsearch is installed (e.g., `cd C:\path\to\elasticsearch-<version>\bin`),
    and execute `elasticsearch.bat` to start Elasticsearch. Substitute `<version>`
    with the specific version of Elasticsearch that was downloaded from their official
    download page.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'macOS/Linux users can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download**: Use the below command in a terminal window, replacing `<version>`
    with the latest version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Install**: Extract the file with the below command and navigate to the newly
    created directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Start**: Open a terminal window and change to the `bin` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start Elasticsearch by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have an ARM-powered MacBook, you may need to install via Homebrew instead.
    Instructions for this, as well as alternative ways of installing Elasticsearch,
    can be found on their website: [https://www.elastic.co/guide/en/elasticsearch/reference/7.17/install-elasticsearch.html](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/install-elasticsearch.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'After starting Elasticsearch, verify that it is running by using the following
    `curl` command in your terminal or command prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see a JSON response from Elasticsearch indicating that it is running
    correctly, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'With Elasticsearch running, you can now connect from Python using the Elasticsearch
    client. Here’s how to set it up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Indexing data in Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we can define the data schema and index data from the dataset. Given
    the diversity of data types and volume of data captured within the dataset, it
    is important to configure our indexing mappings to efficiently handle the range
    and type of data present. This mapping for our index is informed by our earlier
    exploratory analysis that we performed on this dataset, where we discovered that
    our dataset consists of over 3.5 million entries with data types ranging from
    integers and floating-point numbers to categorical and textual data. This diversity
    necessitates a robust indexing strategy to ensure efficient data retrieval and
    query performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Some key aspects of the decisions that were made in defining the above mapping
    types are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numeric and identifier fields**: `product_id`, `category_id`, and `user_id`
    are all stored as integers with significant ranges. Given their use as identifiers,
    they are mapped to Elasticsearch’s integer and long data types, which are optimized
    for numeric operations and comparisons. `price`, a floating-point field, shows
    a wide range of values, from negative (likely indicating refunds) to over 300\.
    This field is mapped as a float to accurately represent and query price data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical and textual fields**: `event_type` and `user_session` are categorical
    fields with high cardinality. These are mapped as keyword types, which support
    the exact matching necessary for filtering and aggregations. `brand` and `category_code`,
    however, contain textual data that might be used for full-text search as well
    as for exact matches. These fields are defined with the text type and a keyword
    multi-field to allow both full-text searching and aggregations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date fields**: `event_time` represents timestamps and is mapped as a date.
    This allows for time-based queries, which are essential for analyzing trends over
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we will create the index based on the mapping defined above using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon execution, this should result in a response such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With these configurations, your Elasticsearch environment is now fully equipped
    for indexing and querying our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingestion into Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ingesting the dataset into the Elasticsearch index is the step that involves
    transferring data from our DataFrame into Elasticsearch. This process uses an
    efficient bulk indexing method, which is essential for handling large volumes
    of data effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from the dataset load step, we include handling of the inconsistent
    and missing values that we identified earlier and replace these with `None` types
    to avoid errors in the indexing process. Depending on your device hardware, the
    indexing process may take some time, ranging from minutes to hours, given the
    dataset size. You can track this progress using the `tqdm` progress bar.
  prefs: []
  type: TYPE_NORMAL
- en: 'To further optimize the indexing performance, we include the `chunk_size` parameter
    in the bulk indexing method. This parameter controls the number of documents processed
    in each bulk request and adjusting its value can impact the speed of indexing
    and memory usage. In addition to `chunk_size`, we also suggest truncating the
    data before indexing if you still experience memory issues. To truncate the data,
    you can use a `pandas` function such as `df = df.head(n)` to limit the DataFrame
    to the first `n` rows. Let’s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon completion of the indexing process, you should see an output similar to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Integrating with LangChain using GPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Integrating LangChain with your Elasticsearch backend involves setting up the
    environment so that LangChain can use an LLM to dynamically generate content based
    on the data retrieved from Elasticsearch. We will demonstrate this using the gpt-3.5-turbo
    LLM, but you should refer to the latest LangChain documentation for the latest
    models available ([https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step involves initializing the GPT model as a LangChain model. You
    may also need to generate an OpenAI API key for the model if you haven’t done
    this step already, a process that currently includes the creation of an OpenAI
    account. Further details on the instructions for this can be found on the OpenAI
    website ([https://platform.openai.com/docs/introduction](https://platform.openai.com/docs/introduction)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the model is set up, the next step involves constructing queries to fetch
    relevant data from Elasticsearch. This data serves as the input for GPT, allowing
    it to generate contextually relevant content based on user behavior.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For example, here is how you can define a function to retrieve data based on
    any matching query from Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This function `retrieve_data_from_es` takes a dictionary representing the Elasticsearch
    query and returns a list of documents that match this query. The example provided
    fetches records associated with a specific user ID, allowing for the generation
    of personalized content based on the user’s previous interactions, such as products
    viewed or added to the cart. For example, to grab content related to `user_id`
    `576802932`, we can execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `data` response retrieved from Elasticsearch includes detailed records
    of the user’s activities, each of which is tagged with precise timestamps, product
    identifiers, category details, and session information. We can see examples of
    these types of interactions via the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: example entries for remove_from_cart and view event types from
    the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: In these examples, one entry shows a user removing an item from the cart around
    midnight, suggesting reconsideration or a preference change. The second entry
    captures a view event, where a user is browsing products but has not yet made
    a purchase decision. Understanding these interactions can help in designing personalized
    re-engagement strategies to encourage users to complete their purchases and enhance
    conversion rates.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have designed, implemented, and tested the framework for our knowledge
    retrieval model, let’s explore the application of this model via an example.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing RAG for micro-targeting based on customer data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having thoroughly analyzed the dataset and constructed a robust retrieval system,
    we now transition from theoretical frameworks to practical implementation. In
    this section, we will learn how to apply RAG to dynamically address common challenges
    in digital marketing, such as outdated information in trained models and capturing
    recent user interactions. Traditional **zero-shot learning** (**ZSL**) and **few-shot
    learning** (**FSL**) models, while powerful, often lag in real-time responsiveness
    and rely on pre-existing data, limiting their effectiveness in such a fast-paced
    marketing scenario.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, we will utilize RAG to generate marketing content
    that is not only up to date but also deeply relevant to current consumer behaviors.
    By integrating our retrieval system with GPT, we can pull the latest user interaction
    data directly from our database. With RAG, we can also generate real-time content
    tailored to individual user scenarios, effectively re-engaging customers who have
    abandoned their carts. This approach allows us to generate marketing strategies
    that are informed by the most recent and relevant user activities, something that
    static models cannot achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Determining the campaign strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we transition from the theoretical frameworks discussed earlier, it’s important
    to understand the strategic underpinnings of the upcoming example where we demonstrate
    how RAG can be applied. Before diving into the real-world examples, we will set
    the stage for the basis of our micro-targeting strategies by performing a couple
    more crucial elements of exploratory analysis on the dataset in order to determine
    our optimal campaign strategy and its goals.
  prefs: []
  type: TYPE_NORMAL
- en: Message timing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Efficiency in digital marketing not only reduces costs but also enhances the
    effectiveness of campaigns. One strategic component for our campaign will be the
    optimization of the timing for our marketing messages. As discussed in *Chapter
    4*, understanding the time-series trends in user interactions can be particularly
    insightful for this. Rather than performing a date-based analysis, since we only
    have access to one month of data, we will instead examine how interactions vary
    by time of day so that we can use this information to give us insight into the
    optimal time of day for user engagement for the following months. We can extract
    the insights needed to make this decision via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: Number of user interactions by event type across different hours
    of the day'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the plotted data, we observe a few distinct user behavior patterns throughout
    the day that have important implications for the timing of our micro-targeting
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Peak viewing hours**: The highest user activity, specifically views, occurs
    between `17:00` and `20:00`. This peak period is optimal for pushing advertisements
    and personalized content to maximize exposure and engagement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**High transaction activities**: Cart additions and purchases are also generally
    high from `17:00` to `20:00`. This indicates not only active browsing but also
    a higher propensity to finalize purchases, making it an ideal time for promotional
    offers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Cart removal insights**:Cart removals closely mirror the two preceding trends
    and peak between `18:00` and `20:00`, suggesting a reconsideration phase among
    users. This period could be strategically targeted with reminders or incentives
    such as discount offers or free shipping to convert hesitations into sales.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Early morning and late night trends**: While there’s a gradual increase in
    activity from the morning, it sharply declines after `21:00`, indicating late
    night hours might not be as effective for targeted campaigns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given these patterns, a targeted strategy can be specifically implemented during
    the `19:00` to `21:00` window to mitigate cart abandonment rates. During this
    time, deploying personalized re-engagement campaigns such as sending reminder
    emails, offering limited-time discount codes, or displaying pop-up messages with
    special offers can be particularly effective.
  prefs: []
  type: TYPE_NORMAL
- en: For your own analysis, it is important to consider that behavior patterns will
    differ when analyzing user interactions across different time zones. It is therefore
    important to confirm that user time zones are accurately recorded and stored in
    your database, or that time zone differences are accounted for after the fact
    based on the region of the user. Failing to do so could result in analysis that
    involves a convolution of user behaviors across regions, leading to poor message
    timing.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the brand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we continue to refine our marketing strategy, it’s crucial to identify which
    brands within our dataset present the highest potential for increasing conversion
    rates. In previous analyses, we explored the frequency of brand appearances to
    gauge their prevalence. Let’s now go deeper by examining the top five brands to
    understand how different types of interactions vary across these brands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.9: Number of user interactions by event type for the five most popular
    brands'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the brand interactions, `bpw.style` stands out as a candidate for
    focused improvement, especially in terms of cart abandonment metrics. While `bpw.style`
    shows a substantial presence in the dataset, there’s a noticeable discrepancy
    between the number of items added to carts (`21995`) and those removed (`18014`).
    This pattern suggests a significant gap between initial interest and final purchasing
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify the opportunity for improvement, let’s calculate the cart abandonment
    rate for the `bpw.style` brand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Addressing the gap in cart abandonment could significantly boost `bpw.style`'s
    performance by converting potential sales into actual purchases.
  prefs: []
  type: TYPE_NORMAL
- en: Using LangChain for micro-targeting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier, we established a retrieval system integrated with GPT to dynamically
    generate content based on the data from Elasticsearch. We will now utilize the
    marketing campaign goals that we just defined to leverage the insights gained
    from our analysis of user behavior by brand and timing to create highly personalized
    content aimed at enhancing customer engagement. We will start with two examples
    that are user-specific, before transitioning to a third example that is targeted
    specifically toward consumers of the `bpw.style` brand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study 1: Targeted product discounts'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given that our data indicates that user interactions, especially cart additions
    and removals, peak roughly between `17:00` and `21:00`, we will use this timeframe
    as the optimal window for sending out marketing messages, capturing when users
    are most active and likely reconsidering their purchase decisions. The preferred
    medium for this outreach will be email, which allows for rich, personalized content
    and can effectively re-engage users by reminding them of items left in their carts
    or providing time-sensitive offers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate tailored marketing content, we deploy a function that constructs
    prompts for GPT based on user interactions, transforming these data points into
    actionable marketing strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This function constructs a series of messages to feed into the LLM, each representing
    a significant user interaction, such as adding to a cart or removing from it.
    These descriptions culminate in a request for a marketing strategy tailored to
    these activities, prompting the LLM to generate a custom message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going back to our earlier analysis of the data stored in our retrieval index
    for “`user_id`": “`576802932`", we can run an analysis on this customer and obtain
    a response such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Our marketing campaign could time this prompt generation to take place based
    on the most recent customer interaction data for this consumer as of roughly `17:00`
    and then send the email shortly thereafter. This not only provides a direct incentive
    to reduce abandonment but also creates a sense of urgency that the customer is
    more likely to act upon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study 2: Product upselling'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on the insights from the first case study, which focused on reducing
    cart abandonment rates through targeted discounts during peak interaction hours,
    this second case study explores a complementary strategy. Here, we aim to enhance
    product upselling by offering personalized product recommendations based on user
    behavior and preferences during similar peak hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is designed to harness user data to not only remind them of their
    abandoned carts but also to upsell related or complementary products based on
    their browsing and purchasing history. By doing this, we not only aim to recover
    abandoned carts but also increase the average order value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This function is designed to construct a narrative that not only addresses the
    immediate need to reduce cart abandonment but also strategically suggests additional
    products that enhance the user’s initial choice, potentially increasing the transaction
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now apply this function to the same consumer as last time and see the
    result, this time prompting the model to return a full email message in its entirety:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following email content:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: Tailored upselling content strategy generated for user 576802932,
    based on their interaction data retrieved from the Elasticsearch database'
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study 3: Real-time content customization for bpw.style'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Building on our strategy to utilize RAG for micro-targeting, this case study
    focuses specifically on enhancing engagement and reducing cart abandonment rates
    for the `bpw.style` brand. Recognizing the disparity between cart additions and
    completions, as previously analyzed, we target users who might benefit from a
    gentle reminder or an incentive to complete their transactions. This query will
    specifically look for cart addition and removal events:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a function with the Elasticsearch query to fetch this specific type
    of brand interaction data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the targeted data retrieved, we can now integrate this into our `generate_reengagement_content`
    function. The modified function will use the data fetched by our new query function
    to generate personalized re-engagement strategies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can run our new query and content generation commands via the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B30999_11_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: Personalized re-engagement strategy created for users interested
    in bpw.style products who have abandoned their carts'
  prefs: []
  type: TYPE_NORMAL
- en: As shown by the output, by honing in on specific brands that exhibit high cart
    abandonment rates, we can tailor marketing strategies that address unique challenges
    related to brand perception and customer engagement. This approach allows for
    the creation of highly specific content that resonates with the brand’s audience,
    potentially transforming browsing behaviors into completed transactions. While
    this strategy emphasizes brand-specific data, it can seamlessly integrate with
    user-specific insights illustrated earlier, including demographic information,
    if available, to enhance the precision and relevance of the marketing messages.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has provided a detailed exploration of RAG and its transformative
    impact on precision marketing. By integrating generative AI with dynamic retrieval
    systems, RAG overcomes the limitations inherent in previous models like ZSL and
    FSL by incorporating real-time, context-specific data into content generation.
    This enables an unprecedented level of personalization in marketing strategies,
    enhancing the relevance and efficacy of marketing content tailored to individual
    consumer preferences and current market conditions.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve used practical examples and mathematical models to demonstrate how RAG
    effectively combines data freshness and specificity, thereby elevating consumer
    engagement and optimizing conversion rates.
  prefs: []
  type: TYPE_NORMAL
- en: The discussion also covered the technical mechanisms that underpin RAG, from
    query generation and information retrieval to the iterative refinement of generated
    content. These elements ensure that the content not only resonates with the audience
    but also stays aligned with the latest trends and data insights, a crucial advantage
    in today’s rapidly evolving digital marketing landscape.
  prefs: []
  type: TYPE_NORMAL
- en: As we look to the future, in the next chapter, we will move on to the broader
    landscape of AI and ML in marketing, consolidating the knowledge acquired throughout
    this book while exploring emerging technologies. We will combine current methods
    and predicted advancements in AI that will revolutionize marketing even further.
    Additionally, we will examine how AI is becoming integral to emerging digital
    platforms that offer novel ways to engage customers and personalize marketing
    efforts. This forward-looking perspective will give you the insights and skills
    that you will need to navigate the evolving AI landscape, preparing you for the
    future of digital marketing.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genai](https://packt.link/genai)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code12856128601808671.png)'
  prefs: []
  type: TYPE_IMG
