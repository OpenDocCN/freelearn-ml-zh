- en: Interfacing Vision Sensors with ROS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at actuators and how to interface the robot's
    sensors using the Tiva-C LaunchPad board. In this chapter, we will mainly look
    at vision sensors and the interface that they use with our robot.
  prefs: []
  type: TYPE_NORMAL
- en: The robot we are designing will have a 3D vision sensor, and we will be able
    to interface it with vision libraries such as **Open Source Computer Vision**
    (**OpenCV**), **Open Natural Interaction (OpenNI)**, and **Point Cloud Library**
    (**PCL**). The main application of the 3D vision sensor in our robot is autonomous
    navigation.
  prefs: []
  type: TYPE_NORMAL
- en: We will also look at how to interface the vision sensors with ROS and process
    the images that it senses using vision libraries such as OpenCV. In the last section
    of this chapter, we will look at the mapping and localization algorithm that we
    will use in our robot, called **SLAM** (**simultaneous localization and mapping**),
    and its implementation using a 3D vision sensor, ROS, and image-processing libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: List of robotic vision sensors and image libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to OpenCV, OpenNI, and PCL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ROS-OpenCV interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Point cloud processing using the PCL-ROS interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversion of point cloud data to laser scan data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to SLAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an Ubuntu 16.04 system with ROS Kinetic installed, as well as
    a web camera and a depth camera in order to try out the example in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the first section, we will look at the 2D and 3D vision sensors that are
    available in the market that can be used in different robots.
  prefs: []
  type: TYPE_NORMAL
- en: List of robotic vision sensors and image libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A 2D vision sensor or an ordinary camera delivers 2D image frames of the surroundings,
    whereas a 3D vision sensor delivers 2D image frames and an additional parameter
    called the depth of each image point. We can find the *x*, *y*, and *z* distance
    of each point from the 3D sensor with respect to the sensor's axis.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few vision sensors available on the market. Some of the 2D
    and 3D vision sensors that can be used in our robot are mentioned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Pixy2/CMUcam5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following picture shows the latest 2D vision sensor, called Pixy2/CMUcam5
    ([https://pixycam.com/pixy-cmucam5/](https://pixycam.com/pixy-cmucam5/)), which
    is able to detect color objects with high speed and accuracy, and can be interfaced
    with an Arduino board. Pixy can be used for fast object detection, and the user
    can teach it which object it needs to track. The Pixy module has a CMOS sensor
    and NXP LPC4330 ([http://www.nxp.com/](http://www.nxp.com/)) based on Arm Cortex
    M4/M0 cores for picture processing. The following image shows the Pixy/CMUcam5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/451ea8e0-1fe5-48cf-9cec-ca024aa0fc3d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Pixy/CMUcam5 ([http://a.co/fZtPqck](http://a.co/1t91hn6))
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly available 2D vision sensors are webcams. They contain a CMOS
    sensor and USB interface, but they do not have any inbuilt vision-processing capabilities
    like Pixy has.
  prefs: []
  type: TYPE_NORMAL
- en: Logitech C920 webcam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following picture shows a popular webcam from Logitech that can capture
    pictures of up to 5-megapixel resolution and HD videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caf46080-ba33-4b2b-a455-c740153411cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Logitech HD C920 webcam (http://a.co/02DUUYd)
  prefs: []
  type: TYPE_NORMAL
- en: Kinect 360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now take a look at some of the 3D vision sensors available on the market.
    Some of the more popular sensors are Kinect, the Intel RealSense D400 series,
    and Orbbec Astra.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/584c1ed3-0591-44d6-a5de-6e86baa35746.png)'
  prefs: []
  type: TYPE_IMG
- en: Kinect sensor
  prefs: []
  type: TYPE_NORMAL
- en: Kinect is a 3D vision sensor originally developed for the Microsoft Xbox 360
    game console. It mainly contains an RGB camera, an infrared projector, an IR depth
    camera, a microphone array, and a motor to alter its tilt. The RGB camera and
    depth camera capture images at a resolution of 640 x 480 at 30 Hz. The RGB camera
    captures 2D color images, whereas the depth camera captures monochrome depth images.
    Kinect has a depth-sensing range of between 0.8 m and 4 m.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the applications of Kinect are 3D motion capture, skeleton tracking,
    face recognition, and voice recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Kinect can be interfaced with a PC using the USB 2.0 interface and programmed
    using Kinect SDK, OpenNI, and OpenCV. Kinect SDK is only available for Windows
    platforms, and SDK is developed and supplied by Microsoft. The other two libraries
    are open source and available for all platforms. The Kinect we are using here
    is the first version of Kinect; the latest versions of Kinect only support Kinect
    SDK when it is running on Windows (see [https://www.microsoft.com/en-us/download/details.aspx?id=40278](https://www.microsoft.com/en-us/download/details.aspx?id=40278)
    for more details).
  prefs: []
  type: TYPE_NORMAL
- en: The production of Kinect series sensors is discontinued, but you can still find
    the sensor on Amazon and eBay.
  prefs: []
  type: TYPE_NORMAL
- en: Intel RealSense D400 series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/d8bd3629-00ce-4fa3-9c73-77e997e8bc7a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Intel RealSense D400 series (https://realsense.intel.com/)
  prefs: []
  type: TYPE_NORMAL
- en: The Intel RealSense D400 depth sensors are stereo cameras that come with an
    IR projector to enhance the depth data (see [https://software.intel.com/en-us/realsense/d400](https://software.intel.com/en-us/realsense/d400)
    for more details), as shown in Figure 4\. The more popular sensor models in the
    D400 series are D415 and D435\. In Figure 4, the sensor on the left is D415 and
    the sensor on the right is D435\. Each consists of a stereo camera pair, an RGB
    camera, and an IR projector. The stereo camera pair computes the depth of the
    environment with the help of the IR projector.
  prefs: []
  type: TYPE_NORMAL
- en: The major features of this depth camera are that it can work in an indoor and
    outdoor environment. It can deliver the depth image stream with 1280 x 720 resolution
    at 90 fps, and the RGB camera can deliver a resolution of up to 1920 x 1080\.
    It has a USB-C interface, which enables fast data transfer between the sensor
    and the computer. It has a small form factor and is lightweight, which is ideal
    for a robotics vision application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The applications of Kinect and Intel RealSense are the same, except for speech
    recognition. They will work in Windows, Linux, and Mac. We can develop applications
    by using ROS, OpenNI, and OpenCV. The following diagram shows the block diagram
    of the D400 series camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78da5af1-86a5-4c4b-a3b8-2a420ae33cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Block diagram of the Intel RealSense D400 series
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the datasheet of the Intel RealSense series at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://software.intel.com/sites/default/files/Intel_RealSense_Depth_Cam_D400_Series_Datasheet.pdf](https://software.intel.com/sites/default/files/Intel_RealSense_Depth_Cam_D400_Series_Datasheet.pdf)
    A research paper about Intel RealSense''s depth sensor can be found at the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1705.05548](https://arxiv.org/abs/1705.05548) You can
    find the Intel RealSense SDK at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/IntelRealSense/librealsense](https://github.com/IntelRealSense/librealsense)'
  prefs: []
  type: TYPE_NORMAL
- en: Orbbec Astra depth sensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The new Orbbec Astra sensor is one of the alternatives to Kinect available
    on the market. It has similar specs compared to Kinect and uses similar technology
    to obtain depth information. Similar to Kinect, it has an IR projector, RGB camera,
    and IR sensor. It also comes with a microphone, which helps for voice recognition
    applications. The following image shows all parts of the Orbbec Astra depth sensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d2d5d043-de46-472a-8b5c-94e6ecda1f28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Orbbec Astra depth sensor (https://orbbec3d.com/product-astra/)
  prefs: []
  type: TYPE_NORMAL
- en: 'The Astra sensor comes in two models: Astra and Astra S. The main difference
    between these two models is the depth range. The Astra has a depth range of 0.6-8
    m, whereas the Astra S has a range of 0.4-2 m. The Astra S is best suited for
    3D scanning, whereas the Astra can be used in robotics applications. The size
    and weight of Astra is much lower than that of Kinect. These two models can both
    deliver depth data and an RGB image of 640 x 480 resolution at 30 fps. You can
    use a higher resolution, such as 1280 x 960, but it may reduce the frame rate.
    They also have the ability to track skeletons, like Kinect.'
  prefs: []
  type: TYPE_NORMAL
- en: The sensor is compliant with the OpenNI framework, so an application built using
    OpenNI can also work using this sensor. We are going to use this sensor in our
    robot.
  prefs: []
  type: TYPE_NORMAL
- en: The SDK is compatible with Windows, Linux, and Mac OS X. For more information,
    you can go to the sensor's development website at [https://orbbec3d.com/develop/](https://orbbec3d.com/develop/).
  prefs: []
  type: TYPE_NORMAL
- en: One of the sensors you can also refer to is the ZED Camera (https://www.stereolabs.com/zed/).
    It is a stereo vision camera system which can able to deliver high resolution
    with good frame rate.  The price is around 450 USD which is higher than above
    sensors. This can be used for high-end robotics applications required good accuracy
    from sensors.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the ROS interfacing for this sensor in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to OpenCV, OpenNI, and PCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at the software frameworks and libraries that we will be using in
    our robots. First, let's look at OpenCV. This is one of the libraries that we
    are going to use in this robot for object detection and other image-processing
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: What is OpenCV?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**OpenCV** is an open source, BSD-licensed computer vision library that includes
    the implementations of hundreds of computer-vision algorithms. The library, mainly
    intended for real-time computer vision, was developed by Intel Russia''s research,
    and is now actively supported by Itseez ([https://github.com/Itseez](https://github.com/Itseez)).
    In 2016, Intel acquired Itseez.'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV is written mainly in C and C++, and its primary interface is in C++.
    It also has good interfaces in Python, Java, and MATLAB/Octave, and also has wrappers
    in other languages (such as C# and Ruby).
  prefs: []
  type: TYPE_NORMAL
- en: In the latest version of OpenCV, there is support for CUDA and OpenCL to enable
    GPU acceleration ([http://www.nvidia.com/object/cuda_home_new.html](http://www.nvidia.com/object/cuda_home_new.html)).
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV will run on most OS platforms (such as Windows, Linux, Mac OS X, Android,
    FreeBSD, OpenBSD, iOS, and BlackBerry).
  prefs: []
  type: TYPE_NORMAL
- en: In Ubuntu, OpenCV, the Python wrapper, and the ROS wrapper are already installed
    when we install the `ros-kinetic-desktop-full` or `ros-melodic-desktop-full` package.
    The following commands install the OpenCV-ROS package individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kinetic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In Melodic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to verify that the OpenCV-Python module is installed on your system,
    take a Linux Terminal, and enter the *python* command. You should then see the
    Python interpreter. Try to execute the following commands in the Python terminal
    to verify the OpenCV installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If this command is successful, this version of OpenCV will be installed on your
    system. The version might be either 3.3.x or 3.2.x.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to try OpenCV in Windows, you can try the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html](https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html)
    The following link will guide you through the installation process of OpenCV on
    Mac OS X:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.learnopencv.com/install-opencv3-on-macos/](https://www.learnopencv.com/install-opencv3-on-macos/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main applications of OpenCV are in the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gesture recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human-computer interaction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mobile robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motion tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial-recognition systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installation of OpenCV from the source code in Ubuntu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenCV installation can be customized. If you want to customize your OpenCV
    installation, you can try to install it from the source code. You can find out
    how to do this installation at [https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html](https://docs.opencv.org/trunk/d7/d9f/tutorial_linux_install.html).
  prefs: []
  type: TYPE_NORMAL
- en: To work with the examples in this chapter, it's best that you work with OpenCV
    installed, along with ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and displaying an image using the Python-OpenCV interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first example will load an image in grayscale and display it on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section of code, we will import the `numpy` module for handling
    the image array. The `cv2` module is the OpenCV wrapper for Python, which we can
    use to access OpenCV Python APIs. NumPy is an extension to the Python programming
    language, adding support for large multidimensional arrays and matrices, along
    with a large library of high-level mathematical functions to operate on these
    arrays (see [https://pypi.python.org/pypi/numpy](https://pypi.python.org/pypi/numpy)
    for more information):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function will read the `robot.jpg` image and load this image
    in grayscale. The first argument of the `cv2.imread()` function is the name of
    the image and the next argument is a flag that specifies the color type of the
    loaded image. If the flag is greater than 0, the image returns a three-channel
    RGB color image; if the flag is 0, the loaded image will be a grayscale image;
    and if the flag is less than 0, it will return the same image as was loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following section of code will show the read image using the `imshow()`
    function. The `cv2.waitKey(0)` function is a keyboard-binding function. Its argument
    is time in milliseconds. If it''s 0, it will wait indefinitely for a key stroke:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `cv2.destroyAllWindows()` function simply destroys all the windows we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the preceding code as `image_read.py` and copy a JPG file and name it
    `robot.jpg`. Execute the code using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will load an image in grayscale because we used `0` as the value
    in the `imread()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f9b4a8a-56d6-4dd0-a6ec-b1b088ce5844.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of read image code
  prefs: []
  type: TYPE_NORMAL
- en: The following example will try to use an open webcam. The program will quit
    when the user presses any button.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing from the web camera
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following code will capture an image using the webcam with the device name
    `/dev/video0` or `/dev/video1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to import the *numpy* and *cv2* modules for capturing an image from
    a camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function will create a `VideoCapture` object. The `VideoCapture`
    class is used to capture videos from video files or cameras. The initialization
    argument of the `VideoCapture` class is the index of a camera or the name of a
    video file. The device index is just a number that is used to specify the camera.
    The first camera index is 0, and has the device name `/dev/video0`-that''s why
    we will put `0` in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following section of code is looped to read image frames from the `VideoCapture`
    object, and shows each frame. It will quit when any key is pressed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a screenshot of the program output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f0f6487-3d71-4b6f-afe4-c1ff2a195530.png)'
  prefs: []
  type: TYPE_IMG
- en: Output of the video capture
  prefs: []
  type: TYPE_NORMAL
- en: You can explore more OpenCV-Python tutorials at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_tutorials.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the OpenNI library and its application.
  prefs: []
  type: TYPE_NORMAL
- en: What is OpenNI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenNI is a multilanguage, cross-platform framework that defines APIs in order
    to write applications using **natural interaction** (**NI**) (see [https://structure.io/openni](https://structure.io/openni)
    for more information). Natural interaction refers to the way in which people naturally
    communicate through gestures, expressions, and movements, and discover the world
    by looking around and manipulating physical objects and materials.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenNI APIs are composed of a set of interfaces that are used to write NI applications.
    The following figure shows a three-layered view of the OpenNI library:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e270787-c408-45fc-ab89-7cc198e14a74.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenNI framework software architecture
  prefs: []
  type: TYPE_NORMAL
- en: The top layer represents the application layer that implements the natural interaction-based
    application. The middle layer is the OpenNI layer, and it will provide communication
    interfaces that interact with sensors and middleware components that analyze the
    data from the sensor. Middleware can be used for full-body analysis, hand-point
    analysis, gesture detection, and so on. One example of a middle layer component
    is NITE ([http://www.openni.ru/files/nite/index.html](http://www.openni.ru/files/nite/index.html)),
    which can detect gestures and skeletons.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom layer contains the hardware devices that capture the visual and audio
    elements of the scene. It can include 3D sensors, RGB cameras, IR cameras, and
    microphones.
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of OpenNI is OpenNI 2, which support sensors such as Asus
    Xtion Pro, and Primesense Carmine. The first version of OpenNI mainly supports
    the Kinect 360 sensor.
  prefs: []
  type: TYPE_NORMAL
- en: OpenNI is cross platform, and has been successfully compiled and deployed on
    Linux, Mac OS X, and Windows.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how we to install OpenNI in Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenNI in Ubuntu
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can install the OpenNI library along with ROS packages. ROS is already interfaced
    with OpenNI, but the ROS desktop full installation may not install OpenNI packages;
    if so, we need to install it from the package manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will install the ROS-OpenNI library (which is mainly
    supported by the Kinect Xbox 360 sensor) in Kinetic and Melodic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command will install the ROS-OpenNI 2 library (which is mainly
    supported by Asus Xtion Pro and Primesense Carmine):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The source code and latest build of OpenNI for Windows, Linux, and MacOS X is
    available at [http://structure.io/openni](http://structure.io/openni).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at how to install PCL.
  prefs: []
  type: TYPE_NORMAL
- en: What is PCL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **point cloud** is a set of data points in space that represent a 3D object
    or an environment. Generally, a point cloud is generated from depth sensors, such
    as Kinect and LIDAR. PCL (Point Cloud Library) is a large scale, open project
    for 2D/3D images and point-cloud processing. The PCL framework contains numerous
    algorithms that perform filtering, feature estimation, surface reconstruction,
    registration, model fitting, and segmentation. Using these methods, we can process
    the point cloud, extract key descriptors to recognize objects in the world based
    on their geometric appearance, create surfaces from the point clouds, and visualize
    them.
  prefs: []
  type: TYPE_NORMAL
- en: PCL is released under the BSD license. It's open source, free for commercial
    use, and free for research use. PCL is cross platform and has been successfully
    compiled and deployed on Linux, macOS X, Windows, and Android/iOS.
  prefs: []
  type: TYPE_NORMAL
- en: You can download PCL at [http://pointclouds.org/downloads/](http://pointclouds.org/downloads/).
  prefs: []
  type: TYPE_NORMAL
- en: PCL is already integrated into ROS. The PCL library and its ROS interface are
    included in a ROS full desktop installation. PCL is the 3D-processing backbone
    of ROS. Refer to http://wiki.ros.org/pcl for details on the ROS-PCL package.
  prefs: []
  type: TYPE_NORMAL
- en: Programming Kinect with Python using ROS, OpenCV, and OpenNI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at how we can interface and work with the Kinect sensor in ROS. ROS
    is bundled with the OpenNI driver, which can fetch the RGB and depth image of
    Kinect. The OpenNI and OpenNI 2 package in ROS can be used for interfacing with
    Microsoft Kinect, Primesense Carmine, Asus Xtion Pro, and Pro Live.
  prefs: []
  type: TYPE_NORMAL
- en: When we install ROS's `openni_launch` package, it will also install its dependent
    packages, such as `openni_camera`. The `openni_camera` package is the Kinect driver
    that publishes raw data and sensor information, whereas the `openni_launch` package
    contains ROS launch files. These launch files launch multiple nodes at a time
    and publish data such as the raw depth, RGB, and IR images, and the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: How to launch the OpenNI driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can connect the Kinect sensor to your computer using a USB interface and
    make sure it is detected on your PC using the `dmesg` command in the terminal.
    After setting up Kinect, we can start ROS's OpenNI driver to get data from the
    device.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will open the OpenNI device and load all nodelets (see
    [http://wiki.ros.org/nodelet](http://wiki.ros.org/nodelet) for more information)
    to convert raw depth/RGB/IR streams to depth images, disparity images, and point
    clouds. The ROS `nodelet` package is designed to provide a way to run multiple
    algorithms in the same process with zero copy transport between algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After starting the driver, you can list out the various topics published by
    the driver using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You can view the RGB image using a ROS tool called `image_view`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn how to interface these images with OpenCV
    for image processing.
  prefs: []
  type: TYPE_NORMAL
- en: The ROS interface with OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV is also integrated into ROS, mainly for image processing. The `vision_opencv`
    ROS stack includes the complete OpenCV library and the interface with ROS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `vision_opencv` meta package consists of individual packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv_bridge`: This contains the `CvBridge` class. This class converts ROS image
    messages to the OpenCV image data type and vice versa.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_geometry`: This contains a collection of methods to handle image and
    pixel geometry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows how OpenCV is interfaced with ROS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46faa908-19fe-479d-a6e2-30d47d4c37bd.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenCV-ROS interfacing
  prefs: []
  type: TYPE_NORMAL
- en: The image data types of OpenCV are `IplImage` and `Mat`. If we want to work
    with OpenCV in ROS, we have to convert `IplImage` or `Mat` to ROS image messages.
    The ROS package `vision_opencv` has the `CvBridge` class; this class can convert
    `IplImage` to a ROS image and vice versa. Once we get the ROS image topics from
    any kind of vision sensor, we can use ROS CvBridge in order to convert it from
    ROS topic to Mat or IplImage format.
  prefs: []
  type: TYPE_NORMAL
- en: The following section shows you how to create a ROS package; this package contains
    a node to subscribe to RGB and depth images, process RGB images to detect edges
    and display all images after converting them to an image type equivalent to OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ROS package with OpenCV support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can create a package called `sample_opencv_pkg` with the following dependencies:
    `sensor_msgs`, `cv_bridge`, `rospy`, and `std_msgs`. The `sensor_msgs` dependency
    defines ROS messages for commonly used sensors, including cameras and scanning-laser
    rangefinders. The `cv_bridge` dependency is the OpenCV interface of ROS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will create the ROS package with the aforementioned dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: After creating the package, create a `scripts` folder inside the package; we
    will use it as a location in which to save the code that will be mentioned in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying Kinect images using Python, ROS, and cv_bridge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first section of the Python code is given in the following code fragment.
    It mainly involves importing `rospy`, `sys`, `cv2`, `sensor_msgs`, `cv_bridge,`
    and the `numpy` module. The `sensor_msgs` dependency imports the ROS data type
    of both image and camera information type. The `cv_bridge` module imports the
    `CvBridge` class for converting the ROS image data type to the OpenCV data type
    and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following section of code is a class definition in Python that we will
    use to demonstrate `CvBridge` functions. The class is called `cvBridgeDemo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the callback to visualize the actual RGB image, processed RGB image,
    and depth image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code gives a callback function of the color image from Kinect.
    When a color image is received on the `/camera/rgb/image_raw` topic, it will call
    this function. This function will process the color frame for edge detection and
    show the edge detected and the raw color image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code gives a callback function of the depth image from Kinect.
    When a depth image is received on the `/camera/depth/raw_image` topic, it will
    call this function. This function will show the raw depth image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is called `process_image(),` and will convert the color
    image to grayscale, then blur the image, and find the edges using the canny edge
    filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function is called `process_depth_image()`. It simply returns
    the depth frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function will close the image window when the node shuts down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is the `main()` function. It will initialize the `cvBridgeDemo()`
    class and call the `rospy.spin()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the preceding code as `cv_bridge_demo.py` and change the permission of
    the node using the following command. The nodes are only visible to the `rosrun`
    command if we give it executable permission:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the commands to start the driver and node. Start the Kinect
    driver using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the node using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a screenshot of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/feee3af4-7c2b-44cd-a94a-f9e951f1dfd3.png)'
  prefs: []
  type: TYPE_IMG
- en: RGB, depth, and edge images
  prefs: []
  type: TYPE_NORMAL
- en: Interfacing Orbbec Astra with ROS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the alternatives to Kinect is Orbbec Astra. There are ROS drivers available
    for Astra, and we can see how to set up that driver and get the image, depth,
    and point cloud from this sensor.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Astra–ROS driver
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The complete instructions to set up the Astra-ROS driver in Ubuntu are mentioned
    at [https://github.com/orbbec/ros_astra_camera](https://github.com/orbbec/ros_astra_camera)
    and [http://wiki.ros.org/Sensors/OrbbecAstra](http://wiki.ros.org/Sensors/OrbbecAstra).
    After installing the driver, you can launch it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also install the Astra driver from the ROS package repository. Here
    is the command to install those packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: After installing these packages, you have to set the permission of the device
    in order to work with the device, as described at [http://wiki.ros.org/astra_camera](http://wiki.ros.org/astra_camera).
    You can check the ROS topics that are generated from this driver using the `rostopic`
    list command in the terminal. In addition, we can use the same Python code for
    image processing that we mentioned in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Working with point clouds using Kinect, ROS, OpenNI, and PCL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A 3D point cloud is a way of representing a 3D environment and 3D objects as
    collection points along the x, y, and z axes. We can get a point cloud from various
    sources: Either we can create our point cloud by writing a program or we can generate
    it from depth sensors or laser scanners.'
  prefs: []
  type: TYPE_NORMAL
- en: PCL supports the OpenNI 3D interfaces natively; thus, it can acquire and process
    data from devices (such as Prime Sensor's 3D cameras, Microsoft Kinect, or Asus
    Xtion Pro).
  prefs: []
  type: TYPE_NORMAL
- en: PCL will be included in the ROS full desktop installation. Let's see how we
    can generate and visualize a point cloud in RViz, a data visualization tool in
    ROS.
  prefs: []
  type: TYPE_NORMAL
- en: Opening the device and generating a point cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open a new terminal and launch the ROS-OpenNI driver, along with the point
    cloud generator nodes, using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This command will activate the Kinect driver and process the raw data into convenient
    outputs, such as a point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Orbbec Astra, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We will use the RViz 3D visualization tool to view our point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will start the RViz tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Set the RViz options for Fixed Frame (at the top of the Displays panel under
    Global Options) to camera_link.
  prefs: []
  type: TYPE_NORMAL
- en: From the left-hand side panel of the RViz panel, click on the Add button and
    choose the PointCloud2 display option. Set its topic to /camera/depth/points (this
    is the topic for Kinect; it will be different for other sensors)
  prefs: []
  type: TYPE_NORMAL
- en: Change the Color Transformer of PointCloud2 to AxisColor.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a screenshot of the RViz point cloud data. You
    can see the nearest objects are marked in red and the farthest objects are marked
    in violet and blue. The objects in front of the Kinect are represented as a cylinder
    and cube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1639b5ac-df7e-4fee-932b-0eaf19437307.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing point cloud data in Rviz
  prefs: []
  type: TYPE_NORMAL
- en: Conversion of point cloud data to laser scan data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are using Astra in this robot to replicate the function of an expensive laser
    range scanner. The depth image is processed and converted to the data equivalent
    of a laser scanner using ROS's `depthimage_to_laserscan` package (see [http://wiki.ros.org/depthimage_to_laserscan](http://wiki.ros.org/depthimage_to_laserscan)
    for more information).
  prefs: []
  type: TYPE_NORMAL
- en: You can either install this package from the source code or use the Ubuntu package
    manager. Here is the command to install this package from the Ubuntu package manager
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The main function of this package is to slice a section of the depth image
    and convert it to an equivalent laser scan data type. The ROS `sensor_msgs/LaserScan`
    message type is used for publishing the laser scan data. This `depthimage_to_laserscan`
    package will perform this conversion and fake the laser scanner data. The laser
    scanner output can be viewed using RViz. In order to run the conversion, we have
    to start the convertor nodelets that will perform this operation. We have to specify
    this in our launch file in order to start the conversion. The following is the
    required code in the launch file to start the `depthimage_to_laserscan` conversion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The topic of the depth image can be changed in each sensor; you have to change
    the topic name according to your depth image topic.
  prefs: []
  type: TYPE_NORMAL
- en: As well as starting the nodelet, we need to set certain parameters of the nodelet
    for better conversion. Refer to [http://wiki.ros.org/depthimage_to_laserscan](http://wiki.ros.org/depthimage_to_laserscan)
    for a detailed explanation of each parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The laser scan of the preceding view is given in the following screenshot.
    To view the laser scan, add the LaserScan option. This is similar to how we add
    the PointCloud2 option and change the Topic value of LaserSan to /scan:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c4b6ceac-d6d3-43eb-a6a8-91d112b6cf71.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing laser scan data in Rviz
  prefs: []
  type: TYPE_NORMAL
- en: Working with SLAM using ROS and Kinect
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main aim of deploying vision sensors in our robot is to detect objects and
    navigate the robot through an environment. SLAM is a algorithm that is used in
    mobile robots to build up a map of an unknown environment or update a map within
    a known environment by tracking the current location of the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Maps are used to plan the robot's trajectory and to navigate through this path.
    Using maps, the robot will get an idea about the environment. The two main challenges
    in mobile robot navigation are mapping and localization.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping involves generating a profile of obstacles around the robot. Through
    mapping, the robot will understand what the world looks like. Localization is
    the process of estimating the position of the robot relative to the map we build.
  prefs: []
  type: TYPE_NORMAL
- en: SLAM fetches data from different sensors and uses it to build maps. The 2D/3D
    vision sensor can be used to input data into SLAM. 2D vision sensors, such as
    web cameras, and 3D sensors, such as Kinect, are mainly used as inputs for the
    SLAM algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A SLAM library called OpenSlam ([http://openslam.org/gmapping.html](http://openslam.org/gmapping.html))
    is integrated with ROS as a package called gmapping. The `gmapping` package provides
    a node to perform laser-based SLAM processing, called `slam_gmapping`. This can
    create a 2D map from the laser and position data collected by the mobile robot.
  prefs: []
  type: TYPE_NORMAL
- en: The `gmapping` package is available at [http://wiki.ros.org/gmapping](http://wiki.ros.org/gmapping).
  prefs: []
  type: TYPE_NORMAL
- en: To use the `slam_gmapping` node, we have to input the odometry data of the robot
    and the laser scan output from the laser range finder, which is mounted horizontally
    on the robot.
  prefs: []
  type: TYPE_NORMAL
- en: The `slam_gmapping` node subscribes to the `sensor_msgs/LaserScan` messages
    and `nav_msgs/Odometry` messages to build the map (`nav_msgs/OccupancyGrid`).
    The generated map can be retrieved via a ROS topic or service.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have used the following launch file to use SLAM in our Chefbot. This launch
    file launches the `slam_gmapping` node and contains the necessary parameters to
    start mapping the robot''s environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the various vision sensors that can be used in
    Chefbot. We used Kinect and Astra in our robot and learned about OpenCV, OpenNI,
    PCL, and their application. We also discussed the role of vision sensors in robot
    navigation, the popular SLAM technique, and its application using ROS. In the
    next chapter, we will see the complete interfacing of the robot and learn how
    to perform autonomous navigation with our Chefbot.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are 3D sensors and how are they different from ordinary cameras?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main features of ROS?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the applications of OpenCV, OpenNI, and PCL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is SLAM?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is RGB-D SLAM and how does it work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can read more about the robotic vision package in ROS at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://wiki.ros.org/vision_opencv](http://wiki.ros.org/vision_opencv)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://wiki.ros.org/pcl](http://wiki.ros.org/pcl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
