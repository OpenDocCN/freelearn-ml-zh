["```py\n# Step 1: Preprocessing the image data\n# Import necessary libraries\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom skimage.transform import resize\n# Load CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n# Normalize pixel values to range [0, 1]\nx_train = x_train.astype('float32') / 255.0\nx_test = x_test.astype('float32') / 255.0\n# Resize images to a standardized size (e.g., 32x32 pixels)\nstandardized_size = (32, 32)\nx_train_resized = np.array([resize(image, standardized_size) for image in x_train])\nx_test_resized = np.array([resize(image, standardized_size) for image in x_test])\n# Display shape of datasets after preprocessing\nprint(\"Shape of x_train:\", x_train_resized.shape)\nprint(\"Shape of x_test:\", x_test_resized.shape) \n```", "```py\n# Step 2: Model Construction\n# Import necessary libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n# Define the baseline CNN model architecture\nmodel = Sequential([\n    # Add a convolutional layer with 32 filters, kernel size of (3, 3), and ReLU activation function\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    # Add a max pooling layer with pool size of (2, 2)\n    MaxPooling2D((2, 2)),\n    # Flatten the output of the convolutional layer\n    Flatten(),\n    # Add a fully connected layer with 64 neurons and ReLU activation function\n    Dense(64, activation='relu'),\n    # Add the output layer with 10 neurons (one for each class) and softmax activation function\n    Dense(10, activation='softmax')\n])\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n# Display model summary\nmodel.summary() \n```", "```py\n# Step 3: Model Training\n# Train the model\nhistory = model.fit(x_train_resized, y_train,\n                    epochs=10, \n                    batch_size=32,\n                    validation_data=(x_test_resized, y_test)) \n```", "```py\n# Step 4: Model Evaluation and Visualization\nimport matplotlib.pyplot as plt\n# Evaluate the model on the testing dataset\ntest_loss, test_accuracy = model.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_accuracy)\n# Plot training history\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show() \n```", "```py\n# Define the modified model with two consecutive layers of 64 filters\nmodified_model_64 = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n# Compile the modified model\nmodified_model_64.compile(optimizer='adam',\n                          loss='sparse_categorical_crossentropy',\n                          metrics=['accuracy'])\n# Display model summary\nmodified_model_64.summary()\n# Train the modified model with two consecutive layers of 64 filters\nhistory_64 = modified_model_64.fit(x_train_resized, y_train,\n                                    epochs=10, \n                                    batch_size=32,\n                                    validation_data=(x_test_resized, y_test))\n# Evaluate the modified model on the testing dataset\ntest_loss_64, test_accuracy_64 = modified_model_64.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss (Modified Model with 64 Filters):\", test_loss_64)\nprint(\"Test Accuracy (Modified Model with 64 Filters):\", test_accuracy_64) \n```", "```py\n# Define the modified model with two consecutive layers of 128 filters\nmodified_model_128 = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n# Compile the modified model\nmodified_model_128.compile(optimizer='adam',\n                           loss='sparse_categorical_crossentropy',\n                           metrics=['accuracy'])\n# Display model summary\nmodified_model_128.summary()\n# Train the modified model with two consecutive layers of 128 filters\nhistory_128 = modified_model_128.fit(x_train_resized, y_train,\n                                     epochs=10, \n                                     batch_size=32,\n                                     validation_data=(x_test_resized, y_test))\n# Evaluate the modified model on the testing dataset\ntest_loss_128, test_accuracy_128 = modified_model_128.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss (Modified Model with 128 Filters):\", test_loss_128)\nprint(\"Test Accuracy (Modified Model with 128 Filters):\", test_accuracy_128) \n```", "```py\nValueError: Exception encountered when calling layer \"conv2d_38\" (type Conv2D).\nNegative dimension size caused by subtracting 3 from 2 for '{{node conv2d_38/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_38/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,64], [3,3,64,128].\nCall arguments received by layer \"conv2d_38\" (type Conv2D):\n  inputs=tf.Tensor(shape=(None, 2, 2, 64), dtype=float32). \n```", "```py\n# Define the modified model with two consecutive layers of 128 filters added to the 64-layer model\nmodified_model_64_to_128 = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),  # Add padding\n    Conv2D(128, (3, 3), activation='relu', padding='same'),  # Add padding\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax')\n])\n# Compile the modified model\nmodified_model_64_to_128.compile(optimizer='adam',\n                                  loss='sparse_categorical_crossentropy',\n                                  metrics=['accuracy'])\n# Display model summary\nmodified_model_64_to_128.summary()\n# Train the modified model with two consecutive layers of 128 filters added to the 64-layer model\nhistory_64_to_128 = modified_model_64_to_128.fit(x_train_resized, y_train,\n                                                 epochs=10, \n                                                 batch_size=32,\n                                                 validation_data=(x_test_resized, y_test))\n# Evaluate the modified model on the testing dataset\ntest_loss_64_to_128, test_accuracy_64_to_128 = modified_model_64_to_128.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss (Modified Model with 64 to 128 Filters):\", test_loss_64_to_128)\nprint(\"Test Accuracy (Modified Model with 64 to 128 Filters):\", test_accuracy_64_to_128) \n```", "```py\nfrom tensorflow.keras.layers import Dropout\n# Define the modified model with dropout regularization\nmodified_model_64_to_128_with_dropout = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    Dropout(0.25),  # Add dropout regularization with a dropout rate of 25%\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    Dropout(0.25),  # Add dropout regularization with a dropout rate of 25%\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dropout(0.5),  # Add dropout regularization with a dropout rate of 50%\n    Dense(10, activation='softmax')\n])\n# Compile the modified model with dropout regularization\nmodified_model_64_to_128_with_dropout.compile(optimizer='adam',\n                                              loss='sparse_categorical_crossentropy',\n                                              metrics=['accuracy'])\n# Display model summary\nmodified_model_64_to_128_with_dropout.summary()\n# Train the modified model with dropout regularization\nhistory_64_to_128_with_dropout = modified_model_64_to_128_with_dropout.fit(x_train_resized, y_train,\n                                                                           epochs=10,\n                                                                           batch_size=32,\n                                                                           validation_data=(x_test_resized, y_test))\n# Evaluate the modified model with dropout regularization on the testing dataset\ntest_loss_64_to_128_with_dropout, test_accuracy_64_to_128_with_dropout = modified_model_64_to_128_with_dropout.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss (Modified Model with Dropout):\", test_loss_64_to_128_with_dropout)\nprint(\"Test Accuracy (Modified Model with Dropout):\", test_accuracy_64_to_128_with_dropout) \n```", "```py\nTest Loss (Modified Model with Dropout): 0.876133382320404 \nTest Accuracy (Modified Model with Dropout): 0.7014999985694885 \n```", "```py\nimport matplotlib.pyplot as plt\n# Plot training history for loss\nplt.plot(history_64_to_128_with_dropout.history['loss'], label='Training Loss')\nplt.plot(history_64_to_128_with_dropout.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n# Plot training history for accuracy\nplt.plot(history_64_to_128_with_dropout.history['accuracy'], label='Training Accuracy')\nplt.plot(history_64_to_128_with_dropout.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show() \n```", "```py\nfrom tensorflow.keras.layers import BatchNormalization\n# Define the modified model with batch normalization\nmodified_model_64_to_128_with_batchnorm = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    BatchNormalization(),  # Add batch normalization\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    BatchNormalization(),  # Add batch normalization\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),  # Add batch normalization\n    MaxPooling2D((2, 2)),\n    Conv2D(128, (3, 3), activation='relu', padding='same'),\n    BatchNormalization(),  # Add batch normalization\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    BatchNormalization(),  # Add batch normalization\n    Dense(10, activation='softmax')\n])\n# Compile the modified model with batch normalization\nmodified_model_64_to_128_with_batchnorm.compile(optimizer='adam',\n                                                loss='sparse_categorical_crossentropy',\n                                                metrics=['accuracy'])\n# Display model summary\nmodified_model_64_to_128_with_batchnorm.summary()\n# Train the modified model with batch normalization\nhistory_64_to_128_with_batchnorm = modified_model_64_to_128_with_batchnorm.fit(x_train_resized, y_train,\n                                                                               epochs=10,\n                                                                               batch_size=32,\n                                                                               validation_data=(x_test_resized, y_test))\n# Evaluate the modified model with batch normalization on the testing dataset\ntest_loss_64_to_128_with_batchnorm, test_accuracy_64_to_128_with_batchnorm = modified_model_64_to_128_with_batchnorm.evaluate(x_test_resized, y_test, verbose=0)\nprint(\"Test Loss (Modified Model with Batch Normalization):\", test_loss_64_to_128_with_batchnorm)\nprint(\"Test Accuracy (Modified Model with Batch Normalization):\", test_accuracy_64_to_128_with_batchnorm) \n```", "```py\n\"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_5 (Conv2D)           (None, 30, 30, 32)        896       \n\n batch_normalization (Batch  (None, 30, 30, 32)        128       \n Normalization)                                                  \n\n max_pooling2d_3 (MaxPoolin  (None, 15, 15, 32)        0         \n g2D)                                                            \n\n conv2d_6 (Conv2D)           (None, 13, 13, 64)        18496     \n\n batch_normalization_1 (Bat  (None, 13, 13, 64)        256       \n chNormalization)                                                \n\n max_pooling2d_4 (MaxPoolin  (None, 6, 6, 64)          0         \n g2D)                                                            \n\n conv2d_7 (Conv2D)           (None, 4, 4, 64)          36928     \n\n conv2d_8 (Conv2D)           (None, 4, 4, 128)         73856     \n\n batch_normalization_2 (Bat  (None, 4, 4, 128)         512       \n chNormalization)                                                \n\n max_pooling2d_5 (MaxPoolin  (None, 2, 2, 128)         0         \n g2D)                                                            \n\n conv2d_9 (Conv2D)           (None, 2, 2, 128)         147584    \n\n batch_normalization_3 (Bat  (None, 2, 2, 128)         512       \n chNormalization)                                                \n\n max_pooling2d_6 (MaxPoolin  (None, 1, 1, 128)         0         \n g2D)                                                            \n\n flatten_1 (Flatten)         (None, 128)               0         \n\n dense_2 (Dense)             (None, 64)                8256      \n\n batch_normalization_4 (Bat  (None, 64)                256       \n chNormalization)                                                \n\n dense_3 (Dense)             (None, 10)                650       \n\n=================================================================\nTotal params: 288330 (1.10 MB)\nTrainable params: 287498 (1.10 MB)\nNon-trainable params: 832 (3.25 KB) \n```", "```py\n# Import optimizers\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\n# Define the optimizers\noptimizers = [SGD(), Adam(), RMSprop()]\n# Define lists to store test accuracies for each optimizer\ntest_accuracies = []\n# Iterate over each optimizer\nfor optimizer in optimizers:\n    # Define the modified model with batch normalization and the current optimizer\n    modified_model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(64, (3, 3), activation='relu'),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Conv2D(128, (3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPooling2D((2, 2)),\n        Flatten(),\n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dense(10, activation='softmax')\n    ])\n    # Compile the model with the current optimizer\n    modified_model.compile(optimizer=optimizer,\n                           loss='sparse_categorical_crossentropy',\n                           metrics=['accuracy'])\n    # Train the model with the current optimizer\n    history = modified_model.fit(x_train_resized, y_train,\n                                 epochs=10,\n                                 batch_size=32,\n                                 validation_data=(x_test_resized, y_test),\n                                 verbose=0)\n    # Evaluate the model with the current optimizer on the testing dataset\n    test_loss, test_accuracy = modified_model.evaluate(x_test_resized, y_test, verbose=0)\n    # Append the test accuracy to the list\n    test_accuracies.append(test_accuracy)\n    # Print the test accuracy for the current optimizer\n    print(f\"Test Accuracy (Optimizer: {optimizer.__class__.__name__}): {test_accuracy}\")\n# Plot the test accuracies for each optimizer\nplt.bar(['SGD', 'Adam', 'RMSprop'], test_accuracies)\nplt.title('Test Accuracy Comparison for Different Optimizers')\nplt.xlabel('Optimizer')\nplt.ylabel('Test Accuracy')\nplt.show() \n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Add\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport numpy as np\ndef davidnet():\n    input_layer = Input(shape=(32, 32, 3))\n    # Initial convolutional layer\n    x = Conv2D(64, kernel_size=3, padding='same')(input_layer)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    # Stack of residual blocks\n    num_residual_blocks = 4\n    for _ in range(num_residual_blocks):\n        y = x\n        x = Conv2D(64, kernel_size=3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv2D(64, kernel_size=3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Add()([x, y])  # Skip connection\n        x = Activation('relu')(x)\n    # Max pooling layer\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Flatten layer\n    x = Flatten()(x)\n    # Dense layers\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dense(10, activation='softmax')(x)\n    # Create model\n    model = Model(inputs=input_layer, outputs=x)\n    return model\n# Learning rate scheduler with warmup\ndef learning_rate_scheduler(epoch):\n    if epoch < 5:\n        return 0.4 * (epoch + 1) / 5\n    else:\n        return 0.4 * np.exp(0.1 * (5 - epoch))\n# Create DavidNet model\nmodel_davidnet = davidnet()\n# Compile the model\noptimizer = SGD(momentum=0.9, decay=0.000125)\nmodel_davidnet.compile(optimizer=optimizer,\n                        loss='sparse_categorical_crossentropy',\n                        metrics=['accuracy'])\n# Print model summary\nmodel_davidnet.summary()\n# Load CIFAR-10 dataset and preprocess if necessary\n# Assuming x_train, y_train, x_test, y_test are loaded and preprocessed\n# Define batch size and number of epochs\nbatch_size = 512\nnum_epochs = 24\n# Train the model with learning rate scheduler\nhistory_davidnet = model_davidnet.fit(x_train, y_train,\n                                      batch_size=batch_size,\n                                      epochs=num_epochs,\n                                      validation_data=(x_test, y_test),\n                                      callbacks=[LearningRateScheduler(learning_rate_scheduler)],\n                                      verbose=1) \n```", "```py\nValueError: decay is deprecated in the new Keras optimizer. Please check the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD. \n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Add\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.callbacks import LearningRateScheduler, Callback\nimport numpy as np\nclass SGDLearningRateDecay(Callback):\n    def __init__(self, initial_lr, warmup_epochs, decay_factor):\n        super(SGDLearningRateDecay, self).__init__()\n        self.initial_lr = initial_lr\n        self.warmup_epochs = warmup_epochs\n        self.decay_factor = decay_factor\n    def on_epoch_begin(self, epoch, logs=None):\n        if epoch < self.warmup_epochs:\n            lr = (self.initial_lr * (epoch + 1)) / self.warmup_epochs\n        else:\n            lr = self.initial_lr * self.decay_factor**(epoch - self.warmup_epochs)\n        self.model.optimizer.lr = lr\n        print(f'Learning rate for epoch {epoch + 1} is {lr}')\ndef davidnet():\n    input_layer = Input(shape=(32, 32, 3))\n    # Initial convolutional layer\n    x = Conv2D(64, kernel_size=3, padding='same')(input_layer)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    # Stack of residual blocks\n    num_residual_blocks = 4\n    for _ in range(num_residual_blocks):\n        y = x\n        x = Conv2D(64, kernel_size=3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Conv2D(64, kernel_size=3, padding='same')(x)\n        x = BatchNormalization()(x)\n        x = Add()([x, y])  # Skip connection\n        x = Activation('relu')(x)\n    # Max pooling layer\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Flatten layer\n    x = Flatten()(x)\n    # Dense layers\n    x = Dense(512)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = Dense(10, activation='softmax')(x)\n    # Create model\n    model = Model(inputs=input_layer, outputs=x)\n    return model\n# Create DavidNet model\nmodel_davidnet = davidnet()\n# Define hyperparameters\nbatch_size = 512\ninitial_lr = 0.4\nnum_epochs = 24\nwarmup_epochs = 5\ndecay_factor = 0.4\n# Compile the model\noptimizer = SGD(momentum=0.9)\nmodel_davidnet.compile(optimizer=optimizer,\n                        loss='sparse_categorical_crossentropy',\n                        metrics=['accuracy'])\n# Print model summary\nmodel_davidnet.summary()\n# Define learning rate scheduler callback\nlr_decay_callback = SGDLearningRateDecay(initial_lr, warmup_epochs, decay_factor)\n# Load CIFAR-10 dataset and preprocess if necessary\n# Assuming x_train, y_train, x_test, y_test are loaded and preprocessed\n# Train the model with learning rate scheduler\nhistory_davidnet = model_davidnet.fit(x_train, y_train,\n                                      batch_size=batch_size,\n                                      epochs=num_epochs,\n                                      validation_data=(x_test, y_test),\n                                      callbacks=[lr_decay_callback],\n                                      verbose=1) \n```", "```py\nModel Summary: \nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_2 (InputLayer)        [(None, 32, 32, 3)]          0         []                            \n\n conv2d_20 (Conv2D)          (None, 32, 32, 64)           1792      ['input_2[0][0]']             \n\n batch_normalization_20 (Ba  (None, 32, 32, 64)           256       ['conv2d_20[0][0]']           \n tchNormalization)                                                                                \n\n activation_10 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_20[0][0]\n                                                                    ']                            \n\n conv2d_21 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_10[0][0]']       \n\n batch_normalization_21 (Ba  (None, 32, 32, 64)           256       ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n\n activation_11 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_21[0][0]\n                                                                    ']                            \n\n conv2d_22 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_11[0][0]']       \n\n batch_normalization_22 (Ba  (None, 32, 32, 64)           256       ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n\n add_4 (Add)                 (None, 32, 32, 64)           0         ['batch_normalization_22[0][0]\n                                                                    ',                            \n                                                                     'activation_10[0][0]']       \n\n activation_12 (Activation)  (None, 32, 32, 64)           0         ['add_4[0][0]']               \n\n conv2d_23 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_12[0][0]']       \n\n batch_normalization_23 (Ba  (None, 32, 32, 64)           256       ['conv2d_23[0][0]']           \n tchNormalization)                                                                                \n\n activation_13 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_23[0][0]\n                                                                    ']                            \n\n conv2d_24 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_13[0][0]']       \n\n batch_normalization_24 (Ba  (None, 32, 32, 64)           256       ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n\n add_5 (Add)                 (None, 32, 32, 64)           0         ['batch_normalization_24[0][0]\n                                                                    ',                            \n                                                                     'activation_12[0][0]']       \n\n activation_14 (Activation)  (None, 32, 32, 64)           0         ['add_5[0][0]']               \n\n conv2d_25 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_14[0][0]']       \n\n batch_normalization_25 (Ba  (None, 32, 32, 64)           256       ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n\n activation_15 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_25[0][0]\n                                                                    ']                            \n\n conv2d_26 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_15[0][0]']       \n\n batch_normalization_26 (Ba  (None, 32, 32, 64)           256       ['conv2d_26[0][0]']           \n tchNormalization)                                                                                \n\n add_6 (Add)                 (None, 32, 32, 64)           0         ['batch_normalization_26[0][0]\n                                                                    ',                            \n                                                                     'activation_14[0][0]']       \n\n activation_16 (Activation)  (None, 32, 32, 64)           0         ['add_6[0][0]']               \n\n conv2d_27 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_16[0][0]']       \n\n batch_normalization_27 (Ba  (None, 32, 32, 64)           256       ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n\n activation_17 (Activation)  (None, 32, 32, 64)           0         ['batch_normalization_27[0][0]\n                                                                    ']                            \n\n conv2d_28 (Conv2D)          (None, 32, 32, 64)           36928     ['activation_17[0][0]']       \n\n batch_normalization_28 (Ba  (None, 32, 32, 64)           256       ['conv2d_28[0][0]']           \n tchNormalization)                                                                                \n\n add_7 (Add)                 (None, 32, 32, 64)           0         ['batch_normalization_28[0][0]\n                                                                    ',                            \n                                                                     'activation_16[0][0]']       \n\n activation_18 (Activation)  (None, 32, 32, 64)           0         ['add_7[0][0]']               \n\n max_pooling2d_9 (MaxPoolin  (None, 16, 16, 64)           0         ['activation_18[0][0]']       \n g2D)                                                                                             \n\n flatten_3 (Flatten)         (None, 16384)                0         ['max_pooling2d_9[0][0]']     \n\n dense_6 (Dense)             (None, 512)                  8389120  \n['flatten_3[0][0]']           \n\n batch_normalization_29 (Ba  (None, 512)                  2048      ['dense_6[0][0]']             \n tchNormalization)                                                                                \n\n activation_19 (Activation)  (None, 512)                  0         ['batch_normalization_29[0][0]\n                                                                    ']                            \n\n dense_7 (Dense)             (None, 10)                   5130      ['activation_19[0][0]']       \n\n==================================================================================================\nTotal params: 8695818 (33.17 MB)\nTrainable params: 8693642 (33.16 MB)\nNon-trainable params: 2176 (8.50 KB) \n```"]