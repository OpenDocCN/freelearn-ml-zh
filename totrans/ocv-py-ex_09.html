<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Object Tracking</h1></div></div></div><p>In this chapter, we are going to learn about tracking an object in a live video. We will discuss the different characteristics that can be used to track an object. We will also learn about the different methods and techniques for object tracking.</p><p>By the end of this chapter, you will know:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to use frame differencing</li><li class="listitem" style="list-style-type: disc">How to use colorspaces to track colored objects</li><li class="listitem" style="list-style-type: disc">How to build an interactive object tracker</li><li class="listitem" style="list-style-type: disc">How to build a feature tracker</li><li class="listitem" style="list-style-type: disc">How to build a video surveillance system</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Frame differencing</h1></div></div></div><p>This is, possibly, the simplest<a id="id287" class="indexterm"/> technique we can use to see what parts of the video are moving. When we consider a live video stream, the difference between successive frames gives us a lot of information. The concept is fairly straightforward! We just take the difference between successive frames and display the differences.</p><p>If I move my laptop rapidly from left to right, we will see something like this:</p><div><img src="img/B04554_09_01.jpg" alt="Frame differencing"/></div><p>If I rapidly move the <a id="id288" class="indexterm"/>TV remote in my hand, it will look something like this:</p><div><img src="img/B04554_09_02.jpg" alt="Frame differencing"/></div><p>As you can see from the <a id="id289" class="indexterm"/>previous images, only the moving parts in the video get highlighted. This gives us a good starting point to see what areas are moving in the video. Here is the code to do this:</p><div><pre class="programlisting">import cv2

# Compute the frame difference
def frame_diff(prev_frame, cur_frame, next_frame):
    # Absolute difference between current frame and next frame
    diff_frames1 = cv2.absdiff(next_frame, cur_frame)

    # Absolute difference between current frame and # previous frame
    diff_frames2 = cv2.absdiff(cur_frame, prev_frame)

    # Return the result of bitwise 'AND' between the # above two resultant images
    return cv2.bitwise_and(diff_frames1, diff_frames2)

# Capture the frame from webcam
def get_frame(cap):
    # Capture the frame
    ret, frame = cap.read()

    # Resize the image
    frame = cv2.resize(frame, None, fx=scaling_factor,
            fy=scaling_factor, interpolation=cv2.INTER_AREA)

    # Return the grayscale image
    return cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)

if __name__=='__main__':
    cap = cv2.VideoCapture(0)
    scaling_factor = 0.5

    prev_frame = get_frame(cap)
    cur_frame = get_frame(cap)
    next_frame = get_frame(cap)

    # Iterate until the user presses the ESC key
    while True:
        # Display the result of frame differencing
        cv2.imshow("Object Movement", frame_diff(prev_frame, cur_frame, next_frame))

        # Update the variables
        prev_frame = cur_frame
        cur_frame = next_frame
        next_frame = get_frame(cap)

        # Check if the user pressed ESC
        key = cv2.waitKey(10)
        if key == 27:
            break

    cv2.destroyAllWindows()</pre></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Colorspace based tracking</h1></div></div></div><p>Frame differencing<a id="id290" class="indexterm"/> gives us some useful information, but <a id="id291" class="indexterm"/>we cannot use it to build anything meaningful. In order to build a good object tracker, we need to understand what characteristics can be used to make our tracking robust and accurate. So, let's take a step in that direction and see how we can use <strong>colorspaces</strong><a id="id292" class="indexterm"/> to come up with a good tracker. As we have discussed in previous chapters, HSVcolorspace<a id="id293" class="indexterm"/> is very informative when it comes to human perception. We can convert an image to the HSV space, and then use <code class="literal">colorspacethresholding</code> to track a given object.</p><p>Consider the following frame in the video:</p><div><img src="img/B04554_09_03.jpg" alt="Colorspace based tracking"/></div><p>If you run it through the colorspace filter and track the object, you will see something like this:</p><div><img src="img/B04554_09_04.jpg" alt="Colorspace based tracking"/></div><p>As we can see here, our<a id="id294" class="indexterm"/> tracker recognizes a particular object <a id="id295" class="indexterm"/>in the video, based on the color characteristics. In order to use this tracker, we need to know the color distribution of our target object. Following is the code:</p><div><pre class="programlisting">import cv2
import numpy as np

# Capture the input frame from webcam
def get_frame(cap, scaling_factor):
    # Capture the frame from video capture object
    ret, frame = cap.read()

    # Resize the input frame
    frame = cv2.resize(frame, None, fx=scaling_factor,
            fy=scaling_factor, interpolation=cv2.INTER_AREA)

    return frame

if __name__=='__main__':
    cap = cv2.VideoCapture(0)
    scaling_factor = 0.5

    # Iterate until the user presses ESC key
    while True:
        frame = get_frame(cap, scaling_factor)

        # Convert the HSV colorspace
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)

        # Define 'blue' range in HSV colorspace
        lower = np.array([60,100,100])
        upper = np.array([180,255,255])

        # Threshold the HSV image to get only blue color
        mask = cv2.inRange(hsv, lower, upper)

        # Bitwise-AND mask and original image
        res = cv2.bitwise_and(frame, frame, mask=mask)
        res = cv2.medianBlur(res, 5)

        cv2.imshow('Original image', frame)
        cv2.imshow('Color Detector', res)

        # Check if the user pressed ESC key
        c = cv2.waitKey(5)
        if c == 27:
            break

    cv2.destroyAllWindows()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Building an interactive object tracker</h1></div></div></div><p>Colorspace based <a id="id296" class="indexterm"/>tracker gives us the freedom to track a colored object, but we are also constrained to a predefined color. What if we just want to pick an object at random? How do we build an object tracker that can learn the characteristics of the selected object and just track it automatically? This is where the <a id="id297" class="indexterm"/><strong>CAMShift</strong> algorithm, which stands for Continuously Adaptive Meanshift, comes into the picture. It's basically an improved version of the <a id="id298" class="indexterm"/><strong>Meanshift</strong> algorithm.</p><p>The concept of Meanshift is actually nice and simple. Let's say we select a region of interest and we want our object tracker to track that object. In that region, we select a bunch of points based on the color histogram and compute the centroid. If the centroid lies at the center of this region, we know that the object hasn't moved. But if the centroid is not at the center of this region, then we know that the object is moving in some direction. The movement of the centroid controls the direction in which the object is moving. So, we move our bounding box to a new location so that the new centroid becomes the center of this bounding box. Hence, this algorithm is called Meanshift, because the mean (i.e. the centroid) is shifting. This way, we keep ourselves updated with the current location of the object.</p><p>But the problem with Meanshift is that the size of the bounding box is not allowed to change. When you move the object away from the camera, the object will appear smaller to the human eye, but Meanshift will not take this into account. The size of the bounding box will remain the same throughout the tracking session. Hence, we need to use CAMShift. The advantage of CAMShift is that it can adapt the size of the bounding box to the size of the object. Along with that, it can also keep track of the orientation of the object.</p><p>Let's consider the following<a id="id299" class="indexterm"/> frame in which the object is highlighted in orange (the box in my hand):</p><div><img src="img/B04554_09_05.jpg" alt="Building an interactive object tracker"/></div><p>Now that we have selected the object, the algorithm computes the histogram <code class="literal">backprojection</code> and extracts all the information. Let's move the object and see how it's getting tracked:</p><div><img src="img/B04554_09_06.jpg" alt="Building an interactive object tracker"/></div><p>Looks like the object is <a id="id300" class="indexterm"/>getting tracked fairly well. Let's change the orientation and see if the tracking is maintained:</p><div><img src="img/B04554_09_07.jpg" alt="Building an interactive object tracker"/></div><p>As we can see, the bounding ellipse has changed its location as well as its orientation. Let's change the perspective of the object and see if it's still able to track it:</p><div><img src="img/B04554_09_08.jpg" alt="Building an interactive object tracker"/></div><p>We are still good! The <a id="id301" class="indexterm"/>bounding ellipse has changed the aspect ratio to reflect the fact that the object looks skewed now (because of the perspective transformation).</p><p>Following is the code:</p><div><pre class="programlisting">import sys

import cv2
import numpy as np

class ObjectTracker(object):
    def __init__(self):
        # Initialize the video capture object
        # 0 -&gt; indicates that frame should be captured
        # from webcam
        self.cap = cv2.VideoCapture(0)

        # Capture the frame from the webcam
        ret, self.frame = self.cap.read()

        # Downsampling factor for the input frame
        self.scaling_factor = 0.5
        self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor,
                    fy=self.scaling_factor, interpolation=cv2.INTER_AREA)

        cv2.namedWindow('Object Tracker')
        cv2.setMouseCallback('Object Tracker', self.mouse_event)

        self.selection = None
        self.drag_start = None
        self.tracking_state = 0

    # Method to track mouse events
    def mouse_event(self, event, x, y, flags, param):
        x, y = np.int16([x, y])

        # Detecting the mouse button down event
        if event == cv2.EVENT_LBUTTONDOWN:
            self.drag_start = (x, y)
            self.tracking_state = 0

        if self.drag_start:
            if flags &amp; cv2.EVENT_FLAG_LBUTTON:
                h, w = self.frame.shape[:2]
                xo, yo = self.drag_start
                x0, y0 = np.maximum(0, np.minimum([xo, yo], [x, y]))
                x1, y1 = np.minimum([w, h], np.maximum([xo, yo], [x, y]))
                self.selection = None

                if x1-x0 &gt; 0 and y1-y0 &gt; 0:
                    self.selection = (x0, y0, x1, y1)

            else:
                self.drag_start = None
                if self.selection is not None:
                    self.tracking_state = 1

    # Method to start tracking the object
    def start_tracking(self):
        # Iterate until the user presses the Esc key
        while True:
            # Capture the frame from webcam
            ret, self.frame = self.cap.read()
            # Resize the input frame
            self.frame = cv2.resize(self.frame, None, fx=self.scaling_factor,
                        fy=self.scaling_factor, interpolation=cv2.INTER_AREA)

            vis = self.frame.copy()

            # Convert to HSV colorspace
            hsv = cv2.cvtColor(self.frame, cv2.COLOR_BGR2HSV)

            # Create the mask based on predefined thresholds.
            mask = cv2.inRange(hsv, np.array((0., 60., 32.)),
                        np.array((180., 255., 255.)))

            if self.selection:
                x0, y0, x1, y1 = self.selection
                self.track_window = (x0, y0, x1-x0, y1-y0)
                hsv_roi = hsv[y0:y1, x0:x1]
                mask_roi = mask[y0:y1, x0:x1]

                # Compute the histogram
                hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )

                # Normalize and reshape the histogram
                cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX);
                self.hist = hist.reshape(-1)

                vis_roi = vis[y0:y1, x0:x1]
                cv2.bitwise_not(vis_roi, vis_roi)
                vis[mask == 0] = 0

            if self.tracking_state == 1:
                self.selection = None

                # Compute the histogram back projection
                prob = cv2.calcBackProject([hsv], [0], self.hist, [0, 180], 1)

                prob &amp;= mask
                term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )

                # Apply CAMShift on 'prob'
                track_box, self.track_window = cv2.CamShift(prob, self.track_window, term_crit)

                # Draw an ellipse around the object
                cv2.ellipse(vis, track_box, (0, 255, 0), 2)

            cv2.imshow('Object Tracker', vis)

            c = cv2.waitKey(5)
            if c == 27:
                break

        cv2.destroyAllWindows()

if __name__ == '__main__':
    ObjectTracker().start_tracking()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec77"/>Feature based tracking</h1></div></div></div><p>Feature based tracking<a id="id302" class="indexterm"/> refers<a id="id303" class="indexterm"/> to tracking individual feature points across successive frames in the video. We use a technique called <strong>optical flow</strong><a id="id304" class="indexterm"/> to track these features. Optical flow is one of the most popular techniques in computer vision. We choose a bunch of feature points and track them through the video stream.</p><p>When we detect the feature points, we compute the displacement vectors and show the motion of those keypoints between consecutive frames. These vectors are called motion vectors. There are many ways to do this, but the Lucas-Kanade method<a id="id305" class="indexterm"/> is perhaps the most popular of all these techniques. You can refer to their original paper <a id="id306" class="indexterm"/>at <a class="ulink" href="http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf">http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf</a>. We start the process by extracting the feature points. For each feature point, we create 3x3 patches with the feature point in the center. The assumption here is that all the points within each patch will have a similar motion. We can adjust the size of this window depending on the problem at hand.</p><p>For each feature point in<a id="id307" class="indexterm"/> the current frame, we take the surrounding 3x3 patch as our reference point. For this patch, we look in its neighborhood in the previous frame to get the best match. This neighborhood is usually bigger than 3x3 because we want to get the patch that's closest to the patch under consideration. Now, the path from the center pixel of the matched patch in the previous frame to the center pixel of the patch under consideration in the current frame will become the motion vector. We do that for all the feature points and extract all the motion vectors.</p><p>Let's consider the following frame:</p><div><img src="img/B04554_09_09.jpg" alt="Feature based tracking"/></div><p>If I move in a horizontal direction, you will see the motion vectors in a horizontal direction:</p><div><img src="img/B04554_09_10.jpg" alt="Feature based tracking"/></div><p>If I move away from the <a id="id308" class="indexterm"/>webcam, you will see something like this:</p><div><img src="img/B04554_09_11.jpg" alt="Feature based tracking"/></div><p>So, if you want to play around with it, you can let the user select a region of interest in the input video (like we did earlier). You can then extract feature points from this region of interest and track the object by drawing the bounding box. It will be a fun exercise!</p><p>Here is the code to perform <a id="id309" class="indexterm"/>optical flow based tracking:</p><div><pre class="programlisting">import cv2
import numpy as np

def start_tracking():
    # Capture the input frame
    cap = cv2.VideoCapture(0)

    # Downsampling factor for the image
    scaling_factor = 0.5

    # Number of frames to keep in the buffer when you
    # are tracking. If you increase this number,
    # feature points will have more "inertia"
    num_frames_to_track = 5

    # Skip every 'n' frames. This is just to increase the speed.
    num_frames_jump = 2

    tracking_paths = []
    frame_index = 0

    # 'winSize' refers to the size of each patch. These patches
    # are the smallest blocks on which we operate and track
    # the feature points. You can read more about the parameters
    # here: http://goo.gl/ulwqLk
    tracking_params = dict(winSize  = (11, 11), maxLevel = 2,
            criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))

    # Iterate until the user presses the ESC key
    while True:
        # read the input frame
        ret, frame = cap.read()

        # downsample the input frame
        frame = cv2.resize(frame, None, fx=scaling_factor,
                fy=scaling_factor, interpolation=cv2.INTER_AREA)

        frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        output_img = frame.copy()

        if len(tracking_paths) &gt; 0:
            prev_img, current_img = prev_gray, frame_gray
            feature_points_0 = np.float32([tp[-1] for tp in tracking_paths]).reshape(-1, 1, 2)

            # Compute feature points using optical flow. You can
            # refer to the documentation to learn more about the
            # parameters here: http://goo.gl/t6P4SE
            feature_points_1, _, _ = cv2.calcOpticalFlowPyrLK(prev_img, current_img, feature_points_0,
                    None, **tracking_params)
            feature_points_0_rev, _, _ = cv2.calcOpticalFlowPyrLK(current_img, prev_img, feature_points_1,
                    None, **tracking_params)

            # Compute the difference of the feature points
            diff_feature_points = abs(feature_points_0- feature_points_0_rev).reshape(-1, 2).max(-1)

            # threshold and keep the good points
            good_points = diff_feature_points &lt; 1

            new_tracking_paths = []

            for tp, (x, y), good_points_flag in zip(tracking_paths,
                        feature_points_1.reshape(-1, 2), good_points):
                if not good_points_flag:
                    continue

                tp.append((x, y))

                # Using the queue structure i.e. first in,
                # first out
                if len(tp) &gt; num_frames_to_track:
                    del tp[0]

                new_tracking_paths.append(tp)

                # draw green circles on top of the output image
                cv2.circle(output_img, (x, y), 3, (0, 255, 0), -1)

            tracking_paths = new_tracking_paths

            # draw green lines on top of the output image
            cv2.polylines(output_img, [np.int32(tp) for tp in tracking_paths], False, (0, 150, 0))

        # 'if' condition to skip every 'n'th frame
        if not frame_index % num_frames_jump:
            mask = np.zeros_like(frame_gray)
            mask[:] = 255
            for x, y in [np.int32(tp[-1]) for tp in tracking_paths]:
                cv2.circle(mask, (x, y), 6, 0, -1)

            # Extract good features to track. You can learn more
            # about the parameters here: http://goo.gl/BI2Kml
            feature_points = cv2.goodFeaturesToTrack(frame_gray,
                    mask = mask, maxCorners = 500, qualityLevel = 0.3,
                    minDistance = 7, blockSize = 7)

            if feature_points is not None:
                for x, y in np.float32(feature_points).reshape (-1, 2):
                    tracking_paths.append([(x, y)])

        frame_index += 1
        prev_gray = frame_gray

        cv2.imshow('Optical Flow', output_img)

        # Check if the user pressed the ESC key
        c = cv2.waitKey(1)
        if c == 27:
            break

if __name__ == '__main__':
    start_tracking()
    cv2.destroyAllWindows()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec78"/>Background subtraction</h1></div></div></div><p>Background subtraction<a id="id310" class="indexterm"/> is very useful in video surveillance. Basically, background subtraction technique performs really well for cases where we have to <a id="id311" class="indexterm"/>detect moving objects in a static scene. As the name indicates, this algorithm works by detecting the background and subtracting it from the current frame to obtain the foreground, that is, moving objects. In order to detect moving objects, we need to build a model of the background first. This is not the same as frame differencing because we are actually modeling the background and using this model to detect moving objects. So, this performs much better than the simple frame differencing technique. This technique tries to detect static parts in the scene and then include it in the background model. So, it's an adaptive technique that can adjust according to the scene.</p><p>Let's consider the following image:</p><div><img src="img/B04554_09_12.jpg" alt="Background subtraction"/></div><p>Now, as we gather more frames in this scene, every part of the image will gradually become a part of the background model. This is what we discussed earlier as well. If a scene is static, the model adapts itself to make sure the background model is updated. This is how it looks in the beginning:</p><div><img src="img/B04554_09_13.jpg" alt="Background subtraction"/></div><p>Notice how a part of <a id="id312" class="indexterm"/>my face has already become a part of the<a id="id313" class="indexterm"/> background model (the blackened region). The following screenshot shows what we'll see after a few seconds:</p><div><img src="img/B04554_09_14.jpg" alt="Background subtraction"/></div><p>If we keep going, everything<a id="id314" class="indexterm"/> eventually becomes part of the <a id="id315" class="indexterm"/>background model:</p><div><img src="img/B04554_09_15.jpg" alt="Background subtraction"/></div><p>Now, if we introduce a new <a id="id316" class="indexterm"/>moving object, it will be detected clearly, as <a id="id317" class="indexterm"/>shown next:</p><div><img src="img/B04554_09_16.jpg" alt="Background subtraction"/></div><p>Here is the <a id="id318" class="indexterm"/>code <a id="id319" class="indexterm"/>to do this:</p><div><pre class="programlisting">import cv2
import numpy as np

# Capture the input frame
def get_frame(cap, scaling_factor=0.5):
    ret, frame = cap.read()

    # Resize the frame
    frame = cv2.resize(frame, None, fx=scaling_factor,
            fy=scaling_factor, interpolation=cv2.INTER_AREA)

    return frame

if __name__=='__main__':
    # Initialize the video capture object
    cap = cv2.VideoCapture(0)

    # Create the background subtractor object
    bgSubtractor = cv2.BackgroundSubtractorMOG()

    # This factor controls the learning rate of the algorithm.
    # The learning rate refers to the rate at which your model
    # will learn about the background. Higher value for
    # 'history' indicates a slower learning rate. You
    # can play with this parameter to see how it affects
    # the output.
    history = 100

    # Iterate until the user presses the ESC key
    while True:
        frame = get_frame(cap, 0.5)

        # Apply the background subtraction model to the # input frame
        mask = bgSubtractor.apply(frame, learningRate=1.0/history)

        # Convert from grayscale to 3-channel RGB
        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)

        cv2.imshow('Input frame', frame)
        cv2.imshow('Moving Objects', mask &amp; frame)

        # Check if the user pressed the ESC key
        c = cv2.waitKey(10)
        if c == 27:
            break

    cap.release()
    cv2.destroyAllWindows()</pre></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec79"/>Summary</h1></div></div></div><p>In this chapter, we learned about object tracking. We learned how to get motion information using frame differencing, and how it can be limiting when we want to track different types of objects. We learned about colorspacethresholding and how it can be used to track colored objects. We discussed clustering techniques for object tracking and how we can build an interactive object tracker using the CAMShift algorithm. We discussed how to track features in a video and how we can use optical flow to achieve the same. We learned about background subtraction and how it can be used for video surveillance.</p><p>In the next chapter, we are going to discuss object recognition, and how we can build a visual search engine.</p></div></div>
</body></html>