<html><head></head><body>
		<div id="_idContainer092">
			<h1 id="_idParaDest-100"><em class="italic"><a id="_idTextAnchor099"/>Chapter 7</em>: Hosting ML Models in the Cloud: Best Practices</h1>
			<p>After you've successfully trained a model, you want to make the model available for inference, don't you? ML models are often the product of a business that is ML-driven. Your customers consume the ML prediction from your model, not your training jobs or processed data. How do you provide a satisfying customer experience, starting with a good experience with your ML models?</p>
			<p>SageMaker has several options for ML hosting and inferencing, depending on your use case. Options are welcomed in many aspects of life, but it can be difficult to find the best option. This chapter will help you understand how to host models for batch inference and for online real-time inference, how to use multi-model endpoints to save costs, and how to conduct resource optimization for your inference needs.</p>
			<p>In this chapter, we will be covering the following topics:</p>
			<ul>
				<li>Deploying models in the cloud after training</li>
				<li>Inferencing in batches with batch transform</li>
				<li>Hosting real-time endpoints</li>
				<li>Optimizing your model deployment</li>
			</ul>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Technical requirements</h1>
			<p>For this chapter, you need to access the code at <a href="https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07">https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07</a>. If you did not run the notebooks in the previous chapter, please run the <a href="http://chapter05/02-tensorflow_sentiment_analysis.ipynb">chapter05/02-tensorflow_sentiment_analysis.ipynb</a> file from the repository before proceeding.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor101"/>Deploying models in the cloud after training</h1>
			<p>ML models <a id="_idIndexMarker441"/>can primarily <a id="_idIndexMarker442"/>be consumed in the cloud in two <a id="_idIndexMarker443"/>ways, <strong class="bold">batch inference</strong> and <strong class="bold">live inference</strong>. Batch inference refers to model inference performed on data that is in batches, often <a id="_idIndexMarker444"/>large batches, and asynchronous in nature. It fits use cases that collect data infrequently, that focus on group statistics rather than individual inference, and that do not need to have inference results right away for downstream processes. Projects that are research oriented, for example, do not require model inference to be returned for a data point right away. Researchers often collect a chunk of data for testing and evaluation purposes and care about overall statistics and performance rather than individual predictions. They can conduct the inference in batches and wait for the prediction for the whole batch to complete before they move on.</p>
			<p>Live inference, on the other hand, refers to model inference performed in real time. It is expected that the inference result for an incoming data point is returned immediately so that it can be used for subsequent decision-making processes. For example, an interactive chatbot would require a live inference capability to support such a service. No one would want to wait until the end of the conversation to get responses from the chatbot model, nor would people want to wait for more than even a couple of seconds. Companies looking to provide the best customer experience would want an inference to be made and returned to the customer instantly. </p>
			<p>Given the different requirements, the architecture and deployment choices also differ between batch inference and live inference. Amazon SageMaker has it covered as it provides <a id="_idIndexMarker445"/>various fully managed options for your inference use cases. <strong class="bold">SageMaker batch transform</strong> is designed to perform batch inference at scale and is cost-effective as the compute infrastructure is fully managed and is de-provisioned <a id="_idIndexMarker446"/>when your inference job is complete. <strong class="bold">SageMaker real-time endpoints</strong> aim to provide a robust live hosting option for your ML use cases. Both the SageMaker hosting options are fully managed, meaning you do not have to worry much about the cloud infrastructure. </p>
			<p>Let's first take a look at SageMaker batch transform, how it works, and when to use it.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor102"/>Inferencing in batches with batch transform</h1>
			<p>SageMaker batch transform is designed to provide offline inference for large datasets. Depending <a id="_idIndexMarker447"/>on how you organize the data, SageMaker batch transform can split a single large text file in S3 by lines into a small and manageable size (mini-batch) that would fit into the memory before making inference against the model; it can also distribute the files by S3 key into compute instances for efficient computation. For example, it could send <strong class="source-inline">test1.csv</strong> to instance 1 and <strong class="source-inline">test2.csv</strong> to instance 2.</p>
			<p>To demonstrate SageMaker batch transform, we can pick up from our training example in the previous chapter. In <a href="B17447_06_ePub_RK.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a>, <em class="italic">Detecting ML Bias and Explaining Models with SageMaker Clarify</em>, we showed you how to train a TensorFlow model using SageMaker managed training for a movie review sentiment prediction use case in <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb.</strong> We can deploy the trained model to make a batch inference using SageMaker batch transform in the following steps:</p>
			<ol>
				<li>Please open the <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter07/01-tensorflow_sentiment_analysis_batch_transform.ipynb</strong> notebook and use the <strong class="bold">Python 3 </strong>(<strong class="bold">TensorFlow 2.3 Python 3.7 CPU Optimized</strong>) kernel.</li>
				<li>Run the first three cells to set up the SageMaker SDK, import the libraries, and prepare the test dataset. There are 25,000 documents in the test dataset. We save the test data as a CSV file and upload the CSV file to our S3 bucket. The file is 27 MB.<p class="callout-heading">Note</p><p class="callout">SageMaker batch transform expects the input CSV files to <em class="italic">not</em> contain headers. That is, the first row of the CSV should be the first data point.</p></li>
				<li>We retrieve the training TensorFlow estimator from a training job we did in <a href="B17447_06_ePub_RK.xhtml#_idTextAnchor090"><em class="italic">Chapter 6</em></a>, <em class="italic">Detecting ML Bias and Explaining Models with SageMaker Clarify</em>. We need to grab the training job name for the <strong class="source-inline">TensorFlow.attach()</strong> method. You can find it in <strong class="bold">Experiments and trials</strong> in the left sidebar, as shown in <em class="italic">Figure 7.1</em>, thanks to the <a id="_idIndexMarker448"/>experiments we used when training. In <strong class="bold">Experiments and trials</strong>, left-click on <strong class="bold">imdb-sentiment-analysis</strong> and you should see your training job as a trial in the list.</li>
			</ol>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B17447_07_01.jpg" alt="Figure 7.1 – Obtaining training job name in Experiments and trials&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Obtaining training job name in Experiments and trials</p>
			<p>You should replace <strong class="source-inline">training_job_name</strong> in the following code with your own:</p>
			<p class="source-code">from sagemaker.tensorflow import TensorFlow</p>
			<p class="source-code">training_job_name='&lt;your-training-job-name&gt;'</p>
			<p class="source-code">estimator = TensorFlow.attach(training_job_name)</p>
			<p>Once you have replaced <strong class="source-inline">training_job_name</strong> and attached it to reload <strong class="source-inline">estimator</strong>, you should see the history of the job printed in the output.</p>
			<ol>
				<li value="4">To run SageMaker batch transform, you only need two lines of SageMaker API code:<p class="source-code">transformer = estimator.transformer(instance_count=1, </p><p class="source-code">                                    instance_type='ml.c5.xlarge',</p><p class="source-code">                                    max_payload = 2, # MB</p><p class="source-code">                                    accept = 'application/jsonlines',</p><p class="source-code">                                    output_path = s3_output_location,</p><p class="source-code">                                    assemble_with = 'Line')</p><p class="source-code">transformer.transform(test_data_s3, </p><p class="source-code">                      content_type='text/csv', </p><p class="source-code">                      split_type = 'Line', </p><p class="source-code">                      job_name = jobname,</p><p class="source-code">                      experiment_config = experiment_config)</p></li>
			</ol>
			<p>The <strong class="source-inline">estimator.transformer()</strong> method creates a <strong class="source-inline">Transformer</strong> object with the compute resource desired for the inference. Here we request one <strong class="source-inline">ml.c5.xlarge</strong> instance for predicting 25,000 movie reviews. The <strong class="source-inline">max_payload</strong> argument allows us to control the size of each mini-batch that SageMaker Batch Transform is splitting. The <strong class="source-inline">accept</strong> argument determines the output type. SageMaker managed Tensorflow serving container supports '<strong class="source-inline">application/json</strong>', and 'a<strong class="source-inline">pplication/jsonlines</strong>'. <strong class="source-inline">assemble_with</strong> controls how you assemble the inference results that are in mini-batches. Then we provide the S3 location of the test data (<strong class="source-inline">test_data_s3</strong>) in the <strong class="source-inline">transformer.transform()</strong>, and indicate that the input content type to be of '<strong class="source-inline">text/csv</strong>' as the file is of CSV format. <strong class="source-inline">split_type</strong> determines how the input files will be split by SageMaker Batch Transform into mini-batch. We put in a unique job name and SageMaker Experiments configuration so that we can track the inference to the associated training job in the same trial. The Batch Transform job would take around 5 minutes to complete. Like a training job, SageMaker manages the provisioning, computation, and de-provisioning of the instances once the job finishes.</p>
			<ol>
				<li value="5">After the job completes, we should take a look at the result. SageMaker batch transform saves the results after assembly to the specified S3 location with <strong class="source-inline">.out</strong> appended to the input filename. You can access the full S3 path in <strong class="source-inline">transformer.output_path</strong> attribute. SageMaker uses TensorFlow Serving, a model serving framework developed by TensorFlow, for model serving, the model output is written in JSON format. The output has the sentiment probabilities in an array with predictions as the JSON key. We can inspect the batch transform results with the following code: <p class="source-code">output = transformer.output_path</p><p class="source-code">output_prefix = 'imdb_data/test_output'</p><p class="source-code">!mkdir -p {output_prefix}</p><p class="source-code">!aws s3 cp --recursive {output} {output_prefix}</p><p class="source-code">!head {output_prefix}/{csv_test_filename}.out</p><p class="source-code"><strong class="bold">{    "predictions": [[0.00371244829], [1.0], [1.0], [0.400452465], [1.0], [1.0], [0.163813606], [0.10115058], [0.793149233], [1.0], [1.0], [6.37737814e-14], [2.10463966e-08], [0.400452465], [1.0], [0.0], [1.0], [0.400452465], [2.65155926e-29], [4.04420768e-11], ……]}</strong></p></li>
			</ol>
			<p>We then collect all 25,000 predictions into a <strong class="source-inline">results</strong> variable: </p>
			<p class="source-code">results=[]</p>
			<p class="source-code">with open(f'{output_prefix}/{csv_test_filename}.out', 'r') as f:</p>
			<p class="source-code">    lines = f.readlines()</p>
			<p class="source-code">    for line in lines:</p>
			<p class="source-code">        print(line)</p>
			<p class="source-code">        json_output = json.loads(line)</p>
			<p class="source-code">        result = [float('%.3f'%(item)) for sublist in json_output['predictions'] </p>
			<p class="source-code">                                       for item in sublist]</p>
			<p class="source-code">        results += result</p>
			<p class="source-code">print(results)</p>
			<ol>
				<li value="6">The rest of the notebook displays one original movie review, the predicted sentiment, and the corresponding ground truth sentiment. The model returns the probabilities of the reviews being positive or negative. We take a <strong class="source-inline">0.5</strong> threshold and mark probabilities over the threshold to be positive and below <strong class="source-inline">0.5</strong> to be negative.</li>
				<li>As we logged the batch transform job in the same trial as the training job, we can find it easily in <strong class="bold">Experiments and trials</strong> in the left sidebar, as shown in <em class="italic">Figure 7.2</em>. You can see more information about this batch transform job in this entry.<div id="_idContainer085" class="IMG---Figure"><img src="image/B17447_07_02.jpg" alt="Figure 7.2 – The batch transform job is logged as a trial component alongside the training component&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 7.2 – The batch transform job is logged as a trial component alongside the training component</p>
			<p>That's <a id="_idIndexMarker449"/>how easy it is to make use of SageMaker batch transform to generate inferences on a large dataset. You may wonder, why can't I just use the notebook to make inferences? What's the benefit of using SageMaker batch transform? Yes, you can use the notebook for quick analysis. The advantages <a id="_idIndexMarker450"/>of SageMaker batch transform are as follows:</p>
			<ul>
				<li>Fully managed mini-batching helps make inferences on a large dataset efficiently.</li>
				<li>You can use a separate SageMaker-managed compute infrastructure that is different from your notebook instance. You can easily run prediction with a cluster of instances for faster prediction.</li>
				<li>You only <a id="_idIndexMarker451"/>pay for the runtime of a batch transform job, even with a much larger compute cluster.</li>
				<li>You can schedule and kick off a model prediction independently in the cloud with SageMaker batch transform. It is not necessary to use a Python notebook in SageMaker Studio to start a prediction job.</li>
			</ul>
			<p>Next, let's see how we can host ML models in the cloud for real-time use cases.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Hosting real-time endpoints</h1>
			<p>SageMaker real-time inference is a fully managed feature for hosting your model(s) on compute <a id="_idIndexMarker452"/>instance(s) for real-time low-latency inference. The deployment process consists of the following steps:</p>
			<ol>
				<li value="1">Create a model, container, and associated inference code in SageMaker. The model refers to the training artifact, <strong class="source-inline">model.tar.gz</strong>. The container is the runtime environment for the code and the model.</li>
				<li>Create an HTTPS endpoint configuration. This configuration carries information about compute instance type and quantity, models, and traffic patterns to model variants.</li>
				<li>Create ML instances and an HTTPS endpoint. SageMaker creates a fleet of ML instances and an HTTPS endpoint that handles the traffic and authentication. The final step is to put everything together for a working HTTPS endpoint that can interact with client-side requests.</li>
			</ol>
			<p>Hosting a real-time endpoint faces one particular challenge that is common when hosting a website or a web application: it can be difficult to scale your compute instances when you have a spike in traffic to your endpoint. You may have 1,000 customers visiting your website per minute in a particular hour and then have 100,000 customers in the next hour. If you only deploy one instance behind your endpoint that is capable of handling 5,000 requests per minute, it would work well in the first hour and would struggle in the next. Autoscaling is a technique in the cloud to help you scale out instances automatically when certain criteria are met so that your application can handle the load at any time.</p>
			<p>Let's walk through a SageMaker real-time endpoint example. Like the batch transform example, we continue the ML use case in <a href="B17447_05_ePub_RK.xhtml#_idTextAnchor077"><em class="italic">Chapter 5</em></a><em class="italic">, Building and Training ML Models with SageMaker Studio IDE</em> and 05/02-tensorflow_sentiment_analysis. ipynb. Please open the notebook in <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter07/02-tensorflow_sentiment_analysis_inference.ipynb</strong> and use the <strong class="bold">Python 3</strong> (<strong class="bold">TensorFlow 2.3 Python 3.7 CPU Optimized</strong>) kernel. We will deploy a trained model to <a id="_idIndexMarker453"/>SageMaker as a real-time endpoint, make some predictions as an example, and finally apply an autoscaling policy to help scale the compute instances behind the endpoint. Please follow these steps:</p>
			<ol>
				<li value="1">In the first four cells, we set up the SageMaker session, load the Python libraries, load the test data that we created in <strong class="source-inline">01-tensorflow_sentiment_analysis_batch_transform.ipynb</strong>, and retrieve the training job that we trained previously using its name. </li>
				<li>We then deploy the model to an endpoint:<p class="source-code">predictor = estimator.deploy(</p><p class="source-code">                 instance_type='ml.c5.xlarge',</p><p class="source-code">                 initial_instance_count=1)</p></li>
			</ol>
			<p>Here, we choose <strong class="source-inline">ml.c5.xlarge</strong> for <strong class="source-inline">instance_type</strong> argument. <strong class="source-inline">initial_instance_ count</strong> argument refers to the number of ML instances behind the endpoint when we make this call. Later, we will show you how to use the autoscaling feature, which is designed to help us scale out the instance fleet when the initial settings become insufficient. The deployment process takes about 5 minutes.</p>
			<ol>
				<li value="3">We can test the endpoint with some sample data. The TensorFlow Serving framework in the container handles the data interface and takes the NumPy array as input so we can pass an entry into the model directly. We can get a response from the endpoint in JSON format, which gets converted to a dictionary in Python in the <strong class="source-inline">prediction</strong> variable:<p class="source-code">prediction=predictor.predict(x_test[data_index])</p><p class="source-code">print(prediction)</p><p class="source-code"><strong class="bold">{'predictions': [[1.80986511e-11]]}</strong></p></li>
			</ol>
			<p>The next two cells retrieve the review in text and print out the ground truth sentiment and the predicted sentiment with a threshold of 0.5, just like in the batch transform example.</p>
			<ol>
				<li value="4">(Optional) You <a id="_idIndexMarker454"/>may be wondering: Can I ask the endpoint to predict the entire <strong class="source-inline">x_test</strong> of 25,000 data points? To find out, feel free to try out the following line:<p class="source-code">predictor.predict(x_test)</p></li>
			</ol>
			<p>This line will run for a couple of seconds and eventually fail. This is because a SageMaker endpoint is designed to take on requests that are 6 MB in size one at a time. You can request inferences for multiple data points, for example, <strong class="source-inline">x_test[:100]</strong>, but not 25,000 all in one call. In contrast, batch transform does the data splitting (mini-batching) automatically and is better suited to handle large datasets.</p>
			<ol>
				<li value="5">Next, we can apply SageMaker's autoscaling feature to this endpoint using the <strong class="source-inline">application-autoscaling</strong> client from the <strong class="source-inline">boto3</strong> SDK:<p class="source-code">sagemaker_client = sess.boto_session.client('sagemaker')</p><p class="source-code">autoscaling_client = sess.boto_session.client('application-autoscaling')</p></li>
				<li>It is a two-step process to configure autoscaling for computing instances in AWS. First, we run <strong class="source-inline">autoscaling_client.register_scalable_target()</strong> to register the target with the desired minimum/maximum capacity for our SageMaker endpoint:<p class="source-code">resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic' </p><p class="source-code">response = autoscaling_client.register_scalable_target(</p><p class="source-code">   ServiceNamespace='sagemaker',</p><p class="source-code">   ResourceId=resource_id,</p><p class="source-code">ScalableDimension='sagemaker:variant:DesiredInstanceCount',</p><p class="source-code">   MinCapacity=1,</p><p class="source-code">   MaxCapacity=4)</p></li>
			</ol>
			<p>Our target, the SageMaker real-time endpoint, is denoted with <strong class="source-inline">resource_id</strong>. We set the minimum capacity to <strong class="source-inline">1</strong> and the maximum to <strong class="source-inline">4</strong>, meaning that when the load is at the lowest, there will be at least one instance running behind the endpoint. Our endpoint is capable of scaling out to four instances at the most. </p>
			<ol>
				<li value="7">Then <a id="_idIndexMarker455"/>we run <strong class="source-inline">autoscaling_client.put_scaling_policy()</strong> to instruct <em class="italic">how</em> we want to autoscale:<p class="source-code">response = autoscaling_client.put_scaling_policy(</p><p class="source-code">   PolicyName='Invocations-ScalingPolicy',</p><p class="source-code">   ServiceNamespace='sagemaker',</p><p class="source-code">   ResourceId=resource_id, </p><p class="source-code">   ScalableDimension='sagemaker:variant:DesiredInstanceCount', </p><p class="source-code">   PolicyType='TargetTrackingScaling', </p><p class="source-code">   TargetTrackingScalingPolicyConfiguration={</p><p class="source-code">       'TargetValue': 4000.0, </p><p class="source-code">       'PredefinedMetricSpecification': {</p><p class="source-code">          'PredefinedMetricType': </p><p class="source-code">             'SageMakerVariantInvocationsPerInstance'},</p><p class="source-code">        'ScaleInCooldown': 600, </p><p class="source-code">        'ScaleOutCooldown': 300})</p></li>
			</ol>
			<p>In this example, we employ a scaling strategy called <strong class="bold">target tracking scaling</strong>. Target <a id="_idIndexMarker456"/>tracking scaling aims to scale in and out the instances based on a specific target metric, such as instance CPU load, or the number of inference requests per instance per minute. We use the latter (<strong class="source-inline">SageMakerVariantInvocationsPerInstance</strong>) in this configuration to make sure each instance can share 4,000 requests per minute before scaling out another instance. <strong class="source-inline">ScaleInCooldown</strong> and <strong class="source-inline">ScaleOutCooldown</strong> refer to the period <a id="_idIndexMarker457"/>of time in seconds after the last scaling activity before autoscaling can scale in and out again. With our configuration, SageMaker will not scale in (remove an instance) within 600 seconds of the last scale-in activity, and will not scale out (add an instance) within 300 seconds of the last scale-out activity.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">There are <a id="_idIndexMarker458"/>two commonly <a id="_idIndexMarker459"/>used advanced scaling strategies for <strong class="source-inline">PolicyType</strong>: <strong class="bold">step scaling</strong> and <strong class="bold">scheduled scaling</strong>. In step scaling, you can define the number of instances to scale in/out based on the size of the alarm <a id="_idIndexMarker460"/>breaches of a certain metric. Read more about step scaling at <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html</a>. In scheduled scaling, you can set up the scaling based on the schedule. This is particularly useful if the traffic is predictable or has some seasonality. Read <a id="_idIndexMarker461"/>more about scheduled scaling at <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html</a>.</p>
			<ol>
				<li value="8">We can verify the configuration of the autoscaling policy with the following code:<p class="source-code">response = autoscaling_client.describe_scaling_policies(</p><p class="source-code">         ServiceNamespace='sagemaker')</p><p class="source-code">for i in response['ScalingPolicies']:</p><p class="source-code">    print('')</p><p class="source-code">    print(i['PolicyName'])</p><p class="source-code">    print('')</p><p class="source-code">    if('TargetTrackingScalingPolicyConfiguration' in i):</p><p class="source-code">        print(i['TargetTrackingS calingPolicyConfiguration']) </p><p class="source-code">    else:</p><p class="source-code">        print(i['StepScalingPolicyConfiguration'])</p><p class="source-code">    print('')</p><p class="source-code"><strong class="bold">Invocations-ScalingPolicy</strong></p><p class="source-code"><strong class="bold">{'TargetValue': 4000.0, 'PredefinedMetricSpecification': {'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'}, 'ScaleOutCooldown': 300, 'ScaleInCooldown': 600}</strong></p></li>
				<li>In <strong class="bold">Amazon SageMaker Studio</strong>, you can easily find the details of an endpoint in the <strong class="bold">Endpoints</strong> registry in the left sidebar, as shown in <em class="italic">Figure 7.3</em>. If you double-click <a id="_idIndexMarker462"/>on an endpoint, you can see more information in the main working area:</li>
			</ol>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B17447_07_03.jpg" alt="Figure 7.3 – Discovering endpoints in SageMaker Studio&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Discovering endpoints in SageMaker Studio</p>
			<p>The purpose of hosting an endpoint is to serve the ML models in the cloud so that you can integrate ML as a microservice into your applications or websites. Your model has to be available at all times as long as your main product or service is available. You can imagine <a id="_idIndexMarker463"/>that there is a great opportunity and incentive for you to optimize the deployment to minimize the cost while maintaining performance. We just learned how to deploy an ML model in the cloud; we should also learn how to optimize the deployment.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Optimizing your model deployment</h1>
			<p>Optimizing model deployment is a critical topic for businesses. No one wants to be spending a <a id="_idIndexMarker464"/>dime more than they need to. Because deployed endpoints are being used continuously, and incurring charges continuously, making sure that the deployment is optimized in terms of cost and runtime performance can save you a lot of money. SageMaker has several options to help you reduce costs while optimizing the runtime performance. In this section, we will be discussing multi-model endpoint deployment and how to choose the instance type and autoscaling policy for your use case.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Hosting multi-model endpoints to save costs</h2>
			<p>A multi-model endpoint is <a id="_idIndexMarker465"/>a type of real-time endpoint in SageMaker that allows multiple models to be deployed behind the same endpoint. There are <a id="_idIndexMarker466"/>many use cases in which you would build models for each customer or for each geographic area, and depending on the characteristics of the incoming data point, you would apply the corresponding ML model. Take the telecommunications churn prediction use case that we tackled in <a href="B17447_03_ePub_RK.xhtml#_idTextAnchor043"><em class="italic">Chapter 3</em></a>, <em class="italic">Data Preparation with SageMaker Data Wrangler,</em> as an example. We may get more accurate ML models if we train them by state because there may be regional differences in terms of competition among local telecommunication providers. And if we do train ML models for each US state, you can also easily imagine that the utilization of each model might not be completely equal. Actually, quite the contrary. </p>
			<p>Model utilization is inevitably proportional to the population of each state. Your New York model is going to be used more frequently than your Alaska model. In this scenario, if you host an endpoint for each state, you will have to pay for instances, even for the least utilized endpoint. With multi-model endpoints, SageMaker helps you reduce costs by reducing the number of endpoints needed for your use case. Let's take a look at how it works with the telecommunications churn prediction use case. Please open the <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter07/03-multimodel-endpoint.ipynb</strong> notebook with the Python 3 (Data Science) kernel and follow the next steps:</p>
			<ol>
				<li value="1">We define the SageMaker session, load up the Python libraries, and load the churn dataset in the first three cells.</li>
				<li>We do <a id="_idIndexMarker467"/>minimal preprocessing to convert the binary columns from strings to <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:<p class="source-code">df[["Int'l Plan", "VMail Plan"]] = df[["Int'l Plan", "VMail Plan"]].replace(to_replace=['yes', 'no'], value=[1, 0])</p><p class="source-code">df['Churn?'] = df['Churn?'].replace(to_replace=['True.', 'False.'], value=[1, 0])</p></li>
				<li>We leave out 10% of the data for ML inference later on:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">df_train, df_test = train_test_split(df_processed,</p><p class="source-code">         test_size=0.1, random_state=42, shuffle=True, </p><p class="source-code">         stratify=df_processed['State'])</p></li>
				<li>After the data is prepared, we set up our state-wise model training process in the function <strong class="source-inline">launch_training_job()</strong> with SageMaker Experiments integrated. The training algorithm we use is SageMaker's built-in XGBoost algorithm, which is fast and accurate for structural data like this. For binary classification, we use a <strong class="source-inline">binary:logtistic</strong> objective with <strong class="source-inline">num_round</strong> set to <strong class="source-inline">20</strong>:<p class="source-code">def launch_training_job(state, train_data_s3, val_data_s3):</p><p class="source-code">    ...</p><p class="source-code">    xgb = sagemaker.estimator.Estimator(image, role,</p><p class="source-code">          instance_count=train_instance_count,</p><p class="source-code">          instance_type=train_instance_type,</p><p class="source-code">          output_path=s3_output,</p><p class="source-code">          enable_sagemaker_metrics=True,</p><p class="source-code">          sagemaker_session=sess)</p><p class="source-code">    xgb.set_hyperparameters(</p><p class="source-code">          objective='binary:logistic',</p><p class="source-code">          num_round=20)</p><p class="source-code">    </p><p class="source-code">    ...    </p><p class="source-code">    xgb.fit(inputs=data_channels, </p><p class="source-code">            job_name=jobname, </p><p class="source-code">            experiment_config=experiment_config, </p><p class="source-code">            wait=False)</p><p class="source-code">    return xgb</p></li>
				<li>With <strong class="source-inline">launch_training_job()</strong>, we could easily create multiple training jobs in a <strong class="source-inline">for</strong> loop <a id="_idIndexMarker468"/>for states. For demonstration purposes, we only train five states in this example: <p class="source-code">dict_estimator = {}</p><p class="source-code">for state in df_processed.State.unique()[:5]:</p><p class="source-code">    print(state)</p><p class="source-code">    output_dir = f's3://{bucket}/{prefix}/{local_prefix}/by_state'</p><p class="source-code">    df_state = df_train[df_train['State']==state].drop(labels='State', axis=1)</p><p class="source-code">    df_state_train, df_state_val = train_test_split(df_state, test_size=0.1, random_state=42, </p><p class="source-code">                                                    shuffle=True, stratify=df_state['Churn?'])</p><p class="source-code">    </p><p class="source-code">    df_state_train.to_csv(f'{local_prefix}/churn_{state}_train.csv', index=False)</p><p class="source-code">    df_state_val.to_csv(f'{local_prefix}/churn_{state}_val.csv', index=False)</p><p class="source-code">    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_train.csv', output_dir)</p><p class="source-code">    sagemaker.s3.S3Uploader.upload(f'{local_prefix}/churn_{state}_val.csv', output_dir)</p><p class="source-code">    </p><p class="source-code">    dict_estimator[state] = launch_training_job(state, out_train_csv_s3, out_val_csv_s3)</p><p class="source-code">    time.sleep(2)</p></li>
			</ol>
			<p>Each training job should take no more than 5 minutes. We will wait for all of them to complete before proceeding to use the <strong class="source-inline">wait_for_training_job_to_complete()</strong> function.</p>
			<ol>
				<li value="6">After the <a id="_idIndexMarker469"/>training is done, we finally deploy our multi-model endpoint. It's a bit different to deploying a single model to an endpoint from a trained estimator object. We use the <strong class="source-inline">sagemaker.multidatamodel.MultiDataModel</strong> class for deployment:<p class="source-code">model_PA = dict_estimator['PA'].create_model(</p><p class="source-code">       role=role, image_uri=image)</p><p class="source-code">mme = MultiDataModel(name=model_name,               </p><p class="source-code">       model_data_prefix=model_data_prefix,</p><p class="source-code">       model=model_PA,</p><p class="source-code">       sagemaker_session=sess)</p></li>
			</ol>
			<p><strong class="source-inline">MultiDataModel</strong> initialization needs to understand the common model configuration, such as the container image and the network configurations, to configure the endpoint configuration. We pass in the model for <strong class="source-inline">PA</strong>. Afterward, we deploy the model to one <strong class="source-inline">ml.c5.xlarge</strong> instance and configure the <strong class="source-inline">serializer</strong> and <strong class="source-inline">deserializer</strong> to take CSV as input and produce JSON as output, respectively:</p>
			<p class="source-code">predictor = mme.deploy(</p>
			<p class="source-code">       initial_instance_count=hosting_instance_count, </p>
			<p class="source-code">       instance_type=hosting_instance_type, </p>
			<p class="source-code">       endpoint_name=endpoint_name,</p>
			<p class="source-code">       serializer = CSVSerializer(),</p>
			<p class="source-code">       deserializer = JSONDeserializer())</p>
			<ol>
				<li value="7">We can then dynamically add models to the endpoint. Note that at this time, there is no model deployed behind an endpoint:<p class="source-code">for state, est in dict_estimator.items():</p><p class="source-code">    artifact_path = est.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']</p><p class="source-code">    model_name = f'{state}.tar.gz'</p><p class="source-code">    mme.add_model(model_data_source=artifact_path, </p><p class="source-code">                  model_data_path=model_name)</p></li>
			</ol>
			<p>That's it. We can verify that there are five models associated with this endpoint:</p>
			<p class="source-code">list(mme.list_models())</p>
			<p class="source-code"><strong class="bold">['MO.tar.gz', 'PA.tar.gz', 'SC.tar.gz', 'VA.tar.gz', 'WY.tar.gz']</strong></p>
			<ol>
				<li value="8">We can <a id="_idIndexMarker470"/>test out the endpoint with some data points from each state. You can specify which model to make inference with using the <strong class="source-inline">target_model</strong> argument in <strong class="source-inline">predictor.predict()</strong>:<p class="source-code">state='PA'</p><p class="source-code">test_data=sample_test_data(state)</p><p class="source-code">prediction = predictor.predict(data=test_data[0], </p><p class="source-code">                               target_model=f'{state}.tar.gz')</p></li>
			</ol>
			<p>In this cell and onwards, we also set up a timer to measure the time it takes models for other states to respond in order to illustrate the nature of dynamic loading of the model from S3 to the endpoint. When the endpoint is first created, there is no model located behind the endpoint. With <strong class="source-inline">add_model()</strong>, it merely upload the models to an S3 location, <strong class="source-inline">model_data_prefix</strong>. When a model is first requested, SageMaker dynamically downloads the requested model from S3 to the ML instance and loads it into the inference container. This process has a longer response time when we first run the prediction for each of the state models, up to 1,000 milliseconds. But once the model is loaded into the memory in the container behind the endpoint, the response time is greatly reduced, to around 20 milliseconds. When a model is loaded, it is persisted in the container until the memory of the instance is exhausted by having too many models loaded at once. Then SageMaker unloads models that are not being used anymore from memory while still keeping <strong class="source-inline">model.tar.gz</strong> on disk in the instance for the next request to avoid downloading it from S3.</p>
			<p>In this <a id="_idIndexMarker471"/>example, we showed how to host a SageMaker multi-model endpoint that is flexible and cost-effective because it drastically reduces the number of endpoints needed for your use case. So, instead of hosting and paying for five endpoints, we would only host and pay for one endpoint. That's an easy 80% cost saving. With hosting models trained for 50 US states in 1 endpoint instead of 50, that's a 98% cost saving! </p>
			<p>With SageMaker multi-model endpoints, you can host as many models as you can in an S3 bucket location. The number of simultaneous models you can load in an endpoint depends on the memory footprint of your models and the amount of RAM on the compute instance. Multi-model endpoints are suitable for use cases where you have models that are built in the same framework (XGBoost in this example), and where it is tolerable to have latency on less frequently used models.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you have models built from different ML frameworks, for example, a mix of TensorFlow, PyTorch, and XGBoost models, you can use a multi-container endpoint, which allows hosting up to 15 distinct framework containers. Another benefit of multi-container <a id="_idIndexMarker472"/>endpoints is that they do not have latency penalties as all containers are running at the same time. Find out more at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html">https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html</a>.</p>
			<p>The other optimization approach is using a technique called load testing to help us choose the instance and autoscaling policy.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Optimizing instance type and autoscaling with load testing</h2>
			<p>Load testing is a <a id="_idIndexMarker473"/>technique that allows us to understand how our ML model hosted in an endpoint with a compute resource configuration responds to online traffic. There are factors such as model size, ML framework, number of CPUs, amount <a id="_idIndexMarker474"/>of RAM, autoscaling policy, and <a id="_idIndexMarker475"/>traffic size that affect how your ML model performs in the cloud. Understandably, it's not easy to predict how many requests can come to an endpoint over time. It is prudent to understand how your model and endpoint behave in this complex situation. Load testing creates artificial traffic and requests to your endpoint and stress tests how your model and endpoint respond in terms of model latency, instance CPU utilization, memory footprint, and so on.</p>
			<p>In this section, let's run some load testing against the endpoint we created in <strong class="source-inline">chapter07/02-tensorflow_sentiment_analysis_inference.ipynb</strong> with some scenarios. In the example, we hosted a TensorFlow-based model to an <strong class="source-inline">ml.c5.xlarge</strong> instance, which has 4 vCPUs and 8 GiB of memory.</p>
			<p>First of all, we need to understand the model's latency and capacity as a function of the type of instance and the number of instances before an endpoint becomes unavailable. Then we vary the instance configuration and autoscaling configuration until the desired latency and traffic capacity has been reached. </p>
			<p>Please open the <strong class="source-inline">Getting-Started-with-Amazon-SageMaker-Studio/chapter07/04-load_testing.ipynb</strong> notebook with the <strong class="bold">Python 3</strong> (<strong class="bold">Data Science</strong>) kernel and an <strong class="source-inline">ml.t3.xlarge</strong> instance and follow these steps:</p>
			<ol>
				<li value="1">We use a Python <a id="_idIndexMarker476"/>load testing framework called <strong class="bold">locust</strong> to perform the load testing in SageMaker Studio. Let's download the library first in the notebook. You <a id="_idIndexMarker477"/>can read more about the library at <a href="https://docs.locust.io/en/stable/index.html">https://docs.locust.io/en/stable/index.html</a>.</li>
				<li>As usual, we set up the SageMaker session in the second cell.</li>
				<li>Create a load testing configuration script, <strong class="source-inline">load_testing/locustfile.py</strong>, which is required by locust. The script is also provided within the repository. This cell overwrites the file. In this configuration, we instruct locust to create simulated users (the <strong class="source-inline">SMLoadTestUser</strong> class) to run model inference against a SageMaker endpoint (the <strong class="source-inline">test_endpoint</strong> class function) provided by the environment variable with a data point loaded from <strong class="source-inline">imdb_data/test/test.csv</strong>. Here, the response time, <strong class="source-inline">total_time</strong>, is measured in <strong class="bold">milliseconds</strong> (<strong class="bold">ms</strong>).</li>
				<li>In the next cell, we start our first load testing job on our already-deployed SageMaker endpoint with an <strong class="source-inline">ml.c5.xlarge</strong> instance. Remember we applied the autoscaling policy in <strong class="source-inline">chapter07/02-tensorflow_sentiment_analysis_inference</strong>? Let's first reverse the policy by setting <a id="_idIndexMarker478"/><strong class="source-inline">MaxCapacity</strong> to <strong class="source-inline">1</strong> to make <a id="_idIndexMarker479"/>sure the endpoint does not scale out to multiple instances during our first test:<p class="source-code">sagemaker_client = sess.boto_session.client('sagemaker')</p><p class="source-code">autoscaling_client = sess.boto_session.client('application-autoscaling')</p><p class="source-code">endpoint_name = '&lt;endpoint-with-ml.c5-xlarge-instance&gt;'</p><p class="source-code">resource_id = f'endpoint/{endpoint_name}/variant/AllTraffic' </p><p class="source-code">response = autoscaling_client.register_scalable_target(</p><p class="source-code">   ServiceNamespace='sagemaker',</p><p class="source-code">   ResourceId=resource_id,</p><p class="source-code">   ScalableDimension='sagemaker:variant:   DesiredInstanceCount',</p><p class="source-code">   MinCapacity=1,</p><p class="source-code">   MaxCapacity=1)</p></li>
				<li>Then we test the endpoint with locust. We set up two-worker distributed load testing on two CPU cores in the following snippet. We instruct <strong class="source-inline">locust</strong> to create 10 users (the <strong class="source-inline">-r 10</strong> argument) per second up to 500 online users (<strong class="source-inline">-u 500</strong>), each making calls to our endpoint for 60 seconds (<strong class="source-inline">-t 60s</strong>). Please replace the <strong class="source-inline">ENDPOINT_NAME</strong> string with your SageMaker endpoint name. You can find the endpoint name in the <strong class="bold">Endpoints</strong> registry, as shown in <em class="italic">Figure 7.3</em>:<p class="source-code">%%sh --bg</p><p class="source-code">export ENDPOINT_NAME='&lt;endpoint-with-ml.c5-xlarge-instance&gt;'</p><p class="source-code">bind_port=5557</p><p class="source-code">locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &amp; </p><p class="source-code">locust -f load_testing/locustfile.py --worker --loglevel ERROR --autostart --autoquit 10 --master-port ${bind_port} &amp;</p><p class="source-code">locust -f load_testing/locustfile.py --headless -u 500 -r 10 -t 60s \</p><p class="source-code">       --print-stats --only-summary --loglevel ERROR \</p><p class="source-code">       --autostart --autoquit 10 --master --expect-workers 2 --master-bind-port ${bind_port}</p></li>
			</ol>
			<p>As it is <a id="_idIndexMarker480"/>running, let's navigate <a id="_idIndexMarker481"/>to the <strong class="bold">Amazon CloudWatch</strong> console to <a id="_idIndexMarker482"/>see what's happening from the endpoint's perspective. Please copy the following URL and replace <strong class="source-inline">&lt;endpoint-with-ml.c5-xlarge-instance&gt;</strong> with your endpoint name and replace the region if you use a region other than us-west-2:  <strong class="source-inline">https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'&lt;endpoint-with-ml.c5-xlarge-instance&gt;~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~</strong><strong class="source-inline">'.~'OverheadLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invoca tion5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~'.~'.~'.))~view~'timeSeries~stacked~false~region~'us-west-2~stat~'Sum~period~60~start~'-PT3H~end~'P0D );query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20&lt;endpoint-with-ml.c5-xlarge-instance&gt;</strong></p>
			<p>You can see a dashboard in <em class="italic">Figure 7.4</em>. The dashboard has captured the most important metrics regarding our SageMaker endpoint's health and status. <strong class="bold">Invocations</strong> and <strong class="bold">InvocationsPerInstance</strong> show the total number of invocations and per-instance counts. <strong class="bold">Invocation5XXErrors</strong> and <strong class="bold">Invocation4XXErrors</strong> are error counts with HTTP codes 5XX and 4XX respectively. <strong class="bold">ModelLatency</strong> (in microseconds) is the time taken by a model inside the container behind a SageMaker endpoint to return a response. <strong class="bold">OverheadLatency</strong> (in microseconds) is the time taken for our SageMaker endpoint to transmit a request and a response. Total latency for a request is <strong class="bold">ModelLatency</strong> plus <strong class="bold">OverheadLatency</strong>. These metrics are emitted by our SageMaker endpoint to Amazon CloudWatch.</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B17447_07_04.jpg" alt="Figure 7.4 – Viewing load testing results on one ml.c5.xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Viewing load testing results on one ml.c5.xlarge instance in Amazon CloudWatch</p>
			<p>In the first load test (<em class="italic">Figure 7.4</em>), we can see that there are around 8,221 invocations per minute, 0 errors, with an average <strong class="bold">ModelLatency</strong> of <strong class="bold">53,825</strong> microseconds, or 53.8 milliseconds.</p>
			<p>With <a id="_idIndexMarker483"/>these numbers in mind <a id="_idIndexMarker484"/>as a baseline, let's scale up the instance, that is, let's use a larger instance.</p>
			<ol>
				<li value="6">We load up the previous IMDb sentiment analysis training job and deploy the TensorFlow model to another endpoint with one <strong class="source-inline">ml.c5.2xlarge</strong> instance, which has 8 vCPU and 16 GiB of memory, twice the capacity of <strong class="source-inline">ml.c5.xlarge</strong>:<p class="source-code">from sagemaker.tensorflow import TensorFlow</p><p class="source-code">training_job_name='&lt;your-training-job-name&gt;'</p><p class="source-code">estimator = TensorFlow.attach(training_job_name)</p><p class="source-code">predictor_c5_2xl = estimator.deploy(</p><p class="source-code">          initial_instance_count=1, </p><p class="source-code">          instance_type='ml.c5.2xlarge')</p></li>
			</ol>
			<p>The <a id="_idIndexMarker485"/>deployment process takes a <a id="_idIndexMarker486"/>couple of minutes. Then we retrieve the endpoint name with the next cell, <strong class="source-inline">predictor_c5_2xl.endpoint_name</strong>.</p>
			<ol>
				<li value="7">Replace <strong class="source-inline">ENDPOINT_NAME</strong> with the output of <strong class="source-inline">predictor_c5_2xl.endpoint_name</strong> and run the cell to kick off another load test against the new endpoint:<p class="source-code">export ENDPOINT_NAME='&lt;endpoint-with-ml.c5-2xlarge-instance&gt;'</p></li>
				<li>In Amazon CloudWatch (replacing <strong class="source-inline">&lt;endpoint-with-ml.c5-xlarge- instance&gt;</strong> in the long URL in <em class="italic">step 4</em> or clicking the hyperlink generated in the next cell in the notebook), we can see how the endpoint responds to traffic in <em class="italic">Figure 7.5</em>:</li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B17447_07_05.jpg" alt="Figure 7.5 – Viewing load testing results on one ml.c5.2xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Viewing load testing results on one ml.c5.2xlarge instance in Amazon CloudWatch</p>
			<p>Similarly, the traffic that locust was able to generate is around 8,000 invocations per minute (<strong class="bold">7,783</strong> in <em class="italic">Figure 7.5</em>). <strong class="bold">ModelLatency</strong> clocks at <strong class="bold">45,871</strong> microseconds (45.8 milliseconds), which is 15% faster than the result from one <strong class="source-inline">ml.c5.xlarge</strong> instance.</p>
			<ol>
				<li value="9">Next, we <a id="_idIndexMarker487"/>deploy the same model <a id="_idIndexMarker488"/>to an <strong class="source-inline">ml.g4dn.xlarge</strong> instance, which is a GPU instance dedicated to model inference use cases. G4dn instances are equipped with NVIDIA T4 GPUs and are cost-effective for ML inference and small neural network training jobs:<p class="source-code">predictor_g4dn_xl = estimator.deploy(</p><p class="source-code">         initial_instance_count=1,        </p><p class="source-code">         instance_type='ml.g4dn.xlarge')</p></li>
				<li>We set up a load testing job similar to the previous ones. The result can also be found on the Amazon CloudWatch dashboard by replacing <strong class="source-inline">&lt;endpoint-with-ml.c5-xlarge- instance&gt;</strong> in the long URL in <em class="italic">step 4</em> or clicking the hyperlink generated in the next cell in the notebook. As shown in <em class="italic">Figure 7.6</em>, with around 6,000 invocations per minute, the average <strong class="bold">ModelLatency</strong> is <strong class="bold">2,070</strong> microseconds (2.07 milliseconds). This is significantly lower than the previous compute configurations, thanks to the GPU device in the <strong class="source-inline">ml.g4dn.xlarge</strong> instance making inference much faster.</li>
			</ol>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B17447_07_06.jpg" alt="Figure 7.6 – Viewing the load test results on one ml.g4dn.xlarge instance in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Viewing the load test results on one ml.g4dn.xlarge instance in Amazon CloudWatch</p>
			<ol>
				<li value="11">The last <a id="_idIndexMarker489"/>approach we should <a id="_idIndexMarker490"/>try is autoscaling. Autoscaling allows us to spread the load across instances, which in turns helps improve the CPU utilization and model latency. We once again set the autoscaling to <strong class="source-inline">MaxCapacity=4</strong> with the following cell:<p class="source-code">endpoint_name = '&lt;endpoint-with-ml.c5-xlarge-instance&gt;'</p><p class="source-code">resource_id=f'endpoint/{endpoint_name}/variant/AllTraffic'</p><p class="source-code">response = autoscaling_client.register_scalable_target(</p><p class="source-code">   ServiceNamespace='sagemaker',</p><p class="source-code">   ResourceId=resource_id,</p><p class="source-code">   ScalableDimension='sagemaker:variant:DesiredInstanceCount',</p><p class="source-code">   MinCapacity=1,</p><p class="source-code">   MaxCapacity=4)</p></li>
			</ol>
			<p>You can confirm the scaling policy attached with the next cell in the notebook.</p>
			<ol>
				<li value="12">We are <a id="_idIndexMarker491"/>ready to perform our last <a id="_idIndexMarker492"/>load testing experiment. Replace <strong class="source-inline">ENDPOINT_NAME</strong> with <strong class="source-inline">&lt;endpoint-with-ml.c5-xlarge-instance&gt;</strong>, and run the next cell to kick off the load test against the endpoint that is now able to scale out up to four instances. This load test needs to run longer in order to see the effect of autoscaling. This is because SageMaker first needs to observe the number of invocations to decide how many new instances are based on our target metric, <strong class="source-inline">SageMakerVariantInvocationsPerInstance=4000</strong>. With our traffic at around 8,000 invocations per minute, SageMaker will spin up one additional instance to have a per-instance invocation at the desired value, 4,000. Spinning up new instances takes around 5 minutes to complete. </li>
			</ol>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B17447_07_07.jpg" alt="Figure 7.7 – Viewing load testing results on an ml.c5.xlarge instance with autoscaling in Amazon CloudWatch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Viewing load testing results on an ml.c5.xlarge instance with autoscaling in Amazon CloudWatch</p>
			<p>We can see the load test result on the Amazon CloudWatch dashboard, as shown in <em class="italic">Figure 7.7</em>. We can see an interesting pattern in the chart. We can clearly <a id="_idIndexMarker493"/>see something happened <a id="_idIndexMarker494"/>between <strong class="source-inline">18:48</strong> and <strong class="source-inline">18:49</strong>. The <strong class="bold">ModelLatency</strong> dropped significantly from around 50,000 microseconds (50 milliseconds) to around <strong class="source-inline">33,839</strong> microseconds (33.8 milliseconds). And the <strong class="bold">InvocationsPerInstance</strong> was cut to half the number of <strong class="bold">Invocations</strong>. We are seeing the effect of SageMaker's autoscaling. Instead of one single instance taking all 8,000 invocations, SageMaker determines that two instances are more appropriate to achieve a target of <strong class="source-inline">SageMakerVariantInvocationsPerInstance=4000</strong> and splits the traffic into two instances. A lower <strong class="bold">ModelLatency</strong> is the preferred outcome of having multiple instances to share the load.</p>
			<p>After the four load testing experiments, we can conclude that at a load of around 6,000 to 8,000 invocations per minute, the following takes place:</p>
			<ul>
				<li>Single-instance performance is measured by average <strong class="bold">ModelLatency</strong>. <strong class="source-inline">ml.g4dn.xlarge</strong> with 1 GPU and 4 vCPUs gives the smallest <strong class="bold">ModelLatency</strong> at 2.07 milliseconds. Next is the <strong class="source-inline">ml.c5.2xlarge</strong> instance with 8 vCPUs at 45.8 milliseconds. Last is the <strong class="source-inline">ml.c5.xlarge</strong> instance with 4 vCPUs at 53.8 milliseconds. </li>
				<li>With autoscaling, two <strong class="source-inline">ml.c5.xlarge</strong> instances with 8 vCPUs achieves 33.8 milliseconds' <strong class="bold">ModelLatency</strong>. This latency is even better than having one <strong class="source-inline">ml.c5.2xlarge</strong> with the same number of vCPUs.</li>
			</ul>
			<p>If we <a id="_idIndexMarker495"/>consider another dimension, the cost <a id="_idIndexMarker496"/>of the instance(s), we can come to an even more interesting situation, as shown in <em class="italic">Figure 7.8</em>. In the table, we create a simple compound metric to measure the cost-performance efficiency of a configuration by multiplying <strong class="bold">ModelLatency</strong> by the price per hour of the instance configuration. </p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B17447_07_08_table.jpg" alt="Figure 7.8 – Cost-performance comparisons&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Cost-performance comparisons</p>
			<p>If we are constrained by cost, we should consider using the last configuration (row d), where the monthly cost is the lowest yet with the second-best cost-performance efficiency while sacrificing some model latency. If we need a model latency of around 40 milliseconds or lower, by paying the same monthly cost, we would get even more bang for our <a id="_idIndexMarker497"/>buck and lower latency with the <a id="_idIndexMarker498"/>third configuration (row c) than the second configuration (row b). The first configuration (row a) gives the best model latency and the best cost-performance efficiency. But it is also the most expensive option. Unless there is a strict single-digit model latency requirement, we might not want to use this option.</p>
			<p>To reduce cost, when you complete the examples, make sure to uncomment and run the last cells in <strong class="source-inline">02-tensorflow_sentiment_analysis_inference.ipynb</strong>, <strong class="source-inline">03-multimodel-endpoint.ipynb</strong>, and <strong class="source-inline">04-load_testing.ipynb</strong> to delete the endpoints in order to stop incurring charges to your AWS account.</p>
			<p>This discussion is based on the example we used, which assumes many factors, such as model framework, traffic pattern, and instance types. You should follow the best practices we introduced for your use case and test out more instance types and autoscaling policies to find the optimal solution for your use case. You can find the full list of instances, specifications, and prices per hour in the <strong class="bold">real-time inference</strong> tab at <a href="https://aws.amazon.com/sagemaker/pricing/">https://aws.amazon.com/sagemaker/pricing/</a> to come up with your own cost-performance efficiency analysis.</p>
			<p>There are other optimization features in SageMaker that help you reduce latency, such as Amazon <a id="_idIndexMarker499"/>Elastic Inference, SageMaker Neo, and Amazon EC2 Inf1 instances. <strong class="bold">Elastic Inference</strong> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html">https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html</a>) attaches fractional GPUs to a SageMaker hosted endpoint. It increases the inference throughput and decreases the model latency for your deep learning models that can benefit from GPU <a id="_idIndexMarker500"/>acceleration. <strong class="bold">SageMaker Neo</strong> (<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html">https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html</a>) optimizes an ML model for inference in the cloud and supported devices at the edge with no loss in accuracy. SageMaker Neo speeds up prediction and reduces cost with a compiled model and optimized container in SageMaker hosted endpoint. <strong class="bold">Amazon EC2 Inf1 instances</strong> (<a href="https://aws.amazon.com/ec2/instance-types/inf1/">https://aws.amazon.com/ec2/instance-types/inf1/</a>) provide high performance <a id="_idIndexMarker501"/>and low cost in the cloud with <strong class="bold">AWS Inferentia</strong> chips designed <a id="_idIndexMarker502"/>and built by AWS for ML inference purposes. You can compile supported ML models using SageMaker Neo and select Inf1 instances to deploy the compiled model in a SageMaker hosted endpoint.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor107"/>Summary</h1>
			<p>In this chapter, we learned how to efficiently make ML inferences in the cloud using Amazon SageMaker. We followed up with what we trained in the previous chapter—an IMDb movie review sentiment prediction—to demonstrate SageMaker's batch transform and real-time hosting. More importantly, we learned how to optimize for cost and model latency with load testing. We also learned about another great cost-saving opportunity by hosting multiple ML models in one single endpoint using SageMaker multi-model endpoints. Once you have selected the best inference option and instance types for your use case, SageMaker makes deploying your models straightforward. With these step-by-step instructions and this discussion, you will be able to translate what you've learned to your own ML use cases.</p>
			<p>In the next chapter, we will take a different route to learn how we can use SageMaker's JumpStart and Autopilot to quick-start your ML journey. SageMaker JumpStart offers solutions to help you see how best practices and ML use cases are tackled. JumpStart model zoos collect numerous pre-trained deep learning models for natural language processing and computer vision use cases. SageMaker Autopilot is an autoML feature that crunches data and trains a performant model without you worrying about data, coding, or modeling. After we have learned the fundamentals of SageMaker—fully managed model training and model hosting—we can better understand how SageMaker JumpStart and Autopilot work.</p>
		</div>
	</body></html>