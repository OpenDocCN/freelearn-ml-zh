- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No-Code Options for Building ML Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, the world of machine learning has undergone a profound transformation,
    breaking free from the realm of expert data scientists and engineers to empower
    a broader audience. The rise of no-code machine learning platforms has ushered
    in a new era, where individuals with diverse skill sets and backgrounds can harness
    the power of artificial intelligence to solve complex challenges, without writing
    a single line of code. This democratization of machine learning has not only expedited
    the development process but has also opened up a myriad of opportunities for businesses
    and individuals alike.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive into the foundations of no-code machine learning,
    shedding light on the remarkable tools and services Google Cloud Vertex AI offers.
    We will explore how users can leverage prebuilt machine learning models, AutoML
    capabilities, and visual interfaces to construct sophisticated and highly accurate
    models with ease. From computer vision to natural language processing and tabular
    data analysis, Google Cloud Vertex AI covers a vast array of use cases, democratizing
    AI application development for everyone.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key topics we will cover in this chapter include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What is AutoML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Vertex AI AutoML?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and deploying a model using Vertex AI AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting predictions from a deployed Vertex AI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first start by looking at the different solutions offered by Google Cloud
    to facilitate model creation without using code.
  prefs: []
  type: TYPE_NORMAL
- en: ML modeling options in Google Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Google Cloud offers several solutions within Vertex AI and the broader **Google
    Cloud Platform** (**GCP**) to build and consume machine learning models. These
    solutions vary widely in terms of required data science and coding skills, catering
    to both advanced ML engineers, relatively less technical business analysts, and
    everyone in between these two personas. The three main GCP solutions for model
    creation are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Query ML (BQML)**: This is part of the BigQuery platform and requires
    only the knowledge of SQL for someone to train and use a model to generate predictions
    on structured data. More details about BQML will be covered in [*Chapter 6*](B17792_06.xhtml#_idTextAnchor079),
    *Low-Code Options for Building* *ML Models*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertex AI AutoML**: This allows users to build models with no coding or even
    SQL knowledge, and it is primarily GUI-based. However, it has APIs that can be
    accessed programmatically if required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertex AI custom training**: This option provides users complete flexibility
    on their model training and deployment but also requires basic coding ability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table shows a comparison of the different options available in
    Google Cloud to create machine learning models:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **BQML** | **Vertex** **AI AutoML** | **Vertex** **AI custom** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Coding requirements | Very Low | None | High |'
  prefs: []
  type: TYPE_TB
- en: '| Required ML engineering expertise | Low | Low | Medium to high, depending
    on the type of model |'
  prefs: []
  type: TYPE_TB
- en: '| Limits on data size | Yes. Standard BigQuery quotas and limits apply. | Yes.
    Dataset limitations vary by the type of dataset being used (see the GCP documentation
    for current limits). | No limit is imposed by GCP for datasets that are not managed.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Types of models supported |'
  prefs: []
  type: TYPE_TB
- en: LINEAR REG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LOGISTIC REG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMEANS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MATRIX FACTORIZATION
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUTOENCODER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUTOML CLASSIFIER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUTOML REGRESSOR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BOOSTED TREE CLASSIFIER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BOOSTED TREE REGRESSOR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RANDOM FOREST CLASSIFIER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RANDOM FOREST REGRESSOR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNN CLASSIFIER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNN REGRESSOR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNN LINEAR COMBINED CLASSIFIER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNN LINEAR COMBINED REGRESSOR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMA PLUS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARIMA PLUS XREG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TENSORFLOW
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TENSORFLOW LITE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBOOST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Image:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video:'
  prefs: []
  type: TYPE_NORMAL
- en: Action recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object tracking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tabular:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Full flexibility to build any type of ML model |'
  prefs: []
  type: TYPE_TB
- en: '| Model development speed | Fast. Some data preparation is required, but training
    can be mostly automated. | Fast. Minimal data preparation and fully automated
    model training. | Slower. More data preparation is required. Significant model
    design and training management. |'
  prefs: []
  type: TYPE_TB
- en: '| Flexibility/control over model generation | Medium | Low | High |'
  prefs: []
  type: TYPE_TB
- en: '| Does the tool support feature engineering? | Yes | No | Yes |'
  prefs: []
  type: TYPE_TB
- en: Table 5.1 – ML model creation options in Google Cloud
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding table, Vertex AI AutoML is the key code-less model
    creation option available as part of Google Cloud. When AutoML was initially launched
    as part of GCP, it used to be a standalone product. Now, it is part of the overall
    Vertex AI platform, and the legacy AutoML product is on the roadmap to be sunset.
    Now, let’s understand what AutoML is and how you can use AutoML features available
    in GCP.
  prefs: []
  type: TYPE_NORMAL
- en: What is AutoML?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AutoML refers to the methodology of automating the process of building machine
    learning models, including data preprocessing, feature engineering, model selection,
    hyperparameter tuning, and model deployment. AutoML aims to make machine learning
    accessible and more efficient for non-experts, saving time and resources for experts
    by reducing the amount of manual work involved in building a model. Different
    types of AutoML products on the market offer different levels of automation. Some
    just automate the training and hyperparameter portion of it, while some do end-to-end
    automation by also automating the steps of data preprocessing and feature generation.
  prefs: []
  type: TYPE_NORMAL
- en: AutoML tools allow users to specify their requirements, such as accuracy, interpretability,
    or training time, and then automatically select and train the best model based
    on these criteria. It can be used for various types of machine learning tasks,
    including classification, regression, and time series forecasting on structured
    and unstructured datasets. AutoML technologies have seen rapid development in
    recent years and are extremely capable of handling many complex ML use cases now,
    with minimal human intervention. However, you need to be careful about being overly
    dependent on AutoML. It is not a substitute for a deep understanding of machine
    learning and data science but, rather, a tool that can help make these processes
    more efficient and accessible. AutoML tools can be risky when used by someone
    who does not understand the fundamentals of machine learning because they can
    generate seemingly high-performing models, while still suffering from common issues
    such as data leakages and overfitting. So, now that we have a basic understanding
    of the concept of AutoML as it relates to model development, let’s look at the
    AutoML features available in Vertex AI.
  prefs: []
  type: TYPE_NORMAL
- en: Vertex AI AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AutoML tools in Google Cloud have existed a lot longer than Vertex AI, which
    was launched to primarily unify most of the separate ML offerings existing in
    GCP. GCP AutoML makes use of models such as NASNet and constantly benefits from
    the AI research happening in other divisions of the Alphabet teams, such as Google
    DeepMind. Few of the interesting papers on the topic are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning Transferable Architectures for Scalable Image* *Recognition*. [https://arxiv.org/pdf/1707.07012.pdf](https://arxiv.org/pdf/1707.07012.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Regularized Evolution for Image Classifier Architecture* *Search*. [https://arxiv.org/pdf/1802.01548.pdf](https://arxiv.org/pdf/1802.01548.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Large-Scale Evolution of Image* *Classifiers*. [https://arxiv.org/pdf/1703.01041.pdf](https://arxiv.org/pdf/1703.01041.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The use cases supported by Vertex AI AutoML are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tabular data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification (an example is covered in this chapter)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series forecasting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification (an example is covered in [*Chapter 8*](B17792_08.xhtml#_idTextAnchor102))
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Natural language:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity extraction
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create a Vertex AI AutoML model using tabular data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following use case, we will walk you through the steps of building a
    classification model, using a public dataset containing hotel reservation data.
    The model’s objective will be to predict the probability of a particular hotel
    reservation being canceled by the customer, helping the hotel to better plan around
    future room occupancy and possibly allow for overbooking in the hotel on dates
    where they expect a high number of cancellations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hotel reservation dataset can be accessed here: [https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand](https://www.kaggle.com/datasets/jessemostipak/hotel-booking-demand)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can download the data from the GitHub repository accompanying this book:
    [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing data to use with Vertex AI AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step when planning to use the Vertex AI AutoML feature is to import
    the data you plan to use to train as Vertex AI datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate to **Vertex AI** | **Datasets** within the Google Cloud console, and
    click **Create** to start creating a new Vertex AI dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Creating a Vertex AI dataset](img/B17792_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Creating a Vertex AI dataset
  prefs: []
  type: TYPE_NORMAL
- en: Type in the name of the dataset, select **Tabular** as the data type, choose
    **Regression/classification**, and then click **CREATE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Selecting a dataset type and model objective](img/B17792_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Selecting a dataset type and model objective
  prefs: []
  type: TYPE_NORMAL
- en: Upload the file named `hotel_reservation_data.csv` that you previously downloaded
    from the GitHub repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17792_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Specifying a data source
  prefs: []
  type: TYPE_NORMAL
- en: Enter a path to the GCS location where you would like to store the imported
    file. If you have not created a GCS bucket before, click on `us-central1`. For
    all other prompts, you can leave the default options already selected (*Figure
    5**.4*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to have consistency in the location of the resources you create
    on GCP, so throughout this book, we will try to use `us-central1` as the location.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Selecting cloud storage location](img/B17792_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Selecting cloud storage location
  prefs: []
  type: TYPE_NORMAL
- en: Next, create a folder within the bucket where you want the imported file to
    be stored. Click the **Folder+** sign at the top right and then provide a name
    for the folder. Then, click **CREATE**. Finally, highlight the folder that was
    just created and click **Select**.![Figure 5.5 – Creating a folder to store the
    dataset](img/B17792_05_5.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 5.5 – Creating a folder to store the dataset
  prefs: []
  type: TYPE_NORMAL
- en: Once the bucket and folder path have been selected, click **Continue** at the
    bottom of the screen, which will start the import of the CSV file into the Vertex
    AI dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the data import is completed, you will be taken to the `hotel_cancellation_prediction`
    or <*whatever name you specified for your dataset*> path. Here, all the feature
    statistics will be blank. To generate these, you can click **Generate Statistics**,
    which will start the process of analyzing the feature data and calculating detailed
    statistics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B17792_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Generating statistics for the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this process is completed, you can click on a specific feature to see
    further details, such as the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The distinct value count
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The missing percentage of data in the field
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature value distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Analyzing key statistics for the dataset](img/B17792_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – Analyzing key statistics for the dataset
  prefs: []
  type: TYPE_NORMAL
- en: The above screenshot shows the graphs explaining the distribution of feature/field
    titled ‘`market_segment`’.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at how we will train an **AutoML Classification** model using
    the dataset discussed above.
  prefs: []
  type: TYPE_NORMAL
- en: Training the AutoML model for tabular/structured data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s look at how you can use Vertex AI AutoML to train ML models on tabular
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: Within the **ANALYZE** tab of the dataset that you want to use to train a model,
    click **TRAIN NEW MODEL** | **Other**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Training a new model](img/B17792_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Training a new model
  prefs: []
  type: TYPE_NORMAL
- en: Since we are trying to classify the reservations based on their cancellation
    likelihood, pick the model objective as **Classification**. Pick **AutoML** as
    the training method, which uses the codeless automated training option. Then,
    click **CONTINUE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Specifying the model type](img/B17792_05_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Specifying the model type
  prefs: []
  type: TYPE_NORMAL
- en: On the following `is_canceled`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want the **Test** results to be exported to BigQuery for further analysis,
    select the **Export test dataset to big query** option, and provide a BigQuery
    table path where these results need to be stored. Vertex AI will create the table
    after the training run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want Vertex AI to randomly split data into **Training**, **Validation**,
    and **Test** datasets, leave the default option, **Random**, selected. If you
    want to control the assignments of samples to the **Training**, **Validation**,
    and **Test** datasets, select the **Manual** option. In this case, you will need
    to provide a column where **Train**/**Validation**/**Test** assignments are provided.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Configuring the model training options](img/B17792_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Configuring the model training options
  prefs: []
  type: TYPE_NORMAL
- en: On the following `reservation_status`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`reservation_status_date`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Selecting the features to be removed](img/B17792_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Selecting the features to be removed
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, check that the Vertex AI has defaulted to the correct transformation
    types for each field. To be used for model training, tabular data must undergo
    a transformation process that is specific to each data feature. This transformation
    process indicates the function of a particular data feature. The supported types
    of transformations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorical**: When training a model with a categorical feature in Vertex
    AI, the feature undergoes data transformations that help with the model training
    process. Vertex AI applies the following transformations to the feature and uses
    any that provide useful information:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The categorical string remains unchanged, with no modifications made to case,
    punctuation, spelling, tense, and other attributes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The category name is converted into a dictionary lookup index, and an embedding
    is generated for each index.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Categories that appear less than five times in the training dataset are considered
    the **unknown** category. The **unknown** category is assigned a unique lookup
    index, and an embedding is generated for this category as well.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: A feature that has undergone a text transformation is treated as
    freeform text and is usually made up of text tokens. The text is tokenized into
    words, and 1-grams and 2-grams are generated from those words. Each *n*-gram is
    then converted into a dictionary lookup index, and an embedding is generated for
    each index. Finally, the embeddings of all the elements are combined into a single
    embedding using the mean.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`float32`, are not included in the training and prediction process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value is converted to `float32`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *z*-score of the value is calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value is bucketed based on quantiles, with a bucket size of 100.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The log of (`value+1`) is calculated when the value is greater than or equal
    to 0\. If the value is less than 0, this transformation is not applied, and the
    value is considered a missing value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *z*-score of the log of (`value+1`) is calculated when the value is greater
    than or equal to 0\. If the value is less than 0, this transformation is not applied,
    and the value is considered a missing value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Boolean value is assigned to indicate whether the value is `null`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timestamp**: The following data transformations are applied to the feature,
    and any that provide useful information are used for training:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The year, month, day, and weekday of the timestamp are determined and treated
    as categorical columns
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Invalid numerical values, such as values outside the typical timestamp range
    or extreme values, are not removed or treated differently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformations for numerical columns are applied to the feature
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows with invalid timestamp inputs, such as an invalid timestamp string, are
    not included in the training and prediction process
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to feed the model additional weights to rebalance the dataset, you
    can provide an additional column that contains weights assigned to each data sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As shown in the following screenshot, by default, the optimization objective
    is set to maximize the AUC ROC curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Selecting the optimization objective](img/B17792_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Selecting the optimization objective
  prefs: []
  type: TYPE_NORMAL
- en: Although not required, you have the option to change the optimization objective.
  prefs: []
  type: TYPE_NORMAL
- en: 'For classification problems, there are five different objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC ROC**: This objective in AutoML maximizes the area under the receiver
    operating characteristic curve. This is the default selection for binary classification
    and can be selected to distinguish between classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log loss**: This aims to minimize the log loss for the model. It is used
    when the goal is to predict probabilities as accurately as possible, and it is
    the only supported objective in AutoML for multi-class classification models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC PR**: This objective aims to maximize the area under the precision-recall
    curve. It optimizes results for predictions for the less common class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision at recall**: This aims to maximize precision at a specific recall
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall at precision**: This aims to maximize recall at a specific precision
    value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For regression problems, there are three different objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RMSE**: This objective aims to minimize the root mean squared error. It captures
    more extreme values accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MAE**: This objective aims to minimize the mean absolute error. It views
    extreme values as outliers with less impact on the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSLE**: This objective aims to minimize the root mean squared log error.
    It penalizes errors in relative size rather than absolute value and is especially
    helpful when both predicted and actual values are quite large.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide the maximum time you want the training to run. The minimum needs to
    be one hour. During the first run, it’s hard to know how long the model will take
    to reach the highest possible accuracy, and you might have to experiment with
    this setting a little for new datasets and model types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Best practice
  prefs: []
  type: TYPE_NORMAL
- en: For most small datasets (< 1 million data samples and < 20 features), two to
    four hours is a good starting point. If the **Enable early stopping** option is
    on, regardless of the number of hours you have budgeted for, the training will
    stop once AutoML determines that no further improvement in the model objective
    is being achieved with further rounds of training.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, as shown in the following screenshot, click **START TRAINING** to kick
    off the training process. In a few hours, you will have a shiny new model ready
    to evaluate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Kicking off model training](img/B17792_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Kicking off model training
  prefs: []
  type: TYPE_NORMAL
- en: Once the model training is completed, let’s see how you can evaluate the model
    created by AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every time you train a machine learning model, it’s crucial to evaluate its
    performance to determine whether it’s reliable for real-world applications. Model
    evaluation metrics are calculated based on the model’s performance against a portion
    of the dataset that was not used during training, referred to as the test dataset.
    This evaluation provides insight into how the model generalizes to new, unseen
    data and helps identify any issues or areas for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Model Registry** in the Vertex AI, and as shown in the following screenshot,
    locate the model you just trained. Click on the model, and then on the next screen,
    click the version of the model you just trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Model Registry](img/B17792_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Model Registry
  prefs: []
  type: TYPE_NORMAL
- en: On the **EVALUATE** tab, you will see the details of the test results generated
    by AutoML, by using the test dataset provided during training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.15 – The model evaluation metrics](img/B17792_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – The model evaluation metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the key information typically shown in the Vertex AI **Evaluate** tab
    depending on the model type:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Confidence threshold: When working with classification models, predictions
    are given a confidence score to indicate the level of certainty that the predicted
    class is correct. The score is a numeric assessment that determines how sure the
    model is that the prediction is accurate. For instance, consider a machine learning
    model that predicts whether a customer will cancel their hotel reservation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To convert the score into a binary decision, a score threshold is used. The
    score threshold is the value at which the model says, “*Yes, the confidence score
    is high enough to conclude that the customer will cancel their reservation*” or
    “*No, the confidence score is not high enough to predict that the customer will
    cancel their reservation.*” The score threshold should be based on a specific
    use case, and a low score threshold increases the risk of misclassification.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision and recall: In classification models, precision and recall are essential
    metrics to assess and summarize how well the model captures information and avoid
    errors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: Precision answers the question, “*Of all the predicted hotel reservation
    cancellations, how many were actually canceled?*” It measures the accuracy of
    the model’s positive predictions – that is, the percentage of true positives out
    of all predicted positives.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall: Recall answers the question, “*Of all the canceled hotel reservations,
    how many did the model correctly predict?*” It measures the model’s ability to
    identify true positive cases – that is, the percentage of true positives out of
    all actual positives.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the use case, you may need to optimize for either precision or
    recall. For instance, if you want to minimize the number of false negatives (i.e.,
    hotel reservations that were canceled but were not identified by the model), then
    you should aim for a high recall. If you want to reduce the number of false positives
    (i.e., hotel reservations that were not canceled but were predicted to be), then
    you should aim for high precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from precision and recall, there are several other classification metrics
    that are useful to evaluate the performance of a machine learning model. Here
    are some of the most commonly used metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC PR**: The area under the **precision-recall** (**PR**) curve measures
    the trade-off between precision and recalls across various score thresholds. The
    **AUC PR** ranges from zero to one, where a higher value indicates a higher-quality
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AUC ROC**: The area under the **receiver operating characteristic** (**ROC**)
    curve measures the model’s performance across all possible score thresholds. The
    AUC ROC also ranges from zero to one, where a higher value indicates a higher-quality
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy**: The fraction of classification predictions produced by the model
    that was correct. This is a simple and intuitive metric that provides an overall
    measure of the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log loss**: Log loss, also known as cross-entropy, serves as a crucial metric
    in assessing the alignment between a model’s predictions and the actual target
    values. This metric quantifies the efficacy of the model’s performance by measuring
    how closely its predictions match the real-world outcomes. With a scale spanning
    from zero to infinity, a lower log loss signifies a higher-quality model, showcasing
    its ability to make more accurate and confident predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score**: When seeking a balance between precision and recall, particularly
    in scenarios with imbalanced class distributions, the F1 score emerges as a valuable
    metric. This score represents the harmonic mean of precision and recall, operating
    on a scale ranging from zero to one. A higher F1 score denotes a model of superior
    quality, signifying its capability to achieve both precision and recall effectively,
    even in challenging class distribution scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using these metrics, you can gain a more comprehensive understanding of your
    model’s performance and make more informed decisions about how to improve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Evaluate** tab also showcases two additional useful pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion matrix**: A confusion matrix is a visualization tool that shows
    the frequency with which a model correctly predicts a result, and for the instances
    when it incorrectly predicts a result, the matrix shows what the model predicted
    instead. The confusion matrix is a helpful way to understand where the model may
    be “confusing” two results, and to diagnose the accuracy and performance of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.16 – The confusion matrix](img/B17792_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – The confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature importance**: Model feature importance is expressed as a percentage
    for each feature, with a higher percentage indicating a stronger impact on model
    training. By looking at the feature importance values, we can gain a better understanding
    of the relative importance of different features in the model and how they contribute
    to the accuracy of predictions. The following screenshot shows the feature importance
    graph for the model we just trained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Feature importance](img/B17792_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Feature importance
  prefs: []
  type: TYPE_NORMAL
- en: So, we have trained a model and evaluated its key performance indicators. Now,
    let’s look at how to deploy it and use it to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions using the recently trained model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the AutoML model is trained, you can generate predictions using one of
    the following two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch predictions**: As the name suggests, batch predictions are asynchronous
    predictions generated for a batch of inputs. This is used when a real-time response
    is unnecessary and you want to submit a single request to process many data instances.
    In Vertex AI, a request for batch predictions can be submitted directly to a model
    residing in the Vertex AI Model registry, without the need to deploy it on an
    endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online predictions**: If you need real-time inference – for example, when
    responding to application input – you need to use the Vertex AI online prediction
    option. To use online prediction, you must first deploy the model to an endpoint.
    This step provisions infrastructure resources and deploys prediction serving mechanism
    using the specified model, enabling it to serve predictions with low latency.
    The steps to deploy the model are shown in the following section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Models deployed on Vertex AI endpoints continuously incur costs, regardless
    of their usage. This could add up quickly, depending on the type of underlying
    VM types, especially if you are using GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will deploy the ML model we trained on Vertex AI to
    generate online predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a model in Vertex AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let us walk you through the steps of deploying the trained model on Vertex
    AI to enable real-time predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to **Model Registry**, click on the model and then the model version you
    want to deploy, and on the **DEPLOY & TEST** tab, click **DEPLOY** **TO ENDPOINT**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Initiating model deployment](img/B17792_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Initiating model deployment
  prefs: []
  type: TYPE_NORMAL
- en: Type in the desired name of the API endpoint being created and click **CONTINUE**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Creating a model endpoint](img/B17792_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Creating a model endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'You can leave all default options unchanged for quick test deployment, but
    these are the settings you need to understand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Traffic split**: If multiple versions of the model are deployed on the same
    API endpoint, this option allows users to define what percentage of total traffic
    is allocated to a specific version. For example, when deploying a new model, you
    might want only 2% of the overall incoming data to be routed to the new model
    so that it can be tested, while continuing to send the rest of the 98% of data
    to the existing version of the model. When deploying a model to a new endpoint,
    you need to leave this value at 100%, since there is no other model to split the
    workload.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum number of compute nodes**: This is the bare minimum number of compute
    nodes always available to handle the inference requests. Even if there are no
    requests being handled, these nodes will constantly be deployed and incur charges.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximum number of compute nodes**: During autoscaling, as the number of incoming
    requests increases, Vertex AI will automatically increase the number of deployed
    nodes up to this number.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine type**: This relates to the configuration of the node supporting
    the online inference in terms of CPUs and memory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explainability options**: If you want the **Feature importance** value to
    be generated with every inference, select this option.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Click **CONTINUE** once you have made the desired changes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Specifying the model deployment configurations](img/B17792_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Specifying the model deployment configurations
  prefs: []
  type: TYPE_NORMAL
- en: On the **Model monitoring** page, turn off monitoring for now. We will cover
    model monitoring in detail in [*Chapter 11*](B17792_11.xhtml#_idTextAnchor153)*,
    MLOps Governance with Vertex AI*. Click **DEPLOY**. This will start the endpoint
    creation process for new endpoints, followed by the model deployment process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the model deployment is complete we can use the model to generate real
    time predictions as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the model deployment is complete, you will see the endpoint listed in the
    **DEPLOY & TEST** tab. Underneath that, there will be a **Test your model** table,
    listing all feature values required to generate predictions. Fields will already
    have starting values based on the data used for AutoML training, but you can type
    in different values and click **Predict** to generate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Testing a deployed model](img/B17792_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Testing a deployed model
  prefs: []
  type: TYPE_NORMAL
- en: Once the model churns through the provided feature values, it will return the
    confidence score associated with each label. The one with the highest confidence
    score is the predicted label.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – The prediction result](img/B17792_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – The prediction result
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s look at the options available to developers to use the Vertex AI models
    programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: Generating predictions programmatically
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To access the Vertex AI prediction service, you can work with the Vertex AI
    SDK for Python, or client libraries available for Python, Java, and Node.js. Check
    out *Install the Vertex AI client libraries* at [https://cloud.google.com/vertex-ai/docs/start/client-libraries](https://cloud.google.com/vertex-ai/docs/start/client-libraries)
    to learn how to install the client library for Java or Node.js. You can find a
    large number of Vertex AI sample notebooks, both community generate and those
    published by Google Cloud team at [https://github.com/GoogleCloudPlatform/vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples).
  prefs: []
  type: TYPE_NORMAL
- en: If your preferred programming language doesn’t have a client library, you can
    make use of the Vertex AI REST API instead.
  prefs: []
  type: TYPE_NORMAL
- en: Submitting prediction requests using the REST API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the **DEPLOY & TEST** tab within Vertex AI makes it easy to test a
    model with a few samples, in most typical use cases, the model’s API endpoint
    will be accessed programmatically by sending the input samples to the API and
    receiving a JSON response from it. Vertex AI makes it easier to get started by
    generating sample code, as shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.23 – A sample REST request](img/B17792_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 – A sample REST request
  prefs: []
  type: TYPE_NORMAL
- en: We will use GCP’s native Cloud Shell to submit a sample prediction request to
    the model deployed in Vertex AI. The first step is to open the cloud shell, as
    shown in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.24 – Opening the cloud shell](img/B17792_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 – Opening the cloud shell
  prefs: []
  type: TYPE_NORMAL
- en: Once Cloud Shell is open, upload the JSON file containing the input data samples
    to the Cloud Shell environment. We will submit this JSON payload as part of the
    request to the API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.25 – Uploading the JSON payload](img/B17792_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.25 – Uploading the JSON payload
  prefs: []
  type: TYPE_NORMAL
- en: In Cloud Shell, copy and paste the following lines to set the environment variables
    after replacing `enpoint_id`, `project_id`, and the path to the input JSON file
    you uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you need to replace `endpoint-id`, `project-id`, and the local path
    of the JSON file you just uploaded from your respective GCP environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, run the following command in Cloud Shell (you can copy and paste the command
    from the **Sample Request** screen we discussed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the prediction request is sent to Vertex AI, it will run the data through
    the model hosted on the endpoint, and if everything goes well, you will receive
    a JSON response from the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the response received from the Vertex AI endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The response contains three main parts – `predictions`, `deployedModelId`, and
    `model`.
  prefs: []
  type: TYPE_NORMAL
- en: The `predictions` section contains the actual predictions made by the model.
    In this case, since we submitted two samples for prediction, two predictions have
    been made, each with a set of scores and predicted classes. The scores represent
    the model’s confidence in each class, and the classes represent the label or category
    that the model has predicted for each prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The first prediction has scores of 0.8947 for class `"0"` and 0.1053 for class
    `"1"`, which suggests that the model is highly confident that the first prediction
    belongs to class `"0"`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second prediction has scores of 0.8826 for class `"0"` and 0.1174 for class
    `"1"`, which also suggests that the model is confident that the second prediction
    belongs to class `"0"` but is less certain than it was about the first prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deployedModelId` is a unique identifier for the deployed model. This is useful
    to keep track of different versions of the same model that may be deployed at
    different times, or in different locations.'
  prefs: []
  type: TYPE_NORMAL
- en: The `model` section provides information about the model that was used to make
    the predictions. It includes the model’s ID, location, and display name, as well
    as the version of the model that was used for these predictions (in this case,
    version 1). This information can help debug and troubleshoot if there are issues
    with the model or its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding end-to-end example, we walked you through building an AutoML
    classification model for tabular/structured data. In the subsequent chapters,
    we will also provide examples of how to build AutoML Vision models using image
    data ([*Chapter 16*](B17792_16.xhtml#_idTextAnchor233)) and examples of AutoML
    NLP models with text data ([*Chapter 17*](B17792_17.xhtml#_idTextAnchor282)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the transformative influence of no-code platforms
    in democratizing AI and machine learning, allowing individuals without specialized
    coding knowledge to develop sophisticated models. You also learned about the no-code
    machine learning options available through Google Cloud’s Vertex AI toolset, and
    we explored the steps to build, evaluate, deploy, and generate predictions with
    an AutoML model. The chapter serves as a practical guide for users interested
    in leveraging Google Cloud’s machine learning capabilities without writing code.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore some low-code options to build a machine
    learning model. Although that would require some additional technical skills compared
    to the no-code options discussed in this book, it would also give you more fine-grained
    control over how your model is built.
  prefs: []
  type: TYPE_NORMAL
