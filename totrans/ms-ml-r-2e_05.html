<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">More Classification Techniques - K-Nearest Neighbors and Support Vector Machines</h1>
            </header>

            <article>
                
<div class="packt_quote">"Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write."<br/>
                                                                                                                       - H.G. Wells</div>
<p>In <a href="d5d39222-b2f8-4c80-9348-34e075893e47.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Logistic Regression and Discriminant Analysis</em>, we discussed using logistic regression to determine the probability that a predicted observation belongs to a categorical response what we refer to as a classification problem. Logistic regression was just the beginning of classification methods, with a number of techniques that we can use to improve our predictions.</p>
<p>In this chapter, we will delve into two nonlinear techniques: <strong>K-Nearest Neighbors</strong> (<strong>KNN</strong>) and <strong>Support Vector Machines</strong> (<strong>SVM</strong>). These techniques are more sophisticated than what we've discussed earlier because the assumptions on linearity can be relaxed, which means a linear combination of the features in order to define the decision boundary is not needed. Be forewarned though, that this does not always equal superior predictive ability. Additionally, these models can be a bit problematic to interpret for business partners and they can be computationally inefficient. When used wisely, they provide a powerful complement to the other tools and techniques discussed in this book. They can be used for continuous outcomes in addition to classification problems; however, for the purposes of this chapter, we will focus only on the latter.</p>
<p>After a high-level background on the techniques, we will lay out the business case and then put both of them to the test in order to determine the best method of the two, starting with KNN.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">K-nearest neighbors</h1>
            </header>

            <article>
                
<p>In our previous efforts, we built models that had coefficients or, said another way, parameter estimates for each of our included features. With KNN, we have no parameters as the learning method is the so-called instance-based learning. In short, <em>The labeled examples (inputs and corresponding output labels) are stored and no action is taken until a new input pattern demands an output value</em>. (Battiti and Brunato, 2014, p. 11). This method is commonly called <strong>lazy learning</strong>, as no specific model parameters are produced. The <kbd>train</kbd> instances themselves represent the knowledge. For the prediction of any new instance (a new data point), the <kbd>train</kbd> data is searched for an instance that most resembles the new instance in question. KNN does this for a classification problem by looking at the closest points-the nearest neighbors to determine the proper class. The <em>k</em> comes into play by determining how many neighbors should be examined by the algorithm, so if <em>k=5</em>, it will examine the five nearest points. A weakness of this method is that all five points are given equal weight in the algorithm even if they are less relevant in learning. We will look at the methods using R and try to alleviate this issue.</p>
<p>The best way to understand how this works is with a simple visual example of a binary classification learning problem. In the following figure, we have a plot of whether a tumor is <strong>benign</strong> or <strong>malignant</strong> based on two predictive features. The <strong>X</strong> in the plot indicates a new observation that we would like to predict. If our algorithm considers <strong>K=3</strong>, the circle encompasses the three observations that are nearest to the one that we want to score. As the most commonly occurring classifications are <strong>malignant</strong>, the <strong>X</strong> data point is classified as <strong>malignant,</strong> as shown in the following figure:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="269" width="342" class="image-border" src="assets/B06473_05_01.png"/></div>
<p>Even from this simple example, it is clear that the selection of <em>k</em> for the nearest neighbors is critical. If <em>k</em> is too small, you may have a high variance on the <kbd>test</kbd> set observations even though you have a low bias. On the other hand, as <em>k</em> grows, you may decrease your variance but the bias may be unacceptable. Cross-validation is necessary to determine the proper <em>k</em>. </p>
<p>It is also important to point out the calculation of the distance or the nearness of the data points in our feature space. The default distance is <strong>Euclidean Distance</strong>. This is simply the straight-line distance from point <kbd>A</kbd> to point <kbd>B</kbd>-as the crow flies-or you can utilize the formula that it is equivalent to the square root of the sum of the squared differences between the corresponding points. The formula for <kbd>Euclidean Distance</kbd>, given point <kbd>A</kbd> and <kbd>B</kbd> with coordinates <kbd>p1</kbd>, <kbd>p2</kbd>, ... <kbd>pn</kbd> and <kbd>q1</kbd>, <kbd>q2</kbd>,... <kbd>qn</kbd> respectively, would be as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="47" width="302" src="assets/image_05_02-2.png"/></div>
<p>This distance is highly dependent on the scale that the features were measured on, so it is critical to standardize them. Other distance calculations <span>as</span><span> well as weights,</span> can be used depending on the distance. We will explore this in the upcoming example.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Support vector machines</h1>
            </header>

            <article>
                
<p>The first time I heard of support vector machines, I have to admit that I was scratching my head, thinking that this was some form of an academic obfuscation or inside joke. However, my open-minded review of SVM has replaced this natural skepticism with a healthy respect for the technique.</p>
<p><em>SVMs have been shown to perform well in a variety of settings and are often considered one of the best "out-of-the-box" classifiers </em>(James, G., 2013). To get a practical grasp of the subject, let's look at another simple visual example. In the following figure, you will see that the classification task is linearly separable. However, the dotted line and solid line are just two among an infinite number of possible linear solutions.<br/>
You would have separating hyperplanes in a problem that has more than two dimensions:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="275" width="352" class="image-border" src="assets/image_05_02-1.png"/></div>
<p>So many solutions can be problematic for generalization because whatever solution you choose, any new observation to the right of the line will be classified as <strong>benign</strong>, and to the left of the line, it will be classified as <strong>malignant</strong>. Therefore, either line has no bias on the <kbd>train</kbd> data but may have a widely divergent error on any data to test. This is where the support vectors come into play. The probability that a point falls on the wrong side of the linear separator is higher for the dotted line than the solid line, which means that the solid line has a higher margin of safety for classification. Therefore, as Battiti and Brunato say, <em>SVMs are linear separators with the largest possible margin and the support vectors the ones touching the safety margin region on both sides</em>.</p>
<p>The following figure illustrates this idea. The thin solid line is the optimal linear separator to create the aforementioned largest possible margin, thus increasing the probability that a new observation will fall on the correct side of the separator. The thicker black lines correspond to the safety margin, and the shaded data points constitute the support vectors. If the support vectors were to move, then the margin and, subsequently, the decision boundary would change. The distance between the separators is known as the <strong>margin</strong>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="277" width="355" class="image-border" src="assets/B06473_05_03.png"/></div>
<p>This is all fine and dandy, but the real-world problems are not so clear cut.</p>
<div class="packt_infobox">In data that is not linearly separable, many observations will fall on the wrong side of the margin (the so-called slack variables), which is a misclassification. The key to building an SVM algorithm is to solve for the optimal number of support vectors via cross-validation. Any observation that lies directly on the wrong side of the margin for its class is known as a <strong>support vector</strong>.</div>
<p>If the tuning parameter for the number of errors is too large, which means that you have many support vectors, you will suffer from a high bias and low variance. On the other hand, if the tuning parameter is too small, the opposite might occur. According to James et al., who refer to the tuning parameter as <kbd>C</kbd>, as <kbd>C</kbd> decreases, the tolerance for observations being on the wrong side of the margin decreases and the margin narrows. This <kbd>C</kbd>, or rather, the cost function, simply allows for observations to be on the wrong side of the margin. If <kbd>C</kbd> were set to zero, then we would prohibit a solution where any observation violates the margin.</p>
<p>Another important aspect of SVM is the ability to model nonlinearity with quadratic or higher order polynomials of the input features. In SVMs, this is known as the <strong>kernel trick</strong>. These can be estimated and selected with cross-validation. In the example, we will look at the alternatives.<br/>
As with any model, you can expand the number of features using polynomials to various degrees, interaction terms, or other derivations. In large datasets, the possibilities can quickly get out of control. The kernel trick with SVMs allows us to efficiently expand the feature space, with the goal that you achieve an approximate linear separation.</p>
<p>To check out how this is done, first look at the SVM optimization problem and its constraints. We are trying to achieve the following:</p>
<ul>
<li>Create weights that maximize the margin</li>
<li>Subject to the constraints, no (or as few as possible) data points should lie within that margin</li>
</ul>
<p>Now, unlike linear regression, where each observation is multiplied by a weight, in SVM, the weights are applied to the inner products of just the support vector observations.</p>
<p>What does this mean? Well, an inner product for two vectors is just the sum of the paired observations' product. For example, if vector one is <em>3</em>, <em>4</em>, and <em>2</em> and vector two is <em>1</em>, <em>2</em>, and <em>3</em>, then you end up with <em>(3x1) + (4x2) + (2x3)</em> or <em>17</em>. With SVMs, if we take a possibility that an inner product of each observation has an inner product of every other observation, this amounts to the formula that there would be <em>n(n-1)/2</em> combinations, where <em>n</em> is the number of observations. With just <em>10</em> observations, we end up with <em>45</em> inner products. However, SVM only concerns itself with the support vectors' observations and their corresponding weights. For a linear SVM classifier, the formula is the following:</p>
<div class="CDPAlignCenter CDPAlign"><img height="68" width="199" src="assets/image_05_05-1.png"/></div>
<p>Here, <kbd>(x, xi)</kbd> are the inner products of the support vectors, as <kbd>α</kbd> is non-zero only when an observation is a support vector.</p>
<p>This leads to far fewer terms in the classification algorithm and allows the use of the <kbd>kernel</kbd> function, commonly referred to as the kernel trick.</p>
<p>The trick in this is that the <kbd>kernel</kbd> function mathematically summarizes the transformation of the features in higher dimensions instead of creating them explicitly. In a simplistic sense, a kernel function computes a dot product between two vectors. This has the benefit of creating the higher dimensional, nonlinear space, and decision boundary while keeping the optimization problem computationally efficient. The <kbd>kernel</kbd> functions compute the inner product in a higher dimensional space without transforming them into the higher dimensional space.</p>
<p>The notation for popular kernels is expressed as the inner (dot) product of the features, with <kbd>x<sub>i</sub></kbd> and <kbd>x<sub>j</sub></kbd> representing vectors, gamma, and <kbd>c</kbd> parameters, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img height="204" width="674" src="assets/image_05_06-1.png"/></div>
<p>As for the selection of the nonlinear techniques, they require some trial and error, but we will walk through the various selection techniques.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business case</h1>
            </header>

            <article>
                
<p>In the upcoming case study, we will apply KNN and SVM to the same dataset. This will allow us to compare the R code and learning methods on the same problem, starting with KNN. We will also spend some time drilling down into the confusion matrix, comparing a number of statistics to evaluate model accuracy.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Business understanding</h1>
            </header>

            <article>
                
<p>The data that we will examine was originally collected by the <strong>National Institute of Diabetes and Digestive and Kidney Diseases</strong> (<strong>NIDDK</strong>). It consists of <kbd>532</kbd> observations and eight input features along with a binary outcome (<kbd>Yes</kbd>/<kbd>No</kbd>). The patients in this study were of Pima Indian descent from South Central Arizona. The NIDDK data shows that for the last 30 years, research has helped scientists to prove that obesity is a major risk factor in the development of diabetes. The Pima Indians were selected for the study as one-half of the adult Pima Indians have diabetes and 95 per cent of those with diabetes are overweight. The analysis will focus on adult women only. Diabetes was diagnosed according to the WHO criteria and was of the type of diabetes that is known as <strong>type 2</strong>. In this type of diabetes, the pancreas is still able to function and produce insulin and it used to be referred to as non-insulin-dependent diabetes.<br/>
Our task is to examine and predict those individuals that have diabetes or the risk factors that could lead to diabetes in this population. Diabetes has become an epidemic in the USA, given the relatively sedentary lifestyle and high-caloric diet. According to the <strong>American Diabetes Association</strong> (<strong>ADA</strong>), the disease was the seventh leading cause of death in the USA in 2010, despite being underdiagnosed. Diabetes is also associated with a dramatic increase in comorbidities, such as hypertension, dyslipidemia, stroke, eye diseases, and kidney diseases. The costs of diabetes and its complications are enormous. The ADA estimates that the total cost of the disease in 2012 was approximately $490 billion. For further background information on the problem, refer to ADA's website at <a href="http://www.diabetes.org/diabetes-basics/statistics/"><span class="URLPACKT">http://www.diabetes.org/diabetes-basics/statistics/</span></a>.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data understanding and preparation</h1>
            </header>

            <article>
                
<p>The dataset for the <kbd>532</kbd> women is in two separate data frames. The variables of interest are as follows:</p>
<ul>
<li><kbd>npreg</kbd>: This is the number of pregnancies</li>
<li><kbd>glu</kbd>: This is the plasma glucose concentration in an oral glucose tolerance test</li>
<li><kbd>bp</kbd>: This is the diastolic blood pressure (mm Hg)</li>
<li><kbd>skin</kbd>: This is triceps skin-fold thickness measured in mm</li>
<li><kbd>bmi</kbd>: This is the body mass index</li>
<li><kbd>ped</kbd>: This is the diabetes pedigree function</li>
<li><kbd>age</kbd>: This is the age in years</li>
<li><kbd>type</kbd>: This is diabetic, <kbd>Yes</kbd> or <kbd>No</kbd></li>
</ul>
<p>The datasets are contained in the R package, <kbd>MASS</kbd>. One data frame is named <kbd>Pima.tr</kbd> and the other is named <kbd>Pima.te</kbd>. Instead of using these as separate <kbd>train</kbd> and <kbd>test</kbd> sets, we will combine them and create our own in order to discover how to do such a task in R.</p>
<p>To begin, let's load the following packages that we will need for the exercise:</p>
<pre>
    <strong>&gt; library(class) #k-nearest neighbors</strong><br/>    <strong>&gt; library(kknn) #weighted k-nearest neighbors</strong><br/>    <strong>&gt; library(e1071) #SVM</strong><br/>    <strong>&gt; library(caret) #select tuning parameters</strong><br/>    <strong>&gt; library(MASS) # contains the data</strong><br/>    <strong>&gt; library(reshape2) #assist in creating boxplots</strong><br/>    <strong>&gt; library(ggplot2) #create boxplots</strong><br/>    <strong>&gt; library(kernlab) #assist with SVM feature selection</strong>
</pre>
<p>We will now load the datasets and check their structure, ensuring that they are the same, starting with <kbd>Pima.tr</kbd>, as follows:</p>
<pre>
    <strong>&gt; data(Pima.tr)</strong><br/>    <strong>&gt; str(Pima.tr)</strong><br/>    <strong>'data.frame':200 obs. of  8 variables:</strong><br/>    <strong> $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...</strong><br/>    <strong> $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...</strong><br/>    <strong> $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...</strong><br/>    <strong> $ skin : int  28 33 41 43 25 27 31 16 15 37 ...</strong><br/>    <strong> $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 <br/>       ...</strong><br/>    <strong> $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...</strong><br/>    <strong> $ age  : int  24 55 35 26 23 52 25 24 63 31 ...</strong><br/>    <strong> $ type : Factor w/ 2 levels "No","Yes": 1 2 1 1 1 2 1 1 1 2 ...</strong><br/>    <strong>&gt; data(Pima.te)</strong><br/>    <strong>&gt; str(Pima.te)</strong><br/>    <strong>'data.frame':332 obs. of  8 variables:</strong><br/>    <strong> $ npreg: int  6 1 1 3 2 5 0 1 3 9 ...</strong><br/>    <strong> $ glu  : int  148 85 89 78 197 166 118 103 126 119 ...</strong><br/>    <strong> $ bp   : int  72 66 66 50 70 72 84 30 88 80 ...</strong><br/>    <strong> $ skin : int  35 29 23 32 45 19 47 38 41 35 ...</strong><br/>    <strong> $ bmi  : num  33.6 26.6 28.1 31 30.5 25.8 45.8 43.3 39.3 29 ...</strong><br/>    <strong> $ ped  : num  0.627 0.351 0.167 0.248 0.158 0.587 0.551 0.183 <br/>       0.704 0.263 ...</strong><br/>    <strong> $ age  : int  50 31 21 26 53 51 31 33 27 29 ...</strong><br/>    <strong> $ type : Factor w/ 2 levels "No","Yes": 2 1 1 2 2 2 2 1 1 2 ...</strong>
</pre>
<p>Looking at the structures, we can be confident that we can combine the data frames into one. This is very easy to do using the <kbd>rbind()</kbd> function, which stands for row binding and appends the data. If you had the same observations in each frame and wanted to append the features, you would bind them by columns using the <kbd>cbind()</kbd> function. You will simply name your new data frame and use this syntax: <kbd>new data = rbind(data frame1, data frame2)</kbd>. Our code thus becomes the following:</p>
<pre>
    <strong>&gt; pima &lt;- rbind(Pima.tr, Pima.te)</strong>
</pre>
<p>As always, double-check the structure. We can see that there are no issues:</p>
<pre>
    <strong>&gt; str(pima)</strong><br/>    <strong>'data.frame':532 obs. of  8 variables:</strong><br/>    <strong> $ npreg: int  5 7 5 0 0 5 3 1 3 2 ...</strong><br/>    <strong> $ glu  : int  86 195 77 165 107 97 83 193 142 128 ...</strong><br/>    <strong> $ bp   : int  68 70 82 76 60 76 58 50 80 78 ...</strong><br/>    <strong> $ skin : int  28 33 41 43 25 27 31 16 15 37 ...</strong><br/>    <strong> $ bmi  : num  30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 <br/>      ...</strong><br/>    <strong> $ ped  : num  0.364 0.163 0.156 0.259 0.133 ...</strong><br/>    <strong> $ age  : int  24 55 35 26 23 52 25 24 63 31 ...</strong><br/>    <strong> $ type : Factor w/ 2 levels "No","Yes": 1 2 1 1 1 2 1 1 1 2 ...</strong>
</pre>
<p>Let's do some exploratory analysis by putting this in boxplots. For this, we want to use the outcome variable, <kbd>"type"</kbd>, as our ID variable. As we did with logistic regression, the <kbd>melt()</kbd> function will do this and prepare a data frame that we can use for the boxplots. We will call the new data frame <kbd>pima.melt</kbd>, as follows:</p>
<pre>
    <strong>&gt; pima.melt &lt;- melt(pima, id.var = "type")</strong>
</pre>
<p>The boxplot layout using the <kbd>ggplot2</kbd> package is quite effective, so we will use it. In the <kbd>ggplot()</kbd> function, we will specify the data to use, the <kbd>x</kbd> and <kbd>y</kbd> variables, and what type of plot, and create a series of plots with two columns. In the following code, we will put the response variable as <kbd>x</kbd> and its value as <kbd>y</kbd> in <kbd>aes()</kbd>. Then, <kbd>geom_boxplot()</kbd> creates the boxplots. Finally, we will build the boxplots in two columns with <kbd>facet_wrap()</kbd>:</p>
<pre>
    <strong>&gt; ggplot(data = pima.melt, aes(x = type, y = value)) + <br/>        geom_boxplot() + facet_wrap(~ variable, ncol = 2)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="250" width="321" class="image-border" src="assets/B06473_05_04.png"/></div>
<p>This is an interesting plot because it is difficult to discern any dramatic differences in the plots, probably with the exception of <strong>glucose</strong> (<strong>glu</strong>). As you may have suspected, the fasting glucose appears to be significantly higher in the patients currently diagnosed with diabetes. The main problem here is that the plots are all on the same y-axis scale. We can fix this and produce a more meaningful plot by standardizing the values and then re-plotting. R has a built-in function, <kbd>scale()</kbd>, which will convert the values to a mean of zero and a standard deviation of one. Let's put this in a new data frame called <kbd>pima.scale</kbd>, converting all of the features and leaving out the <kbd>type</kbd> response. Additionally, while doing KNN, it is important to have the features on the same scale with a mean of zero and a standard deviation of one. If not, then the distance calculations in the nearest neighbor calculation are flawed. If something is measured on a scale of 1 to 100, it will have a larger effect than another feature that is measured on a scale of 1 to 10. Note that when you scale a data frame, it automatically becomes a matrix. Using the <kbd>data.frame()</kbd> function, convert it back to a data frame, as follows:</p>
<pre>
    <strong>&gt; pima.scale &lt;- data.frame(scale(pima[, -8]))</strong><br/>    <strong>&gt; str(pima.scale)</strong><br/>    <strong>'data.frame':532 obs. of  7 variables:</strong><br/>    <strong> $ npreg: num  0.448 1.052 0.448 -1.062 -1.062 ...</strong><br/>    <strong> $ glu  : num  -1.13 2.386 -1.42 1.418 -0.453 ...</strong><br/>    <strong> $ bp   : num  -0.285 -0.122 0.852 0.365 -0.935 ...</strong><br/>    <strong> $ skin : num  -0.112 0.363 1.123 1.313 -0.397 ...</strong><br/>    <strong> $ bmi  : num  -0.391 -1.132 0.423 2.181 -0.943 ...</strong><br/>    <strong> $ ped  : num  -0.403 -0.987 -1.007 -0.708 -1.074 ...</strong><br/>    <strong> $ age  : num  -0.708 2.173 0.315 -0.522 -0.801 ...</strong>
</pre>
<p>Now, we will need to include the response in the data frame, as follows:</p>
<pre>
    <strong>&gt; pima.scale$type &lt;- pima$type</strong>
</pre>
<p>Let's just repeat the boxplotting process again with <kbd>melt()</kbd> and <kbd>ggplot()</kbd>:</p>
<pre>
    <strong>&gt; pima.scale.melt &lt;- melt(pima.scale, id.var = "type")</strong><br/>    <strong>&gt; ggplot(data = pima.scale.melt, aes(x = type, y = value)) +<br/>         geom_boxplot() + facet_wrap(~ variable, ncol = 2)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="441" width="767" class="image-border" src="assets/image_05_05.png"/></div>
<p>With the features scaled, the plot is easier to read. In addition to glucose, it appears that the other features may differ by <kbd>type</kbd>, in particular, <kbd>age</kbd>.</p>
<p>Before splitting this into <kbd>train</kbd> and <kbd>test</kbd> sets, let's have a look at the correlation with the R function, <kbd>cor()</kbd>. This will produce a matrix instead of a plot of the Pearson correlations:</p>
<pre>
    <strong>&gt; cor(pima.scale[-8])</strong><br/>    <strong>            npreg       glu          bp       skin</strong><br/>    <strong>npreg 1.000000000 0.1253296 0.204663421 0.09508511</strong><br/>    <strong>glu   0.125329647 1.0000000 0.219177950 0.22659042</strong><br/>    <strong>bp    0.204663421 0.2191779 1.000000000 0.22607244</strong><br/>    <strong>skin  0.095085114 0.2265904 0.226072440 1.00000000</strong><br/>    <strong>bmi   0.008576282 0.2470793 0.307356904 0.64742239</strong><br/>    <strong>ped   0.007435104 0.1658174 0.008047249 0.11863557</strong><br/>    <strong>age   0.640746866 0.2789071 0.346938723 0.16133614</strong><br/>    <strong>              bmi         ped        age</strong><br/>    <strong>npreg 0.008576282 0.007435104 0.64074687</strong><br/>    <strong>glu   0.247079294 0.165817411 0.27890711</strong><br/>    <strong>bp    0.307356904 0.008047249 0.34693872</strong><br/>    <strong>skin  0.647422386 0.118635569 0.16133614</strong><br/>    <strong>bmi   1.000000000 0.151107136 0.07343826</strong><br/>    <strong>ped   0.151107136 1.000000000 0.07165413</strong><br/>    <strong>age   0.073438257 0.071654133 1.00000000</strong>
</pre>
<p>There are a couple of correlations to point out: <kbd>npreg</kbd>/<kbd>age</kbd> and <kbd>skin</kbd>/<kbd>bmi</kbd>. Multicollinearity is generally not a problem with these methods, assuming that they are properly trained and the hyperparameters are tuned.</p>
<p>I think we are now ready to create the <kbd>train</kbd> and <kbd>test</kbd> sets, but before we do so, I recommend that you always check the ratio of <kbd>Yes</kbd> and <kbd>No</kbd> in our response. It is important to make sure that you will have a balanced split in the data, which may be a problem if one of the outcomes is sparse. This can cause a bias in a classifier between the majority and minority classes. There is no hard and fast rule on what is an improper balance. A good rule of thumb is that you strive for at least a 2:1 ratio in the possible outcomes (He and Wa, 2013):</p>
<pre>
    <strong>&gt; table(pima.scale$type)</strong><br/>    <strong> No Yes</strong><br/>    <strong>355 177</strong>
</pre>
<p>The ratio is 2:1 so we can create the <kbd>train</kbd> and <kbd>test</kbd> sets with our usual syntax using a 70/30 split in the following way:</p>
<pre>
    <strong>&gt; set.seed(502)</strong><br/>    <strong>&gt; ind &lt;- sample(2, nrow(pima.scale), replace = TRUE, prob = c(0.7, <br/>      0.3))</strong><br/>    <strong>&gt; train &lt;- pima.scale[ind == 1, ]</strong><br/>    <strong>&gt; test &lt;- pima.scale[ind == 2, ]</strong><br/>    <strong>&gt; str(train)</strong><br/>    <strong>'data.frame':385 obs. of  8 variables:</strong><br/>    <strong> $ npreg: num  0.448 0.448 -0.156 -0.76 -0.156 ...</strong><br/>    <strong> $ glu  : num  -1.42 -0.775 -1.227 2.322 0.676 ...</strong><br/>    <strong> $ bp   : num  0.852 0.365 -1.097 -1.747 0.69 ...</strong><br/>    <strong> $ skin : num  1.123 -0.207 0.173 -1.253 -1.348 ...</strong><br/>    <strong> $ bmi  : num  0.4229 0.3938 0.2049 -1.0159 -0.0712 ...</strong><br/>    <strong> $ ped  : num  -1.007 -0.363 -0.485 0.441 -0.879 ...</strong><br/>    <strong> $ age  : num  0.315 1.894 -0.615 -0.708 2.916 ...</strong><br/>    <strong> $ type : Factor w/ 2 levels "No","Yes": 1 2 1 1 1 2 2 1 1 1 ...</strong><br/>    <strong>&gt; str(test)</strong><br/>    <strong>'data.frame':147 obs. of  8 variables:</strong><br/>    <strong> $ npreg: num  0.448 1.052 -1.062 -1.062 -0.458 ...</strong><br/>    <strong> $ glu  : num  -1.13 2.386 1.418 -0.453 0.225 ...</strong><br/>    <strong> $ bp   : num  -0.285 -0.122 0.365 -0.935 0.528 ...</strong><br/>    <strong> $ skin : num  -0.112 0.363 1.313 -0.397 0.743 ...</strong><br/>    <strong> $ bmi  : num  -0.391 -1.132 2.181 -0.943 1.513 ...</strong><br/>    <strong> $ ped  : num  -0.403 -0.987 -0.708 -1.074 2.093 ...</strong><br/>    <strong> $ age  : num  -0.7076 2.173 -0.5217 -0.8005 -0.0571 ...</strong><br/>    <strong> $ type : Factor w/ 2 levels "No","Yes": 1 2 1 1 2 1 2 1 1 1 ...</strong>
</pre>
<p>All seems to be in order, so we can move on to building our predictive models and evaluating them, starting with KNN.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Modeling and evaluation</h1>
            </header>

            <article>
                
<p>Now we will discuss various aspects pertaining to modeling and evaluation.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">KNN modeling</h1>
            </header>

            <article>
                
<p>As previously mentioned, it is critical to select the most appropriate parameter (<kbd>k</kbd> or <kbd>K</kbd>) when using this technique. Let's put the <kbd>caret</kbd> package to good use again in order to identify <kbd>k</kbd>. We will create a grid of inputs for the experiment, with <kbd>k</kbd> ranging from <kbd>2</kbd> to <kbd>20</kbd> by an increment of <kbd>1</kbd>. This is easily done with the <kbd>expand.grid()</kbd> and <kbd>seq()</kbd> functions. The <kbd>caret</kbd> package parameter that works with the KNN function is simply <kbd>.k</kbd>:</p>
<pre>
    <strong>&gt; grid1 &lt;- expand.grid(.k = seq(2, 20, by = 1))</strong>
</pre>
<p>We will also incorporate cross-validation in the selection of the parameter, creating an object called <kbd>control</kbd> and utilizing the <kbd>trainControl()</kbd> function from the <kbd>caret</kbd> package, as follows:</p>
<pre>
    <strong>&gt; control &lt;- trainControl(method = "cv")</strong>
</pre>
<p>Now, we can create the object that will show us how to compute the optimal <kbd>k</kbd> value with the <kbd>train()</kbd> function, which is also part of the <kbd>caret</kbd> package. Remember that while conducting any sort of random sampling, you will need to set the <kbd>seed</kbd> value as follows:</p>
<pre>
    <strong>&gt; set.seed(502)</strong>
</pre>
<p>The object created by the <kbd>train()</kbd> function requires the model formula, <kbd>train</kbd> data name, and an appropriate method. The model formula is the same as we've used before-<kbd>y~x</kbd>. The method designation is simply <kbd>knn</kbd>. With this in mind, this code will create the object that will show us the optimal <kbd>k</kbd> value, as follows:</p>
<pre>
    <strong>&gt; knn.train &lt;- train(type ~ ., data = train,<br/>      method = "knn",<br/>      trControl = control,<br/>      tuneGrid = grid1)</strong>
</pre>
<p>Calling the object provides us with the <kbd>k</kbd> parameter that we are seeking, which is <kbd>k=17</kbd>:</p>
<pre>
    <strong>&gt; knn.train</strong><br/>    <strong>k-Nearest Neighbors</strong><br/>    <strong>385 samples</strong><br/>    <strong>  7 predictor</strong><br/>    <strong>  2 classes: 'No', 'Yes'</strong><br/>    <strong>No pre-processing</strong><br/>    <strong>Resampling: Cross-Validated (10 fold)</strong><br/>    <strong>Summary of sample sizes: 347, 347, 345, 347, 347, 346, ...</strong><br/>    <strong>Resampling results across tuning parameters:</strong><br/>    <strong>  k   Accuracy  Kappa  Accuracy SD  Kappa SD</strong><br/>    <strong>   2  0.736     0.359  0.0506       0.1273  </strong><br/>    <strong>   3  0.762     0.416  0.0526       0.1313  </strong><br/>    <strong>   4  0.761     0.418  0.0521       0.1276  </strong><br/>    <strong>   5  0.759     0.411  0.0566       0.1295  </strong><br/>    <strong>   6  0.772     0.442  0.0559       0.1474  </strong><br/>    <strong>   7  0.767     0.417  0.0455       0.1227  </strong><br/>    <strong>   8  0.767     0.425  0.0436       0.1122  </strong><br/>    <strong>   9  0.772     0.435  0.0496       0.1316  </strong><br/>    <strong>  10  0.780     0.458  0.0485       0.1170  </strong><br/>    <strong>  11  0.777     0.446  0.0437       0.1120  </strong><br/>    <strong>  12  0.775     0.440  0.0547       0.1443  </strong><br/>    <strong>  13  0.782     0.456  0.0397       0.1084  </strong><br/>    <strong>  14  0.780     0.449  0.0557       0.1349  </strong><br/>    <strong>  15  0.772     0.427  0.0449       0.1061  </strong><br/>    <strong>  16  0.782     0.453  0.0403       0.0954  </strong><br/>    <strong>  17  0.795     0.485  0.0382       0.0978  </strong><br/>    <strong>  18  0.782     0.451  0.0461       0.1205  </strong><br/>    <strong>  19  0.785     0.455  0.0452       0.1197  </strong><br/>    <strong>  20  0.782     0.446  0.0451       0.1124  </strong><br/>    <strong>Accuracy was used to select the optimal model using the largest <br/>      value.</strong><br/>    <strong>The final value used for the model was k = 17.  </strong>
</pre>
<p>In addition to the results that yield <kbd>k=17</kbd>, we get the information in the form of a table on the <kbd>Accuracy</kbd> and <kbd>Kappa</kbd> statistics and their standard deviations from the cross-validation. <kbd>Accuracy</kbd> tells us the percentage of observations that the model classified correctly. <kbd>Kappa</kbd> refers to what is known as <strong>Cohen's Kappa statistic</strong>. The <kbd>Kappa</kbd> statistic is commonly used to provide a measure of how well two evaluators can classify an observation correctly. It provides an insight into this problem by adjusting the accuracy scores, which is done by accounting for the evaluators being totally correct by mere chance. The formula for the statistic is <em>Kappa = (per cent of agreement - per cent of chance agreement) / (1 - per cent of chance agreement)</em>.<br/>
The <em>per cent of agreement</em> is the rate that the evaluators agreed on for the class (accuracy), and <em>percent of chance agreement</em> is the rate that the evaluators randomly agreed on. The higher the statistic, the better they performed with the maximum agreement being one. We will work through an example when we will apply our model on the <kbd>test</kbd> data.</p>
<p>To do this, we will utilize the <kbd>knn()</kbd> function from the <kbd>class</kbd> package. With this function, we will need to specify at least four items. These would be the <kbd>train</kbd> inputs, the <kbd>test</kbd> inputs, correct labels from the <kbd>train</kbd> set, and <kbd>k</kbd>. We will do this by creating the <kbd>knn.test</kbd> object and see how it performs:</p>
<pre>
    <strong>&gt; knn.test &lt;- knn(train[, -8], test[, -8], train[, 8], k = 17)</strong>
</pre>
<p>With the object created, let's examine the confusion matrix and calculate the accuracy and <kbd>kappa</kbd>:</p>
<pre>
    <strong>&gt; table(knn.test, test$type)</strong><br/>    <strong>knn.test No Yes</strong><br/>    <strong>     No  77  26</strong><br/>    <strong>     Yes 16  28</strong>
</pre>
<p>The accuracy is done by simply dividing the correctly classified observations by the total observations:</p>
<pre>
    <strong>&gt; (77 + 28) / 147</strong><br/>    <strong>[1] 0.7142857</strong>
</pre>
<p>The accuracy of 71 per cent is less than that we achieved on the <kbd>train</kbd> data, which was almost eighty per cent. We can now produce the <kbd>kappa</kbd> statistic as follows: </p>
<pre>
    <strong>&gt; #calculate Kappa</strong><br/>    <strong>&gt; prob.agree &lt;- (77 + 28) / 147 #accuracy</strong><br/>    <strong>&gt; prob.chance &lt;- ((77 + 26) / 147) * ((77 + 16) / 147)</strong><br/>    <strong>&gt; prob.chance</strong><br/>    <strong>[1] 0.4432875</strong><br/>    <strong>&gt; kappa &lt;- (prob.agree - prob.chance) / (1 - prob.chance)</strong><br/>    <strong>&gt; kappa</strong><br/>    <strong>[1] 0.486783</strong>
</pre>
<p>The <kbd>kappa</kbd> statistic of 0.49 is what we achieved with the <kbd>train</kbd> set. Altman(1991) provides a heuristic to assist us in the interpretation of the statistic, which is shown in the following table:</p>
<table class="table">
<thead>
<tr>
<td><strong>Value of <em>K</em></strong></td>
<td><strong>Strength of Agreement</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>&lt;0.20</td>
<td>Poor</td>
</tr>
<tr>
<td>0.21-0.40</td>
<td>Fair</td>
</tr>
<tr>
<td>0.41-0.60</td>
<td>Moderate</td>
</tr>
<tr>
<td>0.61-0.80</td>
<td>Good</td>
</tr>
<tr>
<td>0.81-1.00</td>
<td>Very good</td>
</tr>
</tbody>
</table>
<p>With our <kbd>kappa</kbd> only moderate and with an accuracy just over 70 per cent on the <kbd>test</kbd> set, we should see whether we can perform better by utilizing weighted neighbors. A weighting schema increases the influence of neighbors that are closest to an observation versus those that are farther away. The farther away the observation is from a point in space, the more penalized its influence is. For this technique, we will use the <kbd>kknn</kbd> package and its <kbd>train.kknn()</kbd> function to select the optimal weighting scheme.</p>
<p>The <kbd>train.kknn()</kbd> function uses LOOCV that we examined in the prior chapters in order to select the best parameters for the optimal <kbd>k</kbd> neighbors, one of the two distance measures, and a <kbd>kernel</kbd> function.</p>
<p>The unweighted <kbd>k</kbd> neighbors algorithm that we created uses the Euclidian distance, as we discussed previously. With the <kbd>kknn</kbd> package, there are options available to compare the sum of the absolute differences versus the Euclidian distance. The package refers to the distance calculation used as the <kbd>Minkowski</kbd> parameter.</p>
<p>As for the weighting of the distances, many different methods are available. For our purpose, the package that we will use has ten different weighting schemas, which includes the unweighted ones. They are rectangular (unweighted), triangular, epanechnikov, biweight, triweight, cosine, inversion, gaussian, rank, and optimal. A full discussion of these weighting techniques is available in <em>Hechenbichler K.</em> and <em>Schliep K.P.</em> (2004).</p>
<p>For simplicity, let's focus on just two: <kbd>triangular</kbd> and <kbd>epanechnikov</kbd>. Prior to having the weights assigned, the algorithm standardizes all the distances so that they are between zero and one. The triangular weighting method multiplies the observation distance by one minus the distance. With Epanechnikov, the distance is multiplied by ¾ times (one minus the distance two). For our problem, we will incorporate these weighting methods along with the standard unweighted version for comparison purposes.</p>
<p>After specifying a random seed, we will create the <kbd>train</kbd> set object with <kbd>kknn()</kbd>. This function asks for the maximum number of <em>k</em> values (<kbd>kmax</kbd>), <kbd>distance</kbd> (one is equal to Euclidian and two is equal to absolute), and <kbd>kernel</kbd>. For this model, <kbd>kmax</kbd> will be set to <kbd>25</kbd> and <kbd>distance</kbd> will be <kbd>2</kbd>:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <strong>&gt; kknn.train &lt;- train.kknn(type ~ ., data = train, kmax = 25, <br/>        distance = 2, <br/>        kernel = c("rectangular", "triangular", "epanechnikov"))</strong>
</pre>
<p>A nice feature of the package is the ability to plot and compare the results, as follows:</p>
<pre>
    <strong>&gt; plot(kknn.train)</strong>
</pre>
<p>The following is the output of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img height="210" width="375" class="image-border" src="assets/image_05_06.png"/></div>
<p>This plot shows <strong>k</strong> on the x-axis and the percentage of misclassified observations by <kbd>kernel</kbd>. To my pleasant surprise, the unweighted (<strong>rectangular</strong>) version at <kbd>k: 19</kbd> performs the best. You can also call the object to see what the classification error and the best parameter are in the following way:</p>
<pre>
    <strong>&gt; kknn.train</strong> <br/>    <strong>Call:</strong><br/>    <strong>train.kknn(formula = type ~ ., data = train, kmax = 25, distance = <br/>      2, kernel<br/>     = c("rectangular", "triangular", "epanechnikov"))</strong><br/><strong>    Type of response variable: nominal</strong><br/><strong>    Minimal misclassification: 0.212987</strong><br/><strong>    Best kernel: rectangular</strong><br/><strong>    Best k: 19<br/></strong>
</pre>
<p>So, with this data, weighting the distance does not improve the model accuracy in training and, as we can see here, didn't even do as well on the test set:</p>
<pre>
<strong>    &gt; kknn.pred &lt;- predict(kknn.train, newdata = test)</strong><br/><strong>    &gt; table(kknn.pred, test$type)</strong><br/><strong>    kknn.pred No Yes</strong><br/><strong>           No 76  27</strong><br/><strong>          Yes 17  27</strong>
</pre>
<p>There are other weights that we could try, but as I tried these other weights, the results that I achieved were not more accurate than these. We don't need to pursue KNN any further. I would encourage you to experiment with various parameters on your own to see how they perform.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">SVM modeling</h1>
            </header>

            <article>
                
<p>We will use the <kbd>e1071</kbd> package to build our SVM models. We will start with a linear support vector classifier and then move on to the nonlinear versions. The <kbd>e1071</kbd> package has a nice function for SVM called <kbd>tune.svm()</kbd>, which assists in the selection of the tuning parameters/kernel functions. The <kbd>tune.svm()</kbd> function from the package uses cross-validation to optimize the tuning parameters. Let's create an object called <kbd>linear.tune</kbd> and call it using the <kbd>summary()</kbd> function, as follows:</p>
<pre>
    <strong>&gt; linear.tune &lt;- tune.svm(type ~ ., data = train,<br/>      kernel = "linear",<br/>      cost = c(0.001, 0.01, 0.1, 1, 5, 10))</strong><br/>    <strong>&gt; summary(linear.tune)</strong><br/>    <strong>Parameter tuning of 'svm':</strong><br/>    <strong>- sampling method: 10-fold cross validation</strong><br/>    <strong>- best parameters:</strong><br/>    <strong> cost</strong><br/>    <strong>    1</strong><br/>    <strong>- best performance: 0.2051957</strong><br/>    <strong>- Detailed performance results:</strong><br/>    <strong>   cost     error dispersion</strong><br/>    <strong>1 1e-03 0.3197031 0.06367203</strong><br/>    <strong>2 1e-02 0.2080297 0.07964313</strong><br/>    <strong>3 1e-01 0.2077598 0.07084088</strong><br/>    <strong>4 1e+00 0.2051957 0.06933229</strong><br/>    <strong>5 5e+00 0.2078273 0.07221619</strong><br/>    <strong>6 1e+01 0.2078273 0.07221619</strong>
</pre>
<p>The optimal <kbd>cost</kbd> function is one for this data and leads to a misclassification error of roughly 21 per cent. We can make predictions on the <kbd>test</kbd> data and examine that as well using the <kbd>predict()</kbd> function and applying <kbd>newdata = test</kbd>:</p>
<pre>
    <strong>&gt; best.linear &lt;- linear.tune$best.model</strong><br/>    <strong>&gt; tune.test &lt;- predict(best.linear, newdata = test)</strong><br/>    <strong>&gt; table(tune.test, test$type)</strong><br/>    <strong>tune.test No Yes</strong><br/>    <strong>      No  82  22</strong><br/>    <strong>      Yes 13  30</strong> <br/>    <strong>&gt; (82 + 30)/147</strong><br/>    <strong>[1] 0.7619048</strong>
</pre>
<p>The linear support vector classifier has slightly outperformed KNN on both the <kbd>train</kbd> and <kbd>test</kbd> sets. The <kbd>e1071</kbd> package has a nice function for SVM called <kbd>tune.svm()</kbd> that assists in the selection of the <kbd>tuning parameters/kernel</kbd> functions. We will now see if nonlinear methods will improve our performance and also use cross-validation to select tuning parameters.</p>
<p>The first <kbd>kernel</kbd> function that we will try is <kbd>polynomial</kbd>, and we will be tuning two parameters: a degree of polynomial (<kbd>degree</kbd>) and kernel coefficient (<kbd>coef0</kbd>). The <kbd>polynomial</kbd> order will be <kbd>3</kbd>, <kbd>4</kbd>, and <kbd>5</kbd> and the coefficient will be in increments from <kbd>0.1</kbd> to <kbd>4</kbd>, as follows:</p>
<pre>
    <strong>&gt; set.seed(123)</strong> <br/>    <strong>&gt; poly.tune &lt;- tune.svm(type ~ ., data = train,<br/>      kernel = "polynomial",<br/>      degree = c(3, 4, 5),<br/>      coef0 = c(0.1, 0.5, 1, 2, 3, 4))</strong> <br/>    <strong>&gt; summary(poly.tune)</strong><br/>    <strong>Parameter tuning of 'svm':</strong> <br/>    <strong>- sampling method: 10-fold cross validation</strong><br/>    <strong>- best parameters:</strong><br/>    <strong> degree coef0</strong><br/>    <strong>      3   0.1</strong><br/>    <strong>- best performance: 0.2310391</strong>
</pre>
<p>The model has selected <kbd>degree</kbd> of <kbd>3</kbd> for the polynomial and coefficient of <kbd>0.1</kbd>. Just as the linear SVM, we can create predictions on the <kbd>test</kbd> set with these parameters, as follows:</p>
<pre>
    <strong>&gt; best.poly &lt;- poly.tune$best.model</strong><br/>    <strong>&gt; poly.test &lt;- predict(best.poly, newdata = test)</strong><br/>    <strong>&gt; table(poly.test, test$type)</strong><br/>    <strong>poly.test No Yes</strong><br/>    <strong>      No  81  28</strong><br/>    <strong>      Yes 12  26</strong><br/>    <strong>&gt; (81 + 26) / 147</strong><br/>    <strong>[1] 0.7278912</strong>
</pre>
<p>This did not perform quite as well as the linear model. We will now run the radial basis function. In this instance, the one parameter that we will solve for is <kbd>gamma</kbd>, which we will examine in increments of <kbd>0.1</kbd> to <kbd>4</kbd>. If <kbd>gamma</kbd> is too small, the model will not capture the complexity of the decision boundary; if it is too large, the model will severely overfit:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <strong>&gt; rbf.tune &lt;- tune.svm(type ~ ., data = train, <br/>      kernel = "radial", <br/>      gamma = c(0.1, 0.5, 1, 2, 3, 4))</strong><br/>    <strong>&gt; summary(rbf.tune)</strong><br/>    <strong>Parameter tuning of 'svm':</strong><br/>    <strong>- sampling method: 10-fold cross validation</strong><br/>    <strong>- best parameters:</strong><br/>    <strong> gamma</strong><br/>    <strong>   0.5</strong><br/>    <strong>- best performance: 0.2284076</strong>
</pre>
<p>The best <kbd>gamma</kbd> value is 0.5, and the performance at this setting does not seem to improve much over the other SVM models. We will check for the <kbd>test</kbd> set as well in the following way:</p>
<pre>
    <strong>&gt; best.rbf &lt;- rbf.tune$best.model</strong><br/>    <strong>&gt; rbf.test &lt;- predict(best.rbf, newdata = test)</strong><br/>    <strong>&gt; table(rbf.test, test$type)</strong><br/>    <strong>rbf.test No Yes</strong><br/>    <strong>     No  73  33</strong><br/>    <strong>     Yes 20  21</strong><br/>    <strong>&gt; (73+21)/147</strong><br/>    <strong>[1] 0.6394558</strong>
</pre>
<p>The performance is downright abysmal. One last shot to improve here would be with <kbd>kernel = "sigmoid"</kbd>. We will be solving for two parameters-- <kbd>gamma</kbd> and the kernel coefficient (<kbd>coef0</kbd>):</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <strong>&gt; sigmoid.tune &lt;- tune.svm(type ~ ., data = train,<br/>      kernel = "sigmoid",<br/>      gamma = c(0.1, 0.5, 1, 2, 3, 4),<br/>      coef0 = c(0.1, 0.5, 1, 2, 3, 4))</strong> <br/>    <strong>&gt; summary(sigmoid.tune)</strong><br/>    <strong>Parameter tuning of 'svm':</strong><br/>    <strong>- sampling method: 10-fold cross validation</strong><br/>    <strong>- best parameters:</strong><br/>    <strong> gamma coef0</strong><br/>    <strong>   0.1     2</strong><br/>    <strong>- best performance: 0.2080972</strong>
</pre>
<p>This error rate is in line with the linear model. It is now just a matter of whether it performs better on the <kbd>test</kbd> set or not:</p>
<pre>
    <strong>&gt; best.sigmoid &lt;- sigmoid.tune$best.model</strong><br/>    <strong>&gt; sigmoid.test &lt;- predict(best.sigmoid, newdata = test)</strong><br/>    <strong>&gt; table(sigmoid.test, test$type)</strong><br/>    <strong>sigmoid.test No Yes</strong><br/>    <strong>         No  82  19</strong><br/>    <strong>         Yes 11  35</strong><br/>    <strong>&gt; (82+35)/147</strong><br/>    <strong>[1] 0.7959184</strong>
</pre>
<p>Lo and behold! We finally have a test performance that is in line with the performance on the <kbd>train</kbd> data. It appears that we can choose the sigmoid kernel as the best predictor.</p>
<p>So far we've played around with different models. Now, let's evaluate their performance along with the linear model using metrics other than just the accuracy.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Model selection</h1>
            </header>

            <article>
                
<p>We've looked at two different types of modeling techniques here, and for all intents and purposes, KNN has fallen short. The best accuracy on the <kbd>test</kbd> set for KNN was only around 71 per cent. Conversely, with SVM, we could obtain an accuracy close to 80 per cent. Before just simply selecting the most accurate mode, in this case, the SVM with the sigmoid kernel, let's look at how we can compare them with a deep examination of the confusion matrices.</p>
<p>For this exercise, we can turn to our old friend, the <kbd>caret</kbd> package and utilize the <kbd>confusionMatrix()</kbd> function.  Keep in mind that we previously used the same function from the <kbd>InformationValue</kbd> package.  The <kbd>caret</kbd> package version provides much more detail and it will produce all of the statistics that we need in order to evaluate and select the best model. Let's start with the last model that we built first, using the same syntax that we used in the base <kbd>table()</kbd> function with the exception of specifying the <kbd>positive</kbd> class, as follows:</p>
<pre>
    <strong>&gt; confusionMatrix(sigmoid.test, test$type, positive = "Yes")</strong><br/>    <strong>Confusion Matrix and Statistics</strong><br/>    <strong>          Reference</strong><br/>    <strong>Prediction No Yes</strong><br/>    <strong>       No  82  19</strong><br/>    <strong>       Yes 11  35</strong><br/>    <strong>                Accuracy : 0.7959          </strong><br/>    <strong>                 95% CI : (0.7217, 0.8579)</strong><br/>    <strong>    No Information Rate : 0.6327          </strong><br/>    <strong>    P-Value [Acc &gt; NIR] : 1.393e-05       </strong><br/>    <strong>                  Kappa : 0.5469          </strong><br/>    <strong> Mcnemar's Test P-Value : 0.2012          </strong><br/>    <strong>            Sensitivity : 0.6481          </strong><br/>    <strong>            Specificity : 0.8817          </strong><br/>    <strong>         Pos Pred Value : 0.7609          </strong><br/>    <strong>         Neg Pred Value : 0.8119          </strong><br/>    <strong>             Prevalence : 0.3673          </strong><br/>    <strong>         Detection Rate : 0.2381          </strong><br/>    <strong>   Detection Prevalence : 0.3129          </strong><br/>    <strong>      Balanced Accuracy : 0.7649          </strong><br/>    <strong>       'Positive' Class : Yes    </strong>
</pre>
<p>The function produces some items that we already covered such as <kbd>Accuracy</kbd> and <kbd>Kappa</kbd>. Here are the other statistics that it produces:</p>
<ul>
<li><kbd>No Information Rate</kbd> is the proportion of the largest class; 63 per cent did not have diabetes.</li>
<li><kbd>P-Value</kbd> is used to test the hypothesis that the accuracy is actually better than <kbd>No Information Rate</kbd>.</li>
<li>We will not concern ourselves with <kbd>Mcnemar's Test</kbd>, which is used for the analysis of the matched pairs, primarily in epidemiology studies.</li>
<li><kbd>Sensitivity</kbd> is the true positive rate; in this case, the rate of those not having diabetes has been correctly identified as such.</li>
<li><kbd>Specificity</kbd> is the true negative rate or, for our purposes, the rate of a diabetic that has been correctly identified.</li>
<li>The positive predictive value (<kbd>Pos Pred Value</kbd>) is the probability of someone in the population classified as being diabetic and truly has the disease. The following formula is used:</li>
</ul>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="46" width="435" src="assets/image_05_10.jpg"/></div>
<ul>
<li>The negative predictive value (<kbd>Neg Pred Value</kbd>) is the probability of someone in the population classified as not being diabetic and truly does not have the disease. The formula for this is as follows:</li>
</ul>
<div class="CDPAlignCenter CDPAlign"><img height="49" width="451" src="assets/image_05_11.jpg"/></div>
<ul>
<li><kbd>Prevalence</kbd> is the estimated population prevalence of the disease, calculated here as the total of the second column (the <kbd>Yes</kbd> column) divided by the total observations.</li>
<li><kbd>Detection Rate</kbd> is the rate of the true positives that have been identified, in our case, 35, divided by the total observations.</li>
<li><kbd>Detection Prevalence</kbd> is the predicted prevalence rate, or in our case, the bottom row divided by the total observations.</li>
<li><kbd>Balanced Accuracy</kbd> is the average accuracy obtained from either class. This measure accounts for a potential bias in the classifier algorithm, thus potentially overpredicting the most frequent class. This is simply <em>Sensitivity + Specificity divided by 2</em>.</li>
</ul>
<p>The sensitivity of our model is not as powerful as we would like and tells us that we are missing some features from our dataset that would improve the rate of finding the true diabetic patients. We will now compare these results with the linear SVM, as follows:</p>
<pre>
    <strong>&gt; confusionMatrix(tune.test, test$type, positive = "Yes")</strong><br/>    <strong>         Reference</strong><br/>    <strong>Prediction No Yes</strong><br/>    <strong>       No  82  24</strong><br/>    <strong>       Yes 11  30</strong><br/>    <strong>               Accuracy : 0.7619          </strong><br/>    <strong>                 95% CI : (0.6847, 0.8282)</strong><br/>    <strong>    No Information Rate : 0.6327          </strong><br/>    <strong>    P-Value [Acc &gt; NIR] : 0.0005615       </strong><br/>    <strong>                  Kappa : 0.4605          </strong><br/>    <strong> Mcnemar's Test P-Value : 0.0425225       </strong><br/>    <strong>            Sensitivity : 0.5556          </strong><br/>    <strong>            Specificity : 0.8817          </strong><br/>    <strong>         Pos Pred Value : 0.7317          </strong><br/>    <strong>         Neg Pred Value : 0.7736          </strong><br/>    <strong>             Prevalence : 0.3673          </strong><br/>    <strong>         Detection Rate : 0.2041          </strong><br/>    <strong>   Detection Prevalence : 0.2789          </strong><br/>    <strong>      Balanced Accuracy : 0.7186                                           </strong><br/>    <strong>       'Positive' Class : Yes             </strong>
</pre>
<p>As we can see by comparing the two models, the linear SVM is inferior across the board. Our clear winner is the sigmoid kernel SVM. However, there is one thing that we are missing here and that is any sort of feature selection. What we have done is just thrown all the variables together as the feature input space and let the blackbox SVM calculations give us a predicted classification. One of the issues with SVMs is that the findings are very difficult to interpret. There are a number of ways to go about this process that I feel are beyond the scope of this chapter; this is something that you should begin to explore and learn on your own as you become comfortable with the basics that have been outlined previously.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Feature selection for SVMs</h1>
            </header>

            <article>
                
<p>However, all is not lost on feature selection and I want to take some space to show you a quick way of how to begin exploring this matter. It will require some trial and error on your part. Again, the <kbd>caret</kbd> package helps out in this matter as it will run a cross-validation on a linear SVM based on the <kbd>kernlab</kbd> package.</p>
<p>To do this, we will need to set the random seed, specify the cross-validation method in the caret's <kbd>rfeControl()</kbd> function, perform a recursive feature selection with the <kbd>rfe()</kbd> function, and then test how the model performs on the <kbd>test</kbd> set. In <kbd>rfeControl()</kbd>, you will need to specify the function based on the model being used. There are several different functions that you can use. Here we will need <kbd>lrFuncs</kbd>. To see a list of the available functions, your best bet is to explore the documentation with <kbd>?rfeControl</kbd> and <kbd>?caretFuncs</kbd>. The code for this example is as follows:</p>
<pre>
    <strong>&gt; set.seed(123)</strong><br/>    <strong>&gt; rfeCNTL &lt;- rfeControl(functions = lrFuncs, method = "cv", number <br/>      = 10)<br/>    &gt; svm.features &lt;- rfe(train[, 1:7], train[, 8],<br/>      sizes = c(7, 6, 5, 4), <br/>      rfeControl = rfeCNTL, <br/>      method = "svmLinear")</strong>
</pre>
<p>To create the <kbd>svm.features</kbd> object, it was important to specify the inputs and response factor, number of input features via <kbd>sizes</kbd>, and linear method from <kbd>kernlab</kbd>, which is the <kbd>svmLinear</kbd> syntax. Other options are available using this method, such as <kbd>svmPoly</kbd>. No method for a sigmoid kernel is available. Calling the object allows us to see how the various feature sizes perform, as follows:</p>
<pre>
    <strong>&gt; svm.features</strong><br/>    <strong>Recursive feature selection</strong><br/>    <strong>Outer resampling method: Cross-Validated (10 fold) </strong><br/>    <strong>Resampling performance over subset size:</strong><br/>    <strong> Variables Accuracy  Kappa AccuracySD KappaSD Selected</strong><br/>    <strong>         4   0.7797 0.4700    0.04969  0.1203         </strong><br/>    <strong>         5   0.7875 0.4865    0.04267  0.1096        *</strong><br/>    <strong>         6   0.7847 0.4820    0.04760  0.1141         </strong><br/>    <strong>         7   0.7822 0.4768    0.05065  0.1232         </strong><br/>    <strong>The top 5 variables (out of 5):</strong>
</pre>
<p>Counter-intuitive as it is, the five variables perform quite well by themselves as well as when <kbd>skin</kbd> and <kbd>bp</kbd> are included. Let's try this out on the <kbd>test</kbd> set, remembering that the accuracy of the full model was 76.2 per cent:</p>
<pre>
    <strong>&gt; svm.5 &lt;- svm(type ~ glu + ped + npreg + bmi + age,<br/>      data = train,<br/>      kernel = "linear")</strong><br/>    <strong>&gt; svm.5.predict &lt;- predict(svm.5, newdata = test[c(1, 2, 5, 6, 7)])</strong><br/>    <strong>&gt; table(svm.5.predict, test$type)</strong><br/>    <strong>svm.5.predict No Yes</strong><br/>    <strong>          No  79  21</strong><br/>    <strong>          Yes 14  33</strong>
</pre>
<p>This did not perform as well and we can stick with the full model. You can see through trial and error how this technique can play out in order to determine some simple identification of feature importance. If you want to explore the other techniques and methods that you can apply here, and for blackbox techniques in particular, I recommend that you start by reading the work by Guyon and Elisseeff (2003) on this subject.<br/></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we reviewed two new classification techniques: KNN and SVM. The goal was to discover how these techniques work, and the differences between them, by building and comparing models on a common dataset in order to predict if an individual had diabetes. KNN involved both the unweighted and weighted nearest neighbor algorithms. These did not perform as well as the SVMs in predicting whether an individual had diabetes or not.</p>
<p>We examined how to build and tune both the linear and nonlinear support vector machines using the <kbd>e1071</kbd> package. We used the extremely versatile <kbd>caret</kbd> package to compare the predictive ability of a linear and nonlinear support vector machine and saw that the nonlinear support vector machine with a sigmoid kernel performed the best.</p>
<p>Finally, we touched on how you can use the <kbd>caret</kbd> package to perform a crude feature selection, as this is a difficult challenge with a blackbox technique such as SVM. This can be a major challenge when using these techniques and you will need to consider how viable they are in order to address the business question.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>