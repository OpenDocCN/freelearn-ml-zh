

# ML 部署和数据库操作

*RPA*持续维护的最大来源是处理业务逻辑（流程）和应用程序（对象）的变化。随着 IA 的应用，另一个持续变化的来源被加入其中——更新 BP 以使用新的 ML 模型。想象一下，模型每年只更新一次，而我们有五个模型。这意味着我们每年仍需要五次修改我们的 IA 解决方案的配置，以适应 ML 模型的更新。

即使更新的 ML 模型保持了与之前完全相同的 API 端点定义，我们仍然需要了解正在使用哪种 ML 部署策略，因为这会影响我们计划**回滚**的方式，以防模型的表现不符合预期。

我们在第*8 章*中讨论了**ML 部署者**用户角色。这个用户角色负责定期更新生产中的 ML 模型。在本章中，我们将讨论最常用的 ML 部署策略，它们如何影响 BP 中更新模型所需发生的事情，以及回滚计划。

IA 改变我们操作方式的另一种方式在于数据库维护和 ML 数据提取。*第八章*也讨论了**ML 审计员**用户角色，该角色可以在登录 BP 软件后导出会话日志数据。如果我们不想在 BP 中为这个角色分配职责，我们可以定期直接从数据库中提取这些数据。

在本章中，我们将讨论与 IA 相关的特定操作主题：

+   ML 部署和回滚

+   数据库维护和数据导出

# ML 部署和回滚

ML 通常由 IA 组织之外的团队开发和部署。一旦新的模型可用，IA 团队需要更新 BP 和 ML 模型之间的连接点，以便自动化可以利用它。根据所使用的 ML 部署策略，从 BP 安全更新这些连接点的步骤会有所不同。在本节中，我们将描述最常用的 ML 部署策略。我们还将讨论在特定策略下，如何更改 IA 解决方案的配置以利用新的 ML 模型。

ML 部署策略还影响我们如何从新模型回滚到旧模型。ML 模型的问题通常不会导致容易检测到的会话终止失败。即使有缺陷，模型仍然会继续产生预测。如果发现模型有问题，我们需要准备好回滚到旧模型，以便自动化案例可以继续运行。当然，如果我们使用第三方开发的模型，回滚可能不是一个选项。

回想一下，ML 算法可以从 BP 触发的方式，这些方式在第 1、2 和 3 章中讨论过：*Web* *APIs*、*CLI 脚本*和*代码阶段*。在接下来的部分中，我们将看到每种部署策略的不同之处以及我们的 IA 解决方案应如何配置以保持可审计性。我们还将讨论安全地将 ML 模型回滚到其先前版本所需的步骤。

## Web API 部署策略

部署 ML Web API 的常见方法包括*替换*、*滚动*、*蓝绿*、*金丝雀*和*影子*部署。我将解释这些方法背后的基本概念以及 IA 团队应如何操作以在生产中使用新模型。对于这些策略中的许多，我们还需要考虑 ML 团队是否通过重用*相同的 API 端点*来*覆盖*先前的模型，或者除了先前的端点外还提供*新端点*。新端点允许我们快速切换回旧模型。

### 替换部署

这是需要计划停机的最简单 Web API 部署策略。旧 ML 模型完全离线，新模型被放置到位。需要与 ML 团队协调，以确保在切换期间没有会话正在运行。

#### API 端点被覆盖

如果在部署新模型后重新使用 API 端点，这意味着 URL 完全保持不变。我们需要使用`模型版本`环境变量（存在于所有流程模板中，见*第七章*）来跟踪模型实际更新的时间：

![图 9.1 – 当模型更新时，应更改模型版本环境变量](img/image_00_001.jpg)

图 9.1 – 当模型更新时，应更改模型版本环境变量

更新 BP 解决方案所需的步骤顺序如图所示。在部署新 ML 模型之前，我们必须确保使用该模型的会话已停止运行，因为需要停机时间。新模型上线后，我们更改`模型版本`并重新启动会话：

![图 9.2 – 当现有端点被覆盖时的替换部署](img/image_00_002.jpg)

图 9.2 – 当现有端点被覆盖时的替换部署

如果需要回滚，ML 团队将需要停机以撤销其部署。以下图像显示了示例回滚程序。您可能不想等待正在进行的会话结束，而是请求一个`模型版本`：

![图 9.3 – 当端点被覆盖时回滚替换部署](img/image_00_003.jpg)

图 9.3 – 当端点被覆盖时回滚替换部署

由于这种替换部署策略需要停机时间，因此更适合 IA 流程，其中工作队列的容量较低且 ML 模型更改不频繁。

#### 新的 API 端点已创建

如果有新的 API 端点，更新 IA 解决方案以使用新端点几乎与 *图 9*.2* 中的步骤相同。唯一的区别在于更新 Web API 服务配置以使用新的 API URL，以及 `模型版本` 环境变量。回想一下，在 Web API 服务中更改 URL 的详细信息实际上并没有被保存到审计日志中，这样你就可以检索 URL 的值。如果我们使用的是对象而不是 Web API 服务，我们需要更改存储端点 URL 的环境变量：

![图 9.4 – 创建新端点时的替换部署](img/image_00_004.jpg)

图 9.4 – 创建新端点时的替换部署

在这种情况下回滚很容易，因为之前的 API 仍然可用。我们只需要更改 Web API 服务 URL（或对象环境变量 URL），以及 `模型版本` 回到之前的状态。如果你想更加安全，你可以退役计划，等待所有会话完成后再进行这些更改：

![图 9.5 – 创建新端点时回滚替换部署](img/image_00_005.jpg)

图 9.5 – 创建新端点时回滚替换部署

*替换* 策略可能是最简单的部署类型。本章中讨论的其余策略更为复杂，且不需要停机。由于没有停机时间，因此不需要退役/恢复计划或确保使用 ML 模型的会话没有运行。当然，在停机期间进行解决方案部署和更改仍然更安全。接下来将讨论 *滚动更新* 策略。

### 滚动更新

当有多个服务器在负载均衡器后面托管 API 时，可以执行滚动部署。可以在 *图 9*.6* 中看到简单滚动部署的图像。我们开始时所有 API 服务器都连接到负载均衡器，运行 API 的 *版本 1*（A）。*服务器 1* 被从负载均衡器中移除（*B*），并更新为服务 API 的 *V2*（*C*）。更新后，*服务器 1* 重新连接到负载均衡器（*D*），这意味着两个模型版本（*V1* 在 *服务器 1* 上和 *V2* 在 *服务器 2* 上）正在同时提供服务。接下来，我们移除 *服务器 2* 从负载均衡器（*E*），更新它到 *V2*（*F*），并将其重新连接到负载均衡器（*G*）。我们重复这些步骤，直到所有服务器都更新。

注意，ML API 服务没有停机时间，但也有时间（*D*）可以针对旧模型和新模型进行预测，这会复杂化审计。有可能在旧模型和新模型都可能被服务的时间段内，我们不知道使用了哪个模型进行预测。

![图 9.6 – 滚动更新部署示例](img/image_00_006.jpg)

图 9.6 – 滚动更新部署示例

再次强调，我们必须考虑这种类型部署的两种情况。第一种是现有 API 端点被覆盖，第二种是新 API 端点被创建。

#### API 端点被覆盖

如果 API 端点被重用，当服务器后面的负载均衡器正在更新时，我们可能不知道正在提供哪个版本的 API。我们唯一知道的方式是如果模型版本作为 API 响应的一部分返回，但这并不保证。出于审计原因，在滚动更新开始之前暂停调用 ML 端点，并在我们知道所有服务器都已更新后恢复，是最安全的。然而，我们可能不知道 ML 团队何时开始滚动更新，特别是如果他们是第三方开发者。

在这种情况下，一旦我们收到部署 100%完成的通知，最简单的方法就是更新`模型版本`环境变量。如果您需要确切知道在部署时间段内使用了哪个 ML 模型，您需要要求模型开发者检查他们的服务器日志：

![图 9.7 – 当现有端点被覆盖时的滚动部署](img/image_00_007.jpg)

图 9.7 – 当现有端点被覆盖时的滚动部署

回滚基本上与部署新模型的程序相同。回滚被启动，我们等待收到完成的通知。一旦完成，我们可以更新`模型版本`环境变量。

![图 9.8 – 当端点被覆盖时回滚滚动部署](img/image_00_008.jpg)

图 9.8 – 当端点被覆盖时回滚滚动部署

#### 新 API 端点被创建

如果引入了新的 API 端点，我们需要更新两件事。第一是`模型版本`环境变量。第二取决于我们是否使用*Web* API *服务*或*对象*来调用 API。根据我们使用的是哪一个，我们要么更改 Web API 服务配置以使用新的 API URL，要么更改存储 API URL 的对象环境变量/数据项：

![图 9.9 – 创建新端点时进行滚动部署](img/image_00_009.jpg)

图 9.9 – 创建新端点时进行滚动部署

将 API 回滚到先前版本很简单，因为先前的 API 端点仍然存在。我们撤销对`模型版本`环境变量和 Web API 服务配置 URL（或对象环境变量/数据项）的更改。

![图 9.10 – 创建新端点时回滚滚动部署](img/image_00_010.jpg)

图 9.10 – 创建新端点时回滚滚动部署

滚动部署的一个主要缺点是不知道在部署阶段做出的预测使用了哪个 ML 模型。我们将讨论的下一种策略，称为蓝绿部署，没有这种歧义，也没有停机时间。

### 蓝绿部署

在蓝绿色部署中，负载均衡器后面有重复的基础设施，但在任何给定时间只有一组基础设施正在积极处理请求。一个示例蓝绿色部署可以在 *图 9**.11* 中看到。假设我们目前在 *绿色* 服务器集上运行 *版本 1* 的模型 (*A*)。当模型需要更新到 *版本 2* 时，它被部署到一组单独的蓝色服务器上，这些服务器通常不处理请求 (*B*)。一旦模型版本 2 准备好使用，负载均衡器被更改以仅向蓝色服务器发送请求 (*C*)。带有先前模型的绿色服务器处于闲置状态：

![图 9.11 – 蓝绿色部署示例](img/image_00_011.jpg)

图 9.11 – 蓝绿色部署示例

与滚动更新类似，蓝绿色部署不应有服务中断。与蓝绿色和滚动更新的主要区别是，始终有一个先前模型的副本处于待命状态，可以通过重新配置负载均衡器切换回。也不会存在多个模型同时提供预测的模糊期。这两个点使得蓝绿色部署比滚动更新更适合 IA 审计。

#### API 端点被覆盖

如果 URL 端点被重复使用，我们需要等待通知表明部署已完成，并更改 `模型版本` 环境变量。

![图 9.12 – 覆盖现有端点时的蓝绿色部署](img/image_00_012.jpg)

图 9.12 – 覆盖现有端点时的蓝绿色部署

回滚需要机器学习/基础设施团队更改其负载均衡器的配置。一旦完成，IA 团队可以将 `模型版本` 环境变量改回其先前值。当端点被覆盖时回滚蓝绿色部署的速度应该比回滚滚动更新快得多：

![图 9.13 – 当端点被覆盖时回滚蓝绿色部署](img/image_00_013.jpg)

图 9.13 – 当端点被覆盖时回滚蓝绿色部署

#### 新 API 端点已创建

当我们有新的 API 端点时，我们需要更新 `模型版本` 环境变量，然后是对象环境变量/数据项 API URL 或 Web API 服务配置 URL：

![图 9.14 – 创建新端点时的蓝绿色部署](img/image_00_014.jpg)

图 9.14 – 创建新端点时的蓝绿色部署

由于当前的服务器集合，无论是蓝色还是绿色，都包含机器学习模型的最新和之前版本，因此回滚实际上不需要重新配置负载均衡器。我们只需将环境变量/数据项或 Web API 服务 URL 回滚到之前的状态：

![图 9.15 – 创建新端点时回滚蓝绿色部署](img/image_00_005.jpg)

图 9.15 – 在创建新端点时回滚蓝绿部署

对于我们迄今为止讨论的三个部署策略（替换、滚动、蓝绿），新 ML 模型的 API URL 可以重用（保持不变）或更改。如果新模型有不同的 URL，我们需要在新 ML 模型被调用之前主动更改 IA 解决方案的配置。这与我们将要看到的下两种部署方法形成对比，其中新模型的 URL 将始终保持不变。如果保留模型的前版本，那些将获得新的 URL。

### 金丝雀部署

在金丝雀部署下，新模型和旧模型在 ML 超关注期间并行提供服务（*图 9**.16*，*B*）。最初，只有一小部分请求被路由到新模型；例如，95%的请求将流向旧模型，5%的流量流向新模型（*C*）。随着对新模型性能的信心增加，到达新 ML 模型的请求百分比逐渐增加（*D*）。这最终达到 100%（*E*），托管旧模型的服务器被关闭（*F*）：

![图 9.16 – 一个示例金丝雀部署](img/image_00_016.jpg)

图 9.16 – 一个示例金丝雀部署

IA 团队可能无法看到也无法控制哪些请求会被选择运行在新模型上。这意味着除非 API 响应中提供了该信息，否则我们真的无法确定在切换期间哪些会话将在旧模型和新模型上运行。如果我们需要确定用于审计目的的 ML API 版本，我们需要直接访问 ML 服务的日志，并将它们与会话日志的时间戳进行交叉引用，如果模型是第三方开发的，这可能是不可能的。

金丝雀部署意味着在模型更新时，有一个单一的 URL 端点保持不变。因此，一旦我们收到通知，表示所有流量都被导向新模型，我们只需要更新`模型版本`环境变量一次。类似于滚动更新，我们不会确切知道在部署期间使用了哪个模型进行预测。

![图 9.17 – 金丝雀部署](img/image_00_017.jpg)

图 9.17 – 金丝雀部署

在金丝雀部署中，完全回滚是罕见的，因为其目的是在部署期间捕捉问题。通常发生的情况是，在部署期间，针对新模型的流量成功标准未达到，一切都会立即回滚。这种回滚不需要对 BP 进行任何更改，因为它发生在我们尚未对 IA 解决方案进行任何配置更改之前。

然而，如果在完整部署之后需要回滚，我们只需要将`模型版本`的值改回之前的状态，因为 API URL 保持不变。

![图 9.18 – 回滚金丝雀部署](img/image_00_018.jpg)

图 9.18 – 回滚金丝雀部署

金丝雀部署（类似于滚动更新）在部署期间实际用于生产的模型方面会产生*歧义*。因此，最好避免它们（如果可能的话）或在 API 响应中始终返回模型版本。相反，我们可以考虑使用影子部署（下文将讨论），它也并行运行两个模型。影子部署与金丝雀部署不同，因为旧模型和新模型之间存在明确的切换点。

### 影子部署

使用影子部署时，对旧模型和新模型同时针对实时数据进行调用。然而，在幕后调用的是新模型，并且其 API 响应不会发送回调用者（*图 9.19*，*A*）。影子预测结果直接从服务器日志中检索，由机器学习团队进行分析。一旦机器学习团队对新模型满意，它将被完全切换（*B*），并且旧模型将被移除（*C*）：

![图 9.19 – 一个示例影子部署](img/image_00_019.jpg)

图 9.19 – 一个示例影子部署

影子部署通常是通过将负载均衡器中的流量镜像到托管新机器学习模型的服务器来实现的。请注意，这与在*第六章*中介绍的“*评估新模型的流程模板*”部分在概念上相似。影子部署与模板版本之间的区别在于模板版本明确地按顺序调用两次预测。负载均衡器实现只需要一个 API 请求，并且并行调用。

由于在将新模型用于生产时有一个干净的切换点，影子部署比金丝雀部署更适合 IA。

由于影子部署的实时验证性质，端点 URL 保持不变是隐含的。因此，在部署完成后，我们只需要在 BP 中将`模型版本`更改一次：

![图 9.20 – 影子部署](img/image_00_020.jpg)

图 9.20 – 影子部署

回滚影子部署需要将`模型版本`的更改撤销：

![图 9.21 – 回滚影子部署](img/image_00_021.jpg)

图 9.21 – 回滚影子部署

这就结束了基于 Web API 的部署讨论。让我们总结一下我们看到的五种策略。

### Web API 部署摘要

如果我们可以影响 ML 模型的部署方式，首选的方法是*蓝绿*和*影子*。这是因为从上一个 ML 模型到新模型有一个清晰的截止点，这使得在会话期间使用的模型非常明确。在这两种方法之间，*蓝绿*可能具有更快的回滚速度，因为上一个 ML 模型版本正在待命。您可以将蓝绿部署与*流程*模板结合使用，以评估新模型，产生类似于影子部署的效果。使用模板将在实际模型之后立即调用建议的 ML 模型，为 ML 团队的分析生成日志。

对于*金丝雀*和*滚动更新*的部署期间，关于实际调用的是哪个模型会有模糊性。要找出部署期间实际提供的是哪个 ML 模型，我们需要从托管模型的服务器请求日志，或者让 API 响应指示正在使用哪个 ML 模型版本。

提到的四种部署方法不需要停机时间。如果我们允许停机时间，采用*替换*部署策略是合理的，因为退役计划并确保没有会话处于活动状态是执行任何 BP 解决方案更改的最安全方式。我们当然也可以决定将停机时间与任何其他部署策略相结合。

如果 ML 模型是第三方开发或托管服务，我们几乎不可能影响部署策略。也不太可能知道部署何时发生。我们基本上需要关注 API 文档，以了解何时发生变化，或者希望 API 调用返回模型版本。

从 API 部署过渡到脚本部署，脚本部署包括批处理文件、PowerShell 脚本和 Python 脚本，这些脚本通过命令行直接在数字工作者上执行。

## 脚本部署策略

对于基于脚本的 ML 部署，主要有两种策略。第一种是将脚本及其依赖项*直接*安装在每个数字工作者的 Windows 系统上。第二种是将脚本及其依赖项*虚拟化*。无论选择哪种策略，都要对脚本进行版本控制，这有助于记录依赖项并简化回滚。

### 直接部署

对于直接部署，我们需要考虑脚本的*位置*并安装运行它们所需的*依赖项*。脚本可以位于可执行的网络位置，或者保存在每个数字工作者本地。

部署的难度在于正确配置 Windows 系统，以确保更新的模型所需的依赖项被正确设置。这很可能需要 IT 通过命令行手动安装包。如果需要在许多数字工作者之间重复，这可能会很耗时。

从纯 BP 视角来看，ML 脚本的更新不应导致许多变化，除了修改脚本可执行文件的路径、其参数和 `Model Version` 环境变量。不幸的是，这些更改需要 *停机时间*，因为流程、对象和环境变量的更新存储在 BP 数据库中。对这些更改的修改只能在所有 VM 的部署完成后进行一次。

![图 9.22 – 直接部署](img/image_00_022.jpg)

图 9.22 – 直接部署

在直接部署中为回滚做准备的最简单方法是，在部署新的 ML 模型之前对数字工作者 VM 进行快照。如果我们发现部署后的 ML 模型存在问题，我们可以通过回滚到先前的图像来解决问题。请注意，将数字工作者 VM 回滚到其先前状态仍然需要撤销对 BP 解决方案的更改，例如更改 `Model Version` 环境变量、脚本可执行文件的路径、其参数等。这些更改保存在 BP 数据库中，而不是单个 VM 中。

![图 9.23 – 通过图像备份回滚直接部署](img/image_00_023.jpg)

图 9.23 – 通过图像备份回滚直接部署

如果无法创建图像备份，您将需要手动回滚。手动回滚容易出错，因为降级依赖项很少可能。这很可能需要从头开始卸载和重新安装软件包。在许多数字工作者上这样做将会非常繁琐。

### 虚拟化部署

另一种使回滚变得简单的部署选项是在数字工作者内部使用 *虚拟化*。您可以在本地容器（如 Docker）中部署脚本，VM 图像，Windows Subsystem for Linux 等。虚拟化部署将附带特定版本 ML 模型的所有必要依赖项。

再次强调，对 BP 流程、对象和环境变量的更改将应用于所有会话，因此不可能仅对少数数字工作者（除了克隆整个 BP 发布版本并重命名所有内容之外）部分部署 ML 解决方案。因此，部署虚拟化 ML 解决方案需要停机时间，并需要 IT 的帮助以确保正确启动图像：

![图 9.24 – 虚拟化部署](img/image_00_024.jpg)

图 9.24 – 虚拟化部署

要回滚虚拟化脚本部署，请恢复到图像的先前版本。再次强调，我们仍然需要撤销对 IA 解决方案所做的更改，因为这些更改已保存在 BP 数据库中：

![图 9.25 – 回滚虚拟化部署](img/image_00_025.jpg)

图 9.25 – 回滚虚拟化部署

与直接部署相比，虚拟化部署在数字工作者上对资源的需求（CPU 和 RAM）更大。然而，回滚本地运行的虚拟机可能执行得更快，因为涉及的团队更少。两种方法都有利弊。

我们已经完成了关于如何部署和回滚基于脚本的机器学习模型的讨论。接下来，让我们讨论代码阶段部署。

## 代码阶段部署策略

代码阶段机器学习部署对 RPA 团队来说很熟悉，因为它们可以作为标准的对象 XML 或`.bprelease`导入完成。有些人可能还需要将`.dll`文件复制到对象中指定的位置，到系统路径上的一个文件夹，或者到 BP 安装文件夹。与导入任何其他非机器学习代码阶段相比，唯一的额外步骤是更改`模型版本`环境变量。如果我们需要复制新的`.dll`文件，则需要重启运行时资源以被识别，这将需要停机时间：

![图 9.26 – 代码阶段部署](img/image_00_026.jpg)

图 9.26 – 代码阶段部署

回滚代码阶段部署很简单。重新导入对象或发布的先前版本，复制回正确的`.dll`文件，删除任何新添加的`.dll`文件，并将`模型版本`环境变量值恢复到先前版本。保留先前模型所需的`.dll`文件的副本对于回滚目的很重要。

![图 9.27 – 回滚代码阶段部署](img/image_00_027.jpg)

图 9.27 – 回滚代码阶段部署

这完成了关于 Web API、CLI 脚本和代码阶段部署策略的章节。了解正在使用的特定部署策略对于部署新的机器学习模型非常重要，因为这会影响我们在 IA 解决方案中实施更改的方式，以及我们如何回滚到模型的先前版本。接下来，让我们探讨 IA 如何改变 BP 的数据库管理需求，以及我们如何使用 SQL 提取机器学习日志，而不是通过 BP 软件通过 ML 审计员角色进行。

# 数据库操作

BP 的数据库主要用于操作，并不打算作为记录的永久数据存储。随着数据库表大小的增长，数据库开始运行得更慢，这降低了数字工作者的执行速度。将 IA 步骤添加到流程中会导致额外的日志记录，这增加了某些表的增长速度。持续的数据库维护一直是确保 BP RPA 平稳运行的关键因素，而进行 IA 后，数据库维护的需求只会加剧。

## 表增长维护

受 IA 影响最大的表是`BPAWorkQueueItem`和`BPASessionLog_*`。`BPAWorkQueueItem`存储工作队列项的数据。

重要提示

`BPASessionLog_*` 表示各种会话日志表，包括 `BPASessionLog_NonUnicode`、`BPASessionLog_NonUnicode_pre65`、`BPASessionLog_Unicode` 和 `BPASessionLog_Unicode_pre65`。

### BPAWorkQueueItem

我们很可能会在“工作队列项”数据中存储用于机器学习算法的所有输入数据。如果机器学习算法有多个输入数据列，`BPAWorkQueueItem`的大小会迅速增长。管理此表增长率的四种主要方法如下。第一种是直接从 BP 用户界面删除工作队列项。这不推荐，因为 UI 在选择和删除工作队列项时有限制。

第二种方法是使用 BP 客户支持提供的数据库脚本。此 SQL 脚本会在超过特定日期的行从工作队列相关表中删除。例如，您可以配置脚本以保留 30 天的“工作队列项”，并通过 SQL Server Agent 每天运行脚本。这确保了您的“工作队列项”表不会超过 30 天的数据。

第三种方法是使用**Blue Prism Archiver XBP**资产，该资产可在 DX 上找到，网址为[`digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp`](https://digitalexchange.blueprism.com/dx/entry/3439/solution/blue-prism-archiver-xbp)。此资产允许您将工作队列项从主 BP 数据库移动到不同的数据库。更多详细信息可以在 DX 页面文档中找到。

最后，还有一个 DX 资产，名为`BPAWorkQueueItem`以及各种`BPASessionLog_*`表，将在下文中讨论。

### BPASessionLog*

IA 通过增加执行并记录到数据库的流程和对象阶段数量，简单地增加了各种会话日志表的增长率。如前一段所述，管理此表增长的一种方法是从 DX 使用**DB Servicer XBP**。

`BPASessionLog_*`表也可以通过客户支持提供的 SQL 脚本来管理。该脚本允许我们指定我们想要保留多少天的会话日志。任何超过指定天数的日志都将从数据库中删除。一旦设置好，SQL 脚本需要定期运行。

保持会话日志表大小在可控范围内的第三种选项是使用产品内存档功能。这可以在`.gz`格式下找到。我们必须指定我们想要进行存档的运行时资源（在其空闲时间），在运行时资源上我们想要保存存档文件的位置，以及我们想要在 BP 数据库中保留多少天的日志。

![图 9.28 – 产品内存档器，可以减缓会话日志的增长速度](img/image_00_028.jpg)

图 9.28 – 产品内存档器，可以减缓会话日志的增长速度

最后一种选项是通过 BP 用户界面手动删除会话。对于大多数公司来说，这是不现实的，因为可能定期运行数千个（如果不是更多）会话。

IA 要求我们改进我们的 BP 数据库维护实践。IA 还导致数据导出方面的新要求。例如，ML 审计员想知道哪些输入导致了哪些预测以及这些预测是如何被使用的。数据科学家想知道手动验证结果，以便他们可以更新算法。如果我们的 ML 模型是第三方开发或托管，获取这些日志的唯一方法是通过 BP 数据库，因为我们无法访问底层的 ML 服务器日志。

虽然可以通过 BP 用户界面导出这些数据，但可能更倾向于自动化并直接从数据库导出这些数据。

重要提示

数据导出应在非工作时间进行，最好是在数据库的只读版本上，以最小化对 BP 生产操作的影响。

## 从数据库中提取机器学习预测数据

根据在*第七章*中展示的模板，我们的机器学习预测数据可能从两个数据库位置提取。第一个是从工作队列表，如果我们使用将机器学习预测分离到其自己的工作队列的模板，则此方法有效。如果我们不使用具有专用工作队列的模板，我们可以查询会话日志表以找到这些数据。

### 查询工作队列数据

如果你使用的是具有专用工作队列的 ML 模板，通过 SQL 查询工作队列数据是直接的。以下 SQL 脚本允许你为数据科学家提供算法的输入数据、预测结果和置信度分数。唯一需要更改的是工作队列的名称，你还可以添加日期以限制返回结果的范围：

```py
SELECT data FROM BPAWorkQueueItem wqi, BPAWorkQueue wq
WHERE wq.name = 'ML Queue Name (To Change)'
AND wqi.queueid = wq.id;
```

所选的`data`列是一个 XML 字符串。这种 XML 格式可以直接由 Excel 打开或由数据科学家使用他们选择的编程语言进行解析。

### 查询会话日志数据

在所有的 IA 模板中，我们故意在一些阶段启用了日志记录，以便在会话日志查看器和数据库中更容易提取预测数据。以下图中显示的启用了日志记录的两个阶段在三个 IA 模板中名称相同。这种常见的命名方案是我们 SQL 运行所必需的：

![图 9.29 – 记录机器学习预测结果的常见阶段](img/image_00_029.jpg)

图 9.29 – 记录机器学习预测结果的常见阶段

我们可以通过传递进程的名称来查询会话日志，以特别找到这两个阶段。根据 BP 的配置，你可能还需要将数据库表名从`BPASessionLog_NonUnicode`更改为`BPASessionLog_Unicode`。

```py
SELECT * FROM
(SELECT logid, stagename, LAG(result, 1, 0) OVER(ORDER BY logid) as modelversion, attributexml, startdatetime from BPASessionLog_NonUnicode WHERE stagename in ('Log [Model Version]', 'Set [Prediction] and [Confidence Score]') AND processname = 'To Change') as tbl
WHERE stagename = 'Set [Prediction] and [Confidence Score]';
```

`modelversion`列填充了来自`Log [模型版本]`计算阶段的模型版本。`attributexml`列包含来自`Set [预测]和[置信度分数]`多计算阶段记录的值。

`LAG(result, 1, 0) OVER(ORDER BY logid)` SQL 语法允许我们从 `Log [模型版本]` 和 `Set [预测] 和 [置信度分数]` 的单行中选择值，即使它们属于不同的会话日志条目。这依赖于两个阶段之间只有 `1` 个阶段的差异。如果两个阶段之间存在更多的会话日志记录，您需要将 `1` 的值更改为匹配。

如果您正在使用 **示例 4 – 新模型评估流程模板**（来自 *第六章*）在生产中评估第二个机器学习模型，可以使用以下 SQL 语句：

```py
SELECT logid, stagename, result, attributexml, startdatetime from BPASessionLog_NonUnicode
WHERE stagename = 'Log New Model Evaluation Results'
AND processname = 'To Change';
```

## 从数据库中导出审查后的预测数据

根据使用的模板，审查后的预测结果可能出现在两个位置。如果使用双工作队列或三工作队列模板，我们可以从 `BPAWorkQueueItem` 表中找到它们。如果不使用这些模板，我们可以查看 `BPASessionLog_*` 表。

### 查询工作队列数据

对于 HITL 审查工作队列，我们只检索我们知道已经发生审查的结果。我们通过检查名为 `Confidence` 的收集字段的是否存在来完成此操作：

```py
SELECT data FROM BPAWorkQueueItem wqi, BPAWorkQueue wq
WHERE wq.name = 'ML Queue Name (To Change)'
AND data like '%"Confidence"%'
AND wqi.queueid = wq.id;
```

### 查询会话日志数据

在所有三个模板中，都有一个名为阶段的常见名称，该阶段故意启用了日志记录，以捕获修正后的预测结果和理由。此阶段在以下图中显示：

![图 9.30 – 记录审查结果和理由的常见阶段](img/image_00_030.jpg)

图 9.30 – 记录审查结果和理由的常见阶段

在会话日志表中查找此阶段的 SQL 语句如下：

```py
SELECT logid, attributexml, startdatetime from BPASessionLog_NonUnicode
WHERE stagename = 'Set [Reviewed Prediction] and [Review Justification]'
AND processname = 'attributexml column will need to be parsed by the data scientists manually or opened in Excel.
			Summary
			In this chapter, we discussed two topics which are new, ongoing activities that are required by IA: ML deployments and database operations.
			For deployments of model updates, we discussed the different strategies that are available for Web API, CLI script, and Code Stage-based ML models. Regardless of how the ML model is deployed, we need to think clearly about which steps are needed to update the IA solution, and how to roll it back. This includes whether downtime is needed, what needs to be changed (Objects, Processes, Web API Service Configurations, Environment Variables, copying files, etc.), who performs these changes, and in what order they should be performed in.
			IA affects BP database operations in two main ways. IA leads to faster database table growth, especially for the Work Queue and Session Log tables. The database team that supports IA must really perfect their database maintenance as database growth can negatively affect the performance of BP. Next, IA requires more reports and audits to be generated. It may be desirable to do this from the database. Some sample SQL queries have been provided to extract ML-related logs that work with the Process templates of *Chapter 6* and *Chapter 7*.
			The ongoing IA operations discussed in this chapter, together with the LAM, User Roles and MTE of *Chapter 8* are part of a larger topic, called the BP **Robotic Operating Model** (**ROM**). The ROM is a methodology designed to help firms achieve their automation outcomes. In the next chapter, we’ll be discussing what potential changes IA will have on the ROM.

```
