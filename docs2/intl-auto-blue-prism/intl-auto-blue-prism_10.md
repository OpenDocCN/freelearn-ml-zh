

# 第十章：IA 对机器人操作模型的影响

我所说的**机器人操作模型**（**ROM**）是 RPA 的*管理*方面。它将 BP 20 多年的自动化经验和最佳实践提炼成一个框架，该框架可以应用于所有类型的公司。这个框架的目的是帮助确保公司在自动化方面的持续成功和增长。这包括一些可能不是立即显而易见的问题领域，例如确保获得高管支持、开发职业路径、选择合适的组织结构以及促进文化接受度。

虽然 ROM 不是一个技术产品，但它仍然是 BP 与其竞争对手之间的一个关键差异化因素。任何从事 RPA 工作的人都应该熟悉 ROM，特别是由于其指导是供应商无关的。随着技术和监管环境的变化，我们的管理框架也必须随之变化；ROM 正在不断修订，目前处于 2.0 版本。有关 ROM 的更多信息，请参阅[`community.blueprism.com/content/rom-hub`](https://community.blueprism.com/content/rom-hub)。

这个 ROM 的新版本被组织成五个基础：**战略**、**劳动力**、**设计**、**开发**和**运营**。每个基础进一步细分为六个子主题，总共达到 30 个主题。虽然 IA 在 ROM 中确实被提及，但它并不是主要焦点。作为一个框架，ROM 旨在提供一般性指导，让我们来填补具体细节。试图填补这些细节是我 IA 研究的主要目标之一，你在这里会找到我的许多研究成果。在本章中，我们将讨论 IA 如何影响 ROM 的五个基础，以便你更好地准备在你自己的组织中实施 IA：

+   战略

+   劳动力

+   设计

+   发展

+   运营

# 战略

战略基础中的三个子主题受到 IA 的影响更为明显。这些是*工作未来愿景*、*业务案例和价值*以及*治理、风险和控制*。工作未来愿景子主题讨论了整体自动化计划的愿景声明、使命声明和目标。它还讨论了制定沟通计划的重要性，以确保 IA 愿景在整个组织中传播。业务案例和价值是确保 IA 与公司战略一致，并支持该论点的可衡量 KPI。最后，治理、风险和控制讨论了治理委员会和风险管理。

## 工作未来愿景

如果我们希望从 RPA 过渡到 IA，自动化负责人和执行赞助人明确更新愿景以包括这一点非常重要。对于愿景的更具体部分（使命声明和目标），我们可以指定是否应侧重于集成预构建的 ML 服务，如基于 API 的 OCR、公开可用的 LLMs 等，还是开发内部专业知识以构建定制的 ML 模型。

对于已经运营至少 1 到 2 年的 RPA 团队，建立包含 IA 的愿景是最好的。这至少是 ROM 成熟度模型中的第 3 级：[`community.blueprism.com/content/rom-hub/rom2-maturity-model`](https://community.blueprism.com/content/rom-hub/rom2-maturity-model)。

## 商业案例和价值

由于 ML 是一种趋势技术，许多高管都希望看到它在整个业务中得到应用。尽管有高管的支持，我们仍然需要弄清楚如何衡量 IA 对整体企业战略的贡献。常用的关键绩效指标（KPI），如**全职员工**（**FTE**）节省和返还给业务的小时数，依赖于量化数字工作者与人工工作者节省的时间。但对于 IA，节省的时间可能微不足道——例如，如果我们正在替换一个只需几分钟就能完成的专家决策过程。我们可能需要从基于时间和基于美元节省的 KPI 测量转向基于**业务价值**的评估。这些评估的例子可以在 ROM 2 培训材料中找到。

IA 的**总拥有成本**（**TCO**）也会显著变化。我们仍然有传统的 RPA 成本，但还有 ML 的成本。三个主要场景导致 IA 项目上的 ML 成本不同。第一种情况是将 ML 开发外包给第三方。这可能会产生固定和可变开发成本。如果模型由第三方托管，还可能有持续的服务成本。如果模型在本地部署并由内部团队完全管理，仍将存在内部持续成本。第二种是通过 API 服务消费 ML，这会产生每笔交易的成本。这些成本可以根据预期的作业量和 API 定价进行预测。

第三种情况是如果内部开发机器学习（ML）。通过初步研究，我发现第一次将内部 ML 解决方案部署到生产中大约需要 100,000 美元，不包括 RPA 成本。智能自动化（IA）与引入任何新技术类似，第一次项目的成本很高，但随着更多 IA 项目投入生产，边际成本会降低。决定内部构建 ML 应被视为一项多年努力，而不是一次性或试点项目。内部 ML 的成本主要来自薪资和硬件租赁。

除了完全通过 API 调用消耗的 ML 之外，模型将会有持续的成本，因为需要积极监控数据和模型的表现，并且必须重建模型。还可能有与雇佣 ML 预测审查员相关的成本。

## 治理、风险和控制

考虑 IA 治理的一种方式是将 RPA 的治理关注点添加到 ML 的治理中。单独来看，ML 治理已经是一个巨大的话题，因此*治理委员会*必须要有非常熟悉 ML 生产化并能够跟上其周围不断变化的监管环境的人。

IA 团队发展适当治理的起点是内部寻找现有的数据隐私、数据保留和安全方面的数据和 AI 政策。我们还需要考虑从公平和伦理的角度来看，使用 ML 模型是否可取。目前，很少有公司已经建立了 AI 伦理政策。发展内部伦理指南的一些潜在起点包括*IEEE 全球自主和智能系统伦理倡议*和*蒙特利尔负责任 AI 宣言*。

### 风险

IA 与 RPA 相比，存在许多与之相关的风险。首先，我们需要*遵守*与 AI 相关的现有和即将出台的法律。如*第四章*中所述，各国已经在制定法律来规范商业中 AI 的使用。

另一类风险是*基于模型的*，其中主要的一个是预测精度低。ML 模型的表现也已知会随时间退化，某些类型的 ML 算法容易受到对抗性攻击。

还存在*基于数据*的风险，包括数据偏差、数据质量和数据漂移。数据偏差指的是具有“不良属性”的数据，例如当收集的数据在不应代表某些群体时却未能充分代表。数据质量指的是存在“良好属性”，例如拥有足够的样本数量和具有与我们试图做出的预测高度相关的特征。数据漂移指的是随时间自然发生的底层输入数据分布的变化。数据漂移的一个例子可以在消费者购买行为中看到。即使在相似年龄和薪资范围内，今天人们购买的商品类型与 10 年前人们购买的商品类型不同。

通过 ML 治理可以降低的下一类风险与*安全和数据访问*有关。引入新的服务器来托管模型，以及必须与数据和模型交互的新员工，增加了可以针对 IA 解决方案进行攻击的潜在攻击区域。

### 通过治理应对风险

治理可以帮助解决上一节中提到的所有风险。对于*合规风险*，治理可以强制要求所有 IA 解决方案都有一种方法来禁用 ML 预测（我们在*第六章*中设计的“终止开关”），偏好固有可解释的算法，并为客户提供一个渠道，让他们在处理他们的案件时请求不使用 ML。

ML 治理可以帮助减少*基于模型的风险*。治理可以要求我们实施正式的审查和测试流程，以确保在模型达到生产之前，至少有一定比例的预测是正确的。我们还可以强制要求在模型投入生产之前使用通用的 ML 可解释性方法，如 LIME 和 SHAP 来评估模型。

由于模型随着时间的推移会失去预测能力，治理应该定义关于持续监控模型性能的要求。这包括收集经过人工审查的预测进行定期审查，并了解分类预测标签的频率随时间的变化。

治理可以通过实施数据标准和数据科学家培训要求来应对*数据风险*。建立数据质量的一个例子可能是强制规定数据必须至少有 1,000 个样本每个标签，并且每行必须少于 5%的缺失列。治理可以要求数据科学家在允许他们为 IA 项目开发模型之前，完成关于识别数据偏差的培训。

治理还可以围绕定期监控数据的需求制定标准。首先，需要收集基线统计数据，通常来自训练数据。计算的一些常见统计数据包括数据列的平均值、最大值、最小值和标准差。然后，基于生产中使用的输入数据，定期重新计算新的统计数据。治理可能要求每月进行此操作，并对数据列中超过一个标准差的变化进行手动评估或触发模型重建并部署到生产中。

为了*安全*，我们需要修改现有的治理政策，以定义谁可以更改这些 IA 组件以及相应的程序。这可以利用在*第八章*中开发的*ML Deployer*和*ML Reviewer*角色。

# 人力资源

对于劳动力基础，我们将更深入地探讨 *构建您的组织模型*、*采用新的思维和工作方式* 以及 *角色和职业道路*。构建您的组织模型讨论了 IA 功能在公司中可以采取的不同结构方式。一些例子包括 IA 作为为所有人服务的中央单位，不同业务单元内的个人 IA 团队，或者 IA 完全外包给供应商。*采用新的思维和工作方式* 是关于影响组织并克服对 IA 的阻力以获得支持。最后，*角色和职业道路* 是关于需要哪些技能集和角色来创造成功的 IA 结果并确保 IA 团队内的职业发展。

## 构建您的组织模型

关于组织模型的一些主要考虑因素包括 ML 专业知识在组织中的位置，如何与之互动，以及是否需要直接将 ML 专业知识带入 IA 团队。如果公司已经有一个集中的数据科学团队，我们必须考虑他们的组织模型，以及他们期望如何向公司其他部门提供 ML 专业知识。

如果公司对 IA 严肃认真，预计自动化团队至少会有几名成员能够执行数据科学工作，而不依赖于外部团队。对于 **卓越中心**（**COE**）、*特许经营* 或 *中心和辐射* 组织模型，中央团队应拥有能够将 ML 模型投入生产的内部专业知识。对于 *部门*、*部门联盟* 和 *外包管理服务* 模型，ML 专业知识可以存在于那些团队中。

## 采用新的思维和工作方式

理解 IA 可能受到 *个人员工* 和 *管理层* 反对的方式非常重要。由于潜在可自动化的范围扩大，IA 的采用可能比 RPA 更难管理。在我的研究中，我确定了员工和管理层反对 IA 采用的一些重要方式以及一些对抗这种反对的方法。

### 员工层面的 IA 阻力

如果认为对员工有意义的任务是自动化的，IA 可能会导致 *工作意义的丧失*。一个现实生活中的例子是用聊天机器人和 RPA 替换社会工作者对老年人的福利筛选电话。受影响的员工的访谈显示，他们在自动化后工作满意度较低。可以使用名为 **工作与意义清单**（**WAMI**）的调查问卷来确定员工在 IA 实施后是否经历了工作意义的丧失。

工作的意义可以分为四个组成部分：*个人*、*工作*、*组织*和*社会*。如果我们认为 IA 会降低某人的工作意义，我们可以通过针对这些不同层面的改进来对抗这种影响。例如，在*个人*层面，我们可以通过允许在家工作日来为受影响的员工提供更多的灵活性和自主性。在*工作*层面，我们可以改变受影响人员工作的意义、可见性和范围。在*组织*层面，我们可以增加企业社会责任活动的数量。更实际的方法是询问受影响的员工，所提出的自动化是否有可能降低他们的工作满意度，并避免自动化那些领域。

员工可能遇到的其他风险是*工作准备度降低*。在一家金融服务公司，通过 IA 将文件数字化并自动输入到内部系统中，而不是由案件工作人员自己输入。移除手动数字化和输入任务意味着花在查看客户详细信息上的时间减少。受影响的员工感到更加焦虑，并且不太准备好直接与客户互动。对抗工作准备度降低需要简化数据访问或通过汇总仪表板改进数据的展示。

IA 可能会影响员工对整体*工作安全感*的感觉。衡量工作安全感的两个方面包括：首先，我们可以通过*工作安全感指数*问卷来衡量员工认为他们的工作有多“稳定”；其次，我们可以通过*工作安全感满意度量表*来衡量员工的态度，即他们如何看待自己的工作安全感。如果可行，我们可以让人力资源部门在 IA 实施前后进行测量，以确定是否存在需要解决的工作安全感问题。

如果 IA 的目标不是减少员工人数，那么应明确传达“无岗位流失”的信息，并公布将用于评估 IA 项目成功与否的可衡量指标。SS&C 自实施 IA 以来提出的一个信息是，不是裁员，而是减缓招聘。如果目标是裁员，应准备培训计划，教育将被保留的员工如何与数字工作者协作。还应修订员工的工作角色定义。这两项行动可以帮助减少对工作不安全的感知。

防止对员工产生更广泛负面影响的关键在于在整个组织中培养对 AI 情绪的理解。在所有公司中，都会有支持或希望与 AI 技术合作的人，也会有那些不支持的人。IA 的首次推广应针对那些对 AI 持积极态度的个人或部门，因为这可以提高最初获得积极结果的几率，从而有助于说服业务的其他领域认识到 IA 的好处。

### 管理层对 IA 的抵制

IA 可能会面临*管理层抵制*，因为它通常被描绘为减少人员数量和成本的方式。如果管理层的人员数量保持不变或减少，他们自然会担心他们的预算、影响力范围以及达到 KPI 的能力。IA 可能导致管理层被动抵制、拖延甚至积极破坏 IA 的努力。由不合作的管理团队收集的 IA 指标通常会低估，以破坏 IA。

为了了解管理层对自动化的抵制，已经进行了研究。*高级管理层*的反对主要是因为缺乏知识。*中层管理层*，他们直接监督将被 IA 取代的员工或流程，对反对的原因更为复杂。按重要性排序，这些原因包括缺乏信心、培训不足和对工作重要性的担忧。失去人员数量的恐惧也被直接提及，作为管理层的主要担忧。

研究中提出的解决管理层抵制的方法在实践观点上相当模糊。减少抵制的建议包括教育、使用数据来说服管理层、逐步实施变革和培训。减少人员流失恐惧的一种实际方法（假设这不是 IA 的主要目标）是明确要求经理提交计划，说明 IA 实施后员工将执行的其他增值任务。规划如何利用他们的员工在释放他们的时间后使用，可以帮助缓解恐惧。

经理们也担心 IA 可能会引发*员工流失*并降低员工的留存率。已有针对 AI 采纳及其对员工影响的具体研究。正如预期的那样，对 AI 持负面态度的员工在 AI 被采纳时更有可能离职。再次强调，我们应该了解员工对 AI 的看法，并针对那些对 AI 持正面看法的员工使用用例。当员工感到公司给予他们支持时，离职意向会减弱。这包括许多事情，如团队建设活动、职业规划、员工发展、教育等。

另一个关键的管理层担忧是*财务损失*。这主要体现在两个方面。首先是通过诉讼造成的财务损失——例如，如果属于受保护类别的某个人认为他们因为机器学习模型中的偏见而受到了不公平的对待，并决定采取法律行动。第二种类型的财务损失是由于预测错误导致的进一步错误处理。我们可以通过允许管理层审查现有的治理结构来管理这些担忧，以选择用于生产使用的模型，选择 IA 用例，并审查特定的预测。

## 角色和职业路径

然而，如果策略是内部发展机器学习能力，将出现一些新的角色。第一个是*人工智能数据科学家*。这个人将负责分析数据、构建模型、测试模型以及评估它们。我建议雇佣具有这些技能的人，或者从组织内部的其他地方借用这个人，而不是首先内部培养数据科学家。主要原因是因为成为一名数据科学家所需的时间投资非常长。

这个数据科学家可以组建一个由*IA 开发者*组成的团队，这是一个介于数据科学家和自动化开发者之间的混合角色。IA 开发者将从强大的 RPA 基础开始，并随着时间的推移逐渐建立起他们的数据科学技能。IA 开发者还应关注市场上可用的商业机器学习服务。拥有足够经验的 IA 开发者可以通过横向职业发展成为全职数据科学家。

一个重要的问题是，谁管理机器学习基础设施。随着人工智能（IA）项目的增长，我们可能需要一个专门的*人工智能技术架构师*来定义适当的部署方法，管理部署和回滚，监控数据，以及管理支持人工智能的其他基础设施问题。与通常不是全职职位的“传统”RPA 技术架构师角色不同，由于机器学习模型持续的管理需求，人工智能技术架构师可能会因为人工智能项目的增长而有足够的工作量，最终可能成为全职职位。

期望普通开发者能够构建和部署机器学习模型是不切实际的。普通开发者最终将能够使用 AutoML 服务（例如，在第十三章中讨论的**Decision**），以及**大型语言模型**（LLMs），尽管使用 LLMs 存在许多挑战需要克服，例如处理幻觉结果、幻觉置信度分数以及解析非结构化响应。*治理委员会*和*设计权威机构*将需要制定关于普通开发者如何使用 AI 的指导方针。

# 设计

在本节中，我们将更深入地探讨*评估和优先级排序*的子主题，该主题讨论如何选择开发 IA 流程，以及*需求设计*，该主题讨论捕捉业务流程的步骤和功能需求。

## 评估和优先级

在流程发现过程的**评估阶段**，在使用案例进行分析之前，应该有一个经验丰富的数据科学家的背书，以确定机器学习部分是否可行。分析阶段的一部分应该包括 ML 模型的 POC（原型）开发或测试现成的 API，以确保有足够的准确性（或其他所需指标）来证明继续进行 IA 用例的合理性。再次强调，治理委员会上必须有一位对 ML 有经验的成员，以便批准 IA 开发流程。

一个重要的评估领域是 IA 将如何**影响现有的 SLA**。虽然我们预计 IA 会增加可以完成的工作的整体吞吐量，但 SLA 也可能包括基于**质量的目标**，这取决于所使用的模型的准确性。一个基于质量的目标示例是保持客户满意度评分在 3/5 以上。

新基础设施的引入对部署机器学习（ML）也产生影响，因为它是一个新的**停机源**。许多机器学习模型托管在主要的云平台上，如 GCP、AWS 和 Azure。2021 年 12 月，AWS 在工作日期间出现了多次故障。Azure 在 2023 年 1 月也发生了重大故障。如果您的模型当时托管在这些平台上，那么由于机器学习模型不可用，您可能会违反服务等级协议（SLA）。

## 需求设计

在 IA（智能自动化）方面，我们需要帮助业务定义之前不存在的要求。例如，触发预测的 HITL（人类-机器交互）审查的**标准是什么**？如第四章所示，这可以包括随机抽样、不同标签的不同阈值，或基于公式的方 法。具体的阈值值本身还不必选择，因为它们应该基于候选模型的实验来选择。

业务还需要定义**如何向审阅者展示预测数据**，例如通过 Excel、自定义开发的网站或数据库。一旦做出预测，我们需要知道是否存在一个最大允许的**完成预测与人工审查之间的延迟**，这对于 SLA 目的而言。

对于数据和模型本身也可能有要求。在某些用例中，一些数据字段，如年龄和性别，受到保护，不能用于模型。如果数据被认为是**敏感的**，那么它可能不能发送到组织外部，这会排除在线机器学习 API 在我们的解决方案设计中被考虑。

如果模型需要**可解释性**，这意味着应该优先考虑某些类型的回归和树模型。我们可能已经考虑了**期望的算法**，如深度学习或 LLM（大型语言模型），这会对解决方案提出硬件要求（GPU）。我们还需要了解预测是否**时间敏感**，因为这也会告知数据科学家哪些算法是可能的，以及部署解决方案的大致硬件需求。

# 开发

**方法和团队合作**子部分受到 IA 的影响，因为 ML 模型有一个并行于传统 RPA 开发的独立开发周期。在**交付控制**方面，一个足够高质量的模型也常常作为继续开发 IA 解决方案的通过与否的决定因素。在 IA 下，**测试和质量保证**永远不会真正停止，因为 ML 模型需要持续的监控以确保质量水平得到维持。

## 方法与团队合作

机器学习模型开发通常独立于 RPA 开发。虽然 BP 有六个阶段的交付方法（定义、设计、构建、测试、UAT 和部署），但这并不完全适用于机器学习模型构建。例如，许多机器学习模型开发方法都有数据分析和完善阶段，这些在 RPA 中是不需要的。

如果机器学习模型是由不同的团队构建的，我们不需要过多考虑正在使用什么特定的机器学习方法。然而，如果机器学习是内部构建的，IA 团队应该寻求标准化他们的机器学习模型开发方法。网上有许多这样的例子——例如，来自 AWS ([`docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html`](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/well-architected-machine-learning-lifecycle.html)) 和 GCP ([`cloud.google.com/blog/products/ai-machine-learning/making-the-machine-the-machine-learning-lifecycle`](https://cloud.google.com/blog/products/ai-machine-learning/making-the-machine-the-machine-learning-lifecycle))。

机器学习模型开发的生命周期在**流程定义文档**最终确定之前开始是很正常的，尤其是在收集和分析训练数据时。在开发的**设计**阶段，我们需要与数据科学家合作，选择机器学习模型与 BP 之间的接口，无论是 Web API 调用、可执行文件、程序脚本还是代码阶段。我们还需要数据科学家提供不同的预测响应示例，以便在 BP 中进行模拟。这允许 RPA 开发者即使机器学习程序尚未准备好，也能在本地测试他们的工作。还应该讨论功能需求，例如所需的 SLA、响应时间、我们是否可以批量发送预测请求等。所有这些都应该记录在**解决方案设计文档**中。

对于最初的几个 IA 项目，RPA 团队通常需要从组织内部的其他地方借用专业知识。如果数据科学家没有完全致力于项目，IA 项目的工作可能会被降级，交付时间表可能会推迟。

设计权威机构应有一名具有实际开发模型经验的成员。如果模型是内部开发的，需要考虑模型是否可以设计为跨多个 IA 用例重用，或者模型是否必须保持独立。设计权威机构的工作部分将扩展到跟踪正在使用的机器学习模型及其应用位置。

## 交付控制

在 RPA 或等效机器学习开发阶段的构建阶段，我们需要记住，预测精度不足可能导致在整体 IA 解决方案的开发方面做出不继续开发的决策。从某种意义上说，机器学习模型开发应略领先于 RPA 开发，以最大限度地减少开发工作浪费的风险。

机器学习模型的用户验收测试可能完全独立于 RPA 进行。机器学习模型 UAT 的完成应作为开始 RPA UAT 前的入门标准之一。

## 测试和质量保证

在机器学习模型的用户验收测试（UAT）期间，捕捉预测结果和置信度得分非常重要。这些数据有助于我们校准不同标签所需的适当置信度得分，以防使用阈值来确定是否需要人工审查。

IA 与 RPA 之间的主要区别之一是机器学习模型的质量保证（QA）永远不会真正停止。机器学习运营团队应定期监控机器学习预测的结果，以检测潜在的漂移。

# 运营

智能分析（IA）会影响部署和发布，因为即使业务逻辑（流程）或应用程序（对象）没有发生变化，也预期会进行机器学习部署。由于机器学习的加入带来了许多需要执行的操作，包括监控模型、监控数据、导出 HITL 审查结果等，因此支持模型有众多变更。

## 部署和发布

如前一章所述，机器学习模型应定期部署到生产环境中，部署时间表应独立于流程和对象更新。用于机器学习模型的部署方法应通知控制室操作员如何更新整体智能分析（IA）解决方案，以保持可审计性，以及如何在部署的新模型出现问题时进行回滚。建议选择一种不需要停机即可回滚的部署方法，并且模型的预测响应应返回用于做出预测的模型版本，以便进行审计。部署新的机器学习模型还应需要正式的变更请求，并需获得批准。

## 支持模型

IA（智能代理）为支持模型带来了重大补充。我们现在必须担心 ML 模型的正常运行时间（业务连续性），确保 ML 模型的准确性（这可以通过随机抽样进行评估），以及可能的人类审查的 SLA（服务等级协议）。每个 ML 模型都将有自己的 ML 审查员，他们可能也需要培训和访问 BP（业务流程），因为他们可能会手动更改工作队列项的状态或编辑会话变量以重新创建审查数据。

如果 ML 模型由 IA 团队维护，模型及其数据需要持续监控，以防止性能下降和数据漂移。这通常是通过基于 Web 的仪表板完成的，该仪表板读取 ML 服务器日志。如果 ML 通过 Web API 部署，则这种方法有效，但如果模型是通过 CLI（命令行界面）或代码阶段调用，则不适用。在这种情况下，支持模型需要包括从工作队列或会话日志中导出 ML 结果，供数据科学家进行分析，并将其纳入监控系统中。无论 BP 使用何种方法进行 ML 预测，我们仍然需要将关于已审查预测的信息反馈给数据科学家，因为该数据不在 ML API 服务器日志中。

监控数据和模型可能导致 ML 模型的*定期更新和部署*。其他关注点，如 ML 库的更新、通过 HITL（人机交互学习）审查接收到的新的训练数据，或对新算法的实验，也可能导致模型更新。业务逻辑的变化也可能触发 ML 模型的更改。IA 支持团队需要非常熟悉将新模型部署到生产环境中。

另一个主要关注点（虽然它与引用处理和异常处理相关，但并不是这两者之一）是所谓的*时间滞后效应*。想象一下，我们是一家使用机器学习（ML）来判定账户开立申请是否欺诈的银行。如果 ML 模型存在缺陷，许多欺诈性账户创建请求将被进一步处理，数据将被发送到许多其他系统。在模型缺陷被检测到之前，可能已经处理了数千个账户开立申请。在做出错误的 ML 预测和发现它之间需要撤销或纠正的工作量，就是时间滞后效应。

除非 ML 模型完美无缺，否则时间滞后问题是不可避免的。支持模型需要考虑如何处理个别案例——例如，如果业务用户标记了一个由错误的 ML 预测导致错误处理的案例。支持模型还需要处理批量案例，这些案例可能是由有缺陷的 ML 模型引起的。甚至可能需要创建一个专门的自动化流程，专门用于撤销 IA 解决方案在预测后执行的操作。

支持模型应尝试解决*如何分配预测责任*的问题，该预测导致某种形式的损失。在某个时候，需要有人或团队对由错误的机器学习预测引起的错误处理负责。是 IA 团队或数据科学团队的人？是签署了模型测试结果批准的业务用户？是审查员（假设所讨论的预测已被审查过）？这是在指责发生之前所有各方需要达成一致的事项。

机器学习责任是法律研究的一个活跃领域。我发现法律专家认为，使用预测的公司将被追究责任，即使机器学习模型开发完全外包。虽然使用预测的公司可以尝试寻求对第三方提起法律诉讼，但几乎不可能证明算法有缺陷。

对责任归属的直观分配可能是给维护模型或审查预测的个人或团队，但这两者都可能被外包。如果这些功能已经被外包，那么下一个直观的责任分配将转向在 UAT（用户验收测试）后签署模型批准的业务用户。

最后，如果 IA（可解释人工智能）实践较为先进，那么实施一些通用的*机器学习可解释性方法*是值得考虑的，例如 LIME 和 SHAP。虽然这些可解释性算法通常被认为是在将模型部署到生产之前的一个审计步骤，但它们也可以在帮助 IA 团队调查对业务用户有问题的预测以及找到避免未来出现这些问题的模型调整方法时发挥作用。

# 摘要

BP ROM 是一个框架和方法论，指导整个自动化程序。对 ROM 有良好的理解可以显著提高实现自动化目标并更快地扩展数字工作力的可能性。虽然 ROM 是一个无价的资源，但它不提供如何将机器学习预测添加到 RPA 流程的具体指导。

在本章中，我介绍了 ROM 2 的五个基础，并讨论了从 RPA 转向 IA 最受影响的子主题。讨论是基于我的 IA 研究发现和我在机器学习方面的经验。IA 主要通过在现有的 RPA 问题之上增加一套独立的机器学习关注点来影响 ROM。

这就结束了关于 BP（业务流程自动化）管理方面的章节。在下一章中，我们将通过考察两个真实案例来巩固本书所学的内容。

# 第四部分：真实场景和其他 Blue Prism 产品

在*第四部分*中，我们将通过考察两个基于现实案例的情景，来整合本书的*第二部分*和*第三部分*的内容。第十一章描述了一个包含三个不同机器学习模型的 IA 用例。在这里，我们将分析机器学习模型的要求，以选择合适的设计方案，并通过使用第七章中开发的 IA 流程模板来实施解决方案结构。

第十二章描述了一个不同的用例，其中机器学习可审计性是首要关注点。在这里，我们将通过实例说明如何部署机器学习模型的新版本以及如何回滚。我们还将探讨如何通过 BP 数据库提取机器学习审计数据。

在整本书中，我们只讨论了主要的 BP 产品。BP 生态系统中还有许多其他产品，它们也与 IA 直接或间接相关。第十三章描述了这些附加产品、它们的目的以及如何用于 IA。最后，我们通过讨论三个未来的 IA 趋势来结束本书。

本部分包含以下章节：

+   第十一章，*处理退款*

+   第十二章，*电力服务中断*

+   第十三章，*其他智能蓝宝石产品*

+   *附录，IA 风险管理*
