

# 调试以实现负责任人工智能

开发成功的机器学习模型并不仅仅是关于实现高性能。当我们提高我们模型的表现时，我们都感到兴奋。我们觉得自己有责任开发一个高性能的模型。但我们也有责任构建公平和安全的模型。这些超越性能改进的目标，是*负责任机器学习*或更广泛地，*负责任人工智能*的目标之一。作为负责任机器学习建模的一部分，我们在训练和预测模型时应该考虑透明度和问责制，并考虑我们数据和建模过程的治理系统。

在本章中，我们将涵盖以下主题：

+   机器学习中的无偏建模公平性

+   机器学习中的安全和隐私

+   机器学习建模中的透明度

+   可问责和可检查的建模

+   数据和模型治理

到本章结束时，您将了解负责任机器学习建模中的需求和不同关注点以及挑战。您还将了解不同的技术，这些技术可以帮助我们在负责任建模的同时确保隐私和安全，在开发机器学习模型时。

# 技术要求

在阅读本章之前，您需要了解机器学习生命周期的组成部分，因为这将帮助您更好地理解这些概念，并能够在您的项目中使用它们。

# 机器学习中的无偏建模公平性

机器学习模型会犯错误。但是当错误发生时，它们可能存在偏见，例如在*第一章*中提供的 COMPAS 示例，*超越代码调试*。我们需要调查我们的模型是否存在这种偏见，并修改它们以消除这些偏见。让我们通过更多示例来阐明调查我们的数据和模型是否存在这种偏见的重要性。

招聘对每家公司来说都是一个具有挑战性的过程，因为他们必须从提交简历和求职信的数百名申请人中确定最合适的候选人进行面试。2014 年，亚马逊开始开发一个招聘工具，使用机器学习来筛选求职者，并根据他们在简历中提供的信息选择最佳候选人。这是一个文本处理模型，它使用简历中的文本来识别关键信息并选择顶级候选人。但最终，亚马逊决定放弃该系统，因为该模型在招聘过程中对男性比对女性有偏见。这种偏见背后的主要原因是数据，这些数据主要是男性简历，被输入到机器学习模型中。模型学会了如何识别男性简历中的语言和关键信息，但面对女性简历时并不有效。因此，该模型在保持性别无偏见的同时，无法对工作申请的候选人进行排名。

一些机器学习模型被设计用来预测住院的可能性。这些模型可以帮助降低个人和人群的医疗保健成本。然而，这样的有益模型也可能存在自己的偏差。例如，住院需要获得和使用医疗服务，这受到社会经济条件差异的影响。这意味着用于构建预测住院可能性的模型的数据集，与贫困家庭相比，将包含更多社会经济条件较高人群的正面数据。这种不平等可能导致机器学习模型在住院决策中的偏差，从而进一步限制社会经济条件较低人群获得住院的机会。

医疗保健环境中机器学习应用中偏差的另一个例子出现在遗传研究中。这些研究因未能妥善考虑人群的多样性而受到批评，这可能导致所研究疾病的误诊。

偏差的两个主要来源包括数据，这些数据可能源自数据源，也可能在模型训练之前的数据处理中引入，以及算法偏差。让我们来回顾一下这两者。

## 数据偏差

你可能已经听说过计算机科学中的“垃圾输入，垃圾输出”概念。这个概念是关于这样一个事实：如果无意义的数据进入计算机工具，例如机器学习模型，输出也将是无意义的。用于帮助训练机器学习算法的数据可能存在各种问题，最终导致偏差，正如之前提到的。例如，数据可能未能充分代表某个群体，类似于亚马逊模型中输入的招聘数据中的女性。回想一下，拥有这种偏差数据不应该阻止我们构建模型，但我们必须在设计生命周期组件时考虑到这些偏差，例如数据选择和整理或模型训练，并在将模型投入生产之前测试我们的模型以检测偏差。以下是一些数据偏差的来源。

### 数据收集偏差

收集的数据可能包含偏差，例如，在亚马逊申请人分类示例中存在的性别偏差，在 COMPAS 中存在的种族偏差，在医院化示例中存在的社会经济偏差，或其他类型的偏差。作为另一个例子，想象一个用于自动驾驶的机器学习模型仅训练于白天拍摄的街道、汽车、人和交通标志的图像。该模型在夜间将具有偏差且不可靠。这种偏差可以通过在机器学习生命周期中的数据探索或数据整理步骤提供反馈后从数据收集和选择中移除。但如果在模型训练、测试和部署之前没有修订，那么在检测到预测偏差时，需要立即从模型监控中提供反馈，并在生命周期中使用以提供更少偏差的数据进行建模。

### 样本偏差

数据偏差的另一个来源可能是生命周期中数据收集阶段的样本数据点或人群样本。例如，当抽样学生填写调查问卷时，我们的抽样过程可能会偏向于女孩或男孩、富裕或贫困的学生家庭，或者高年级或低年级学生。这类偏差仅通过添加其他群体的样本难以轻易纠正。填写调查问卷或为新药测试设计临床试验的抽样过程就是数据收集过程中添加数据并不一定被允许的例子。其中一些数据收集过程需要在过程中预先定义人群，且人群定义在过程中不能改变。在这种情况下，在设计数据抽样过程时需要确定和考虑不同类型的可能偏差。

### 排除偏差

在数据清洗和整理过程中，在开始训练和测试机器学习模型之前，可能由于统计推理（如数据点的低信息含量或方差，或没有期望的特征）而删除特征。这些特征删除有时会导致我们的建模偏差。尽管不是排除在外，一些特征也可能导致最终机器学习模型预测的偏差。 

### 测量或标注偏差

测量和标注偏差可能由技术、专家或非专家数据标注员在生成或标注用于模型训练、测试和生产预测的数据时遇到的问题或差异引起。例如，如果使用一种类型的相机收集数据来训练用于图像分类的机器学习模型，那么在生产中如果使用另一种生成不同质量的图像的相机进行图像捕捉，生产中的预测可能可靠性较低。

## 算法偏差

与机器学习模型的算法和训练过程相关的系统错误可能是存在的。例如，在人脸识别工具中，数据可能被偏向于特定的种族或肤色，而算法可能会对具有特定肤色或种族的群体产生有偏见的预测。考虑到机器学习生命周期，在*第二章*中展示的模块化方式，*机器学习生命周期*将帮助你在模型监控等阶段识别问题。然后，可以为相关步骤提供反馈，例如数据收集或数据处理，以消除已识别的偏见。我们将在未来的章节中介绍检测偏见和解决偏见的方法。例如，我们可以使用机器学习可解释性技术来识别可能导致预测偏见的特征或其组合的贡献。

除了消除模型中的偏见外，在经历机器学习生命周期时，我们还需要考虑安全和隐私问题，这是我们接下来要讨论的主题。

# 机器学习中的安全和隐私

对于所有拥有物理或虚拟产品和服务的企业来说，安全都是一个关注点。60 年前，每家银行都必须确保其分支机构中现金和重要文件等实物资产的安全。但是，在进入数字世界后，他们必须建立新的安全系统，以确保其客户的数据、金钱和资产的安全，这些资产现在可以以数字方式传输和更改。机器学习产品和技术也不例外，需要拥有适当的安全系统。机器学习环境中的安全担忧可能与数据的安全、模型本身或模型预测有关。在本节中，我们将介绍与机器学习建模中的安全和隐私相关的三个重要主题：**数据隐私**、**数据中毒**和**对抗攻击**。

## 数据隐私

在生产中用户数据的隐私性，或者您用于模型训练和测试存储和使用的数据库是机器学习技术安全系统设计的重要方面。出于许多原因，数据需要保持安全：

+   如果数据包含从用户、个人或组织接收的机密信息

+   如果数据是根据法律合同从商业数据提供商那里许可的，并且不应通过您的服务或技术对他人可访问

+   如果数据是为您生成的，并被认为是您团队和组织的资产之一

在所有这些情况下，您需要确保数据的安全。您可以使用数据库和数据集的安全系统。如果部分数据需要在两个服务器之间以数字方式传输，您还可以在此之上设计加密过程。

### 数据隐私攻击

一些攻击旨在访问您数据集和数据库中的私有和机密数据，例如医院中的患者信息、银行系统中的客户数据或政府机构员工的个人信息。其中三种攻击是*数据重建攻击*、*身份识别攻击*和*个人追踪攻击*，所有这些都可以通过**互联网协议**（**IP**）跟踪来实现，例如。

## 数据中毒

数据的意义和质量的变化是数据安全中的另一个担忧。数据可能会被中毒，预测结果的变化可能对个人、团队和组织在财务、法律和道德方面产生严重影响。想象一下，你和你的朋友们设计了一个用于股票市场预测的机器学习模型，并且你的模型使用过去几天的新闻和股票价格作为输入特征。这些数据是从雅虎财经和不同的新闻来源等不同资源中提取的。如果你的数据库被中毒，通过改变某些特征值或收集的数据的变化，例如某只股票的价格历史，你可能会遭受严重的经济损失，因为你的模型可能会建议你购买一周内价值下降超过 50%的股票，而不是上涨。这是一个有财务后果的例子。然而，如果发生在医疗或军事系统中，数据中毒可能会产生致命的后果。

## 对抗性攻击

有时，通过在特征值上做出非常简单的改变，例如添加少量的噪声或扰动，你可以欺骗机器学习模型。这是生成对抗样本和对抗攻击背后的概念。

例如，在一个医疗人工智能系统中，一个良性（即，无害）的痣的图像可能会被诊断为恶性（即，在一般意义上有害和危险的）通过在图像中添加人眼无法识别的对立噪声或简单地旋转图像。将“*患者有背部疼痛和慢性酒精滥用史，最近还出现过几次...*”替换为“*患者有腰痛和慢性酒精依赖史，最近还出现过几次...*”这样的同义文本替换可能会将诊断从良性变为恶性（Finlayson 等人，2019 年）。在其他图像分类的应用中，例如在自动驾驶汽车中，简单的黑白贴纸有时会欺骗模型将停车标志的图像或停车标志视频帧分类（Eykholt 等人，2018 年）。

对抗性样本可能会在推理或训练过程中误导你的系统，并验证它们是否被注入到你的建模数据中并使其中毒。了解你的对手有三个重要方面可以帮助你保护你的系统——即攻击者的目标、知识和能力（*表 3.1*）：

| **关于对手的知识类型** | **不同类型知识的方面** | **定义** |
| --- | --- | --- |

| 攻击者的目标 | 安全违规 | 攻击者试图做以下事情：

+   避免检测

+   破坏系统功能

+   获取访问私有信息

|

| 攻击特异性 | 针对特定或随机数据点以生成错误结果 |
| --- | --- |
| 攻击者的知识 | 完美知识白盒攻击 | 攻击者了解系统的一切 |
| 零知识黑盒攻击 | 攻击者对系统本身没有任何了解，但通过生产中模型的预测收集信息 |
| 有限知识灰盒攻击 | 攻击者有有限的知识 |
| 攻击者的能力 | 攻击影响 | **因果性**：攻击者可以毒害训练数据并操纵测试数据**探索性**：攻击者只能操纵测试数据 |
| 数据操纵约束 | 对数据操纵的约束，以消除数据操纵或使其具有挑战性 |

表 3.1 – 关于对手的知识类型（Biggio 等人，2018）

## 输出完整性攻击

这种攻击通常不会影响数据处理、模型训练和测试，甚至不会影响生产中的预测。它出现在你的模型输出和展示给用户的内容之间。根据这个定义，这种攻击并不特定于机器学习环境。但在我们的机器学习系统中，仅基于展示给用户的输出理解这种攻击可能具有挑战性。例如，如果你的模型在分类设置中的预测概率或标签偶尔发生变化，展示给用户的结果将是错误的，但如果用户相信我们的系统，他们可能会接受这些结果。确保这类攻击不会挑战我们模型在生产中的结果完整性是我们的责任。

## 系统操纵

你的机器学习系统可能被故意设计的合成数据操纵，这些数据可能不存在，或者可能从未存在于模型训练和测试集中。这种在预测层面的操纵不仅可能导致调查模型错误预测的时间浪费，还可能毒害你的模型，并改变你的模型在测试和生产中的性能，如果数据进入你的训练、评估或测试数据。

## 安全和隐私机器学习技术

一些技术帮助我们开发用于数据存储、传输以及在机器学习建模中使用的数据的安全和隐私保护流程和工具：

+   **匿名化**：这项技术侧重于删除有助于识别个人数据点（如医疗数据集中的单个患者）的信息。这些信息可能非常具体，例如健康卡号码，在不同国家可能有不同的名称，或者更一般的信息，如性别和年龄。

+   **脱敏**：与匿名化不同，脱敏不是删除信息，而是将可识别的个人数据替换为合成替代品作为脱敏的一部分。

+   **数据和算法加密**：加密过程将信息（无论是数据还是算法）转换成新的（加密）形式。如果个人有权访问加密密钥（即用于解密过程的密码式密钥），则加密数据可以被解密（使其成为人类可读或机器可理解）。这样，在没有加密密钥的情况下获取数据和算法将几乎不可能或非常困难。我们将在第十六章“机器学习中的安全和隐私”中回顾加密技术，如**高级加密标准**（**AES**）*第十六章*。

+   **同态加密**：这是一种加密技术，通过机器学习模型进行预测时无需对数据进行解密。模型使用加密数据进行预测，因此数据可以在机器学习管道中的整个数据传输和使用过程中保持加密状态。

+   **联邦机器学习**：联邦机器学习依赖于将学习、数据分析、推理去中心化的想法，从而允许用户数据保持在单个设备或本地数据库中。

+   **差分隐私**：差分隐私试图确保删除或添加单个数据点不会影响建模的结果。它试图从数据点的组内模式中学习。例如，通过添加来自正态分布的随机噪声，它试图使单个数据点的特征变得模糊。如果可以访问大量数据点，基于大数定律（[`www.britannica.com/science/law-of-large-numbers`](https://www.britannica.com/science/law-of-large-numbers)），可以消除学习中的噪声影响。

这些技术并不适用于所有环境，也不一定有用。例如，当你有一个内部数据库并且只需要确保其安全性时，联邦机器学习将不会有所帮助。对于小型数据源，差分隐私也可能不可靠。

加密和解密过程

加密是将可读数据转换为人类不可读形式的过程。另一方面，解密是将加密数据转换回其原始可读格式的过程。您可以在[`docs.oracle.com/`](https://docs.oracle.com/)和[`learn.microsoft.com/en-ca/`](https://learn.microsoft.com/en-ca/)找到更多关于这个主题的信息。

在本节中，我们讨论了机器学习建模中的隐私和安全问题。即使我们构建了一个具有最小隐私担忧的安全系统，我们也需要考虑其他因素来建立对我们模型的信任。透明度就是这些因素之一。我们将在下一节介绍这一点。

# 机器学习建模中的透明度

透明度通过帮助用户理解模型的工作原理和构建方式，使用户信任您的模型。它还帮助您、您的团队、您的合作者和您的组织收集关于机器学习生命周期不同组件的反馈。了解生命周期不同阶段的透明度要求以及实现它们的挑战是值得的：

+   **数据收集**：数据收集的透明度需要回答两个主要问题：

    +   你在收集哪些数据？

    +   你想用这些数据做什么？

例如，当用户在注册手机应用时点击数据使用协议按钮，他们是在同意将他们在应用中提供的信息用于。但协议需要明确说明将要使用哪些用户数据以及用于什么目的。

+   **数据选择和探索**：在这些生命周期阶段，您数据选择的过程以及您如何实现探索性结果需要清晰。这有助于您从其他项目合作者和同事那里收集反馈。

+   **数据整理和建模数据准备**：在此步骤之前，数据几乎就像所谓的原始数据，没有对特征定义进行任何更改，也没有将数据分成训练集和测试集。如果您将这些生命周期组件设计成一个黑盒且不透明，您可能会失去其他专家在将来访问您的数据和结果时的信任和反馈机会。例如，想象一下，您本不应该使用医院的患者的遗传信息，而在生命周期这些步骤之后，您提供了称为 Feature1、Feature2 等特征。如果没有解释这些特征是如何生成的以及使用了哪些原始特征，人们就不能确定您是否使用了患者的遗传信息。您还需要对您如何设计测试策略以及如何将训练数据与验证和测试数据分开进行透明度说明。

+   **模型训练和评估**：模型训练的透明度有助于在从数据中学习时理解模型的决策和模式识别方面。训练和评估的透明度有助于直接用户、开发人员和审计员更好地评估这些过程。例如，确实有超过 99%的亚马逊 Prime 用户不想了解背后的机器学习建模。然而，我们的模型有时会被用户直接使用，例如医院中的医生进行诊断或制造设施中的员工。我们将讨论的*第六章*，“机器学习建模中的可解释性和可理解性”，是机器学习建模中的一个重要概念，它与透明度密切相关，并有助于用户更好地理解模型的工作原理。

+   **测试**：在生命周期测试阶段的决策透明度有助于更好地理解选择或拒绝模型的过程，或规划改进和调试相关代码、数据处理、或模型训练和评估。

+   **模型部署和监控**：除了从组织或团队中的其他开发人员和专家，或公众专家那里获取反馈外，您还需要让用户和审计员（无论在组织内部还是外部）清楚模型在生产中的效果以及收集用于监控的数据。

尽管需要透明度，但透明度的增加可能会从安全和隐私的角度使您的系统变得脆弱。因此，您不能在不考虑隐私和安全的情况下追求最大程度的透明度。

公平性、偏差消除、隐私和透明度都是负责任机器学习建模的因素。但我们需要记住，实现公平和安全模型及系统的责任在我们身上。

# 负责任且开放接受检查的建模

我们作为不同企业或公司、研究机构或大学，或自由职业者的员工开发的模型，最终可能会应用于不同的应用，如医疗保健、金融、制造、营销、零售、交通、媒体和娱乐。我们的模型可以使用患者数据来预测他们是否会患上癌症或糖尿病，或者他们是否会响应治疗。或者，它们可以使用银行客户的财务历史和其他信息来评估他们获得贷款的资格。另一个例子是，我们的模型可以使用人们购买的历史记录向他们推荐新产品。

正如我们在本章中讨论的，我们必须注意数据的隐私和模型的隐私，提供一个公平无私的模型，并尽可能使我们的模型透明。但我们必须记住，我们在开发可靠和负责任的机器学习模型时，必须对这些所有问题负责。我向著名的机器学习聊天机器人 ChatGPT 提问，“*你对你的回应负责吗？*”它的回答如下：

*“作为一个机器学习模型，我没有能力像人类一样对我的回应承担责任。我被设计成根据接收到的输入生成回应，但我没有个人信念、意图和动机。我无法独立做出决策或选择。我的回应是我被创建时使用的训练数据和算法的反映，我的创造者负责确保我按预期运行。”*

注意

本章接下来的几段引用了法律来源的引文，这些引文未作改动，以确保法律内容不会被不恰当地改写。然而，这不是一本法律书籍，本书的任何内容都不应被视为法律建议。

但问责制不是可选择的。在过去的几年中，已经出台了立法和法规，以使机器学习模型和产品的开发者和所有者对我们本章中讨论的担忧负责。例如，**欧盟的**（**EU 的**）**通用数据保护条例**（**GDPR**）列出了正在处理个人数据的个人的权利，以赋予他们对其数据的控制权。它是通过以下方面实现的：

+   需要个人明确同意处理其数据

+   数据主体更容易访问其数据

+   纠正权、删除权和被遗忘权

+   包括对个人数据进行画像在内的反对权

+   从一个服务提供商到另一个服务提供商的数据可移植权

欧盟还建立了一个司法救济和赔偿系统（来源：[`www.consilium.europa.eu/en/policies/data-protection/`](https://www.consilium.europa.eu/en/policies/data-protection/))。

欧盟后来制定了**人工智能**（**AI**）**法案**，这是主要监管机构制定的第一部人工智能法律（来源：[`artificialintelligenceact.eu/`](https://artificialintelligenceact.eu/))。

但这些法规不仅限于欧盟。例如，*白宫科学和技术政策办公室*发布了以下人工智能权利法案蓝图，以保护人工智能时代的美国公众（来源：[`www.whitehouse.gov/ostp/ai-bill-of-rights/`](https://www.whitehouse.gov/ostp/ai-bill-of-rights/))。

加拿大后来还提出了 C-27 人工智能法案，该法案“通过一系列主要罪行建立其基本义务，保护公民免受错误的人工智能侵害，并要求对数据使用进行普遍记录的义务”（来源：[`www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624`](https://www.lexology.com/library/detail.aspx?g=4b960447-6a94-47d1-94e0-db35c72b4624))。

本章我们要讨论的最后一个主题是机器学习建模中的治理。在下一节中，你将了解治理如何帮助你和你所在的组织开发机器学习模型。

# 数据和模型治理

机器学习建模中的治理是关于使用工具和程序来帮助你、你的团队和你的组织开发可靠和负责任的机器学习模型。你不应该将其视为对如何进行项目的任何限制，而应将其视为减少未检测错误风险的机会。机器学习中的治理应该设计成帮助你和你所在的组织实现帮助人类和商业的目标，避免可能产生道德、法律或财务后果的过程和模型。以下是一些在团队和组织中建立治理体系的方法示例：

+   **定义指南和协议**：鉴于我们希望检测模型中的问题，并在性能和责任方面改进模型，我们需要设计用于简化和一致性的指南和协议。我们需要定义被认为是模型问题的标准和方法，例如从安全角度考虑，以及被认为是值得花费时间和精力进行模型改进的机会。我们需要记住，考虑到本章讨论的主题和生命周期中的不同步骤，机器学习建模并非易事，你不应该期望与你合作的每个开发者都像专家一样了解所有这些。

+   **培训和指导**：如果你是管理者，你需要寻找导师和培训项目，阅读书籍和文章，然后为你的团队提供这些机会。但你也需要将你或你的团队学到的知识应用到实践中。机器学习建模中的每个概念都有其挑战。例如，如果你决定使用防御机制来对抗对抗性攻击，这并不像加载一个 Python 库并希望永远都不会发生任何事情那样简单。所以，实践你所学的，并为你的团队提供将所学知识应用到实践中的机会。

+   **定义责任和问责制**：照顾机器学习生命周期的所有方面以构建技术和处理本章中讨论的所有责任主题，并不是一个人的工作。话虽如此，团队和组织中个人的责任和问责制需要明确界定，以减少工作冗余并确保没有遗漏任何事项。

+   **使用反馈收集系统**：我们需要设计简单易用且最好是自动化的系统来收集反馈并在机器学习生命周期中采取行动。这种反馈将帮助负责生命周期每个步骤的开发者，并最终导致在生产中推出更好的模型。

+   **使用质量控制流程**：我们需要定量和预定义的方法和协议来评估训练后或生产中的机器学习模型的质量，或者评估机器学习生命周期每个阶段输出的处理数据。拥有定义和记录的质量控制流程有助于我们实现可扩展的系统，以便更快、更一致地进行质量评估。然而，这些流程可以根据新的标准和与数据和相应的机器学习模型相关的风险进行修订和调整。

现在我们已经了解了负责任机器学习建模的重要性，并回顾了实现它的关键因素和技术，我们准备进入本书的下一部分，并深入了解开发可靠、高性能和公平的机器学习模型和技术的技术细节。

# 摘要

在本章中，我们讨论了负责任人工智能的不同要素，例如数据隐私、机器学习系统的安全性、不同类型的攻击以及设计针对它们的防御系统、机器学习时代的透明度和问责制，以及如何在实际中利用数据和管理模型来开发可靠和负责任的模式。

本章和前两章，构成了本书的*第一部分*，介绍了机器学习建模和模型调试的重要概念。*第二部分*包括如何改进机器学习模型的主题。

在下一章中，你将了解检测机器学习模型问题以及提高此类模型性能和泛化性的方法。我们将涵盖使用真实生活示例进行模型调试的统计、数学和可视化技术，以帮助您快速实施这些方法，以便您可以调查和改进您的模型。

# 问题

1.  你能解释两种类型的数据偏差吗？

1.  白盒攻击和黑盒攻击有什么区别？

1.  你能解释一下数据与算法加密如何帮助保护你系统的隐私和安全吗？

1.  你能解释差分隐私和联邦机器学习之间的区别吗？

1.  透明度如何帮助你在增加你的机器学习模型用户数量方面？

# 参考文献

+   Zou, James, 和 Londa Schiebinger. *AI 可能是性别歧视和种族歧视的 – 是时候让它变得公平了*。（2018）：324-326.

+   Nushi, Besmira, Ece Kamar, 和 Eric Horvitz. *迈向可问责的 AI：用于表征系统失败的混合人机分析.* AAAI 人机计算与众包会议论文集。第 6 卷。2018.

+   Busuioc, Madalina. *可问责的人工智能：对算法负责.* 公共行政评论 81.5 (2021): 825-836.

+   Unceta, Irene, Jordi Nin, 和 Oriol Pujol. *算法问责制中的风险缓解：机器学习副本在其中的作用.* Plos one 15.11 (2020): e0241286.

+   Leonelli, Sabina. *数据治理是关键：重新概念化数据在数据科学中的角色.* 哈佛数据科学评论 1.1 (2019): 10-1162.

+   Sridhar, Vinay, 等人. *模型治理：减少生产 {ML} 的无政府状态.* 2018 USENIX 年度技术会议 (USENIX ATC 18)。2018.

+   Stilgoe, Jack. *机器学习、社会学习和自动驾驶汽车的治理.* 科学研究 48.1 (2018): 25-56.

+   Reddy, Sandeep, 等人. *AI 在医疗保健中的应用治理模型.* 美国医学信息学协会杂志 27.3 (2020): 491-497.

+   Gervasi, Stephanie S., 等人. *机器学习中的潜在偏见及其对健康保险公司的应对机会：文章探讨了机器学习中的潜在偏见以及健康保险公司应对它的机会.* 健康事务 41.2 (2022): 212-218.

+   Gianfrancesco, M. A., Tamang, S., Yazdany, J., & Schmajuk, G. (2018). *使用电子健康记录数据的机器学习算法中的潜在偏见.* JAMA 内科学杂志，178(11)，1544.

+   Finlayson, Samuel G., 等人. *对抗医疗机器学习攻击.* 科学 363.6433 (2019): 1287-1289.

+   Eykholt, Kevin, 等人. *对深度学习视觉分类的鲁棒物理世界攻击.* IEEE 计算机视觉与模式识别会议论文集。2018.

+   Biggio, Battista, 和 Fabio Roli. *野模式：对抗机器学习兴起十年后.* 模式识别 84 (2018): 317-331.

+   Kaissis, Georgios A., 等人. *医学成像中的安全、隐私保护和联邦机器学习.* 自然机器智能 2.6 (2020): 305-311.

+   Acar, Abbas, 等人. *同态加密方案综述：理论与实现.* ACM 计算机调查 (Csur) 51.4 (2018): 1-35.

+   Dwork, Cynthia. *差分隐私：结果综述.* 国际计算模型理论与应用会议。Springer，柏林，海德堡，2008.

+   Abadi, Martin, 等人. *具有差分隐私的深度学习.* 2016 ACM SIGSAC 计算机与通信安全会议论文集。2016.

+   杨强，等。*联邦机器学习：概念与应用*。ACM 智能系统与技术交易（TIST）10.2（2019）：1-19。

# 第二部分：提高机器学习模型

这一部分将帮助我们过渡到精炼和理解机器学习模型的关键方面。我们将从深入探讨检测模型中的性能和效率瓶颈开始，接着提出可操作的战略来提升它们的性能。随后，叙述转向可解释性和可说明性的主题，阐明不仅构建出能工作的模型，而且我们能够理解和信任的模型的重要性。我们将通过介绍减少偏差的方法来结束这一部分，强调机器学习中公平性的必要性。

本部分包含以下章节：

+   *第四章*, *检测机器学习模型中的性能和效率问题*

+   *第五章*, *提高机器学习模型的性能*

+   *第六章*, *机器学习建模中的可解释性和可说明性*

+   *第七章*, *减少偏差和实现公平性*
