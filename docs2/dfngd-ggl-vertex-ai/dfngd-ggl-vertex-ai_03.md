# 3

# 一切都是关于数据——存储和转换 ML 数据集的选项

在机器学习项目中，真正的工作只有在项目开发环境中可用所需数据之后才开始。有时，当数据变化非常频繁或用例需要实时数据时，我们可能需要设置一些数据管道，以确保所需数据始终可用于分析和建模目的。最佳的数据传输、存储或转换方式也取决于底层数据的大小、类型和性质。在现实世界中收集的原始数据通常规模庞大，可能属于多种类型，如文本、音频、图像、视频等。由于现实世界数据的性质、大小和类型的多样性，设置正确的基础设施来大规模存储、传输、转换和分析数据变得非常重要。

在本章中，我们将学习将数据移动到 Google Cloud 环境的不同选项、不同的数据存储系统以及高效地对大规模数据进行转换的方法。

在本章中，我们将探讨以下关于数据的话题：

+   将数据移动到 Google Cloud

+   数据存储位置

+   数据转换

# 将数据移动到 Google Cloud

当我们在**Google Cloud Platform**（**GCP**）上启动机器学习项目时，第一步是将所有与项目相关的数据移动到 Google Cloud 环境中。在向云传输数据时，需要关注的关键问题是可靠性、安全性、可扩展性和管理传输过程的便捷性。考虑到这些因素，Google Cloud 提供了四个主要的数据传输工具，以满足各种用例的客户需求。一般来说，这些工具对任何类型的数据传输目的都很有用，包括数据中心迁移、数据备份、内容存储和机器学习。鉴于我们目前的重点是使数据可用于机器学习用例，我们可以利用以下任何一种传输解决方案：

+   Google Cloud Storage 传输工具

+   BigQuery 数据传输服务

+   存储传输服务

+   传输设备

让我们了解每种传输解决方案。

## Google Cloud Storage 传输工具

当我们的数据集不是太大（几 TB 以内即可）且我们希望将其存储在**Google Cloud Storage**（**GCS**）桶中时（GCS 是一个对象存储系统，与我们的计算机中的本地文件系统非常相似；我们将在下一节中了解更多关于它的信息），此选项是合适的。Google Cloud 提供了从我们的计算机直接上传数据到这些 GCS 桶的工具。我们可以使用以下方法之一上传一个或多个文件，甚至是一个包含文件的文件夹，并使用 Google Cloud 控制台中的**上传进度**窗口跟踪上传进度。

这里是上传文件或文件夹到 GCS 桶的三种方法：

+   使用 Google Cloud 控制台 UI

+   使用命令行

+   使用 REST API（JSON API）

让我们更详细地看看这些方法。

使用 Google Cloud 控制台 UI

使用云控制台用户界面上传文件或包含文件的文件夹到 GCS 非常简单。当我们上传一个文件夹时，文件夹内的层次结构也会被保留。按照以下简单步骤使用 UI 上传数据到 GCS 存储桶：

1.  打开浏览器并访问**Google Cloud** **控制台**页面。

1.  在左侧面板中，点击**云存储**并打开**存储桶**页面。它将列出我们项目中所有现有的存储桶。

1.  如果相关的存储桶已经存在，请点击其名称；否则，创建一个新的存储桶来存储上传的文件或文件夹。

1.  一旦我们进入存储桶，我们将看到存储桶详情和现有内容。现在，我们可以直接使用**上传文件**或**上传文件夹**按钮上传数据。UI 还提供了创建新文件夹和下载或删除文件选项。

注意

文件夹上传仅支持使用 Chrome 浏览器。它可能与其他浏览器不兼容。

### 使用命令行

Google Cloud 还提供了一个名为**GSUTIL**的开源命令行实用工具。我们可以利用 GSUTIL 进行脚本数据传输，也可以使用简单的命令来管理我们的 GCS 存储桶。对于大规模流式数据，它支持多线程/多进程数据传输以推送脚本输出。它可以在*rsync*模式下操作并传输数据的增量副本。

GSUTIL 命令与 Unix 命令非常相似。以下示例展示了如何将文件复制到我们之前创建的 GCS 存储桶中：

```py
$ gsutil cp Desktop/file.txt gs://my-bucket-name
```

同样，我们可以使用`ls`命令列出存储桶的内容：

```py
$ gsutil ls gs://my-bucket-name
$ gsutil ls gs://my-bucket-name/my-data-folder
```

### REST API (JSON API)

JSON API 接口让我们能够以编程方式访问或操作 GCS 数据。这种方法更适合熟悉网络编程和创建使用 HTTP 请求消费网络服务的应用程序的软件开发人员。例如，我们可以使用以下 HTTP 请求列出特定存储桶的对象：

```py
GET https://storage.googleapis.com/storage/v1/b/my-bucket/o
```

重要提示

要使用上述方法访问、操作或上传数据，我们必须拥有适当的**身份和访问管理**(**IAM**)权限。项目所有者可以向项目开发团队提供相关权限列表。

## BigQuery 数据传输服务

BigQuery 数据传输服务目前支持从 Google **软件即服务** (**SaaS**)应用、外部云提供商、数据仓库如**Teradata**或**Redshift**以及一些第三方源加载数据。一旦数据可用，我们就可以直接在**BigQuery** (**BQ**)中进行分析或机器学习。它也可以用作数据仓库解决方案；我们将在接下来的章节中了解更多关于 BQ 的信息。

## 存储传输服务

与 GSUTIL 相比，存储传输服务是一种托管服务，适用于在不同云（AWS 和 Azure）、本地或谷歌云中不同存储桶之间快速且安全地传输数据。数据传输过程非常快，因为它利用了高带宽网络。它还处理重试并提供详细的传输日志。

## 转移设备

当我们想要迁移一个非常大的数据集且带宽有限时，此选项是合适的。转移设备是一种具有高内存容量的物理设备，可用于将数据传输并安全地运送到谷歌上传设施，在那里数据被上传到云存储。我们可以从云控制台订购设备，一旦收到设备，我们就可以开始复制我们的数据。最后，我们可以将设备运回谷歌，以将数据传输到指定的 GCS 存储桶。

对于大多数与机器学习相关的用例，前两种方法应该足以快速、安全且一致地传输数据。接下来，让我们更多地了解谷歌云上的 GCS 和 BQ 数据存储系统。

# 数据存储位置

GCS 和 BQ 是存储任何与机器学习用例相关的数据集的两个推荐选项，旨在实现高安全性和效率。如果底层数据是有结构的或半结构的，由于 BQ 具有现成的功能来操作或处理结构化数据集，因此 BQ 是推荐选项。如果数据包含图像、视频、音频和非结构化数据，那么 GCS 是存储它的合适选项。让我们更详细地了解这两个数据存储系统。

## GCS – 对象存储

我们从现实世界应用中收集的大量数据是无结构的。一些例子包括图像、视频、电子邮件、音频文件、网页和传感器数据。以经济高效的方式管理和存储如此大量的非结构化数据是一项相当具有挑战性的任务。如今，对象存储已成为存储如此大量静态数据和备份的首选解决方案。对象存储是一种计算机数据架构，旨在高效地处理大量结构化数据。在基于对象的存储系统中，每个数据对象都被视为一个独立的单元，包含元数据和唯一的标识符，这对于快速检索和定位数据非常有用。

GCS 是 Google Cloud 中的一个基于对象的存储系统。由于它是基于云的，GCS 数据可以全球访问并提供大规模存储。它是一个适合从小型企业到大型企业以经济高效且易于检索的方式存储大量数据的理想选择。对于您只需写入一次数据但需要非常频繁地读取数据的应用程序，对象存储效率更高。虽然它非常适合静态数据，但不是动态数据的良好解决方案。如果数据不断变化，我们不得不反复写入整个数据对象来修改它，这是低效的。

由于其多样化的好处，GCS 经常被 Google Cloud 上的机器学习从业者使用。以下是存储数据在如 GCS 这样的对象存储系统中的常见好处：

+   **大规模可扩展性**：通过简单地添加更多服务器或设备，对象存储可以无限扩展。

+   **更简单**：与文件存储系统不同，对象存储中没有层次结构或文件夹结构，因此检索相当简单。

+   **可搜索性**：由于元数据也是对象的一部分，因此很容易搜索特定对象。我们可以使用标签使对象更容易过滤。

+   **容错性**：由于它可以自动复制数据并在多个设备或地理位置之间存储，因此无需担心数据丢失。

+   **低成本**：对象存储经济高效，因此非常适合存储大量数据。其次，我们只为使用的容量付费。

虽然有这么多优点，但基于对象的存储系统也有一些局限性：

+   由于延迟问题，在设计 Web 应用程序时不能替代传统数据库

+   它们不适合数据快速变化且需要非常频繁地大量写入文件的情况

+   它们与操作系统挂载不太兼容，需要额外的客户端或适配器才能工作

现在我们对对象存储系统的优点和局限性有了相当的了解，我们将在未来的项目中根据相关需求利用它们。现在，让我们更多地了解 BQ。

## BQ – 数据仓库

BQ 是 Google Cloud 上提供的一个全托管和无服务器的数据仓库。它是一个可扩展分析大型数据集的皮字节级平台。BQ 的无服务器架构支持 SQL 查询以切片和切块大型数据集。其分析引擎非常可扩展，支持分布式分析，这样我们可以在几秒钟内查询千兆字节的数据，在几分钟内查询皮字节的数据。BQ 还支持机器学习，这意味着我们只需使用几个类似 SQL 的命令即可在 BQ 中训练和测试常见的 ML 模型。我们将在接下来的章节中学习**BigQuery 机器学习**（**BQML**）。

在幕后，BQ 以优化的列式存储格式存储数据，该格式针对分析查询进行了优化。BQ 内的数据通过具有行和列的表以数据库的形式呈现。它提供了类似于事务型数据库管理系统的**原子性、一致性、隔离性和持久性**（**ACID**）属性的全支持。BQ 通过在 Google Cloud 的多个位置和区域自动复制数据，为数据提供高可用性。除了 BQ 存储内部的数据外，它还提供了从外部源查询数据的灵活性，包括 GCS、Bigtable、Google Sheets 和 Spanner。

如果机器学习用例涉及结构化或半结构化数据，BQ 可以是存储和分析的最佳选择。在随后的章节中，我们将了解 BQ 是数据分析师和机器学习实践者极其有用的工具。以下是使用 BQ 作为数据仓库解决方案的一些常见好处：

+   **速度和规模**：使用 BQ，对大规模数据集进行查询只需几秒钟。BQ 被设计成轻松分析和存储非常大的数据集。它可以从千兆字节无缝扩展到艾字节。

+   **实时分析**：BQ 支持流数据摄取，并使其立即可用于查询。它与 BI 工具 Looker Studio 的集成使其能够提供实时和交互式的分析能力。

+   **机器学习**：BQ 提供了使用简单的 SQL 在结构化和非结构化数据上构建和运行机器学习模型的能力，时间非常短。BQ 模型可以直接导出，以便在 Vertex AI 预测服务上提供服务。

+   **安全性**：BQ 提供强大的安全性和细粒度的治理控制。数据默认在静态和传输过程中加密。

+   **多云支持**：BQ 允许在多个云平台之间进行数据分析。BQ 可以在不将数据移动到其他位置的情况下对数据进行分析，这使得它更加经济高效且灵活。

我们现在对两个流行的数据存储系统——GCS 和 BQ——及其优缺点有了相当的了解。根据具体的使用场景，我们现在应该能够选择合适的存储位置来存放我们的机器学习数据集。接下来，我们将深入了解 Google Cloud 上的数据转换选项。

# 数据转换

在现实世界应用中存在的原始数据通常是未结构化和嘈杂的。因此，不能直接将其输入到机器学习算法中。我们通常需要对原始数据进行多次转换，并将其转换为机器学习算法广泛支持的格式。在本节中，我们将了解在 Google Cloud 上以可扩展和高效的方式转换数据的多种选项。

在 GCP 环境中，这里有三种常见的数据转换选项：

+   在 Jupyter Notebooks 内进行临时转换

+   Cloud Data Fusion

+   可扩展的数据转换数据流管道

让我们更详细地了解这三种方法。

## 在 Jupyter Notebook 中的临时转换

机器学习算法是数学的，只能理解数值数据。例如，在计算机视觉问题中，图像在输入模型之前被转换为数值像素值。同样，在音频数据的情况下，它通常使用不同的转换（如**快速傅里叶变换**（FFT））转换为时频域。如果数据以表格格式存在，并且包含行和列，那么一些列可能包含非数值或分类类型的数据。这些分类列首先使用合适的转换转换为数值形式，然后输入到机器学习模型或神经网络中。这些转换中的一些可以直接在 Jupyter 笔记本中使用 Python 功能应用。

在将数据读入 Jupyter Notebook 后，我们可以应用任何所需的转换，使数据集准备好进行建模。一旦数据集准备就绪，我们可以将其保存到某个位置（例如，BQ 或 GCS），并附上版本号，以便可以直接将其读入多个不同的模型训练实验中。机器学习从业者经常通过应用不同的转换或特征工程来创建和保存多个版本的训练数据集。这使得比较不同版本数据的实验性能变得更容易。让我们了解一下在机器学习项目中非常频繁应用的一些常见转换。

从高层次来看，数据可以分为以下两类：

+   **数值数据**：正如其名所示，这种数据是数值的或可量化的——例如，年龄、身高和体重

+   **分类数据**：这种数据是字符串格式或定性数据——例如，性别、语言和口音

现在，让我们了解可以应用于这两种类型数据列的一些常见转换。

### 处理数值数据

数值数据可以是离散的或连续的。离散数据是可以计数的，例如，一个篮球队中的球员数量或一个国家中的城市数量。离散数据只能取某些值，例如 10、22、35、41 等等。另一方面，任何可测量的数据都称为连续数据——例如，一个人的身高或两辆赛车之间的距离。连续数据几乎可以取任何值，例如 2.3、11、0.0001 等等。

对数值数据应用两种常见的转换：

+   **归一化**：这是为了将不同的数值列调整到相同的尺度。建议将数值数据列归一化到相同的尺度，因为它有助于基于梯度下降的机器学习算法的收敛。如果一个数据列具有非常大的值，那么归一化它可以防止在训练模型时出现**NaN**错误。

+   **分桶**：在这种类型的转换中，数值数据被转换为分类数据。分桶通常在连续数值数据上执行，当数值列和目标列之间没有线性关系时。例如，一家汽车制造公司观察到价格最低或最高的汽车销售频率较低，而价格中等的汽车销售频率较高。在这种情况下，如果我们想使用汽车价格作为特征来预测销售的汽车数量，将汽车价格分桶到价格范围将是有益的。这样，模型将能够识别出销售汽车数量最多的中档价格桶，并为此特征分配更多的权重。

### 处理分类数据

分类数据可以分为两类——**有序**和**名义**。有序分类数据与其相关联的某些顺序有关——例如，电影评分（最差、差、好、优秀）和反馈（负面、中性、正面）。有序数据总是可以标记在量表上。另一方面，名义数据没有顺序。名义数据的例子包括性别、国家和语言。

一些算法，如决策树，可以很好地处理分类数据，但大多数机器学习算法不能直接处理分类数据。这些算法需要将分类数据转换为数值形式。在将分类数据转换为数值形式时，机器学习从业者会面临一些挑战：

+   **高基数**：基数意味着数据中的唯一性。具有高基数的数据列可能有很多不同的值——例如，国家层面的 ZIP 代码。

+   **稀疏或频繁出现**：数据列可能有一些很少出现或一些非常频繁出现的值。在这两种情况下，由于非常高的或非常低的方差，这个列可能不足以对模型产生影响。

+   **动态值**：数据列会不时地改变一些值——例如，在市列中，如果新城市被频繁添加或删除。

克服这些挑战的最佳方式高度依赖于我们处理的问题或数据类型。现在，让我们了解一些将分类数据转换为数值形式的常见方法：

+   **标签编码**：在这种技术中，我们将分类数据替换为从 0 到*N*-1 的整数。在这里，每个整数代表一个具有*N*个唯一值的分类数据列中的值。例如，如果有一个表示颜色的分类数据列，并且可能有 10 种独特的颜色值，在这种情况下，每种颜色将被映射并替换为 0 到 9 范围内的一个整数。这种方法可能不是所有情况都理想，因为模型可能会将数值视为分配给数据的权重。因此，这种方法更适合有序分类数据。

+   `1` 如果该颜色在给定行中存在；否则，它将是 `0`。One-hot 编码通常用于对分类数据进行编码。

+   **嵌入**：由于 One-hot 编码为每个唯一值创建一个新列，当唯一值的数量很大时，这可能会创建一个非常稀疏的数据表示。例如，如果我们有一个包含 20k 个唯一 ZIP 代码的 ZIP 代码列，One-hot 编码方法将创建 20k 个新的二进制列。这种稀疏数据需要大量的内存来存储，并增加了机器学习训练的复杂性。为了处理和表示具有大量唯一值的此类分类数据列，可以使用密集嵌入。然而，这些嵌入通常使用神经网络生成，因此它是一种现成的编码技术。这些嵌入将分类列中的每个值编码为一个小型的密集实数向量。训练和生成这些嵌入的一个简单方法是用内置的 Keras 嵌入层。

现在我们已经对在 Jupyter 笔记本中使用 Python 容易应用的一些常见数据转换有了很好的理解，让我们了解一些在 GCP 上进行数据转换的更多可扩展方法，例如 Cloud Data Fusion 和 Dataflow。

## Cloud Data Fusion

Cloud Data Fusion 是 GCP 上的一个完全托管服务，用于快速构建和管理可扩展的数据管道。使用 Data Fusion UI，我们可以构建和部署数据管道，而无需编写任何代码（使用可视化点选界面）。Data Fusion UI 允许我们以完全托管的方式构建可扩展的数据集成解决方案，以清理、准备、混合、转换和传输数据（这意味着我们不需要管理基础设施）。Cloud Data Fusion 为批处理和实时数据处理提供了数百个预构建的转换，并快速构建 ETL/ELT 管道。

Cloud Data Fusion 的一些关键特性如下：

+   **可移植性**：Cloud Data Fusion 是使用名为 **CDAP** 的开源项目构建的，从而确保数据管道的可移植性

+   **简单集成**：Cloud Data Fusion 与 Google Cloud 的功能（如 GCS、Dataproc 和 BigQuery）的简单集成使开发更快、更简单，并确保安全性

+   **无代码管道**：即使非技术用户也可以使用 Cloud Data Fusion 的 Web 界面快速构建数据管道

+   **混合**：由于 Cloud Data Fusion 是一个开源项目，它提供了在混合和多云环境中构建标准化数据管道的灵活性

+   **安全性**：Cloud Data Fusion 提供了企业级的安全性和访问管理，结合 Google Cloud 的数据保护和合规性。

除了 Web UI 之外，我们还可以使用命令行工具创建和管理 Cloud Data Fusion 实例和管道。接下来，让我们了解 GCP 上的另一个数据转换工具——Dataflow。

## 可扩展的数据转换的 Dataflow 管道

Dataflow 是一个托管服务，用于在 Google Cloud 上执行使用**Apache Beam SDK**开发的数据处理或转换管道。它支持快速、成本效益和无服务器（这意味着我们不需要管理基础设施）的统一批处理和流数据处理。由于 Dataflow 是无服务器的，它让我们能够专注于表达数据管道的业务逻辑（使用 SQL 或代码），而无需担心操作任务和基础设施管理。由于其流式特性，Dataflow 非常适合构建用于异常检测、模式识别和预测等用例的实时管道。

通过将 Dataflow 与其他管理的 Google Cloud 服务相结合，我们可以简化与自管理解决方案相比的生产化数据管道的许多方面。例如，它可以与 Google Cloud 提供的产品，如 Pub/Sub 和 BQ 结合，开发一个能够处理、分析和生成宝贵实时业务洞察的流式解决方案。由于它是管理的，它会自动配置和扩展所需资源，从而减少在流分析解决方案上工作的数据工程师或数据分析师的时间和复杂性。

Dataflow 的一些关键特性如下：

+   **智能自动扩展**：Dataflow 支持工作节点的水平和垂直扩展。扩展是自动执行的，以确保以高效（成本效益）或最佳匹配的方式满足工作节点和其他管道扩展需求。

+   **实时管道**：Dataflow 的流式特性在构建实时流分析、机器学习预测和异常检测管道时非常有用。它还适用于以最小延迟同步或复制跨多个数据源（如 BQ、PostgreSQL 或 Cloud Spanner）的数据。

+   **Dataflow SQL**：可以使用简单的 SQL 命令直接从 BQ 构建 Dataflow 流式管道。

+   **灵活调度**：需要调度的批处理作业（如夜间作业）可以轻松地使用 Dataflow 的**灵活资源调度**（**FlexRS**）在成本效益的环境中进行调度。

# 摘要

有效地管理数据对于每个组织节省时间、成本和复杂性至关重要。机器学习从业者应该了解传输、存储和转换数据以更有效地构建机器学习解决方案的最佳选项。在本章中，我们学习了将数据带入 Google Cloud 环境的多种方式。我们讨论了根据数据特性存储的最佳选项。最后，我们讨论了多种不同的工具和方法，以可扩展的方式转换/处理数据。

阅读本章后，你应该对根据用例需求选择最佳方案将数据移动或传输到你的 Google Cloud 环境中充满信心。选择最佳的数据存储位置以及最佳的数据分析和转换策略应该更容易，因为我们现在已经了解了不同选项的优缺点。在下一章中，我们将深入探讨**Vertex AI Workbench**，这是一个 Vertex AI 内部的托管笔记本平台。
