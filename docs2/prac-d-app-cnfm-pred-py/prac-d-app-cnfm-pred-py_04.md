

# 符合性预测的有效性和效率

在本章中，我们将更深入地探讨概率预测模型中的有效性和效率概念，在前面章节的基础上进行构建。

有效性和效率是确保预测模型在广泛行业应用中的实用性和鲁棒性的关键方面。理解这些概念及其影响将使你能够开发出无偏且高性能的模型，这些模型可以可靠地支持决策和风险评估过程。

在本章中，我们将探讨有效和高效模型的定义、指标和示例，并讨论由**符合性预测**提供的自动有效性保证，这是一种前沿的不确定性量化方法。到本章结束时，你将具备评估和改进你预测模型有效性和效率所需的知识，为你在各自领域内更可靠和有效的应用铺平道路。

本章将涵盖以下主题：

+   概率预测器的有效性

+   概率预测器的效率

# 概率预测器的有效性

我们首先总结一下为什么无偏点预测模型在各个领域和应用中都很重要：

+   **准确性和可靠性**：无偏模型确保其生成的预测在平均上是准确和可靠的，这意味着模型既不会系统地高估也不会低估真实值。这种准确性对于做出明智的决策、最小化风险和提升系统的整体性能至关重要。

+   **信任和可信度**：无偏预测模型有助于在利益相关者之间建立信任和可信度，因为它们为决策提供了可靠的基础。用户可以更有信心地依赖无偏模型生成的输出，知道它不会偏斜或偏向任何特定结果。

+   **公平性和公正性**：在某些应用中，例如金融、医疗保健和人力资源，无偏模型对于确保不同群体或个人之间的公平性和公正性至关重要。有偏模型可能会无意中加剧现有的不平等或创造新的不平等，导致不公平的待遇或资源分配。

+   **泛化性**：无偏模型更有可能在新数据或未见数据上良好泛化，因为它们准确地代表了数据中的潜在关系。相比之下，有偏模型在应用于新数据或不同条件下可能会表现不佳，导致意外的错误或次优结果。

+   **合规性**：在某些行业中，无偏模型是合规性的要求。例如，在金融、医疗保健和保险业，模型必须无偏见以满足监管标准，并确保客户得到公平对待，风险得到有效管理。

在一致预测的背景下，有效性指的是预测模型提供准确、可靠和无偏估计其预测相关不确定性的能力。更具体地说，一个有效的一致预测器会生成包含目标变量真实值的预测区间，并具有预定义的覆盖概率，确保模型的不确定性量化是可靠的且经过良好校准。

从不同角度可以理解一致预测中有效性的重要性：

+   **对预测的信心**：有效的预测器允许用户对其生成的预测区间有信心，因为他们知道这些区间真正反映了预测的不确定性。例如，如果一个一致预测器为某个数据点生成一个 95%的预测区间，用户可以相信有 95%的概率真实值位于该区间内。这种信心对于各种应用中的决策和风险管理至关重要。

+   **对模型误设的鲁棒性**：一致预测的一个关键优势是即使在底层预测模型误设或不完美的情况下，它也能提供有效的确定性估计。这种对模型误设的鲁棒性在现实世界场景中特别有价值，因为在这些场景中，真实的数据生成过程往往是未知的或复杂的，而可用的模型可能只能提供底层关系的近似。

+   **非参数性质**：一致预测是一种非参数方法，这意味着它不依赖于关于数据分布或预测误差的任何特定假设。这种非参数特性进一步促进了一致预测器的有效性，因为它们可以适应不同的数据结构，并在不需要了解底层分布的显式知识的情况下提供准确的不确定性估计。

+   **跨领域的适用性**：一致预测的有效性是一个普遍的特性，适用于各种领域和应用。这种普遍性允许从业者利用一致预测在多个领域，如金融、医疗保健、能源和交通等领域，知道一致预测器提供的不确定性估计将有效且可靠，无论具体情境如何。

+   **自动有效性保证**：与传统方法相比，一致预测的一个关键优势是它能够提供自动有效性保证，这意味着它产生的不确定性估计在轻微的假设下（如数据的可交换性）是有效的。这种自动有效性确保一致预测器即使在添加新数据点或底层关系随时间演变时也能保持其可靠性。

在符合性预测中，有效性从数学上定义为预测区间或由符合性预测器生成的区域的覆盖概率。如果对于任何期望的置信水平 *(1−α)*，真实目标值在其相应预测区间内包含的比例平均而言至少为 *(1−α)*，则符合性预测器是有效的，在多个实例中。

从数学上讲，让我们将目标变量表示为 *Y*，预测区间（在回归中）或集合（在分类中）表示为 *I(x, α)*，其中 *x* 代表测试数据点的特征，*α* 是显著性水平 *(α* ∈ *[0, 1])*。如果以下条件成立，则符合性预测器是有效的：

P(Y ∈ I(x, α)) ≥ 1 – α

此条件表明，对于任何给定的输入数据点 *x*，真实目标值 *Y* 在预测区间 *I(x, α)* 内的概率至少为 *(1−α)*。

符合性预测中的有效性与概率预测中的校准概念密切相关。校准预测器生成的预测区间具有正确的覆盖概率，确保它提供的不确定性估计与数据中的真实潜在不确定性良好对齐。

重要的一点是，在可交换性的假设下，符合性预测的有效性是有保证的，这要求观察到的数据点可以与未来的、未见过的数据点交换。对于 **独立同分布**（**IID**）数据，这个假设是成立的。此外，已经开发出成功的符合性预测修改方法来处理不可交换的数据，包括许多针对时间序列的成功模型。

## 分类器校准

分类器校准确保预测事件的概率与该事件发生的真实概率或频率相匹配。例如，在天气预报中，校准确保预测的降雨概率与一系列预测中实际降雨的发生相一致。

分类器校准的概念自 20 世纪 50 年代以来就被应用于天气预报，由 Glen Brier 开创。在降雨预测的情况下，预报员可能会宣布有 80%的降雨概率。如果平均而言，在这样的话语之后，降雨发生 60%的时间，我们认为这样的预测校准良好。

让我们考虑一位天气预报员，他声称某一天有 *x*% 的降雨概率。为了评估预报员预测的校准，我们会收集一系列类似的预测及其相应的结果（是否下雨）。例如，假设我们收集了 100 个实例，预报员预测了 60%的降雨概率。如果预报员的预测校准良好，那么大约会有 60 天下雨，从而使得观察到的降雨频率与预测的概率相匹配。

但这在实践中意味着什么呢？让我们通过一个例子来更好地理解这个概念。

假设，随着时间的推移，一位天气预报员做出了 10 次关于有 *x*% 降雨概率的预测。在下表中，我们展示了这些预测和实际结果：

| **日期** | **预测降雨** **概率** | **实际** **结果（降雨）** |
| --- | --- | --- |
| 1 | 80% | 是 |
| 2 | 60% | 否 |
| 3 | 90% | 是 |
| 4 | 30% | 否 |
| 5 | 70% | 是 |
| 6 | 50% | 否 |
| 7 | 80% | 是 |
| 8 | 20% | 否 |
| 9 | 40% | 是 |
| 10 | 60% | 是 |

表 4.1 – 预测降雨概率与实际结果对比

为了确定这个预测是否校准良好，我们需要比较每个概率水平下的预测降雨概率与实际结果。我们可以按概率水平分组预测，并计算每个组的降雨观察频率。

| **降雨** **概率** | 20% | 30% | 40% | 50% | 60% | 70% | 80% | 90% |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| **预测** | 1 天 | 1 天 | 1 天 | 1 天 | 2 天 | 1 天 | 2 天 | 1 天 |
| **降雨** | 0 天 | 0 天 | 1 天 | 0 天 | 1 天 | 1 天 | 2 天 | 1 天 |
| **观察** **频率** | 0% | 0% | 100% | 0% | 50% | 100% | 100% | 100% |

表 4.2 – 预测降雨概率与观察频率对比

根据观察频率，我们可以看到预测并未校准良好。观察频率与大多数概率水平的预测概率不一致。例如，在两个有 60% 降雨概率的日子里，只下了雨一次（预测频率的 50%），而在有 50% 降雨概率的日子里，一点雨都没下（预测频率的 0%）。

我们如何将这些结果汇总成某些指标？我们可以使用**Brier 分数**，这是我们之前章节中遇到的。Brier 分数是二分类问题中常用的校准指标。

回想一下，Brier 分数是预测概率与真实二元结果（0 或 1）之间平均平方差的计算：

Brier 分数 = (1 / N) ∑ (prediction_i - outcome_i)²

其中 *N* 是预测次数，*prediction_i* 是第 *i* 次实例的预测概率，*outcome_i* 是第 *i* 次实例的真实二元结果（降雨为 1，无降雨为 0）。

现在我们可以计算 Brier 分数：

Brier 分数 = (1 / 10) * (0.04 + 0.36 + 0.01 + 0.09 + 0.09 + 0.25 + 0.04 + 0.04 + 0.36 + 0.16) = 0.144

较低的 Brier 分数表示更好的模型性能和，因此，更好的校准。然而，与其他模型进行比较或参考，更容易确定 Brier 分数为 0.144 是否良好。此外，重要的是要记住，这种评估仅基于只有 10 天的有限样本量，这可能无法准确代表预报在更长时期内的校准。

我们还可以创建一个校准图，也称为可靠性图，以评估概率预测模型的校准。该图在 *x* 轴上绘制预测概率（分组到不同的区间），在 *y* 轴上绘制事件的观察频率。一个校准良好的模型将沿对角线（45 度角）有多个点，这表明预测概率与观察到的频率相匹配。

如我们从 *表 4.2* 中可以看到，预测并没有很好地校准。观察到的频率与大多数概率水平上的预测概率不匹配。

存在两种类型的校准错误：*信心不足*和*信心过强*。当一个分类器表现出信心不足时，它会低估其区分类别的能力，在实际表现上比其预测更好。相反，一个信心过强的分类器会高估其分离类别的能力，其表现比预测的概率更差。

可以用来评估校准的另一个指标是对数损失，也称为对数损失或交叉熵，它是一种用于评估产生每个类别概率估计的分类模型性能的指标。它衡量真实和预测概率分布之间的差异，对错误和不准确的预测进行惩罚。

对数损失的概念基于这样一个想法，即分类器不仅应该预测正确的类别，而且对其预测应该有信心。对数损失通过对预测概率与实际结果进行比较来量化预测概率的不确定性。

对于二元分类，对数损失定义为以下：

对数损失 = − (y * log(p) + (1 − y) * log(1 − p))

在这种情况下，*y* 代表真实类别标签，可以是 0 或 1。符号 *p* 表示对正类（类别 1）的预测概率。术语 *log* 表示自然对数。对数损失是对每个实例进行计算，然后对所有实例进行平均，以获得最终的对数损失值。

在校准的上下文中，对数损失可以用来评估预测概率与真实结果匹配得有多好。一个校准良好的模型将具有较低的对数损失，因为预测概率将更接近实际的类别标签。相反，一个校准不良的模型将具有较高的对数损失，这表明预测概率与真实结果之间存在差异。

需要注意的是，仅对数损失本身可能不足以评估校准，因为它还取决于分类精度。然而，当与其他指标结合使用时，如校准图，对数损失可以提供关于分类模型校准的有价值见解。在实践中，对数损失通常与 Brier 分数一起使用来评估模型的校准。当这两个指标在两个模型的相对校准上达成一致时，这比仅依赖单个校准指标（如对数损失或 Brier 损失）提供了更强的证据。通过考虑这两个指标，可以更全面地评估模型的校准。

回想一下之前提到的降雨预测示例。

要计算此例的对数损失，我们将使用这个公式：

对数损失 = − (y * log(p) + (1 − y) * log(1 − p))

在这里，*y* 是真实的类别标签（0 或 1），而 *p* 是预测的正类概率（降雨）。

让我们计算每一天的对数损失：

+   第 1 天：(1 * log(0.8) + (1 - 1) * log(1 - 0.8)) = 0.223

+   第 2 天：(0 * log(0.8) + (1 - 0) * log(1 - 0.8)) = 1.609

+   第 3 天：(1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = 0.511

+   第 4 天：(1 * log(0.7) + (1 - 1) * log(1 - 0.7)) = 0.357

+   第 5 天：(1 * log(0.9) + (1 - 1) * log(1 - 0.9)) = 0.105

+   第 6 天：(0 * log(0.7) + (1 - 0) * log(1 - 0.7)) = 1.204

+   第 7 天：(1 * log(0.6) + (1 - 1) * log(1 - 0.6)) = 0.511

+   第 8 天：(1 * log(0.5) + (1 - 1) * log(1 - 0.5)) = 0.693

+   第 9 天：(0 * log(0.6) + (1 - 0) * log(1 - 0.6)) = 0.916

+   第 10 天：(0 * log(0.4) + (1 - 0) * log(1 - 0.4)) = 0.511

现在，我们可以计算 10 天内的平均对数损失：

平均对数损失 = (0.223 + 1.609 + 0.511 + 0.357 + 0.105 + 1.204 + 0.511 + 0.693 + 0.916 + 0.511) / 10 ≈ 0.664

此降雨预测示例的平均对数损失约为 0.664。

一个自然而然的问题：在统计学习、机器学习和深度学习中，哪些是校准良好的，哪些不是？

作为一般准则，重要的是要记住，大多数机器学习模型在某种程度上都存在校准不当，严重程度不同。然而，逻辑回归有其局限性，可能只适用于某些应用，因为其建模能力相对简单。

一个关于校准的经典研究是论文 *Predicting Good Probabilities With Supervised Learning (2005):* [`www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf`](https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf)，该论文研究了各种监督分类算法的校准特性。研究发现，最大边缘，如支持向量机和提升树，产生了校准不当的类别得分，并且倾向于将预测值推向 0 和 1，而其他方法，如朴素贝叶斯，则将预测值推向相反方向。

虽然最初认为简单的神经网络会产生校准预测，但这种结论后来被重新评估。在一篇题为《传统神经网络是否校准良好？》的更近期的论文中（[`ieeexplore.ieee.org/document/8851962`](https://ieeexplore.ieee.org/document/8851962)），作者们表明，单个多层感知器以及多层感知器的集成，经常显示出校准不良。

# 概率预测器的效率

效率是一个用于评估概率预测器的性能指标。它衡量预测区间或区域有多精确或信息量有多大。换句话说，效率表明预测的概率分布有多紧密或狭窄。更小的区间或区域被认为是更高效的，因为它们传达了关于预测结果的更多确定性。

当有效性关注于确保错误率得到控制时，效率则评估预测的有用性和精确度。一个高效的预测器提供了关于可能结果的更具体信息，而一个效率较低的预测器则生成更宽的区间或区域，导致信息不够精确。

在有效性和效率之间存在固有的权衡。一致性预测器可以通过输出包含所有可能结果的非常宽的预测集来始终实现完美的有效性。然而，这缺乏效率，因为预测过于保守且不精确。

另一方面，一个模型可以输出非常狭窄、精确的预测，但可能会因为错误预测超过允许的阈值而未能满足有效性标准。这源于过度自信和不可靠的概率估计。

理想情况下，一致性预测器会找到一个最佳平衡点；预测尽可能紧密，同时仍然满足有效性保证。这确保了准确性和精确性，而不会过于保守或超过错误率阈值。

在一致性预测中，效率通常通过评估一致性预测器生成的预测区间或区域的大小来衡量。更小的区间或区域被认为是更高效的，因为它们提供了关于可能结果的更精确信息。以下是一些在一致性预测中衡量效率的常见方法：

+   **预测区间长度**：对于回归问题，可以通过找到每个区间的上下限之间的差异，然后对所有实例的平均这些差异来计算预测区间的平均长度。更小的平均长度表明更高的效率。

+   **预测集大小**：对于分类问题，可以评估预测集的大小。较小的预测集包含较少的类别标签，被认为是更高效的。一种衡量方法是计算所有实例的预测集的平均大小。较低的平均集大小表明更好的效率。

+   **覆盖率概率**：覆盖率概率衡量了真实结果落在预测区间或区域内的比例。虽然它主要用于评估一致性预测器的有效性，但它也可以提供关于效率的见解。具有紧密区间或区域的预测器将具有更高的覆盖率概率，这表明更好的效率。

+   **P 值直方图**：在一致性预测中，为每个实例和类别标签计算 p 值。检查 p 值的分布可以提供关于效率的见解。p 值的均匀分布表明预测器是有效的，但并不一定高效，而更集中的分布（例如，p 值接近 0 或 1）则意味着更高的效率。

在前面的章节中，我们已经看到一致性预测如何通过构建带有保证错误率的预测区间（用于回归）或预测集（用于分类）来保证预测集的自动有效性，该错误率由用户定义的置信水平确定。一致性预测背后的关键思想是利用过去数据和给定机器学习模型的观察行为来估计其预测的不确定性。

让我们回顾一下**归纳一致性预测**的阶段，它包括两个主要阶段：校准阶段和预测阶段。以下是其工作原理的概述：

+   **校准阶段**：在这个阶段，机器学习模型在数据集上训练，并为数据集中的每个实例计算非一致性度量。非一致性度量量化了实例相对于其余数据的不寻常性或异常性。

+   **预测阶段**：当需要预测新实例时，使用与校准阶段相同的非一致性度量函数计算非一致性度量。然后，将实例的非一致性得分与校准实例的非一致性得分进行比较。为每个可能的输出计算 p 值，反映具有高于或等于新实例非一致性得分的校准实例的比例。p 值可以解释为实例属于每个类别（用于分类）或落在某个范围（用于回归）的可能性度量。

+   **预测区间或集合**：基于计算出的 p 值和用户定义的置信水平，构建预测区间（用于回归）或预测集合（用于分类）。这些区间或集合保证以等于所选置信水平的概率包含真实结果。例如，如果置信水平设置为 0.95，则真实结果将有 95%的时间落在预测区间或集合内。

通过确保预测区间或集合以所需的概率包含真实结果，一致性预测为预测提供了自动的有效性。值得注意的是，虽然一致性预测保证了有效性，但它并不一定保证效率，这取决于预测区间或集合的精度。

在*第四章*中，我们将通过实际例子了解和学习不同类型的一致性预测。

# 摘要

在本章中，我们深入探讨了概率预测模型中的有效性和效率的概念，基于前几章奠定的基础。我们研究了有效性和效率的定义，并了解了可以用来评估和比较不同模型有效性和效率的各种指标。

在下一章中，我们将学习不同类型的一致性预测，并探讨量化不确定性的各种方法。
