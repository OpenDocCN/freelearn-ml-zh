# 第五章：数据存储的 AWS 服务

AWS 提供了一系列服务来安全地存储您的数据。在 AWS 上提供了多种存储选项，例如块存储、文件存储和对象存储。由于硬件投资较高、管理开销和系统升级管理，本地数据存储管理成本较高。使用 AWS 存储服务，您只需为使用的部分付费，无需管理硬件。我们还将了解 Amazon S3 提供的各种存储类别，以实现数据的智能访问和降低成本。您可以在考试中期待有关存储类别的题目。随着我们继续本章的学习，我们将掌握 Amazon RDS 的单 AZ 和多 AZ 实例，以及**RTO**（**恢复时间目标**）和**RPO**（**恢复点目标**）的概念。

在本章中，我们将通过以下部分学习如何通过以下方式安全地存储我们的数据，以便进行进一步的分析：

+   在 Amazon S3 上存储数据

+   控制 S3 存储桶和对象的访问

+   保护 Amazon S3 上的数据

+   在静态和传输中保护 S3 对象

+   使用其他类型的数据存储

+   关系型数据库服务（RDS）

+   管理 Amazon RDS 的故障转移

+   自动备份、RDS 快照、恢复和读取副本

+   使用具有多主能力的 Amazon Aurora 进行写入

+   在 Amazon Redshift 上存储列式数据

+   作为 NoSQL 数据库服务的 Amazon DynamoDB

# 技术要求

您在本章中需要的只是 AWS 账户和配置好的 AWS CLI。Amazon 详细解释了如何为您的账户配置 AWS CLI 的步骤：[`docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html`](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)。

您可以从 GitHub 下载代码示例，这里：[`github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/`](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-5/)。

# 在 Amazon S3 上存储数据

**S3**是亚马逊基于云的对象存储服务，可以通过互联网从任何地方访问。它是大型数据集的理想存储选项。它是基于区域的，因为您的数据存储在特定区域，直到您将数据移动到另一个区域。除非您进行配置，否则您的数据永远不会离开该区域。在特定区域中，数据在该区域的可用区中进行复制；这使得 S3 在区域上具有弹性。如果该区域中的任何可用区发生故障，则其他可用区将处理您的请求。它可以通过 AWS 控制台 UI、AWS CLI、AWS API 请求或标准 HTTP 方法进行访问。

S3 有两个主要组件：**存储桶**和**对象**。

+   存储桶是在特定的 AWS 区域中创建的。存储桶可以包含对象，但不能包含其他存储桶。

+   对象有两个主要属性。一个是**键**，另一个是**值**。值是存储的内容，键是名称。对象的最大大小可以是 5 TB。根据此处可用的亚马逊 S3 文档 [`docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html`](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)，对象还具有版本 ID、元数据、访问控制信息和子资源。

    重要提示

    根据亚马逊的文档，S3 为新对象的 PUT 操作提供写后一致性，这意味着如果你放置了一个新对象或创建了一个新对象，然后立即使用其键来读取该对象，那么你会得到你刚刚上传的确切数据。然而，对于覆盖和删除操作，它以**最终一致性**的方式表现。这意味着如果你在删除或覆盖操作后立即读取一个对象，那么你可能会读取到旧副本或过时的对象版本。将对象内容复制到三个可用区需要一些时间。

可以通过使用前缀来逻辑上维护文件夹结构。让我们以一个例子来说明，一个图像被上传到一个名为 `bucket-name-example` 的桶中，前缀为 `folder-name`，对象名称为 `my-image.jpg`。整个结构看起来像这样：`/bucket-name-example/folder-name/my-image.jpg`。

可以使用桶名 `bucket-name-example` 和键 `/folder-name/my-image.jpg` 来读取对象的内容。

亚马逊为存储在 S3 中的对象提供了几种存储类：

+   **标准存储**：这是频繁访问的对象和快速访问的存储类。S3 标准具有毫秒级的首字节延迟，并且对象可以被公开访问。

+   **降低冗余 (RR)**：这个选项比标准存储类提供更少的冗余。可以在此类中存储非关键且可重复的数据。AWS S3 文档建议不要使用此类，因为标准存储类更经济。

+   **标准不频繁访问 (IA)**：当你需要数据快速返回但不是频繁访问时，使用此选项。对象大小必须至少为 128 KB。最小存储时间为 30 天。如果对象在 30 天内被删除，你仍然会被收取 30 天的费用。**标准 IA** 对象对可用区的丢失具有弹性。

+   **单区不频繁访问**：这种存储类别的对象仅存储在一个可用区，这使得它比**标准 IA**更便宜。最小对象大小和存储时间与**标准 IA**相同。从这个存储类别的对象更不可用且更不具弹性。当你有另一个副本或数据可以重新创建时，应使用此存储类。对于长期存储的非关键且可替换的数据，以及访问不频繁的情况，应使用**单区 IA**存储类。

+   **冰川**：此选项用于长期存档和备份。检索此类存储类中的对象可能需要几分钟到几小时。最小存储时间为 90 天。您不能使用 Amazon Glacier API 访问作为对象生命周期管理一部分从 S3 移动到冰川的对象。

+   **冰川深存档**：此类的最小存储时间为 180 天。这是最便宜的存储类，默认检索时间为 12 小时。

+   **智能分层**：这个存储类旨在减少运营开销。用户支付监控费用，AWS 根据对象的访问模式在标准（频繁访问层）和标准 IA（低成本不频繁访问层）之间选择一个存储类。此选项旨在用于具有未知或不可预测访问模式的长久数据。

通过一系列规则，可以轻松管理存储类之间的转换和对象的删除，这些规则被称为**S3 生命周期配置**。这些规则包括操作。这些操作可以应用于一个桶或该桶中由前缀或标签定义的一组对象。操作可以是**转换操作**或**过期操作**。转换操作定义了在创建*用户定义*天数后的对象的存储类转换。过期操作配置了版本化对象的删除，或删除删除标记或不完整的分片上传。这对于管理成本非常有用。

在*图 5.1*中给出了一个说明。您可以在以下链接中找到更多详细信息：[`docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html`](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)

![图 5.1 – S3 存储类的比较表]

![图片/B16735_05_001.jpg]

图 5.1 – S3 存储类的比较表

## 创建桶以存储数据

现在，让我们看看如何使用 AWS CLI 创建桶、上传对象和读取对象。

1.  在第一步中，我们将使用`aws s3 ls`命令检查是否创建了任何桶：

    ```py
    $ pwd
    /Users/baba/AWS-Certified-Machine-Learning-Specialty-
    2020-Certification-Guide/Chapter-5/s3demo/demo-files
    $ aws s3 ls
    ```

1.  此命令在此处没有返回任何内容。因此，我们将使用`mb`参数创建一个桶。假设桶名为`demo-bucket-baba`，位于`us-east-1`区域：

    ```py
    $ aws s3 mb s3://demo-bucket-baba --region us-east-1
    make_bucket: demo-bucket-baba
    $ aws s3 ls
    2020-11-04 14:39:50 demo-bucket-baba
    ```

1.  我们已经创建了一个桶，下一步是将文件复制到我们的桶中，使用`cp`参数，如下面的代码所示：

    ```py
    $ aws s3 cp sample-file.txt s3://demo-bucket-baba/
    upload: ./sample-file.txt to s3://demo-bucket-
    baba/sample-file.txt
    ```

1.  要通过 AWS 控制台验证文件上传操作，请登录您的 AWS 账户并转到 AWS S3 控制台查看相同的内容。AWS S3 控制台列出了如图*图 5.2*所示的结果。控制台在您阅读本书时可能已经发生变化！！[图 5.2 – AWS S3 截图以列出您的文件]

    ![图片/B16735_05_002.jpg]

    ```py
    --recursive will do the job for you:

    ```

    `cp`命令和`--recursive`参数。为了实现这一点，您将需要创建两个桶，demo-bucket-baba-copied 和 demo-bucket-baba-moved。步骤如下：

    ```py
    $ aws s3 mb s3://demo-bucket-baba-copied --region us-east-2
    $ aws s3 mb s3://demo-bucket-baba-moved --region us-east-2
    $ aws s3 cp s3://demo-bucket-baba s3://demo-bucket-baba-copied/ --recursive
    $ aws s3 mv s3://demo-bucket-baba s3://demo-bucket-baba-moved/ --recursive
    $ aws s3 ls 
    2020-11-04 14:39:50 demo-bucket-baba
    2020-11-04 15:44:28 demo-bucket-baba-copied
    2020-11-04 15:44:37 demo-bucket-baba-moved
    $ aws s3 ls s3://demo-bucket-baba/
    ```

    如果所有命令都成功运行，那么原始存储桶最终应该是空的（因为所有文件现在都已移动）。注意：在认证考试中，您不会找到很多关于存储桶和对象级操作的问题。然而，了解基本操作和所需步骤总是更好的。

    ```py

    ```

1.  一旦动手实践完成，必须删除存储桶以避免成本。在提供`rb`命令之前，存储桶必须为空：

    ```py
    $ aws s3 rb s3://demo-bucket-baba
    $ aws s3 rb s3://demo-bucket-baba-moved
    remove_bucket failed: s3://demo-bucket-baba-moved An error occurred (BucketNotEmpty) when calling the DeleteBucket operation: The bucket you tried to delete is not empty
    ```

1.  `demo-bucket-baba-moved`存储桶不为空，因此我们无法删除该存储桶。在这种情况下，使用`--force`参数删除整个存储桶及其所有内容，如下所示：

    ```py
    rm command with the --recursive parameter. 
    ```

1.  让我们以一个具有前缀`images`的存储桶`test-bucket`为例。这个前缀包含四个名为`animal.jpg`、`draw-house.jpg`、`cat.jpg`和`human.jpg`的图像文件。

1.  现在，为了删除图像中的内容，命令如下：**aws s3 rm s3://test-bucket/images –recursive**

1.  现在存储桶应该是空的。

在下一节中，我们将学习关于对象标签和对象元数据的内容。

## 区分对象标签和对象元数据

让我们比较这两个术语：

+   **对象标签**：对象标签是一个**键值**对。AWS S3 对象标签可以帮助您过滤分析和指标，对存储进行分类，根据某些分类对对象进行安全保护，根据对象的某些分类跟踪成本，以及更多。对象标签可以用来创建生命周期规则，将对象移动到更便宜的存储层。您可以为对象添加最多 10 个标签，为存储桶添加最多 50 个标签。标签键可以包含 128 个 Unicode 字符，而标签值可以包含 256 个 Unicode 字符。

+   **对象元数据**：对象元数据是描述对象的描述性数据。它由**名称-值**对组成。对象元数据作为 HTTP 头在对象上返回。它们有两种类型：一种是**系统元数据**，另一种是**用户定义元数据**。用户定义元数据是用户添加到对象中的自定义名称-值对。名称必须以**x-amz-meta**开头。您可以更改对象上的所有系统元数据，如存储类、版本控制和加密属性。更多详细信息请参阅[`docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html`](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html)。

    重要提示

    元数据名称不区分大小写，而标签名称区分大小写。

在下一节中，我们将学习如何通过不同的策略控制 Amazon S3 上存储桶和对象的访问，包括资源策略和身份策略。

# 控制对 Amazon S3 上存储桶和对象的访问

一旦对象存储在存储桶中，下一个主要步骤就是管理访问。S3 默认是私有的，并且通过多种方法向其他用户、组或资源提供访问权限。这意味着可以通过 **访问控制列表（ACLs**）、**公共访问设置**、**身份策略** 和 **存储桶策略** 来管理对象的访问权限。

让我们详细看看其中的一些。

## S3 存储桶策略

`Principal` 被渲染为 `*`：

```py
{
  "Version":"2012-10-17"
  "Statement":[
    {
      "Sid":"AnyoneCanRead",
      "Effect":"Allow",
      "Principal":"*",
      "Action":["s3:GetObject"],
      "Resource":["arn:aws:s3:::my-bucket/*"]
    }
    ]
}
```

默认情况下，S3 中的所有内容都对所有者私有。如果我们想将前缀公开给全世界，那么资源将变为 `arn:aws:s3:::my-bucket/some-prefix/*`，同样，如果它是为特定的 IAM 用户或 IAM 组设计的，那么这些详细信息将进入策略中的主体部分。

也可以在存储桶策略中添加条件。让我们考察一个组织希望保持存储桶公开并白名单特定 IP 地址的用例。策略可能看起来像这样：

```py
{ 
  "Version":"2012-10-17" 
  "Statement":[ 
    { 
      "Sid":"ParticularIPRead", 
      "Effect":"Allow", 
      "Principal":"*", 
      "Action":["s3:GetObject"], 
      "Resource":["arn:aws:s3:::my-bucket/*"], 
      "Condition":{
        "NotIpAddress":{"aws:SourceIp":"2.3.3.6/32"}
      }
    } 
    ] 
}
```

更多示例可在 AWS S3 开发者指南中找到，链接如下：[`docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html`](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html)。

**阻止公共访问** 是为存储桶所有者提供的单独设置，以避免在存储桶策略中犯任何错误。在实际场景中，存储桶可能会由于策略错误而公开；为了避免此类错误或数据泄露，AWS 提供了此设置。它提供了比存储桶策略更高的安全级别。你可以在创建存储桶时选择此设置，也可以在创建存储桶后设置。

（示例中的 `us-east-1`）：

```py
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "rds:*",
            "Resource": ["arn:aws:rds:us-east-1:*:*"]
        },
        {
            "Effect": "Allow",
            "Action": ["rds:Describe*"],
            "Resource": ["*"]
        }
    ]
}
```

**访问控制列表（ACLs**）用于授予高级权限，通常用于授予对其他 AWS 账户的访问权限。ACLs 是存储桶或对象的 **子资源** 之一。存储桶或对象可以通过 ACLs 快速公开。AWS 不建议这样做，你也不应该在测试中期待有关此问题的问题。了解这一点是好的，但它不如 **S3 存储桶策略** 灵活。

现在，让我们在下一节中了解保护我们数据的方法。

# 保护 Amazon S3 上的数据

在本节中，我们将学习如何记录对象的每个版本。除了持久性之外，Amazon 还提供了几种技术来保护 S3 中的数据。其中一些技术涉及启用版本控制和加密对象。

版本控制可以帮助你在更新、删除或上传操作中，如果当前对象出现任何问题，回滚到之前的版本。

通过加密，你可以控制对象的访问。你需要适当的密钥来读取和写入对象。我们还将学习**多因素认证（MFA）**用于删除操作。亚马逊还允许**跨区域复制（CRR）**，在另一个区域中维护对象的副本，这可以在任何灾难期间用于数据备份，以提供额外的冗余，或者用于提高不同区域的数据访问速度。

## 应用存储桶版本控制

让我们现在通过一些实际操作示例来了解如何使用帮助启用存储桶版本控制。可以在创建存储桶时从 AWS S3 控制台应用存储桶版本控制：

1.  要从命令行启用存储桶的版本控制，必须首先创建存储桶，然后才能启用版本控制，如下例所示。在这个例子中，我已经创建了一个存储桶，`version-demo-mlpractice`，并通过`put-bucket-versioning`命令启用了版本控制：

    ```py
    $ aws s3 mb s3://version-demo-mlpractice/
    $ aws s3api put-bucket-versioning --bucket version-demo-mlpractice --versioning-configuration Status=Enabled
    $ aws s3api get-bucket-versioning --bucket version-demo-mlpractice
    {
        "Status": "Enabled"
    }
    ```

1.  我们没有使用任何加密方式创建这个存储桶。因此，如果你运行`aws s3api get-bucket-encryption --bucket version-demo-mlpractice`，它将输出一个错误，显示以下内容：

    ```py
    The server side encryption configuration was not found
    ```

1.  `put-bucket-encryption` API。命令看起来是这样的：

    ```py
    $ aws s3api put-bucket-encryption --bucket version-demo-mlpractice --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":
    {"SSEAlgorithm":"AES256"}}]}'
    ```

1.  同样可以使用以下命令进行验证：`aws s3api get-bucket-encryption --bucket version-demo-mlpractice`。

我们将在下一节中了解更多关于加密的内容。

## 应用存储桶加密

你还需要了解在存储桶上启用版本控制将如何帮助。有一些用例中，文件会定期更新，并且将为同一文件创建版本。为了模拟这种场景，请尝试以下示例：

1.  在这个例子中，我们将创建一个包含版本信息的文件。我们将覆盖它并检索它以检查该文件中的版本：

    ```py
    $ echo "Version-1">version-doc.txt
    $ aws s3 cp version-doc.txt s3://version-demo-mlpractice
    $ aws s3 cp s3://version-demo-mlpractice/version-doc.txt 
    check.txt
    $ cat check.txt
    Version-1
    $ echo "Version-2">version-doc.txt
    $ aws s3 cp version-doc.txt s3://version-demo-mlpractice
    $ aws s3 cp s3://version-demo-mlpractice/version-doc.txt 
    check.txt
    $ cat check.txt
    Version-2
    ```

1.  在检索时，我们得到了文件的最新版本，换句话说，在这个例子中是`Version-2`。要检查每个版本以及它们的最新版本，S3 提供了`list-object-versions` API，如下所示。从 JSON 结果中，你可以推断出最新版本：

    ```py
    $ aws s3api list-object-versions 
    --bucket version-demo-mlpractice
    {
        "Versions": [
            {
                "ETag": 
    "\"b6690f56ca22c410a2782512d24cdc97\"",
                "Size": 10,
                "StorageClass": "STANDARD",
                "Key": "version-doc.txt",
                "VersionId": 
    "70wbLG6BMBEQhCXmwsriDgQoXafFmgGi",
                "IsLatest": true,
                "LastModified": "2020-11-07T15:57:05+00:00",
                "Owner": {
                    "DisplayName": "baba",
                    "ID": "XXXXXXXXXXXX"
                }
            },
            {
                "ETag": "\"5022e6af0dd3d2ea70920438271b21a2\"",
                "Size": 10,
                "StorageClass": "STANDARD",
                "Key": "version-doc.txt",
                "VersionId": "f1iC.9L.MsP00tIb.sUMnfOEae240sIW",
                "IsLatest": false,
                "LastModified": "2020-11-07T15:56:27+00:00",
                "Owner": {
                    "DisplayName": "baba",
                    "ID": " XXXXXXXXXXXX"
                }
            }
        ]
    }
    ```

1.  可能会有这样的情况，你需要将当前对象的早期版本回滚。在前面的例子中，最新的是*Version-2*。你可以通过将*VersionId*子资源解析到`get-object` API 调用中，并重新上传该对象，将任何所需的版本设置为最新或当前版本。另一种方法是，通过在`delete-object` API 请求中将`versionId`传递到`–version-id`参数中，删除当前或最新版本。更多关于 API 的详细信息，请参阅此处：[`docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html`](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html)。

1.  当你在启用了版本控制的存储桶中删除一个对象时，并不会从存储桶中删除该对象。它只是创建了一个名为**DeleteMarker**的标记。它看起来是这样的：

    ```py
    $ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt
    {
        "DeleteMarker": true,
        "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"
    }
    ```

1.  这意味着对象没有被删除。你可以使用以下命令列出它：

    ```py
    aws s3api list-object-versions --bucket version-demo-mlpractice
    ```

1.  现在存储桶中没有名为`version-doc.txt`的对象，您可以使用`aws s3 ls`命令来验证这一点，因为该标记已成为具有新 ID 的对象的当前版本。如果您尝试检索已删除的对象，这意味着删除标记正在充当对象的当前版本，那么您将获得一个`VersionId`，如下面的示例命令所示。一个简单的删除请求**（不带版本 ID）**不会删除删除标记并创建另一个具有唯一版本 ID 的删除标记。因此，对于同一对象可以有多个删除标记。在此需要注意的是，这将消耗您的存储空间，并且您将为此付费：

    ```py
    $ aws s3 ls s3://version-demo-mlpractice/
    $ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP
    {
        "DeleteMarker": true,
        "VersionId": "BKv_Cxixtm7V48MWqBO_KUkKbcOaH5JP"
    }
    ```

1.  在现在列出存储桶时，可以看到较旧的对象：

    ```py
    $ aws s3 ls s3://version-demo-mlpractice/
    2020-11-07 15:57:05         10 version-doc.txt
    ```

    由于我们已经涵盖了考试主题并练习了大多数所需的概念，我们应该删除存储桶中的对象，然后删除存储桶以节省成本。此步骤将删除对象的版本，从而永久删除对象。

1.  在这里，通过向其提供版本 ID，然后是另一个版本 ID 来删除最新版本：

    ```py
    $ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id 70wbLG6BMBEQhCXmwsriDgQoXafFmgGi
    $ aws s3api delete-object --bucket version-demo-mlpractice --key version-doc.txt --version-id f1iC.9L.MsP00tIb.sUMnfOEae240sIW
    $ aws s3api list-object-versions --bucket version-demo-mlpractice
    ```

    我们现在可以清楚地看到空存储桶了。

    重要提示

    AWS 最佳实践建议通过**多因素认证删除**添加另一层保护。可以防止意外删除存储桶，并确保存储桶中对象的安全性。可以通过控制台和 CLI 启用或禁用多因素认证删除。如 AWS 文档所述，多因素认证删除需要两种认证方式同时进行：您的安全凭证，以及一个有效序列号、一个空格和显示在批准的认证设备上的六位代码的组合。

CRR 可以帮助您在不同地理区域之间分离数据。一个典型的用例是在灾难期间的正常业务活动。如果某个区域发生故障，则另一个区域可以在 CRR 启用的情况下支持用户。这提高了数据可用性。另一个用例是如果同一数据被另一个计算资源使用，例如在另一个区域启动的 EC2 或 AWS Lambda，可以减少延迟。您还可以使用 CRR 将对象复制到属于不同所有者的另一个 AWS 账户。对于认证考试，以下是一些重要要点值得记录：

+   为了使用 CRR，必须在源和目标存储桶上启用版本控制。

+   通过添加规则，在源存储桶上启用了复制。作为源，可以是整个存储桶、前缀或标签进行复制。

+   通过分配适当的加密密钥，加密对象也可以进行复制。

+   目标存储桶可以位于同一账户或另一个账户。您可以在目标存储桶中更改对象的存储类型和所有权。

+   对于 CRR，可以选择现有的角色或创建新的 IAM 角色。

+   源存储桶上可以有多个复制规则，并赋予其优先级。优先级较高的规则会覆盖优先级较低的规则。

+   当你添加复制规则时，只有规则启用后创建的新版本对象才会被复制。

+   如果从源存储桶中删除了版本，则它们不会被从目标存储桶中删除。

+   当你从源存储桶中删除一个对象时，它会在该源存储桶中创建一个删除标记。S3 不会将此删除标记复制到目标存储桶。

在下一节中，我们将介绍保护 S3 对象的概念。

# 保护静态和传输中的 S3 对象

在前一节中，我们学习了存储桶默认加密的概念，这与对象级加密完全不同。存储桶没有被加密，而对象被加密。这里可能会出现一个问题：*什么是存储桶默认加密？* 我们将在本节中学习这些概念。在传输过程中，可以使用**安全套接字层（SSL）**或**传输层安全性（TLS）**来保护 HTTPS 请求的传输。下一步是保护数据，授权人员可以编码和解码数据。

在同一个存储桶中，可以对不同的对象有不同的加密设置。S3 支持对象静态存储的**客户端加密（CSE）**和**服务器端加密（SSE）**：

+   **客户端加密**：客户端通过 S3 端点将对象上传到 S3。在客户端加密（CSE）中，数据在上传到 S3 之前由客户端加密。尽管用户和 S3 端点之间的传输发生在加密通道中，但通道中的数据已经被客户端加密，无法被看到。在传输过程中，默认通过 HTTPS 进行加密。因此，AWS S3 存储加密对象，在任何时候都无法以任何格式读取数据。在 CSE 中，客户端负责加密对象内容。因此，在密钥管理和加密解密过程中，控制权始终在客户端。这导致 CPU 使用量巨大。S3 仅用于存储。

+   **服务器端加密**：客户端通过 S3 端点将对象上传到 S3。尽管传输中的数据通过使用 HTTPS 的加密通道，但对象本身在通道内没有被加密。一旦数据到达 S3，它就会被 S3 服务加密。在 SSE 中，你信任 S3 执行加密解密、对象存储和密钥管理。S3 对象有三种 SSE 技术可用：

    a) SSE-C

    b) SSE-S3

    c) SSE-KMS

+   在进行`PUT`操作时，用户必须向 S3 提供密钥和对象。S3 使用提供的密钥加密对象，并将哈希（加密文本）附加到对象上。一旦对象被存储，S3 就会丢弃加密密钥。这个生成的哈希是一次的，不能用来生成新的密钥。当用户提供带有解密密钥的`GET`操作请求时，哈希会识别是否使用了特定的密钥进行加密。然后，S3 解密并丢弃密钥。

+   `PUT`操作中，用户只需提供未加密的对象。S3 创建一个用于加密过程的密钥。由于这个密钥是在内部创建、旋转并由 S3 端到端管理的，因此没有人可以更改这个主密钥。这是对象的唯一密钥。它默认使用 AES-256 算法。

+   **使用存储在 AWS 密钥管理服务（SSE-KMS）中的客户主密钥进行服务器端加密**：**AWS 密钥管理服务（KMS）**管理**客户主密钥（CMK）**。AWS S3 与 AWS KMS 协作并生成一个 AWS 管理的 CMK。这是 SSE-KMS 默认使用的密钥。每次上传对象时，S3 都会使用一个专用的密钥来加密该对象，而这个密钥是一个**数据加密密钥（DEK）**。DEK 由 KMS 使用 CMK 生成。S3 提供了 DEK 的明文版本和加密版本。DEK 的明文版本用于加密对象然后被丢弃。DEK 的加密版本与加密对象一起存储。当您使用 SSE-KMS 时，不需要使用 S3 创建的默认 CMK。您可以创建并使用客户管理的 CMK，这意味着您可以控制其上的权限以及密钥材料的旋转。因此，如果您所在的组织中有一个关注密钥旋转或加密用户与解密用户之间角色分离的监管委员会，那么 SSE-KMS 是解决方案。在 SSE-KMS 上还可以进行日志记录和审计，以跟踪针对密钥的 API 调用。

+   `PUT`操作。

在下一节中，我们将了解一些与 EC2 实例一起使用的数据存储。

# 使用其他类型的数据存储

**弹性块存储（EBS）**用于在可用区创建卷。该卷只能连接到同一可用区内的 EC2 实例。Amazon EBS 提供**SSD（固态硬盘）**和**HDD（硬盘驱动器）**类型的卷。对于基于 SSD 的卷，主导的性能属性是**IOPS（每秒输入输出）**，而对于 HDD 则是吞吐量，通常以 MiB/s 来衡量。以下表格在*图 5.3*中提供了不同卷和类型的概述：

![图 5.3 – 不同卷及其用例]

![图片/B16735_05_003.jpg]

图 5.3 – 不同卷及其用例

**EBS**对可用区（AZ）具有容错性。如果由于某种原因，一个 AZ 失败，那么卷将无法访问。为了避免这种情况，可以从 EBS 卷创建**快照**，并且快照存储在 S3 中。一旦快照到达 S3，快照中的数据就具有区域容错性。第一个快照是卷上数据的完整副本，从那时起，快照是增量式的。快照可以用来克隆卷。由于快照存储在 S3 中，可以在该区域的任何 AZ 中克隆卷。快照可以在区域之间共享，并且可以在灾难恢复期间从它们克隆卷。

AWS KMS 管理 CMK。AWS KMS 可以使用 AWS 管理的 CMK 来管理 EBS，或者 AWS KMS 可以使用客户管理的 CMK。当创建加密卷时，EBS 会使用 CMK。CMK 用于创建一个加密的 DEK，该 DEK 与卷一起存储在物理磁盘上。这个 DEK 只能通过 KMS 解密，前提是该实体有解密权限。当从加密卷创建快照时，快照会使用相同的 DEK 进行加密。从该快照创建的任何卷也将使用该 DEK。

**实例存储**卷是物理连接到 EC2 实例的块存储设备。由于实例附加的**临时存储**来自实例启动的主机，因此它们提供最高的性能。EBS 可以在任何时候附加到实例，但实例存储必须在实例启动时附加；一旦实例启动，就不能再附加。如果 EC2 实例的底层主机存在问题，则相同的实例将在另一个主机上启动，并带有新的实例存储卷，而早期的实例存储（临时存储）和旧数据将丢失。附加卷的大小和能力取决于实例类型，更详细的信息可以在这里找到：[`aws.amazon.com/ec2/instance-types/`](https://aws.amazon.com/ec2/instance-types/)。

**弹性文件系统（EFS）**提供了一种基于网络的文件系统，可以在 Linux EC2 实例内部挂载，并且可以一次由多个实例使用。它是**NFSv4**的实现。它可以用于通用模式、最大 I/O 性能模式（用于科学分析或并行计算）、突发模式和预配吞吐量模式。

正如我们所知，在实例存储的情况下，数据是易变的。一旦实例丢失，数据就会从实例存储中丢失。对于 EFS 来说并非如此。EFS 与 EC2 实例存储是分开的。EFS 是一个文件存储，通过 VPC 内部的挂载目标被多个 EC2 实例访问。本地系统可以通过混合网络访问 EFS 存储，例如**VPN**或**Direct Connect**。EFS 还支持两种存储类别：标准和高频访问。标准用于频繁访问的数据。高频访问是长期存储、较少访问数据的成本效益存储类别。可以使用生命周期策略在存储类别之间转换数据。

重要提示

对于最大 I/O 需求，实例存储是首选，如果数据可替换且临时。

# 关系型数据库服务（RDSes）

这是在 AWS 考试中最常见的考试主题之一。在考试之前，你应该具备足够的知识。在本节中，我们将学习关于 Amazon 的 RDS。

AWS 为用户提供了几种关系型数据库作为服务。用户也可以在 EC2 实例上运行他们想要的数据库。最大的缺点是实例在一个区域的可用区中仅可用。EC2 实例必须被管理和监控，以避免任何类型的故障。需要自定义脚本来维护数据备份。任何数据库主要或次要版本更新都会导致停机。在 EC2 实例上运行的数据库实例在数据库负载增加时无法轻松扩展，因为复制不是一项容易的任务。

RDS 提供可管理的数据库实例，这些实例本身可以包含一个或多个数据库。想象一下，在一个 EC2 实例上运行数据库服务器，而你不需要管理或维护它。你只需要访问服务器并在其中创建数据库。AWS 将管理其他一切，例如实例的安全性、实例上运行的操作系统、数据库版本以及数据库服务器的高可用性。RDS 支持多种引擎，如 MySQL、Microsoft SQL Server、MariaDB、Amazon Aurora、Oracle 和 PostgreSQL。你可以根据自己的需求选择其中任何一种。

Amazon RDS 的基础是一个数据库实例，它可以支持多种引擎，并且用户可以创建多个数据库。一个数据库实例只能通过使用主实例的数据库**CNAME**（CNAME 是域名系统数据库中规范名称的别名）来访问。RDS 使用标准数据库引擎。因此，使用某种工具在自管理的数据库服务器上访问数据库与访问 Amazon RDS 相同。

既然我们已经了解了 Amazon RDS 的需求，让我们了解 Amazon RDS 中的故障转移过程。我们将介绍如果 RDS 实例出现问题，Amazon 提供哪些服务。

# 管理 Amazon RDS 的故障转移

RDS 实例可以是**单可用区**或**多可用区**。在多可用区中，多个实例协同工作，类似于活动-被动故障转移设计。

对于单可用区 RDS 实例，可以为该实例分配存储。简而言之，单可用区 RDS 实例在同一可用区中有一个可用的附加块存储（EBS 存储）。这使得 RDS 实例的数据库和存储容易受到可用区故障的影响。分配给块存储的存储可以是 SSD（gp2 或 io1）或磁性存储。为了确保 RDS 实例的安全，建议使用安全组并根据需求提供访问权限。

多可用区始终是避免任何故障并保持应用程序高可用性的最佳架构设计方式。利用多可用区功能，备用副本会与主体实例**同步**保持同步。备用实例在其指定的可用区有自己的存储。备用副本不能直接访问，因为所有 RDS 访问都是通过单个数据库 CNAME。除非发生故障转移，否则无法访问备用实例。备用实例不提供性能优势，但它确实在 RDS 实例的可用性方面构成改进。它只能在同一区域发生，即在 VPC 内同一区域的另一个可用区的子网中。当多可用区 RDS 实例在线时，你可以从备用副本中备份，而不会影响性能。在单可用区实例中，备份操作期间可能会出现可用性和性能问题。

要理解多可用区（multi-AZ）的工作原理，让我们以一个单可用区（single-AZ）的实例为例，并将其扩展到多可用区。

假设你有一个运行在名为`db-vpc`的 VPC 内`us-east-1`区域`AZ-A`的 RDS 实例。这成为了一个 RDS 实例单可用区设计的主体实例。在这种情况下，将在*AZ-A*可用区为该实例分配存储。一旦你选择在另一个名为*AZ-B*的可用区进行多可用区部署，AWS 将在*db-vpc* VPC 内*us-east-1*区域的*AZ-B*可用区创建一个备用实例，并在*us-east-1*区域的*AZ-B*为备用实例分配存储。除此之外，RDS 将从主体实例到备用副本启用**同步复制**。正如我们之前所学的，访问我们的 RDS 实例的唯一方式是通过数据库 CNAME，因此，访问请求会发送到 RDS 主体实例。一旦写入请求到达端点，它就会写入主体实例。然后它将数据写入硬件，这是附加到主体实例的块存储。同时，主体实例将相同的数据复制到备用实例。最后，备用实例将数据提交到其块存储。

主实例将数据写入硬件，并并行地将数据复制到备用实例，因此在它们各自的硬件中的数据提交操作之间有最小的时间延迟（几乎为零）。如果主实例发生错误，RDS 会检测到这一点并将数据库端点更改为备用实例。访问数据库的客户端可能会经历非常短暂的中断。这种故障转移在 60-120 秒内发生。它不提供容错系统，因为在故障转移操作期间会有一些影响。

您现在应该已经理解了 Amazon RDS 的故障转移管理。现在，让我们学习如何自动备份 RDS，使用快照在发生任何故障时进行恢复，以及下一节中的读取副本。

# 自动备份、RDS 快照、恢复和读取副本

在本节中，我们将了解 RDS 的**自动备份**和**手动快照**是如何工作的。这些功能是 Amazon RDS 的一部分。

让我们考虑一个每天早上 5 点进行备份的数据库。如果应用程序在上午 11 点失败，那么可以从上午 11 点进行的备份中重新启动应用程序，但会丢失 6 小时的数据。这被称为 6 小时**RPO（恢复点目标）**。因此，RPO 是指最近一次备份和事件之间的时间，这定义了数据丢失的数量。如果您想减少这个时间，那么您必须安排更多的增量备份，这将增加成本和备份频率。如果您的业务需要更低的 RPO 值，那么业务必须为满足技术解决方案投入更多的资金。

现在，根据我们的例子，一位工程师被分配了这个任务，在灾难发生时尽快将系统上线。工程师通过向现有系统添加一些额外的硬件组件并安装一些更新的软件版本，在同一天下午 2 点成功将数据库上线。这被称为 3 小时**RTO（恢复时间目标）**。因此，RTO 是指灾难恢复和完全恢复之间的时间。通过拥有备用硬件和记录恢复过程，可以降低 RTO 值。如果业务需要更低的 RTO 值，那么您的业务必须在备用硬件和有效的系统设置上投入更多的资金以执行恢复过程。

在 RDS 中，RPO 和 RTO 在选择**自动备份**和**手动快照**时发挥着重要作用。这两个备份服务都使用 AWS 管理的 S3 存储桶，这意味着它不能在用户的 AWS S3 控制台中可见。它是区域弹性，因为备份被复制到 AWS 区域中的多个可用区。对于单 AZ RDS 实例，备份是从单个可用的数据存储进行的，而对于已启用多 AZ 的 RDS 实例，备份是从备用数据存储（主存储在备份方面保持不变）进行的。

快照是针对 RDS 实例的手动操作，并且这些快照存储在 AWS 管理的 S3 桶中。RDS 实例的第一个快照是数据的完整副本，后续的快照是增量备份，反映了数据的变化。在快照过程的耗时方面，第一个快照所需时间较长，从那时起，增量备份会更快。当任何快照发生时，它可能会影响单 AZ RDS 实例的性能，但不会影响多 AZ RDS 实例的性能，因为这是在备用数据存储上发生的。手动快照不会过期，需要自动清除，并且它们会存在于 RDS 实例终止之后。当你删除 RDS 实例时，它会建议为你创建一个最后的快照，并且这个快照将包含你 RDS 实例内部的所有数据库（RDS 实例中不仅仅只有一个数据库）。当你从手动快照恢复时，你将恢复到单个时间点，这会影响 RPO。

要自动化整个流程，你可以选择一个时间窗口来捕捉这些快照。这被称为自动备份。这些时间窗口可以明智地管理，从而本质上降低业务的 RPO（恢复点目标）值。自动备份的保留期为 0 到 35 天，其中 0 表示禁用，最大值为 35 天。根据 AWS 文档，保留的自动备份包含数据库实例的系统快照和事务日志。它们还包括数据库实例属性，如分配的存储和数据库实例类别，这些是将其恢复为活动实例所必需的。数据库生成事务日志，这些日志包含特定数据库中的实际数据变化。这些事务日志每 5 分钟由 RDS 写入 S3。事务日志还可以在快照之上重放，以恢复到 5 分钟粒度的时间点。理论上，RPO 可以是一个 5 分钟的时间点。

当您执行还原操作时，RDS 会创建一个新的 RDS 实例，这意味着一个新的数据库端点来访问该实例。使用这些实例的应用程序必须指向新的地址，这会显著影响 RTO。这意味着还原过程并不非常快，这会影响 RTO。为了在故障期间最小化 RTO，您可以考虑复制数据。使用副本，复制损坏数据的机会很高。克服这一点的唯一方法是拥有快照，并且 RDS 实例可以恢复到损坏之前的时间点。**Amazon RDS 读取副本**与多区域副本不同。在多区域 RDS 实例中，备用副本不能直接用于任何操作，除非主实例故障，而**读取副本**可以直接使用，但仅限于读取操作。读取副本有自己的数据库端点，读取密集型应用程序可以直接指向此地址。它们与主实例**异步**保持同步。读取副本可以在与主实例相同的区域或不同区域创建。其他区域的读取副本称为**跨区域读取副本**，这提高了应用程序的全球性能。

根据 AWS 文档，每个数据库实例允许有五个直接读取副本，这有助于扩展读取性能。由于异步复制，读取副本具有非常低的 RPO 值。在主实例故障的情况下，它们可以被提升为读写数据库实例。这可以快速完成，并且提供了相当低的 RTO 值。

在下一节中，我们将了解亚马逊自己的数据库引擎，Amazon Aurora。

# 使用多主功能写入 Amazon Aurora

Amazon Aurora 是亚马逊开发的最可靠的数据库引擎，以简单和成本效益的方式提供速度。Aurora 使用单个主实例和零个或多个副本的集群。Aurora 的副本可以为您提供 RDS 中的读取副本和多区域实例的优势。Aurora 使用共享集群卷进行存储，并可供集群中所有计算实例（最多 64 TiB）使用。这允许 Aurora 集群更快地提供资源，并提高可用性和性能。Aurora 使用基于 SSD 的存储，提供高 IOPS 和低延迟。与其它 RDS 实例不同，Aurora 不要求您分配存储，它基于您使用的存储。

Aurora 集群有多个端点，包括**集群端点**和**读取端点**。如果没有副本，则集群端点与读取端点相同。如果有可用的副本，则读取端点在读取端点之间进行负载均衡。集群端点用于读写，而读取端点旨在从集群中读取。如果您添加更多副本，则 AWS 在幕后为新副本管理负载均衡。

当发生故障转移时，副本会被提升到读写模式，这需要一些时间。在 Aurora 集群的**多主**模式下可以避免这种情况。这允许多个实例同时执行读写操作。

# 在 Amazon Redshift 上存储列式数据

Amazon Redshift 不用于实时事务处理，但用于数据仓库目的。它设计用于支持 PB 级规模的大量数据。它是一个基于列的数据库，用于分析、长期处理、维护和聚合。**Redshift Spectrum**可以用于查询 S3 上的数据，而无需将数据加载到 Redshift 集群中（尽管需要 Redshift 集群）。它不是 OLTP，而是 OLAP。**AWS QuickSight**可以与 Redshift 集成进行可视化，它提供了一个类似 SQL 的界面，允许您使用 JDBC/ODBC 连接进行数据查询。

Redshift 在 VPC 中的一个可用区使用集群架构，节点间具有更快的网络连接。它不是设计为高可用性，因为它与可用区紧密耦合。一个 Redshift 集群有一个**主节点**，该节点负责客户端与集群计算节点之间的所有通信、查询规划和聚合。**计算节点**负责运行由主节点提交的查询并存储数据。默认情况下，Redshift 使用公共网络与外部服务或任何 AWS 服务进行通信。通过**增强 VPC 路由**，可以通过自定义网络设置进行控制。

# 作为 NoSQL 数据库即服务的 Amazon DynamoDB

Amazon DynamoDB 是 AWS 中的一种 NoSQL 数据库即服务产品。它是一个完全管理的键/值和文档数据库。通过其端点可以轻松访问 DynamoDB。输入和输出吞吐量可以手动或自动管理或扩展。它还支持数据备份、时间点恢复和数据加密。在本章中，我们不会涵盖 DynamoDB 表结构或键结构，因为这对于认证考试不是必需的。然而，了解它们的基本知识是好的。有关更多详细信息，请参阅此处可用的 AWS 文档：[`docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html`](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html)。

# 摘要

在本章中，我们学习了亚马逊的各种数据存储服务以及如何通过各种策略来保护数据，并使用这些服务。如果您正在处理机器学习用例，那么您可能会遇到这样的场景，您必须为您的需求选择一个有效的数据存储服务。

在下一章中，我们将学习存储数据的处理。

## 问题

1.  要设置 S3 存储桶存储的区域，您必须首先创建存储桶，然后单独设置区域。

    A. 是的

    B. 错误

1.  为了复制 S3 存储桶的内容，是否必须将源存储桶和目标存储桶放在同一个区域？

    A. 是的

    B. 错误

1.  默认情况下，存储桶中的对象是私有的。

    A. 是的

    B. 错误

1.  通过 WS，S3 对象是不可变的，您只能执行 put 和 delete。重命名是具有不同名称的同一对象的 GET 和 PUT。

    A. 是的

    B. 错误

1.  如果用户在同一个存储桶中存储了一个未版本化的对象和一个版本化的对象，那么用户只能删除未版本化的对象。版本化对象不能被删除。

    A. 是的

    B. 错误

    答案

    版本控制适用于整个存储桶而不是对象。如果存储桶上启用了版本控制，则只能暂停；不能禁用。

1.  对删除标记的简单删除请求将：

    A. 删除删除标记

    B. 创建删除标记的副本

    C. 不删除删除标记，但会创建另一个删除标记

    D. 删除对象的原始版本

1.  通过 RDS 多可用区实例可以提高扩展性和性能。

    A. 是的

    B. 错误

    答案

    RDS 多可用区与扩展性和性能无关。它用于故障转移。

1.  EFS 使用什么协议？

    A. SMB

    B. NFS

    C. EBS

    D. HTTP

1.  EFS 支持哪些操作系统？

    A. 仅 Linux

    B. 仅 Windows

    C. Windows 和 Linux 都支持

    D. 既不是 Windows 也不是 Linux

1.  EFS 是私有服务吗？

    A. 是的。

    B. 不，它是一个公共服务。

    C. 它既是私有服务也是公共服务。

    D. 既不是私有服务也不是公共服务。

1.  以下哪两个是正确的？

    A. 多可用区：同一区域::只读副本：多个区域

    B. 多可用区：多个区域::只读副本：同一区域

    C. 多可用区：同步复制::只读副本：异步复制

    D. 多可用区：异步复制::只读副本：同步复制

1.  以下哪一个是正确的？

    A. 只读副本是只读实例。

    B. 只读副本是可读写实例。

    C. 只读副本是只写实例。

    D. 只读副本是只读实例，直到提升为读写。

1.  EFS 可以从哪里访问？

    A. 仅在 VPC 内部

    B. 通过 AWS 端点

    C. 任何有公共互联网连接的地方

    D. 在 VPC 内部或通过混合网络连接到该 VPC 的任何本地位置

1.  以下哪三个是正确的？

    A. 实例存储卷是持久存储。

    B. 实例存储卷是临时（短暂）存储。

    C. 如果发生硬件故障，存储在实例存储卷上的数据可能会丢失。

    D. 当 EC2 实例重启时，存储在实例存储卷上的数据可能会丢失。

    E. 当 EC2 实例停止和启动时，存储在实例存储卷上的数据可能会丢失。

    答案

    硬件故障可能会改变底层主机。因此，实例存储卷没有保证。当您停止实例并重新启动它时，由于主机变化，实例存储卷会丢失。实例重启不同于停止和启动；它意味着操作系统重启。

1.  为了使用 EC2 和 EBS 启用静态加密，您需要...

    A. 在创建 EBS 卷时配置加密。

    B. 使用适当的操作系统的文件系统配置加密。

    C. 使用 X.509 证书配置加密。

    D. 在 S3 中挂载 EBS 卷，然后使用存储桶策略加密存储桶。

1.  以下哪项是在 EBS 快照上无法执行的操作？

    A. 从快照创建镜像。

    B. 从快照创建 EBS 卷。

    C. 与另一个 AWS 账户共享快照。

    D. 创建加密快照的非加密副本。

1.  使用 EBS，您需要执行以下操作（选择两项）。

    A. 从另一个加密卷的快照创建一个加密卷。

    B. 从加密快照创建加密卷。

    C. 通过创建未加密快照的加密副本，从未加密快照创建一个加密快照。

    D. 加密现有卷。

    答案

    更多关于 EBS 的信息，请访问以下链接：[`docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_considerations`](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#EBSEncryption_considerations)。

## 答案

1\. B

2\. B

3\. A

4\. A

5\. B

6\. C

7\. B

8\. B

9\. A

10\. B

11\. A, C

12\. D

13\. D

14\. B, C, E

15\. A

16\. D

17\. A, C
