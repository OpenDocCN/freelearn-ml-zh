# 云应用扩展

本章将指导您了解如何通过为开发者提供访问传统、云原生和现代应用开发框架和资源，包括生产级容器服务和开放 API，来支持下一代云应用开发。这些将在共同的 vSphere 平台上使用，并将支持与云原生和容器化应用并行运行的遗留或传统应用，在整个虚拟化环境中。

你将学习如何通过定义参数和假设情景来优化资源以获得最大产出。这些将考虑未来的可扩展性，以便我们可以在不同的云环境中配置和自动扩展参数。

在本章中，我们将涵盖以下主题：

+   云原生应用

+   基于 vSphere 的**Pivotal 容器服务**（**PKS**）

# 技术要求

您可以从[`cloud.vmware.com/vmware-enterprise-pks/resources`](https://cloud.vmware.com/vmware-enterprise-pks/resources)下载 VMware Enterprise PKS。

# 云原生应用

由于今天的动态商业目标，数字技术总是在变化。通过移动、社交网络、可穿戴设备、联网汽车等一切事物都连接在一起，并且它们都在影响着我们今天的行为和与技术互动的方式。由于这种技术创新，客户对体验产品和服务的要求更加创新、灵活和快速。

让我们看看相互独立运作的系统、责任和技能集。我们正处于数字化转型中，需要跨各个段落的这些操作。数字化转型在很多环境中重新设计组织结构，以便它们能够协作。技术可以提升性能，并使组织在全球范围内扩大其影响力。

云原生应用具有四个特征：

+   **云原生应用由微服务组成**：云原生应用采用微服务架构，其中每个应用程序都是一组小型服务，可以独立于彼此运行。微服务通常由个人开发团队拥有，这些团队在自己的时间表下开发、部署、扩展和升级他们的服务。

+   **云原生应用打包在容器中**：容器为微服务提供隔离的上下文。它们高度可访问、可扩展，易于从一个环境便携到另一个环境，创建或拆除速度快，这使得它们非常适合构建和运行由微服务组成的应用程序。

+   **云原生应用运行在持续交付模型中**：软件开发人员和 IT 运维团队在这个模型下协作，以便在软件准备就绪时立即构建、测试和发布软件更新，而不会影响最终用户或其他团队的开发人员。

+   **云原生应用程序在云中动态管理**：它们通常在提供易于扩展和硬件解耦的现代云原生平台上构建和运行，这有助于应用程序的编排、管理和自动化。

# 使用容器进行自动化

对于已经大量部署 VMware 自动化工具的客户来说，可以轻松推动敏捷性和简化 IT 服务的消费。VMware 将帮助客户提供应用程序和容器服务。这个平台将 BOSH（自动扩展、自我修复、负载均衡等）的好处扩展到**容器即服务**（**CaaS**）解决方案（PKS）。BOSH 是一个开源工具，有助于分布式系统的部署和生命周期管理。PKS 是唯一能够提供本地完全管理的 Kubernetes 集群以及公共**基础设施即服务**（**IaaS**）的 CaaS 解决方案。这个平台还将包括**函数即服务**（**FaaS**）。这将允许组织通过在一个平台上提供应用程序部署和运行时结构来确保其抽象规划的安全性，无论 IaaS 如何。因此，我们必须与负责应用程序合理化和与业务和技术需求相关的后续迁移的各个团队进行详细规划。

**Pivotal Cloud Foundry**（**PCF**）包括**Pivotal 应用程序服务**（**PAS**）和 PKS 作为关键组件。PAS 是用于部署和运行现代应用程序的云原生平台。PKS 使客户和服务提供商能够在 VMware SDDC 和其他公共云环境中提供生产就绪的 Kubernetes。

例如，如果我们有一个在容器中运行的 10 个应用程序的系统，那么这 10 个应用程序将会有 10 个隔离的用户空间实例。想象一下，如果两个应用程序安装在同一操作系统上，但每个都需要该文件的不同版本。我们可以通过使用一个公共共享库文件来管理这种条件。容器（更具体地说，Linux 容器）已经存在了一段时间，像 Oracle、HP 和 IBM 这样的公司已经使用容器几十年了。然而，Docker 在用户中变得更加流行。

部署支持命名空间和资源限制的应用程序的易于使用的 API 和 CLI 工具简化了部署和管理容器的复杂性。容器是一个运行中的镜像实例，该镜像运行容器。我们需要下载一个镜像来使用它。镜像是一个分层文件系统，其中每一层都有自己的文件系统。

当你想进行更改时，没有必要打开一个单一的大型单体应用程序并将新更改塞进去。如果我们必须进行更改，我们只需将它们添加到一个新层即可。

容器正在对操作系统做的是虚拟机对服务器硬件所做的事情。运行和操作容器所需的工具和组织流程通常没有明确定义。VMware 和 Pivotal 处于独特的位置，能够解决这些新的挑战并成为**既得利益者**。容器通过限制我们需要在操作系统上安装的应用程序依赖项的数量来虚拟化操作系统。

# 容器的用例

以下是一些容器的用例：

+   **开发者沙盒的需求**：开发者经常希望访问运行特定框架的一组机器，以构建或快速测试和验证他们的应用程序。配置这样的环境是耗时的，通常涉及工单和审批。因此，开发者要么请求虚拟机并根据他们的需求进行定制，创建雪花部署，要么他们永远不会放弃这些资源，因为他们担心获得新的资源可能是一个繁琐的过程。

+   **应用程序重新打包**：客户可以将现有的应用程序打包为容器。您不需要重构代码或更改架构。虽然这构成了客户容器化旅程中的第一个逻辑步骤，但它使客户能够获得某些好处。修补和维护应用程序是一个主要的好处，其中更新可以仅限于镜像的各个单独层。这确保了其他层保持完整，减少了可能出现的错误和配置问题。

+   **可移植性**：将应用程序打包为容器可以使其具有可移植性。由于容器镜像不仅打包了应用程序代码，还包括了所有依赖项，因此可以保证在任何地方都能运行。现在我们能够将这个镜像从开发者的笔记本电脑移动到您的测试/开发或生产环境，而无需投入时间和资源来确保目标环境与开发环境（或反之）完全一致。

# 容器面临的挑战

我们专注于使开发者的代码能够实例化开发者所需的所有资源，即使是对于遗留系统，以便在瀑布方法中提供高水平的自动化，并使客户能够自助满足其资源需求。

传统模型使用传统的应用程序架构、工具和流程，在云交付模型中，开发者需要提交工单以获取资源。资源通过自助服务提供。云原生应用程序通过代码发起这些请求，并提供**基础设施即代码**（**IaC**）服务。

代码取代了服务票证，API 在其中扮演着关键角色。通过提供帮助运行容器的 API，如 OpenStack、PCF 等，可以使用自动化的 VMware SDDC 工具实现开发者就绪的基础设施。由于开发者获得了所有的好处，因此可以从现有的运营模式中管理容器。这是因为 IT 必须以一致的方式管理底层资源。

在微服务架构中，全局一致的基础设施层具有优势，因为每个服务定义了其与其他微服务的关系。如果底层网络复杂且没有可见性，则可能会破坏这种关系。网络应该完全开放以避免这个问题。Pivotal 重视 VMware NSX 和开发者就绪基础设施具有相同的代码，该代码定义了微服务之间的关系并实例化了安全的微分段网络连接。即使是无服务器架构也可能出现内部服务器错误消息。

# PKS 在 vSphere 上

**vSphere 集群组**是一组具有共同计算实体的 ESXi 主机；当在集群级别激活 vSphere HA 和 DRS 时，每个 vSphere 集群有 2 到 64 个主机。在 vSphere 集群实例下创建资源池，vCenter 能够管理多个 vSphere 集群实例，因为没有对 vSphere 集群数量的硬性限制。我们可以创建不同类型的 vSphere 集群，如管理集群、计算集群和边缘集群，因为 PKS 完全利用了 vSphere 集群结构。

在典型的 PKS 部署中，以下 vSphere 集群是推荐的：

+   **管理集群**：

    +   **托管组件**：vCenter、NSX 管理器和控制器虚拟机

    +   vSphere HA 和 DRS 已启用

    +   ESXi 主机需要为 NSX 准备，因为托管虚拟机上强制执行微分段

+   **计算集群**：

    +   **托管组件**：Kubernetes (K8s) 集群节点虚拟机

    +   应启用 vSphere HA 和 DRS，因为 BOSH 将检查 DRS 是否已开启

    +   ESXi 主机需要为 NSX 准备

+   **边缘集群**：

    +   **托管组件**：NSX 边缘节点虚拟机

    +   vSphere HA 和 DRS 已启用

    +   ESXi 主机不需要为 NSX 准备

PKS 管理平面可以位于管理集群或计算集群，具体取决于所选的设计场景。PKS 管理平面虚拟机包括 Ops Manager、BOSH、PKS 控制平面和 Harbor。

PKS 数据平面（或计算平面）将仅位于计算集群中。每个 K8s 集群允许最多三个主节点和 50 个工作节点，并且可以在同一个 PKS 环境中创建多个 K8s 集群。

K8s 主节点也托管 etcd 组件。vSphere 计算集群上必须启用 vSphere DRS 和 HA。vSphere DRS 自动化必须设置为部分自动化或完全自动化。vSphere HA 设置为主机故障 = 重启虚拟机。

以下是对 PKS 组件的计算和存储要求的说明：

| **PKS 组件** | **CPU** | **RAM (GB)** | **存储 (GB)** |
| --- | --- | --- | --- |
| Ops Manager | 1 | 8 | HD1: 160 |
| PKS 控制平面虚拟机 | 2 | 8 | HD1: 3HD2: 16HD3: 10 |
| BOSH | 2 | 8 | HD1: 3HD2: 50HD3: 50 |
| Harbor | 2 | 8 | HD1: 3HD2: 64HD3: 30 |
| K8s 主节点 | 每个 PKS 计划可配置 | 每个 PKS 计划可配置 | 临时磁盘：8 到 256 GB 持久磁盘：1 GB 到 32 TB（每个 PKS 计划可配置） |
| K8s 工作节点 | 每个 PKS 计划可配置 | 每个 PKS 计划可配置 | 临时磁盘：8 到 256 GB 持久磁盘：1 GB 到 32 TB（每个 PKS 计划可配置） |

# PKS 可用区

PKS 支持可用区（**AZ**）的概念，即 **AZ = vSphere 集群 + 资源池**。可用区决定了由 BOSH/PKS 创建的虚拟机放置到相应的 vSphere 集群/资源池。

有两种类型的可用区：

+   **管理可用区**：用于 BOSH、PKS 控制平面和 Harbor 虚拟机

+   **计算可用区**：用于 K8s 主节点和工作节点虚拟机

PKS 支持多个计算可用区，并且每个 PKS 计划最多支持三个不同的可用区。每个 K8s 主节点（最多三个）将落在单独的一个可用区。K8s 工作节点将在三个区域之间分配。

允许三个 PKS 计划（总共九个不同的计算区域）。每个 PKS 计划可以使用相同的三个区域或完全不同的三个可用区。可用区通常用于设置虚拟机相对于不同位置的本地域；或者，我们可以说 AZ = 物理机架（或房间）。

以下是 PKS 设计拓扑：

+   **物理拓扑（与 vSphere 集成）**：可以通过 PKS/NSX-T 集成部署多种拓扑。

+   **管理集群中的 PKS 管理平面**：多计算集群：

    +   PKS 管理平面托管在管理集群中，并连接到 DVS 虚拟交换机

    +   多个计算集群，以支持 K8s 集群节点

    +   每个可用区映射到不同的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）。

    +   **AZ 可以代表一个物理位置**：每个计算集群可以位于专用机架或房间

+   **单个计算集群的管理集群中的 PKS 管理平面**：

    +   PKS 管理平面托管在管理集群中，并连接到 DVS 虚拟交换机

    +   单个计算集群以支持 K8s 集群节点

    +   每个可用区映射到一个唯一的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）

    +   可用区可以用来限制每个 PKS 计划的 CPU/内存

+   **多计算集群中的 PKS 管理平面**：

    +   PKS 管理平面托管在计算集群中，并连接到 NSX-T 逻辑交换机

    +   多个计算集群，以支持 K8s 集群节点

    +   每个可用区映射到不同的 vSphere 集群（可用区与 vSphere 集群之间 1:1 映射）

    +   **一个可用区可以代表一个物理位置**：每个计算集群可以位于专用机架或房间

+   **计算集群中的 PKS 管理平面，或单个计算集群**：

    +   PKS 管理平面托管在计算集群中，并连接到 NSX-T 逻辑交换机

    +   单个计算集群，以支持 K8s 集群节点

    +   每个可用区映射到一个唯一的 vSphere 集群/不同的资源池：

    +   可用区可以用于每个 PKS 计划的 CPU/内存限制

+   **PKS 可用区（单/多个计算和管理集群）设计模型**：

    +   **具有单个 vSphere 计算集群的 PKS 可用区**：默认情况下，无法保证 K8s 主节点落在不同的 ESXi 主机上。一种解决方案是在 vSphere 计算集群上创建一个 DRS 亲和规则。

    +   **类型**：独立的虚拟机。

    +   **成员**：所有 K8s 主节点虚拟机。

    +   vSphere 集群必须至少有三个 ESXi 主机（这是 vSAN 的先决条件）。然而，为了防止单个主机故障（并确保 DRS 亲和规则能够正常工作），建议在集群中启动四个 ESXi 主机。

    +   NSX-T 2.2 支持在 N-VDS 上的所有类型流量。这意味着计算集群中的 ESXi 主机可以从两个物理网卡开始。

生产环境的最低 vSphere 集群配置如下：

+   **管理集群**：

    +   **非-vSAN**: 最小主机数：两个

    +   **vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须有两个副本和一个见证）

+   **计算集群**：

    +   **单个计算集群拓扑**：

        +   **非-vSAN**: 最小主机数：三个（通过使用 DRS 亲和规则保证每个 ESXi 主机有一个 K8s 主节点虚拟机）

        +   **vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须有两个副本和一个见证）

    +   **多个计算集群拓扑**：

        +   **非-vSAN**: 最小主机数：每个可用区两个，总共三个可用区（K8s 主节点实例化在不同的计算集群中。每个计算集群与一个可用区一一对应）

        +   **vSAN**: 最小主机数：每个可用区三个，总共三个可用区（为保证 vSAN 对象的数据保护，必须使用两个副本和一个见证）

+   **边缘集群**：

    +   **非-vSAN**: 最小主机数：两个。

    +   **vSAN**: 最小主机数：三个（为保证 vSAN 对象的数据保护，必须使用两个副本和一个见证。）注意：如果需要减少启动 ESXi 主机的数量，可以将边缘集群与计算集群（甚至管理集群）合并。

以下表格提供了关于 PKS/NSX-T 网络的信息：

| **网络** | **描述** | **CIDR** |
| --- | --- | --- |
| **PKS 管理网络** |

+   此网络托管 Ops Manager、BOSH、PKS 控制平面和 Harbor

+   如果需要，可以与 vCenter、NSX-T 管理和控制平面共置

+   PKS 管理网络是可路由或不可路由的，这取决于 NO-NAT 或 NAT 拓扑

| 192.168.1.0/28（例如）CIDR，/28 是一个良好的起点。 |
| --- |
| **节点 IP 块** |

+   此块将被分割以创建一个网络，该网络将托管 K8s 集群节点虚拟机

+   每个 K8s 集群将被分配一个 /24 的块

+   节点 IP 块是可路由或不可路由的，这取决于 NO-NAT 或 NAT 拓扑

| 取决于 NAT 或 NO-NAT 拓扑。172.23.0.0/16（例如） |
| --- |
| **Pods IP 块** |

+   此块将被分割以创建一个网络，该网络将托管属于同一 K8s 命名空间的 K8s Pods

+   每个 k8s 命名空间将分配到 /24 块的部分

+   Pods IP 块始终不可路由

| 172.16.0.0/16（例如） |
| --- |
| **浮动 IP 池** |

+   此池将用于以下两个目的：

    +   T0 上每个 K8s 命名空间的 SNAT 规则（用于 Pods 网络）

    +   LB 虚拟服务器 IP 分配

+   浮动 IP 池始终可路由

| 192.168.20.2-192.168.20.254（例如） |
| --- |

**节点 IP 块的 CIDR**：

+   在可路由场景中必须是唯一的（NO-NAT 拓扑）

+   在不可路由场景中可以重复（NAT 拓扑）

在所有情况下，`172.17.0.0/16`CIDR 都不得使用，因为 K8s 工作节点上的 Docker 正在使用该子网。

如果与 Harbor 部署 PKS，则必须不使用以下 CIDR，因为 Harbor 正在使用它为其内部 Docker 桥接器：

```py
 172.18.0.0/16 ;172.19.0.0/16 ;172.20.0.0/16 ;172.21.0.0/16 ;172.22.0.0/16
```

每个 K8s 集群使用以下 IP 块用于 Kubernetes 服务，因此请避免将其用于节点 IP 块：`10.100.200.0/24`。

# PKS/NSX-T 逻辑拓扑

当与 NSX-T 集成时，PKS 支持两种类型的拓扑。NAT 和 NO-NAT 拓扑选择在 PKS 瓦片 | 网络部分完成。NAT 拓扑是默认的，但您可以取消选中 NAT 模式以使用 NO-NAT 拓扑。NAT 和 NO-NAT 术语基本上适用于 PKS 管理网络和 K8s 集群节点网络（即是否使用可路由子网）。无论 NAT 还是 NO-NAT 拓扑，访问 K8s API 都使用相同的程序。

在分配给 K8s 集群的 NSX-T LB 实例上创建的虚拟服务器用于以下目的：

+   从 PKS 浮动 IP 池中提取一个 IP（此处为 `1x.x0.1x.1xx`），端口号为 `8443`

+   `pks cluster <cluster name>` 命令的输出显示了相同的 IP 地址

以下是不同 NAT 拓扑的目标：

+   **NAT 拓扑**：对于在他们的数据中心拥有有限的可路由 IP 地址并且希望使用 concourse 管道（例如）自动化 PKS 部署的客户

+   **NO-NAT 拓扑**：对于避免 NAT（NAT 会破坏完整路径可见性并且拥有大量可路由 IP 地址资源）的客户

# 不同配置的使用案例

以下是与不同配置相关的使用案例：

+   从企业网络访问 PKS 管理平面组件（Ops Manager、BOSH、PKS 控制平面 VM、Harbor）：

    +   **NO-NAT 拓扑**：无需采取任何行动，因为这些组件使用可路由 IP 地址

    +   **NAT 拓扑**：用户需要在 T0 上创建 DNAT 规则

+   访问 K8s API（例如使用 kubectl CLI）：

    +   **NO-NAT 拓扑**：1 个虚拟服务器（在专用于 K8s 集群的 NSX-T LB 实例上）自动创建，使用 1 个可路由 IP 从 PKS 浮动 IP 块

    +   **NAT 拓扑**：用户需要指向此 IP 以访问 K8s API

+   使用 PKS 浮动 IP 块中的一个可路由 IP 地址，自动创建一个虚拟服务器（在专门用于 K8s 集群的 NSX-T 负载均衡器实例上）：

    +   **无 NAT 拓扑结构**：用户需要指向此 IP 以访问 K8s API

    +   **NAT 拓扑结构**：用户需要访问 K8s 节点 VM（例如，BOSH SSH）

+   使用可路由 IP 地址的组件：

    +   **无 NAT 拓扑结构**：用户需要 SSH 到 Ops Manager 以执行针对 K8s 节点 VM 的 BOSH 命令

    +   **NAT 拓扑结构**：一种替代方案是在同一子网上安装一个跳转盒服务器，而不是 PKS 管理平面组件

+   使用 K8s 节点 VM 访问公司网络（或互联网）：

    +   **无 NAT 拓扑结构**：不需要采取任何行动，因为这些组件使用可路由 IP 地址

    +   **NAT 拓扑结构**：PKS 会自动为每个 K8s 集群在 T0 上创建一个 SNAT 规则，使用 PKS 浮动 IP 池中的一个 IP 地址

# PKS 和 NSX-T 边缘节点和边缘集群

PKS 仅支持大型尺寸的 NSX-T 边缘节点 VM 配置。PKS 仅支持一个 T0（8 vCPU，16 GB RAM）的 Edge Cluster 实例。T0 路由器必须配置在活动/备用模式，因为 PKS 将在那里应用 SNAT 规则。NSX-T 边缘集群可以包含多达八个边缘**传输节点**（**TN**）。您可以在边缘集群中添加新的边缘节点（最多八个）以增加整体容量（例如 LB）并为 NSX-T 边缘集群提供可伸缩性。您可以使用两个不同的边缘节点为 T0 上行链路 IP 地址（总共两个 IP 地址）提供 NSX-T T0 在边缘集群中的高可用性。我们应该在 T0 上启用 HA VIP，以确保其始终处于运行状态，即使一个 T0 上行链路出现故障。物理路由器将仅与 T0 HA VIP 交互操作。

以下列出了 NSX-T 和负载均衡器的缩放数字：

|   | **小型 LB** | **中型 LB** | **大型 LB** | **池成员** |
| --- | --- | --- | --- | --- |
| **NSX-T 版本** | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 | 2.1 | 2.2 |
| **边缘 VM：小型** | - | - | - | - | - | - | - | - |
| **边缘 VM：中型** | 1 | 1 | - | - | - | - | 30 | 30 |
| **边缘 VM：大型** | 4 | 40 | 1 | 1 | - | - | 120 | 1,200 |
| **裸金属边缘** | 100 | 750 | 10 | 100 | 1 | 1 | 3,000 | 22,500 |

# PKS 和 NSX-T 通信

多个 PKS 组件需要与 NSX-T 管理器通信。需要一个使用 NSX-T 超级用户主体身份证书作为认证机制的 PKS 控制平面 VM，以创建每个 K8s 集群节点网络的 T1/LS 以及每个 K8s 集群的 LB 实例。

BOSH 使用凭证作为认证机制，为 VM 的所有逻辑端口添加特殊的 BOSH ID 标签和 NCP pod。它使用 NSX-T 超级用户主体身份证书作为认证机制，为每个命名空间创建 T1/LS，为每个命名空间在 T0 上创建 SNAT 规则，并为每个类型的 LB K8s 服务创建 LB 虚拟服务器。

以下是为每个 K8s 集群创建的 NSX-T 对象的列表。

当创建一个新的 K8s 集群时，以下 NSX-T 对象默认创建：

+   **NSX-T LS**：

    +   为 K8s 主节点和工作节点创建一个 LS

    +   为每个 K8s 命名空间创建一个 LS，即 kube-public、kube-system 和 pks-infrastructure

    +   一个与 K8s 集群关联的 NSX-T 负载均衡器 LS

+   **NSX-T T1**：

    +   为 K8s 主节点和工作节点创建一个 T1（称为 cluster-router）

    +   为每个 K8s 命名空间（默认，kube-public，kube-system 和 pks-infrastructure）创建一个 T1

    +   为与 K8s 集群关联的 NSX-T 负载均衡器创建一个 T1

+   **NSX-T 负载均衡器**：

    +   一个包含以下对象的 NSX-T 负载均衡器小实例：

        +   一个虚拟服务器用于访问 K8s 控制平面 API（端口 8443）

        +   包含三个 K8s 主节点的服务器池

        +   为入口控制器创建一个虚拟服务器（HTTP）

        +   为入口控制器创建一个虚拟服务器（HTTPS）

        +   每个虚拟服务器分配一个从 PKS 浮动 IP 池派生的 IP 地址

当创建一个新的 K8s 集群时，以下 NSX-T 对象默认创建：

+   **NSX-T DDI/IPAM**：从节点 IP 块中提取并分配一个/24 子网给 K8s 主节点和工作节点。

+   **NSX-T DDI/IPAM**：从 PODs IP 块中提取并分配一个/24 子网给每个 K8s 命名空间（默认，kube-public，kube-system 和 pks-infrastructure）。

    +   **NSX-T T0 路由器**：

        +   为每个 K8s 命名空间（默认，kube-public，kube-system，pks-infrastructure）创建一个 SNAT 规则，使用浮动 IP 池中的一个 IP 作为翻译的 IP 地址。

        +   为每个 K8s 集群（如果使用 NAT 拓扑）创建一个 SNAT 规则，使用浮动 IP 池中的一个 IP 作为翻译的 IP 地址。K8s 集群子网是从节点 IP 块中派生的，使用/24 子网掩码。

    +   **NSX-T DFW**：

        +   为 kubernetes-dashboard 创建一个 DFW 规则：源=K8s 工作节点（托管仪表板 POD）/ 目标=仪表板 POD IP/端口：TCP/8443/操作：允许

        +   为 kube-dns 创建一个 DFW 规则：源=K8s 工作节点（托管 DNS POD）/ 目标 = DNS POD IP/端口：TCP/8081 和 TCP/10054/操作：允许

# K8s 集群节点 VM 的存储

您可以通过使用**持久卷**（**PV**）为 K8s POD 提供存储。可以通过使用**vCP**（代表**云提供商**）插件将 PV 映射到 vSphere 上的**虚拟机磁盘**（**VMDK**）文件。然后，将 VMDK 文件作为磁盘附加到工作节点 VM。然后我们可以从该磁盘挂载卷。

# 数据存储

以下是一个关于数据存储的信息表：

| **部署拓扑/存储技术** | **vSAN 数据存储** | **VMFS 通过 NFS/iSCSI/FC 数据存储** |
| --- | --- | --- |
| 单个 vSphere 计算集群（单个可用区，或使用 RPs 时的多个可用区）具有本地数据存储 |

+   **静态 PV 配置**：是

+   **动态 PV 配置**：是

|

+   **静态 PV 配置**：是

+   **动态 PV 配置**：是

|

| 多个 vSphere 计算集群（多个可用区）具有本地数据存储 |
| --- |

+   **静态 PV 配置**：否*

+   **动态 PV 提供程序**：否*

|

+   **静态 PV 提供程序**：否*

+   **动态 PV 提供程序**：否*

|

| 多个 vSphere 计算集群（多个 AZs）共享所有 vSphere 计算集群的数据存储（s） |
| --- |

+   N/A

+   vSAN 不支持跨 vSphere 集群的共享数据存储

|

+   **静态 PV 提供程序**：是

+   **动态 PV 提供程序**：是

|

以下是为静态 PV 提供的步骤：

1.  手动创建 VMDK 文件

1.  创建一个引用上述 VMDK 文件的 PV

1.  创建 PVC

1.  通过引用 PVC 来部署有状态的 POD 或 StatefulSets

以下是为动态 PV 提供程序提供的步骤：

1.  创建 PVC（vCP K8s 存储插件；hatchway 将自动创建 PV 和 VMDK 文件）

1.  使用 PVC 引用部署有状态的 POD 或 StatefulSets

以下是一些关于 PKS/NSX-T 的 vSAN 考虑事项：

+   使用 vSAN，vSphere 集群必须从至少三个 ESXi 主机开始，以确保数据保护（在这种情况下，对于 RAID1，容错设置为 1）

+   PKS 的 AZ 无法与 vSAN 故障域进行映射

+   目前支持带有 vSAN 的单个计算集群的 PKS（所有 ESXi 主机都位于同一站点）

+   **注意**: 目前为止，带有 vSAN 拉伸集群的 PKS 不是一个受支持的配置（因为没有将 AZs 与 vSAN 故障域进行映射）

+   带有多个计算集群的 PKS 在仅使用 vSAN 数据存储的情况下不是一个受支持的配置

+   主节点和工作节点可以跨不同的 ESXi 集群创建（BOSH 瓦片允许您为虚拟机指定多个持久和临时数据存储）

+   仅为单个 vSAN 数据存储创建 PV VMDK 磁盘（并且不会自动在不同 vSAN 数据存储之间执行复制）

数据中心维护独立的 PKS 实例、NSX 部署、Kubernetes（K8s）集群和 vSphere 基础设施。一个**全局服务器负载均衡器**（**GSLB**），通过第三方提供，监控站点 K8s 集群 API 和 PKS 控制器 API 的可用性。操作和开发将 API 请求直接指向 GSLB 虚拟服务器 URL 以创建和管理 K8s 集群以及部署应用程序。手动部署的应用程序（例如通过 kubectl）不会在环境之间自动复制，并在站点 B 的故障转移后需要重新部署。

您可以配置一个 CI/CD 自动化服务器，以执行针对每个环境中的 K8s URL 的构建管道，或者针对 GSLB 虚拟服务器 URL 的单个构建。基于 Harbor 策略的复制，这是一个内置功能，负责将镜像克隆到备用位置。您可以在环境之间复制数据存储以支持 PV。在站点 A 失败后，Pods 将在站点 B 重新部署，挂载原始持久卷的 VMDK 文件。

# 摘要

在这个数字化趋势的背后，有一个名为云原生的新 IT 方法，它是商业数字化的推动力之一。云原生方法允许企业大幅提高开发者的生产力，使他们能够比以前更快地将新应用和服务推向市场；因此，他们可以改善客户体验和满意度。如果成功采用，云原生方法还可以帮助降低运营和基础设施成本，以及增强应用安全性。

在下一章，第十四章，《机器学习的高性能计算》，你将了解可以增强高性能计算（**HPC**）环境生产力的虚拟化具体方面。我们将探讨 VMware vSphere 提供的功能，以满足研究计算、学术、科学和工程高性能计算工作负载的需求。
