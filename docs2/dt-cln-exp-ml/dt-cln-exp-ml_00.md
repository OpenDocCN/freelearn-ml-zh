# 前言

研究人员为数据分析准备数据的工作——包括提取、转换、清洗和探索——在机器学习工具日益普及的情况下，其本质并没有发生根本变化。30 年前，当我们为多元分析准备数据时，我们同样关注缺失值、异常值、变量分布的形状以及变量之间的相关性，就像我们现在使用机器学习算法时一样。虽然确实，广泛使用相同的机器学习库（如 scikit-learn、TensorFlow、PyTorch 等）确实鼓励了方法上的更大一致性，但良好的数据清洗和探索实践在很大程度上并未改变。

我们谈论机器学习的方式仍然非常侧重于算法；只需选择正确的模型，随之而来的就是组织变革的洞见。但我们必须为过去几十年中我们一直在进行的数据学习留出空间，其中我们从数据中做出的预测、在数据中建模关系以及我们对数据的清洗和探索都是对话的一部分。确保我们的模型正确与从直方图或混淆矩阵中获取尽可能多的信息一样重要，需要仔细调整超参数。

同样，数据分析师和科学家的工作并不从清洗、探索、预处理、建模到评估这一过程有序推进。我们在过程的每一步都怀有潜在模型的想法，并定期更新我们之前的模型。例如，我们最初可能认为我们将使用逻辑回归来建模特定的二元目标，但当我们看到特征的分布时，我们可能至少需要尝试使用随机森林分类。我们将在整篇文章中讨论建模的影响，即使在解释相对常规的数据清洗任务时也是如此。我们还将探讨在早期过程中使用机器学习工具，以帮助我们识别异常、插补值和选择特征。

这指向了数据分析师和科学家在过去十年中工作流程的另一个变化——对“单一模型”的重视减少，以及对模型构建作为迭代过程的接受度提高。一个项目可能需要多个机器学习算法——例如，主成分分析来降低维度（特征的数量），然后是逻辑回归进行分类。

话虽如此，我们在数据清洗、探索和建模方面的方法有一个关键的区别——随着机器学习工具在我们的工作中扮演越来越重要的角色，我们对预测的重视超过了对底层数据的理解。我们更关心我们的特征（也称为自变量、输入或预测因子）如何预测我们的目标（因变量、输出、响应），而不是特征之间的关系以及我们数据的底层结构。我在本书的前两章中指出了这一点如何改变我们的关注点，即使我们在清洗和探索数据时也是如此。

# 本书面向的对象

在撰写这本书时，我考虑了多个受众，但我最经常想到的是我的一个好朋友，30 年前她买了一本 Transact-SQL 书，立刻对自己的数据库工作有了极大的信心，最终围绕这些技能建立起了自己的职业生涯。如果有人刚开始作为数据科学家或分析师的职业生涯，通过这本书并获得了与我朋友相似的经历，我将感到非常高兴。最重要的是，我希望你通过阅读这本书后感到满意和兴奋，对你可以做到的事情感到自豪。

我还希望这本书对那些已经从事这类工作一段时间的人来说是一本有用的参考书。在这里，我想象有人打开这本书，自己问自己，*在我的逻辑回归模型网格搜索中，应该使用哪些好的值？*

为了保持本书的实践性质，书中的每一部分输出都可以通过本书中的代码进行重现。我始终坚持一个规则，即使有时会遇到挑战。除了概念性章节外，每个章节都是从原始下载文件中基本未变的数据开始的。你将在每个章节中从数据文件到模型进行转换。如果你忘记了某个特定对象是如何创建的，你只需要翻回一页或两页就能看到。

对于那些对 pandas 和 NumPy 有一定了解的读者来说，在处理一些代码块时会更加得心应手，同样，对 Python 和 scikit-learn 有一定了解的人也是如此。尽管如此，这些都不是必需的。有些部分你可能需要花更多的时间去仔细阅读。如果你需要关于使用 Python 进行数据工作的额外指导，我认为我的 *Python 数据清洗食谱* 是一本很好的配套书籍。

# 本书涵盖的内容

*第一章*，*检查特征和目标的分布*，探讨了使用常见的 NumPy 和 pandas 技术来更好地了解数据的属性。我们将生成汇总统计量，如`均值`、`最小值`和`最大值`，以及标准差，并计算缺失值的数量。我们还将创建关键特征的可视化，包括直方图和箱线图，以比仅查看汇总统计量更好地了解每个特征的分布。我们将暗示特征分布对数据转换、编码和缩放以及我们在后续章节中用相同数据进行建模的影响。

*第二章*，*检查特征和目标之间的双变量和多变量关系*，专注于可能特征和目标变量之间的相关性。我们将使用 pandas 方法进行双变量分析，并使用 Matplotlib 进行可视化。我们将讨论我们发现的特征工程和建模的影响。我们还在本章中使用多元技术来理解特征之间的关系。

*第三章*，*识别和修复缺失值*，将介绍识别每个特征或目标缺失值的技术，以及识别大量特征值缺失的观测值。我们将探讨填充值的方法，例如将值设置为整体均值、给定类别的均值或前向填充。我们还将检查用于填充缺失值的多元技术，并讨论它们何时适用。

*第四章*，*编码、转换和缩放特征*，涵盖了各种特征工程技术。我们将使用工具删除冗余或高度相关的特征。我们将探索最常见的编码类型——独热编码、有序编码和哈希编码。我们还将使用转换来改善特征的分布。最后，我们将使用常见的分箱和缩放方法来解决偏斜、峰度和异常值，以及调整范围差异较大的特征。

*第五章*，*特征选择*将介绍多种特征选择方法，从过滤器到包装器，再到嵌入式方法。我们将探讨它们如何与分类和连续目标一起工作。对于包装器和嵌入式方法，我们将考虑它们与不同算法结合时的效果。

*第六章*，*为模型评估做准备*，将展示我们构建第一个完整的流水线，将数据分为测试集和训练集，并学习如何在没有数据泄露的情况下进行预处理。我们将实现 k 折交叉验证，并更深入地研究评估模型性能的方法。

*第七章*，*线性回归模型*，是关于使用许多数据科学家喜爱的老方法——线性回归来构建回归模型的几个章节中的第一个。我们将运行一个经典的线性模型，同时考察使特征空间成为线性模型良好候选者的特性。我们将探讨在必要时如何通过正则化和变换来改进线性模型。我们将研究随机梯度下降作为**普通最小二乘法**（**OLS**）优化的替代方案。我们还将学习如何使用网格搜索进行超参数调整。

*第八章*，*支持向量回归*，讨论了关键的支持向量机概念以及它们如何应用于回归问题。特别是，我们将考察诸如 epsilon 敏感管和软边界等概念如何为我们提供灵活性，以在数据和领域相关挑战下获得最佳拟合。我们还将首次但肯定不是最后一次探索非常实用的核技巧，它允许我们在不进行变换或增加特征数量的情况下建模非线性关系。

*第九章*，*K 近邻、决策树、随机森林和梯度提升回归*，探讨了最流行的非参数回归算法中的一些。我们将讨论每个算法的优点，何时可能想要选择一个而不是另一个，以及可能的建模挑战。这些挑战包括如何通过仔细调整超参数来避免欠拟合和过拟合。

*第十章*，*逻辑回归*，是关于使用逻辑回归构建分类模型的几个章节中的第一个，逻辑回归是一种高效且偏差低的算法。我们将仔细检查逻辑回归的假设，并讨论数据集和建模问题中使逻辑回归成为良好选择的属性。我们将使用正则化来解决高方差问题或当我们有许多高度相关的预测因子时。我们将通过多项式逻辑回归将算法扩展到多类问题。我们还将讨论如何首次但肯定不是最后一次处理类别不平衡问题。

*第十一章*, *决策树和随机森林分类*，回到在第九章中介绍的决策树和随机森林算法，这次处理分类问题。这为我们提供了另一个学习如何构建和解释决策树的机会。我们将调整包括树深度在内的关键超参数，以避免过拟合。然后我们将探索随机森林和梯度提升决策树作为决策树的优秀、低方差替代方案。

*第十二章*, *用于分类的 K 近邻*，回到**k 近邻**（**KNNs**）来处理二元和多类建模问题。我们将讨论并展示 KNN 的优势——构建无装饰模型的简便性以及需要调整的超参数数量有限。到本章结束时，我们将了解两个问题——如何进行 KNN 以及何时应该考虑它来进行我们的建模。

*第十三章*, *支持向量机分类*，探讨了实现**支持向量分类**（**SVC**）的不同策略。我们将使用线性 SVC，当我们的类别是线性可分时，它可以表现得非常好。然后我们将考察如何使用核技巧将 SVC 扩展到类别不是线性可分的情况。最后，我们将使用一对一和一对多分类来处理具有两个以上值的标签。

*第十四章*, *朴素贝叶斯分类*，本章讨论了朴素贝叶斯的基本假设以及该算法如何被用来解决我们已探讨的一些建模挑战，以及一些新的挑战，例如文本分类。我们将考虑何时朴素贝叶斯是一个好的选择，何时则不是。我们还将探讨朴素贝叶斯模型的解释。

*第十五章*, *主成分分析*，考察**主成分分析**（**PCA**），包括其工作原理以及我们可能想要使用它的时机。我们将学习如何解释 PCA 创建的成分，包括每个特征如何贡献到每个成分以及解释了多少方差。我们将学习如何可视化成分以及如何在后续分析中使用成分。我们还将考察如何使用核函数进行 PCA 以及何时这可能给我们带来更好的结果。

*第十六章*，*K-Means 和 DBSCAN 聚类*，探讨了两种流行的聚类技术，k-means 和**基于密度的空间聚类应用噪声**（**DBSCAN**）。我们将讨论每种方法的优点，并培养在何时选择一种聚类算法而不是另一种算法的感觉。我们还将学习如何评估我们的聚类以及如何更改超参数以改进我们的模型。

# 要充分利用本书

要运行本书中的代码，您需要安装一个科学版的 Python，例如 Anaconda。所有代码都使用 scikit-learn 版本 0.24.2 和 1.0.2 进行了测试。

# 下载示例代码文件

您可以从 GitHub（[`github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning`](https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning)）下载本书的示例代码文件。如果代码有更新，它将在 GitHub 仓库中更新。

我们还有其他来自我们丰富的图书和视频目录的代码包，可在[`github.com/PacktPublishing/`](https://github.com/PacktPublishing/)找到。查看它们吧！

# 下载彩色图像

我们还提供了一份包含本书中使用的截图和图表的彩色图像的 PDF 文件。您可以从这里下载：[`packt.link/aLE6J`](https://packt.link/aLE6J)。

# 使用的约定

本书使用了多种文本约定。

`文本中的代码`：表示文本中的代码词汇、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 昵称。以下是一个示例：“为了学习目的，我们在 GitHub 的`chapter08`文件夹下提供了两个示例`mlruns`工件和`huggingface`缓存文件夹。”

代码块设置如下：

```py
client = boto3.client('sagemaker-runtime') 
```

```py
response = client.invoke_endpoint(
```

```py
        EndpointName=app_name, 
```

```py
        ContentType=content_type,
```

```py
        Accept=accept,
```

```py
        Body=payload
```

```py
        )
```

当我们希望将您的注意力引到代码块的一个特定部分时，相关的行或项目将以粗体显示：

```py
loaded_model = mlflow.pyfunc.spark_udf(
```

```py
    spark,
```

```py
    model_uri=logged_model, 
```

```py
    result_type=StringType())
```

任何命令行输入或输出都如下所示：

```py
mlflow models serve -m models:/inference_pipeline_model/6
```

**粗体**：表示新术语、重要词汇或屏幕上看到的词汇。例如，菜单或对话框中的单词以**粗体**显示。以下是一个示例：“要执行此单元格中的代码，您只需在右上角的下拉菜单中点击**运行单元格**。”

小贴士或重要提示

看起来是这样的。

# 联系我们

欢迎读者反馈。

`customercare@packtpub.com`并在邮件主题中提及书名。

**勘误表**：尽管我们已经尽最大努力确保内容的准确性，但错误仍然可能发生。如果您在这本书中发现了错误，如果您能向我们报告，我们将不胜感激。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)并填写表格。

`copyright@packt.com`并附上材料链接。

**如果您有兴趣成为作者**：如果您在某个领域有专业知识，并且有兴趣撰写或为书籍做出贡献，请访问[authors.packtpub.com](http://authors.packtpub.com)。

# 分享您的想法

一旦您阅读了《使用机器学习进行数据清洗和探索》，我们非常想听听您的想法！请点击此处直接进入此书的亚马逊评论页面并分享您的反馈。

您的评论对我们和科技社区都非常重要，并将帮助我们确保我们提供高质量的内容。
